Result of /data/leuven/370/vsc37064/new_queries_named_graph/query_47.txt:
OpenLink Virtuoso Interactive SQL (Virtuoso)
Version 07.20.3240 as of Mar 11 2025
Type HELP; for help and EXIT; to exit.
Connected to OpenLink Virtuoso
Driver: 07.20.3240 OpenLink Virtuoso ODBC Driver
model                                                                             evaluation
LONG VARCHAR                                                                      LONG VARCHAR
_______________________________________________________________________________

CapsNet-BERT                                                                                  Acc evaluation for CapsNet-BERT model: 83.391
D2SBERT using Sequence Attention                                                                                  Micro-F1 evaluation for D2SBERT using Sequence Attention model: 68.555
TAS-BERT                                                                                  F1 (Laptop) evaluation for TAS-BERT model: 27.31,
F1 (R15) evaluation for TAS-BERT model: 57.51,
F1 (R16) evaluation for TAS-BERT model: 65.89,
F1 (Restaurant) evaluation for TAS-BERT model: 33.53
BioLinkBERT (base)                                                                                  Accuracy evaluation for BioLinkBERT (base) model: 40.0,
Accuracy evaluation for BioLinkBERT (base) model: 70.2,
Accuracy evaluation for BioLinkBERT (base) model: 80.81,
Accuracy evaluation for BioLinkBERT (base) model: 91.4,
F1 evaluation for BioLinkBERT (base) model: 84.35,
Macro F1 word level evaluation for BioLinkBERT (base) model: 73.97 ,
Pearson Correlation evaluation for BioLinkBERT (base) model: 0.9325
AlephBERT-base Pipeline                                                                                  F1 evaluation for AlephBERT-base Pipeline model: 80.15
BERT + Dep-GCN - Const-GCN                                                                                  F1 evaluation for BERT + Dep-GCN - Const-GCN model: 50.21
KPI-BERT                                                                                  Relation F1 evaluation for KPI-BERT model: 43.76
VE-BERT                                                                                  F1 evaluation for VE-BERT model: 0.362
SRoBERTa-NLI-large                                                                                  Spearman Correlation evaluation for SRoBERTa-NLI-large model: 0.7429,
Spearman Correlation evaluation for SRoBERTa-NLI-large model: 0.7453,
Spearman Correlation evaluation for SRoBERTa-NLI-large model: 0.7682,
Spearman Correlation evaluation for SRoBERTa-NLI-large model: 0.8185
BERT (Input: Abstract)                                                                                  Accuracy evaluation for BERT (Input: Abstract) model: 0.8471
RobBERT v2                                                                                  Accuracy evaluation for RobBERT v2 model: 95.144%,
F1 evaluation for RobBERT v2 model: 95.144%
Transition-based (+BERT + MTL)                                                                                  Full MRP F1 evaluation for Transition-based (+BERT + MTL) model: 64.1,
Full UCCA F1 evaluation for Transition-based (+BERT + MTL) model: 35.6,
LPP MRP F1 evaluation for Transition-based (+BERT + MTL) model: 73.1,
LPP UCCA F1 evaluation for Transition-based (+BERT + MTL) model: 50.3
XLM-RoBERTa-large                                                                                  Dutch evaluation for XLM-RoBERTa-large model: 79.7,
Dutch evaluation for XLM-RoBERTa-large model: 82.3,
German evaluation for XLM-RoBERTa-large model: 74.5,
German evaluation for XLM-RoBERTa-large model: 76.9,
Spanish evaluation for XLM-RoBERTa-large model: 78.9,
Spanish evaluation for XLM-RoBERTa-large model: 79.5
c2f + SpanBERT-Large                                                                                  Avg F1 evaluation for c2f + SpanBERT-Large model: 80.2
BERT large (LAMB optimizer)                                                                                  F1 evaluation for BERT large (LAMB optimizer) model: 90.584
BERT-base 110M (fine-tuned)                                                                                  Accuracy evaluation for BERT-base 110M (fine-tuned) model: 63.1
DistilBERT-uncased-PruneOFA (90% unstruct sparse)                                                                                  EM evaluation for DistilBERT-uncased-PruneOFA (90% unstruct sparse) model: 76.91,
F1 evaluation for DistilBERT-uncased-PruneOFA (90% unstruct sparse) model: 84.82,
Matched evaluation for DistilBERT-uncased-PruneOFA (90% unstruct sparse) model: 80.68,
Mismatched evaluation for DistilBERT-uncased-PruneOFA (90% unstruct sparse) model: 81.47
SemEHR+WS (rules+BlueBERT)                                                                                  F1 evaluation for SemEHR+WS (rules+BlueBERT) model: 0.702,
F1 evaluation for SemEHR+WS (rules+BlueBERT) model: 0.858
GenBERT (+ND+TD)                                                                                  F1 evaluation for GenBERT (+ND+TD) model: 72.4
BERT-HateXplain [LIME]                                                                                  AUROC evaluation for BERT-HateXplain [LIME] model: 0.851,
Macro F1 evaluation for BERT-HateXplain [LIME] model: 0.687
Multi-passage BERT                                                                                  EM evaluation for Multi-passage BERT model: 51.1,
EM evaluation for Multi-passage BERT model: 65.1
ProtoBERT                                                                                  10 way 1~2 shot evaluation for ProtoBERT model: 15.05±0.44,
10 way 1~2 shot evaluation for ProtoBERT model: 32.45±0.79,
10 way 5~10 shot evaluation for ProtoBERT model: 35.40±0.13,
10 way 5~10 shot evaluation for ProtoBERT model: 52.92±0.37,
5 way 1~2 shot evaluation for ProtoBERT model: 20.76±0.84,
5 way 1~2 shot evaluation for ProtoBERT model: 38.83±1.49,
5 way 5~10 shot evaluation for ProtoBERT model: 42.54±0.94,
5 way 5~10 shot evaluation for ProtoBERT model: 58.79±0.44
BERT (Input: Headline)                                                                                  Accuracy evaluation for BERT (Input: Headline) model: 0.7727
IndicBERT Base                                                                                  Accuracy evaluation for IndicBERT Base model: 59.03,
Accuracy evaluation for IndicBERT Base model: 71.32,
Accuracy evaluation for IndicBERT Base model: 78.45
TW-BERT                                                                                  MAP evaluation for TW-BERT model: 0.2025
BERT-TL                                                                                  R10@1 evaluation for BERT-TL model: 0.927,
R10@2 evaluation for BERT-TL model: 0.974,
R10@5 evaluation for BERT-TL model: 0.997
Late Fusion (Bert + InceptionV3)                                                                                  Accuracy (%) evaluation for Late Fusion (Bert + InceptionV3) model: 84.59
BioMegatron BERT-cased                                                                                  F1 evaluation for BioMegatron BERT-cased model: 87.8
MotionBERT (Scratch)                                                                                  2D detector evaluation for MotionBERT (Scratch) model: SH,
Average MPJPE (mm) evaluation for MotionBERT (Scratch) model: 39.2,
Frames Needed evaluation for MotionBERT (Scratch) model: 243,
Need Ground Truth 2D Pose evaluation for MotionBERT (Scratch) model: No,
Use Video Sequence evaluation for MotionBERT (Scratch) model: Yes
SAIS-BERT-base                                                                                  F1 evaluation for SAIS-BERT-base model: 62.77,
Ign F1 evaluation for SAIS-BERT-base model: 60.96
V2W-BERT                                                                                  5 fold cross validation evaluation for V2W-BERT model: 89-98
RoBERTa-WinoGrande 355M                                                                                  Accuracy evaluation for RoBERTa-WinoGrande 355M model: 90.1
Liu et al. - BERT                                                                                  EM evaluation for Liu et al. - BERT model: 46.4,
F1 evaluation for Liu et al. - BERT model: 64.3
SETE-Roberta-large                                                                                  F1 evaluation for SETE-Roberta-large model: 63.74,
Ign F1 evaluation for SETE-Roberta-large model: 61.78
RoBERTa Focal Loss                                                                                  AUROC evaluation for RoBERTa Focal Loss model: 0.9818,
GMB BNSP evaluation for RoBERTa Focal Loss model: 0.9581,
GMB BPSN evaluation for RoBERTa Focal Loss model: 0.901,
GMB Subgroup evaluation for RoBERTa Focal Loss model: 0.8807,
Macro F1 evaluation for RoBERTa Focal Loss model: 0.4648,
Micro F1 evaluation for RoBERTa Focal Loss model: 0.5524,
Precision evaluation for RoBERTa Focal Loss model: 0.4017,
Recall evaluation for RoBERTa Focal Loss model: 0.8839
BERT-MRC+DSC                                                                                  F1 evaluation for BERT-MRC+DSC model: 84.47,
F1 evaluation for BERT-MRC+DSC model: 92.07,
F1 evaluation for BERT-MRC+DSC model: 93.33,
F1 evaluation for BERT-MRC+DSC model: 96.72
CorefQA + SpanBERT-base                                                                                  Avg F1 evaluation for CorefQA + SpanBERT-base model: 79.9
RoBERTa-single                                                                                  Test evaluation for RoBERTa-single model: 58.9,
Test evaluation for RoBERTa-single model: 63.5
SciBERT (full data)                                                                                  Exact Span F1 evaluation for SciBERT (full data) model: 65.5
DeBERTa (single model)                                                                                  Accuracy evaluation for DeBERTa (single model) model: 0.6086
cfilt/HiNER-collapsed-xlm-roberta-large                                                                                  F1-score (Weighted) evaluation for cfilt/HiNER-collapsed-xlm-roberta-large model: 92.22
XLM-RoBERTa-Base                                                                                  EM evaluation for XLM-RoBERTa-Base model: 75.3,
F1 evaluation for XLM-RoBERTa-Base model: 85.9
DiffCSE-RoBERTa-base                                                                                  Spearman Correlation evaluation for DiffCSE-RoBERTa-base model: 0.7005,
Spearman Correlation evaluation for DiffCSE-RoBERTa-base model: 0.7549,
Spearman Correlation evaluation for DiffCSE-RoBERTa-base model: 0.8212,
Spearman Correlation evaluation for DiffCSE-RoBERTa-base model: 0.8281,
Spearman Correlation evaluation for DiffCSE-RoBERTa-base model: 0.8343
BERT$_{ssenet}^{c}$                                                                                  Accuracy (%) evaluation for BERT$_{ssenet}^{c}$ model: 67.25,
Accuracy of Agreeableness evaluation for BERT$_{ssenet}^{c}$ model: 85.89,
Accuracy of Conscientiousness evaluation for BERT$_{ssenet}^{c}$ model: 63.48,
Accuracy of Extraversion evaluation for BERT$_{ssenet}^{c}$ model: 78.21,
Accuracy of Neurotism evaluation for BERT$_{ssenet}^{c}$ model: 53.27,
Accuracy of Openness evaluation for BERT$_{ssenet}^{c}$ model: 55.42,
Macro-F1 evaluation for BERT$_{ssenet}^{c}$ model: 74.08
Baseline BERT (task A)                                                                                  F1 evaluation for Baseline BERT (task A) model: 0.77
lstm+bert                                                                                  1:3 Accuracy evaluation for lstm+bert model: 97
BioLinkBERT (large)                                                                                  Accuracy evaluation for BioLinkBERT (large) model: 72.2,
Accuracy evaluation for BioLinkBERT (large) model: 83.5,
Accuracy evaluation for BioLinkBERT (large) model: 94.8,
F1 evaluation for BioLinkBERT (large) model: 79.98,
F1 evaluation for BioLinkBERT (large) model: 80.06,
F1 evaluation for BioLinkBERT (large) model: 83.35,
F1 evaluation for BioLinkBERT (large) model: 84.88,
F1 evaluation for BioLinkBERT (large) model: 84.90,
F1 evaluation for BioLinkBERT (large) model: 85.18,
F1 evaluation for BioLinkBERT (large) model: 86.39,
F1 evaluation for BioLinkBERT (large) model: 88.1,
F1 evaluation for BioLinkBERT (large) model: 88.76,
F1 evaluation for BioLinkBERT (large) model: 90.22,
F1 evaluation for BioLinkBERT (large) model: 94.04,
Macro F1 word level evaluation for BioLinkBERT (large) model: 74.19,
Micro F1 evaluation for BioLinkBERT (large) model: 79.98,
Micro F1 evaluation for BioLinkBERT (large) model: 83.35,
Micro F1 evaluation for BioLinkBERT (large) model: 84.87,
Micro F1 evaluation for BioLinkBERT (large) model: 84.90,
Pearson Correlation evaluation for BioLinkBERT (large) model: 0.9363
BERT-Large 340M                                                                                  Accuracy evaluation for BERT-Large 340M model: 47.3,
Accuracy evaluation for BERT-Large 340M model: 66.7
RoBERTa-Large (replicated by Adaseq)                                                                                  F1 evaluation for RoBERTa-Large (replicated by Adaseq) model: 43.8
BERT-CRF (Replicated in AdaSeq)                                                                                  Exact Span F1 evaluation for BERT-CRF (Replicated in AdaSeq) model: 97.18,
F1 evaluation for BERT-CRF (Replicated in AdaSeq) model: 59.69,
F1 evaluation for BERT-CRF (Replicated in AdaSeq) model: 68.97,
F1 evaluation for BERT-CRF (Replicated in AdaSeq) model: 72.77,
F1 evaluation for BERT-CRF (Replicated in AdaSeq) model: 93.35,
F1 evaluation for BERT-CRF (Replicated in AdaSeq) model: 96.69,
F1 evaluation for BERT-CRF (Replicated in AdaSeq) model: 96.87
BERT [BERT]                                                                                  Accuracy evaluation for BERT [BERT] model: 79
PNBERT                                                                                  ROUGE-1 evaluation for PNBERT model: 42.69,
ROUGE-2 evaluation for PNBERT model: 19.60,
ROUGE-L evaluation for PNBERT model: 38.85
HuBERT with Libri-Light                                                                                  Word Error Rate (WER) evaluation for HuBERT with Libri-Light model: 1.8,
Word Error Rate (WER) evaluation for HuBERT with Libri-Light model: 2.9
E2GRE-BERT-base                                                                                  F1 evaluation for E2GRE-BERT-base model: 58.72,
Ign F1 evaluation for E2GRE-BERT-base model: 55.22
GTS-BERT                                                                                  F1 evaluation for GTS-BERT model: 70.20
XLM-RoBERTa                                                                                  Average F1 evaluation for XLM-RoBERTa model: 90.8419,
Exact Match evaluation for XLM-RoBERTa model: .36,
F1 evaluation for XLM-RoBERTa model: 66%,
Weighted Average F1-score evaluation for XLM-RoBERTa model: 0.60,
Weighted Average F1-score evaluation for XLM-RoBERTa model: 0.87
StructBERTRoBERTa ensemble                                                                                  Accuracy evaluation for StructBERTRoBERTa ensemble model: 69.2%,
Accuracy evaluation for StructBERTRoBERTa ensemble model: 89.7,
Accuracy evaluation for StructBERTRoBERTa ensemble model: 89.7%,
Accuracy evaluation for StructBERTRoBERTa ensemble model: 90.7,
Accuracy evaluation for StructBERTRoBERTa ensemble model: 90.7%,
Accuracy evaluation for StructBERTRoBERTa ensemble model: 91.5%,
Accuracy evaluation for StructBERTRoBERTa ensemble model: 97.1,
Accuracy evaluation for StructBERTRoBERTa ensemble model: 99.2%,
F1 evaluation for StructBERTRoBERTa ensemble model: 74.4,
F1 evaluation for StructBERTRoBERTa ensemble model: 93.6%,
Pearson Correlation evaluation for StructBERTRoBERTa ensemble model: 0.928,
Spearman Correlation evaluation for StructBERTRoBERTa ensemble model: 0.924
Trans-Encoder-BERT-large-bi (unsup.)                                                                                  Spearman Correlation evaluation for Trans-Encoder-BERT-large-bi (unsup.) model: 0.7133,
Spearman Correlation evaluation for Trans-Encoder-BERT-large-bi (unsup.) model: 0.7819,
Spearman Correlation evaluation for Trans-Encoder-BERT-large-bi (unsup.) model: 0.8137,
Spearman Correlation evaluation for Trans-Encoder-BERT-large-bi (unsup.) model: 0.8481,
Spearman Correlation evaluation for Trans-Encoder-BERT-large-bi (unsup.) model: 0.8616,
Spearman Correlation evaluation for Trans-Encoder-BERT-large-bi (unsup.) model: 0.8816,
Spearman Correlation evaluation for Trans-Encoder-BERT-large-bi (unsup.) model: 0.8851
Query-doc RobeCzech (Roberta-base)                                                                                  P@10 evaluation for Query-doc RobeCzech (Roberta-base) model: 46.73
GAIN-BERT-large                                                                                  F1 evaluation for GAIN-BERT-large model: 62.76,
Ign F1 evaluation for GAIN-BERT-large model: 60.31
BERT-ADA                                                                                  Laptop (Acc) evaluation for BERT-ADA model: 80.23,
Mean Acc (Restaurant + Laptop) evaluation for BERT-ADA model: 84.06,
Restaurant (Acc) evaluation for BERT-ADA model: 87.89
ASA + BERT-base                                                                                  Accuracy evaluation for ASA + BERT-base model: 64.3,
Accuracy evaluation for ASA + BERT-base model: 91.4%,
Accuracy evaluation for ASA + BERT-base model: 94.1,
F1 evaluation for ASA + BERT-base model: 49.8,
F1 evaluation for ASA + BERT-base model: 72.3,
Matched evaluation for ASA + BERT-base model: 85,
Spearman Correlation evaluation for ASA + BERT-base model: 0.865
RIN (BERT, K=2)                                                                                  F1 evaluation for RIN (BERT, K=2) model: 87.8,
F1 evaluation for RIN (BERT, K=2) model: 90.1
Two-Step+BERT-base                                                                                  F1 evaluation for Two-Step+BERT-base model: 53.92,
Ign F1 evaluation for Two-Step+BERT-base model: 54.42
BERT Finetune + UDA                                                                                  Accuracy (10 classes) evaluation for BERT Finetune + UDA model: -,
Accuracy (2 classes) evaluation for BERT Finetune + UDA model: 95.8,
Accuracy evaluation for BERT Finetune + UDA model: 67.92%,
Accuracy evaluation for BERT Finetune + UDA model: 97.95%,
Error evaluation for BERT Finetune + UDA model: 3.5,
Error evaluation for BERT Finetune + UDA model: 37.12
MUPPET Roberta base                                                                                  Accuracy evaluation for MUPPET Roberta base model: 96.7
RYANSQL (BERT)                                                                                  Exact Match Accuracy (Dev) evaluation for RYANSQL (BERT) model: 66.6,
Exact Match Accuracy (Test) evaluation for RYANSQL (BERT) model: 58.2
BioMobileBERT                                                                                  F1 evaluation for BioMobileBERT model: 80.13,
F1 evaluation for BioMobileBERT model: 84.62,
F1 evaluation for BioMobileBERT model: 85.26,
F1 evaluation for BioMobileBERT model: 87.21,
F1 evaluation for BioMobileBERT model: 94.23
DeBERTaV3large                                                                                  Acc evaluation for DeBERTaV3large model: 92.2,
Accuracy evaluation for DeBERTaV3large model: 92.7%,
Accuracy evaluation for DeBERTaV3large model: 93.4,
Accuracy evaluation for DeBERTaV3large model: 96%
pucpr/biobertpt-clin                                                                                  Micro F1 evaluation for pucpr/biobertpt-clin model: 0.602
CGM2IR-BERTbase                                                                                  F1 evaluation for CGM2IR-BERTbase model: 62.06,
Ign F1 evaluation for CGM2IR-BERTbase model: 60.24
BERT + Const-GCN                                                                                  F1 evaluation for BERT + Const-GCN model: 49.71
BertSumExt                                                                                  ROUGE-1 evaluation for BertSumExt model: 43.85,
ROUGE-2 evaluation for BertSumExt model: 20.34,
ROUGE-L evaluation for BertSumExt model: 39.9
RoBERTa-Base Joint MSPP                                                                                  Accuracy evaluation for RoBERTa-Base Joint MSPP model: 74.39,
MAP evaluation for RoBERTa-Base Joint MSPP model: 0.673,
MAP evaluation for RoBERTa-Base Joint MSPP model: 0.887,
MRR evaluation for RoBERTa-Base Joint MSPP model: 0.737,
MRR evaluation for RoBERTa-Base Joint MSPP model: 0.900
Stacked_LinkedBERT                                                                                  F1 (micro) evaluation for Stacked_LinkedBERT model: 78.1,
F1 (micro) evaluation for Stacked_LinkedBERT model: 78.5
BERT-GT                                                                                  F1 evaluation for BERT-GT model: 56.5,
F1 evaluation for BERT-GT model: 72.1
SemBERT(ensemble)                                                                                  EM evaluation for SemBERT(ensemble) model: 86.166,
F1 evaluation for SemBERT(ensemble) model: 88.886
BEV-BERT                                                                                  spl evaluation for BEV-BERT model: 0.60
PubMedBERT uncased                                                                                  Accuracy evaluation for PubMedBERT uncased model: 55.84,
Accuracy evaluation for PubMedBERT uncased model: 87.56,
F1 evaluation for PubMedBERT uncased model: 73.38,
F1 evaluation for PubMedBERT uncased model: 79.1,
F1 evaluation for PubMedBERT uncased model: 84.52,
F1 evaluation for PubMedBERT uncased model: 87.82,
Macro F1 word level evaluation for PubMedBERT uncased model: 73.38,
Micro F1 evaluation for PubMedBERT uncased model: 77.24,
Micro F1 evaluation for PubMedBERT uncased model: 82.32,
Micro F1 evaluation for PubMedBERT uncased model: 82.34,
Micro F1 evaluation for PubMedBERT uncased model: 82.36
FlauBERT (base)                                                                                  Accuracy evaluation for FlauBERT (base) model: 80.6
BERT-Large-CAS                                                                                  Number of params evaluation for BERT-Large-CAS model: 395M,
Params evaluation for BERT-Large-CAS model: 395M,
Test perplexity evaluation for BERT-Large-CAS model: 20.4,
Test perplexity evaluation for BERT-Large-CAS model: 31.3,
Test perplexity evaluation for BERT-Large-CAS model: 34.1,
Validation perplexity evaluation for BERT-Large-CAS model: 19.6,
Validation perplexity evaluation for BERT-Large-CAS model: 36.1,
Validation perplexity evaluation for BERT-Large-CAS model: 37.7
Adv-RoBERTa ensemble                                                                                  Accuracy evaluation for Adv-RoBERTa ensemble model: 88.7%,
Matched evaluation for Adv-RoBERTa ensemble model: 91.1,
Mismatched evaluation for Adv-RoBERTa ensemble model: 90.7
Trans-Encoder-BERT-base-cross (unsup.)                                                                                  Spearman Correlation evaluation for Trans-Encoder-BERT-base-cross (unsup.) model: 0.6952,
Spearman Correlation evaluation for Trans-Encoder-BERT-base-cross (unsup.) model: 0.8444,
Spearman Correlation evaluation for Trans-Encoder-BERT-base-cross (unsup.) model: 0.8559
Bert-base                                                                                  Task 1 Accuracy: all evaluation for Bert-base model: 75.3,
Task 1 Accuracy: domain specific evaluation for Bert-base model: 77.9,
Task 1 Accuracy: general purpose evaluation for Bert-base model: 73.3,
Task 2 Accuracy: all evaluation for Bert-base model: 71.7,
Task 2 Accuracy: domain specific evaluation for Bert-base model: 74.7,
Task 2 Accuracy: general purpose evaluation for Bert-base model: 68.6,
Task 3 Accuracy: all evaluation for Bert-base model: 76.6,
Task 3 Accuracy: domain specific evaluation for Bert-base model: 80.4,
Task 3 Accuracy: general purpose evaluation for Bert-base model: 73.5
KnowBERT                                                                                  F1 evaluation for KnowBERT model: 71.5,
F1 evaluation for KnowBERT model: 89.1
SSAN-RoBERTa-base                                                                                  F1 evaluation for SSAN-RoBERTa-base model: 59.94,
Ign F1 evaluation for SSAN-RoBERTa-base model: 57.71
BERT+SIEF                                                                                  F1 (v1) evaluation for BERT+SIEF model: 61.8,
F1c (v1) evaluation for BERT+SIEF model: 58.4
SpeechBERT                                                                                  F1 score evaluation for SpeechBERT model: 71.75
BERT + Doc2query                                                                                  MRR evaluation for BERT + Doc2query model: 0.368,
mAP evaluation for BERT + Doc2query model: 36.5
TagRec(BERT+Sent BERT)                                                                                  R@10 evaluation for TagRec(BERT+Sent BERT) model: 0.93,
R@15 evaluation for TagRec(BERT+Sent BERT) model: 0.95,
R@20 evaluation for TagRec(BERT+Sent BERT) model: 0.97,
R@5 evaluation for TagRec(BERT+Sent BERT) model: 0.85
BERT-SparseLT+CONTainNER                                                                                  10 way 1~2 shot evaluation for BERT-SparseLT+CONTainNER model: 40.48,
10 way 5~10 shot evaluation for BERT-SparseLT+CONTainNER model: 53.04,
5 way 1~2 shot evaluation for BERT-SparseLT+CONTainNER model: 47.20,
5 way 5~10 shot evaluation for BERT-SparseLT+CONTainNER model: 59.67
Glyce + BERT                                                                                  F1 evaluation for Glyce + BERT model: 67.6,
F1 evaluation for Glyce + BERT model: 80.62,
F1 evaluation for Glyce + BERT model: 95.54,
F1 evaluation for Glyce + BERT model: 96.54,
F1 evaluation for Glyce + BERT model: 96.7,
F1 evaluation for Glyce + BERT model: 97.9,
F1 evaluation for Glyce + BERT model: 98.3,
Precision evaluation for Glyce + BERT model: 67.68,
Precision evaluation for Glyce + BERT model: 81.87,
Precision evaluation for Glyce + BERT model: 95.57,
Precision evaluation for Glyce + BERT model: 96.6,
Precision evaluation for Glyce + BERT model: 96.62,
Precision evaluation for Glyce + BERT model: 97.1,
Precision evaluation for Glyce + BERT model: 97.9,
Precision evaluation for Glyce + BERT model: 98.2,
Recall evaluation for Glyce + BERT model: 67.71,
Recall evaluation for Glyce + BERT model: 81.4,
Recall evaluation for Glyce + BERT model: 95.51,
Recall evaluation for Glyce + BERT model: 96.4,
Recall evaluation for Glyce + BERT model: 96.48,
Recall evaluation for Glyce + BERT model: 96.8,
Recall evaluation for Glyce + BERT model: 98,
Recall evaluation for Glyce + BERT model: 98.3
AV-HuBERT Large + Relaxed Attention + LM                                                                                  Word Error Rate (WER) evaluation for AV-HuBERT Large + Relaxed Attention + LM model: 25.51
PromptNER [BERT-large]                                                                                  F1 evaluation for PromptNER [BERT-large] model: 87.21,
F1 evaluation for PromptNER [BERT-large] model: 88.16,
F1 evaluation for PromptNER [BERT-large] model: 92.41
ALBERT-xxlarge 223M (fine-tuned)                                                                                  Average (%) evaluation for ALBERT-xxlarge 223M (fine-tuned) model: 27.1
JEREX-BERT-base                                                                                  F1 evaluation for JEREX-BERT-base model: 60.40,
Ign F1 evaluation for JEREX-BERT-base model: 58.44
RoBERTa-large-typed-marker                                                                                  F1 evaluation for RoBERTa-large-typed-marker model: 74.6,
F1 evaluation for RoBERTa-large-typed-marker model: 91.1
DistilBioBERT                                                                                  F1 evaluation for DistilBioBERT model: 79.97,
F1 evaluation for DistilBioBERT model: 85.42,
F1 evaluation for DistilBioBERT model: 86.6,
F1 evaluation for DistilBioBERT model: 87.93,
F1 evaluation for DistilBioBERT model: 94.53
GraphCodeBERT-MT4TS                                                                                  Average Accuracy evaluation for GraphCodeBERT-MT4TS model: 63.42
Second-best learning and decoding + BERT + Flair                                                                                  F1 evaluation for Second-best learning and decoding + BERT + Flair model: 77.36,
F1 evaluation for Second-best learning and decoding + BERT + Flair model: 84.34,
F1 evaluation for Second-best learning and decoding + BERT + Flair model: 85.82
SRoBERTa-NLI-STSb-large                                                                                  Spearman Correlation evaluation for SRoBERTa-NLI-STSb-large model: 0.8615
BERT classifier w/o Table                                                                                  Test evaluation for BERT classifier w/o Table model: 50.5,
Val evaluation for BERT classifier w/o Table model: 50.9
InfoBERT (RoBERTa)                                                                                  A1 evaluation for InfoBERT (RoBERTa) model: 75,
A2 evaluation for InfoBERT (RoBERTa) model: 50.5,
A3 evaluation for InfoBERT (RoBERTa) model: 47.7
BERT$_{senet}^{c}$                                                                                  Accuracy (%) evaluation for BERT$_{senet}^{c}$ model: 66.02,
Accuracy of Agreeableness evaluation for BERT$_{senet}^{c}$ model: 81.99,
Accuracy of Conscientiousness evaluation for BERT$_{senet}^{c}$ model: 61.59,
Accuracy of Extraversion evaluation for BERT$_{senet}^{c}$ model: 77.71,
Accuracy of Neurotism evaluation for BERT$_{senet}^{c}$ model: 53.4,
Accuracy of Openness evaluation for BERT$_{senet}^{c}$ model: 55.42,
Macro-F1 evaluation for BERT$_{senet}^{c}$ model: 71.89
BERTEM+MTB                                                                                  F1 (1% Few-Shot) evaluation for BERTEM+MTB model: 43.4,
F1 (10% Few-Shot) evaluation for BERTEM+MTB model: 64.8,
F1 evaluation for BERTEM+MTB model: 71.5,
F1 evaluation for BERTEM+MTB model: 89.5
DeBERTa++                                                                                  Accuracy evaluation for DeBERTa++ model: 93
Twin BERT                                                                                  R@10 evaluation for Twin BERT model: 0.86,
R@15 evaluation for Twin BERT model: 0.91,
R@20 evaluation for Twin BERT model: 0.94,
R@5 evaluation for Twin BERT model: 0.72
RoBERTa (base)                                                                                  ICAT Score evaluation for RoBERTa (base) model: 67.50
RoBERTa-Large                                                                                  D(BERT): F1 evaluation for RoBERTa-Large model: 65.5,
D(BiDAF): F1 evaluation for RoBERTa-Large model: 74.1,
D(RoBERTa): F1 evaluation for RoBERTa-Large model: 53.4,
Overall: F1 evaluation for RoBERTa-Large model: 64.4
NeST-CCG + BERT                                                                                  Accuracy evaluation for NeST-CCG + BERT model: 96.25
ULIP-2 + Point-BERT                                                                                  Overall Accuracy evaluation for ULIP-2 + Point-BERT model: 89.0
BERT LARGE Baseline                                                                                  ANLS evaluation for BERT LARGE Baseline model: 0.655,
Accuracy evaluation for BERT LARGE Baseline model: 54.48
AfriBERTa                                                                                  weighted-F1 score evaluation for AfriBERTa model: 0.439
Trans-Encoder-BERT-large-cross (unsup.)                                                                                  Spearman Correlation evaluation for Trans-Encoder-BERT-large-cross (unsup.) model: 0.7192,
Spearman Correlation evaluation for Trans-Encoder-BERT-large-cross (unsup.) model: 0.8831
Q8BERT (Zafrir et al., 2019)                                                                                  Accuracy evaluation for Q8BERT (Zafrir et al., 2019) model: 65.0,
Accuracy evaluation for Q8BERT (Zafrir et al., 2019) model: 84.8,
Accuracy evaluation for Q8BERT (Zafrir et al., 2019) model: 89.7,
Accuracy evaluation for Q8BERT (Zafrir et al., 2019) model: 93.0,
Accuracy evaluation for Q8BERT (Zafrir et al., 2019) model: 94.7,
Matched evaluation for Q8BERT (Zafrir et al., 2019) model: 85.6,
Pearson Correlation evaluation for Q8BERT (Zafrir et al., 2019) model: 0.911
VLN-Bert                                                                                  error evaluation for VLN-Bert model: 3.09,
length evaluation for VLN-Bert model: 686.62,
oracle success evaluation for VLN-Bert model: 0.99,
spl evaluation for VLN-Bert model: 0.01,
success evaluation for VLN-Bert model: 0.73
BERT-GCN                                                                                  Accuracy evaluation for BERT-GCN model: 0.589,
Macro-F1 evaluation for BERT-GCN model: 0.563,
Micro-F1 evaluation for BERT-GCN model: 0.707
MatchSum (BERT-base)                                                                                  ROUGE-1 evaluation for MatchSum (BERT-base) model: 31.85,
ROUGE-1 evaluation for MatchSum (BERT-base) model: 41.21,
ROUGE-1 evaluation for MatchSum (BERT-base) model: 44.22,
ROUGE-2 evaluation for MatchSum (BERT-base) model: 14.91,
ROUGE-2 evaluation for MatchSum (BERT-base) model: 20.62,
ROUGE-2 evaluation for MatchSum (BERT-base) model: 8.98,
ROUGE-L evaluation for MatchSum (BERT-base) model: 29.58,
ROUGE-L evaluation for MatchSum (BERT-base) model: 36.75,
ROUGE-L evaluation for MatchSum (BERT-base) model: 40.38
RoBERTa-large 355M + Entailment as Few-shot Learner                                                                                  % Test Accuracy evaluation for RoBERTa-large 355M + Entailment as Few-shot Learner model: 93.1,
Accuracy evaluation for RoBERTa-large 355M + Entailment as Few-shot Learner model: 86.0,
Accuracy evaluation for RoBERTa-large 355M + Entailment as Few-shot Learner model: 86.4%,
Accuracy evaluation for RoBERTa-large 355M + Entailment as Few-shot Learner model: 90.5%,
Accuracy evaluation for RoBERTa-large 355M + Entailment as Few-shot Learner model: 90.8,
Accuracy evaluation for RoBERTa-large 355M + Entailment as Few-shot Learner model: 92.5,
Accuracy evaluation for RoBERTa-large 355M + Entailment as Few-shot Learner model: 94.5%,
Accuracy evaluation for RoBERTa-large 355M + Entailment as Few-shot Learner model: 96.1,
Accuracy evaluation for RoBERTa-large 355M + Entailment as Few-shot Learner model: 96.9,
Accuracy evaluation for RoBERTa-large 355M + Entailment as Few-shot Learner model: 97.1,
F1 evaluation for RoBERTa-large 355M + Entailment as Few-shot Learner model: 89.2,
F1 evaluation for RoBERTa-large 355M + Entailment as Few-shot Learner model: 91.0,
Parameters evaluation for RoBERTa-large 355M + Entailment as Few-shot Learner model: 355,
Pearson Correlation evaluation for RoBERTa-large 355M + Entailment as Few-shot Learner model: 0.918
PubMedBERT                                                                                  F1 evaluation for PubMedBERT model: 0.8236,
F1 evaluation for PubMedBERT model: 58.9,
F1 evaluation for PubMedBERT model: 72.9,
Micro F1 evaluation for PubMedBERT model: 82.36
RUN+BERT                                                                                  ROUGE-L evaluation for RUN+BERT model: 93.5,
Rewriting F3 evaluation for RUN+BERT model: 47.7
RoBERTa-wwm-ext-large                                                                                  F1 evaluation for RoBERTa-wwm-ext-large model: 95.8,
Micro F1 evaluation for RoBERTa-wwm-ext-large model: 55.9
RoBERTa+CRF                                                                                  F1 evaluation for RoBERTa+CRF model:  87.27
BERT-SL                                                                                  R10@1 evaluation for BERT-SL model: 0.776,
R10@1 evaluation for BERT-SL model: 0.884,
R10@2 evaluation for BERT-SL model: 0.919,
R10@2 evaluation for BERT-SL model: 0.946,
R10@5 evaluation for BERT-SL model: 0.990,
R10@5 evaluation for BERT-SL model: 0.991,
R2@1 evaluation for BERT-SL model: 0.975
G2GT SpanBERT-large overlap                                                                                  F1 evaluation for G2GT SpanBERT-large overlap model: 80.2
AlBERT                                                                                  AUROC evaluation for AlBERT model: 0.979,
GMB BNSP evaluation for AlBERT model: 0.9499,
GMB BPSN evaluation for AlBERT model: 0.8982,
GMB Subgroup evaluation for AlBERT model: 0.8734,
Macro F1 evaluation for AlBERT model: 0.3541,
Micro F1 evaluation for AlBERT model: 0.4845,
Precision evaluation for AlBERT model: 0.3247,
Recall evaluation for AlBERT model: 0.9104
MERIt(MERIt-deberta-v2-xxlarge )                                                                                  Test evaluation for MERIt(MERIt-deberta-v2-xxlarge ) model: 79.3
BERT_{utt}                                                                                  Accuracy of Sentiment evaluation for BERT_{utt} model: 48.96,
Macro-F1 of Sentiment evaluation for BERT_{utt} model: 45.18
Pretrained Sent BERT                                                                                  R@10 evaluation for Pretrained Sent BERT model: 0.40,
R@15 evaluation for Pretrained Sent BERT model: 0.47,
R@20 evaluation for Pretrained Sent BERT model: 0.52,
R@5 evaluation for Pretrained Sent BERT model: 0.30
BERT (Devlin et al., 2019)-Base                                                                                  Dev Set (Acc-%) evaluation for BERT (Devlin et al., 2019)-Base model: 0.35,
Test Set (Acc-%) evaluation for BERT (Devlin et al., 2019)-Base model: 0.33
BERT (base)                                                                                  ICAT Score evaluation for BERT (base) model: 71.21
RoBERTa+MLP                                                                                  Laptop (Acc) evaluation for RoBERTa+MLP model: 83.78,
Mean Acc (Restaurant + Laptop) evaluation for RoBERTa+MLP model: 85.58,
Restaurant (Acc) evaluation for RoBERTa+MLP model: 87.37
FinQANet (FinBert)                                                                                  Execution Accuracy evaluation for FinQANet (FinBert) model: 53.71,
Program Accuracy evaluation for FinQANet (FinBert) model: 51.71
A-GCN + BERT                                                                                  F1 evaluation for A-GCN + BERT model: 89.85
DRN-BERT-base                                                                                  F1 evaluation for DRN-BERT-base model: 61.37,
Ign F1 evaluation for DRN-BERT-base model: 59.15
M-BERT+Word+Char                                                                                  F1 evaluation for M-BERT+Word+Char model: 59.3,
F1 evaluation for M-BERT+Word+Char model: 71.8
Sequence tagging + token-level transformations + two-stage fine-tuning (+RoBERTa, XLNet)                                                                                  F0.5 evaluation for Sequence tagging + token-level transformations + two-stage fine-tuning (+RoBERTa, XLNet) model: 73.7
CGM2IR-RoBERTalarge                                                                                  F1 evaluation for CGM2IR-RoBERTalarge model: 63.89,
Ign F1 evaluation for CGM2IR-RoBERTalarge model: 61.96
EmoBERTa                                                                                  Accuracy of Sentiment evaluation for EmoBERTa model: 48.09,
Macro-F1 of Sentiment evaluation for EmoBERTa model: 44.60,
Weighted-F1 evaluation for EmoBERTa model: 65.61,
Weighted-F1 evaluation for EmoBERTa model: 68.57
SBERT-STSb-large                                                                                  Spearman Correlation evaluation for SBERT-STSb-large model: 0.8445
HateBERT                                                                                  AUROC evaluation for HateBERT model: 0.9791,
GMB BNSP evaluation for HateBERT model: 0.9589,
GMB BPSN evaluation for HateBERT model: 0.8915,
GMB Subgroup evaluation for HateBERT model: 0.8744,
Macro F1 evaluation for HateBERT model: 0.3679,
Macro F1 evaluation for HateBERT model: 0.494,
Macro F1 evaluation for HateBERT model: 0.742,
Macro F1 evaluation for HateBERT model: 0.805,
Micro F1 evaluation for HateBERT model: 0.4844,
Precision evaluation for HateBERT model: 0.3297,
Recall evaluation for HateBERT model: 0.9165
ConvBERT                                                                                  Accuracy evaluation for ConvBERT model: 94.5,
F1 evaluation for ConvBERT model: 94.6
BERT (gold POS/lemmas)                                                                                  Full F1 (Preps) evaluation for BERT (gold POS/lemmas) model: 71.4,
Function F1 (Preps) evaluation for BERT (gold POS/lemmas) model: 81.7,
Role F1 (Preps) evaluation for BERT (gold POS/lemmas) model: 72.4,
Tags (Full) Acc evaluation for BERT (gold POS/lemmas) model: 81.0
BERT Base                                                                                  AUROC evaluation for BERT Base model: 70.40,
AUROC evaluation for BERT Base model: 81.13,
AUROC evaluation for BERT Base model: 85.84,
Accuracy evaluation for BERT Base model: 53.2,
Accuracy evaluation for BERT Base model: 91.2,
F1 evaluation for BERT Base model: 65.24,
F1 evaluation for BERT Base model: 86.37
SciBERT (SciVocab)                                                                                  F1 evaluation for SciBERT (SciVocab) model: 65.71,
F1 evaluation for SciBERT (SciVocab) model: 67.57,
F1 evaluation for SciBERT (SciVocab) model: 71.18,
F1 evaluation for SciBERT (SciVocab) model: 74.64,
F1 evaluation for SciBERT (SciVocab) model: 76.09,
F1 evaluation for SciBERT (SciVocab) model: 84.99,
F1 evaluation for SciBERT (SciVocab) model: 86.45,
F1 evaluation for SciBERT (SciVocab) model: 88.94,
F1 evaluation for SciBERT (SciVocab) model: 91.41,
F1 evaluation for SciBERT (SciVocab) model: 92.46
BioLinkBERT-large                                                                                  F1 Micro evaluation for BioLinkBERT-large model: 84.7
KVL-BERTLARGE                                                                                  Accuracy evaluation for KVL-BERTLARGE model: 60.3,
Accuracy evaluation for KVL-BERTLARGE model: 76.4,
Accuracy evaluation for KVL-BERTLARGE model: 78.6
SimCSE-BERT-unsup                                                                                  Accuracy evaluation for SimCSE-BERT-unsup model: 62.5,
Spearman Correlation evaluation for SimCSE-BERT-unsup model: 31.15,
Spearman Correlation evaluation for SimCSE-BERT-unsup model: 74.33,
V-Measure evaluation for SimCSE-BERT-unsup model: 29.04
DanFEVER XLM-RoBERTa Large                                                                                  F1 evaluation for DanFEVER XLM-RoBERTa Large model: 0.902
TGCN + BERT                                                                                  Acc evaluation for TGCN + BERT model: 83.68,
Macro-F1 evaluation for TGCN + BERT model: 83.07
BERTlarge-flow (target)                                                                                  Spearman Correlation evaluation for BERTlarge-flow (target) model: 0.6520,
Spearman Correlation evaluation for BERTlarge-flow (target) model: 0.6942,
Spearman Correlation evaluation for BERTlarge-flow (target) model: 0.7226,
Spearman Correlation evaluation for BERTlarge-flow (target) model: 0.7339,
Spearman Correlation evaluation for BERTlarge-flow (target) model: 0.7492,
Spearman Correlation evaluation for BERTlarge-flow (target) model: 0.7763
XLM-RoBERTA                                                                                  Accuracy evaluation for XLM-RoBERTA model: 75.2,
F1 evaluation for XLM-RoBERTA model: 77.0
DistilBERT-uncased-PruneOFA (90% unstruct sparse, QAT Int8)                                                                                  EM evaluation for DistilBERT-uncased-PruneOFA (90% unstruct sparse, QAT Int8) model: 75.62,
F1 evaluation for DistilBERT-uncased-PruneOFA (90% unstruct sparse, QAT Int8) model: 83.87,
Matched evaluation for DistilBERT-uncased-PruneOFA (90% unstruct sparse, QAT Int8) model: 78.8,
Mismatched evaluation for DistilBERT-uncased-PruneOFA (90% unstruct sparse, QAT Int8) model: 80.4
Li and Zhao - BERT                                                                                  EM evaluation for Li and Zhao - BERT model: 46.9,
EM evaluation for Li and Zhao - BERT model: 49.2,
F1 evaluation for Li and Zhao - BERT model: 63.9,
F1 evaluation for Li and Zhao - BERT model: 64
Dense-CCNet-SciBERTbase                                                                                  F1 evaluation for Dense-CCNet-SciBERTbase model: 77.06,
F1 evaluation for Dense-CCNet-SciBERTbase model: 86.44
roberta-base-mnli                                                                                  1:1 Accuracy evaluation for roberta-base-mnli model: 48.5
BERT-LARGE                                                                                  Accuracy evaluation for BERT-LARGE model: 55.9,
Accuracy evaluation for BERT-LARGE model: 60.5%,
Accuracy evaluation for BERT-LARGE model: 70.1%,
Accuracy evaluation for BERT-LARGE model: 92.7%,
Accuracy evaluation for BERT-LARGE model: 94.9,
Average evaluation for BERT-LARGE model: 82.1,
Dev evaluation for BERT-LARGE model: 86.6,
F1 evaluation for BERT-LARGE model: 72.1,
F1 evaluation for BERT-LARGE model: 89.3,
F1 evaluation for BERT-LARGE model: 92.8,
Matched evaluation for BERT-LARGE model: 86.7,
Mismatched evaluation for BERT-LARGE model: 85.9,
Spearman Correlation evaluation for BERT-LARGE model: 0.865,
Test evaluation for BERT-LARGE model: 86.3
GAZP+BERT                                                                                  interaction match accuracy evaluation for GAZP+BERT model: 12.8,
question match accuracy evaluation for GAZP+BERT model: 39.7
MUPPET Roberta Base                                                                                  Accuracy evaluation for MUPPET Roberta Base model: 83.8
DeBERTa-Large + SSP                                                                                  MAP evaluation for DeBERTa-Large + SSP model: 0.901,
MRR evaluation for DeBERTa-Large + SSP model: 0.914
ALBERT-xxlarge 235M                                                                                  Accuracy evaluation for ALBERT-xxlarge 235M model: 58.7,
Accuracy evaluation for ALBERT-xxlarge 235M model: 78.8
CodeBERT (MLM+RTD)                                                                                  Smoothed BLEU-4 evaluation for CodeBERT (MLM+RTD) model: 14.56,
Smoothed BLEU-4 evaluation for CodeBERT (MLM+RTD) model: 15.41,
Smoothed BLEU-4 evaluation for CodeBERT (MLM+RTD) model: 15.99,
Smoothed BLEU-4 evaluation for CodeBERT (MLM+RTD) model: 21.32,
Smoothed BLEU-4 evaluation for CodeBERT (MLM+RTD) model: 26.66,
Smoothed BLEU-4 evaluation for CodeBERT (MLM+RTD) model: 8.46,
Smoothed BLEU-4 evaluation for CodeBERT (MLM+RTD) model: 9.54
BERT-Tagger                                                                                  F1-Measure evaluation for BERT-Tagger model: 67.13,
Precision evaluation for BERT-Tagger model: 65.56,
Recall evaluation for BERT-Tagger model: 68.78
Sequence Labeling with edits using BERT, Faster inference (Single Model)                                                                                  F0.5 evaluation for Sequence Labeling with edits using BERT, Faster inference (Single Model) model: 59.7
Auxiliary IndicBert                                                                                  F1 score evaluation for Auxiliary IndicBert model: 0.5725,
F1 score evaluation for Auxiliary IndicBert model: 0.7741
DeBERTa-Large 304M (classification-based)                                                                                  Accuracy evaluation for DeBERTa-Large 304M (classification-based) model: 79.9,
Accuracy evaluation for DeBERTa-Large 304M (classification-based) model: 85.9,
Accuracy evaluation for DeBERTa-Large 304M (classification-based) model: 95.6
FaceXHuBERT                                                                                  FDD evaluation for FaceXHuBERT model: 4.96,
Lip Vertex Error evaluation for FaceXHuBERT model: 4.56,
Mean Face Vertex Error evaluation for FaceXHuBERT model: 4.80
RoBERTa-Base + PSD                                                                                  MAP evaluation for RoBERTa-Base + PSD model: 0.903,
MRR evaluation for RoBERTa-Base + PSD model: 0.951
BERT Large Augmented (single model)                                                                                  In-domain evaluation for BERT Large Augmented (single model) model: 82.5,
Out-of-domain evaluation for BERT Large Augmented (single model) model: 77.6,
Overall evaluation for BERT Large Augmented (single model) model: 81.1
ELC-BERT-base 98M                                                                                  Accuracy evaluation for ELC-BERT-base 98M model: 82.6
Table-BERT-Horizontal-T+F-Template                                                                                  Test evaluation for Table-BERT-Horizontal-T+F-Template model: 65.12,
Val evaluation for Table-BERT-Horizontal-T+F-Template model: 66.1
SpanBERT                                                                                  Accuracy evaluation for SpanBERT model: 64.3%,
Accuracy evaluation for SpanBERT model: 79.0%,
Accuracy evaluation for SpanBERT model: 89.5,
Accuracy evaluation for SpanBERT model: 90.9%,
Accuracy evaluation for SpanBERT model: 94.3%,
Accuracy evaluation for SpanBERT model: 94.8,
Avg F1 evaluation for SpanBERT model: 64.6,
EM evaluation for SpanBERT model: 85.7,
Exact Span F1 evaluation for SpanBERT model: 34.64,
F1 evaluation for SpanBERT model: 70.8,
F1 evaluation for SpanBERT model: 71.9,
F1 evaluation for SpanBERT model: 73.6,
F1 evaluation for SpanBERT model: 75.71,
F1 evaluation for SpanBERT model: 79.6,
F1 evaluation for SpanBERT model: 82.5,
F1 evaluation for SpanBERT model: 83.6,
F1 evaluation for SpanBERT model: 84.8,
F1 evaluation for SpanBERT model: 85.3,
F1 evaluation for SpanBERT model: 86.8,
F1 evaluation for SpanBERT model: 88.7,
F1 evaluation for SpanBERT model: 88.8,
F1(Neg) evaluation for SpanBERT model: 86.02,
F1(Pos) evaluation for SpanBERT model: 60.00,
Matched evaluation for SpanBERT model: 88.1,
Pearson Correlation evaluation for SpanBERT model: 0.899
Multimodal(ViT+BERT, Input: Image + Caption) - Concatenate                                                                                  Accuracy evaluation for Multimodal(ViT+BERT, Input: Image + Caption) - Concatenate model: 0.7951
NCBI_BERT(large) (P)                                                                                  F1 evaluation for NCBI_BERT(large) (P) model: 74.4,
F1 evaluation for NCBI_BERT(large) (P) model: 79.9,
F1 evaluation for NCBI_BERT(large) (P) model: 87.3
Fei et al. 2021 (HeSyFu) + RoBERTa                                                                                  F1 evaluation for Fei et al. 2021 (HeSyFu) + RoBERTa model: 88.59
NCBI_BERT(base) (P+M)                                                                                  F1 evaluation for NCBI_BERT(base) (P+M) model: 0.792,
F1 evaluation for NCBI_BERT(base) (P+M) model: 84,
Pearson Correlation evaluation for NCBI_BERT(base) (P+M) model: 0.848,
Pearson Correlation evaluation for NCBI_BERT(base) (P+M) model: 0.9159999999999999
RoBERTa+ALBERT                                                                                  F1 evaluation for RoBERTa+ALBERT model: 70.4
BioDistilBERT                                                                                  F1 evaluation for BioDistilBERT model: 79.1,
F1 evaluation for BioDistilBERT model: 85.61,
F1 evaluation for BioDistilBERT model: 86.97,
F1 evaluation for BioDistilBERT model: 87.61,
F1 evaluation for BioDistilBERT model: 94.48
BERT-PT                                                                                  F1 (%) evaluation for BERT-PT model: 78.8,
Laptop (Acc) evaluation for BERT-PT model: 78.07,
Laptop (F1) evaluation for BERT-PT model: 84.26,
Mean Acc (Restaurant + Laptop) evaluation for BERT-PT model: 81.51,
Restaurant (Acc) evaluation for BERT-PT model: 84.95,
Restaurant (F1) evaluation for BERT-PT model: 77.97
PAR BERT Base                                                                                  Accuracy evaluation for PAR BERT Base model: 89.2,
Accuracy evaluation for PAR BERT Base model: 91.6
BERT Solawetz and Larson (2021)                                                                                  F1 evaluation for BERT Solawetz and Larson (2021) model: 47.54
Bert                                                                                  10 fold Cross validation evaluation for Bert model: 90,
1:1 Accuracy evaluation for Bert model: 0.80352,
Accuracy (%) evaluation for Bert model: 84.41,
Accuracy (max) evaluation for Bert model: 86.34,
Accuracy (max) evaluation for Bert model: 93.99,
Accuracy (mean) evaluation for Bert model: 87.66,
Accuracy (mean) evaluation for Bert model: 93.86,
Accuracy evaluation for Bert model: 65.0,
F1 - macro evaluation for Bert model: 0.20803,
Micro F1 evaluation for Bert model: 0.85431
Transition-based (+BERT)                                                                                  Full MRP F1 evaluation for Transition-based (+BERT) model: 77.7,
Full UCCA F1 evaluation for Transition-based (+BERT) model: 57.4,
LPP MRP F1 evaluation for Transition-based (+BERT) model: 82.2,
LPP UCCA F1 evaluation for Transition-based (+BERT) model: 65.9
SMART_RoBERTa (single model)                                                                                  Accuracy evaluation for SMART_RoBERTa (single model) model: 0.5371
HuBERT-B-LS960 (e2e approach, uses LM)                                                                                  F1 (%) evaluation for HuBERT-B-LS960 (e2e approach, uses LM) model: 61.9,
label-F1 (%) evaluation for HuBERT-B-LS960 (e2e approach, uses LM) model: 70.3
BERT + Dep-GCN [?] Const-GCN                                                                                  F1 evaluation for BERT + Dep-GCN [?] Const-GCN model: 49.89
GlossBert-ws                                                                                  Task 1 Accuracy: all evaluation for GlossBert-ws model: 75.9,
Task 1 Accuracy: domain specific evaluation for GlossBert-ws model: 76.7,
Task 1 Accuracy: general purpose evaluation for GlossBert-ws model: 75.2
PubmedBERT(Gu et al., 2022)                                                                                  Dev Set (Acc-%) evaluation for PubmedBERT(Gu et al., 2022) model: 0.40,
Test Set (Acc-%) evaluation for PubmedBERT(Gu et al., 2022) model: 0.41
BioKMNER + BioBERT                                                                                  F1 evaluation for BioKMNER + BioBERT model: 76.33,
F1 evaluation for BioKMNER + BioBERT model: 77.83,
F1 evaluation for BioKMNER + BioBERT model: 85.29,
F1 evaluation for BioKMNER + BioBERT model: 90.08,
F1 evaluation for BioKMNER + BioBERT model: 94.22
DNABERT-2-117M                                                                                  Avg F1 evaluation for DNABERT-2-117M model: 71.02,
MCC evaluation for DNABERT-2-117M model: 55.98,
MCC evaluation for DNABERT-2-117M model: 67.99,
MCC evaluation for DNABERT-2-117M model: 70.10,
MCC evaluation for DNABERT-2-117M model: 70.52,
MCC evaluation for DNABERT-2-117M model: 84.21,
MCC evaluation for DNABERT-2-117M model: 84.99
DocBERT [DOCBERT]                                                                                  Accuracy evaluation for DocBERT [DOCBERT] model: 82.3
GenPT (RoBERTa)                                                                                  F1 evaluation for GenPT (RoBERTa) model: 91.1
RoBERTa-Large + ICDA                                                                                  Accuracy (%) evaluation for RoBERTa-Large + ICDA model: 82.45,
Accuracy (%) evaluation for RoBERTa-Large + ICDA model: 84.01,
Accuracy (%) evaluation for RoBERTa-Large + ICDA model: 87.41,
Accuracy (%) evaluation for RoBERTa-Large + ICDA model: 89.79,
Accuracy (%) evaluation for RoBERTa-Large + ICDA model: 92.57,
Accuracy (%) evaluation for RoBERTa-Large + ICDA model: 92.62,
Accuracy (%) evaluation for RoBERTa-Large + ICDA model: 94.42,
Accuracy (%) evaluation for RoBERTa-Large + ICDA model: 94.84,
Accuracy (%) evaluation for RoBERTa-Large + ICDA model: 97.12,
Accuracy evaluation for RoBERTa-Large + ICDA model: 94.42
BERT-Base cased (fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus')                                                                                  F1 evaluation for BERT-Base cased (fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus') model: 89.12,
Precision evaluation for BERT-Base cased (fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus') model: 88.25,
Recall evaluation for BERT-Base cased (fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus') model: 90.1
ALBERT (single model)                                                                                  Accuracy evaluation for ALBERT (single model) model: 0.5922,
EM evaluation for ALBERT (single model) model: 88.107,
F1 evaluation for ALBERT (single model) model: 90.902
c2f-coref + BERT-large                                                                                  Avg F1 evaluation for c2f-coref + BERT-large model: 76.9
RobBERT                                                                                  Accuracy evaluation for RobBERT model: 94.422%,
F1 evaluation for RobBERT model: 94.422%
LSR+BERT-base                                                                                  F1 evaluation for LSR+BERT-base model: 59.05,
Ign F1 evaluation for LSR+BERT-base model: 56.97
BERT-to-BERT                                                                                  BLEU evaluation for BERT-to-BERT model: 44,
PARENT evaluation for BERT-to-BERT model: 52.6
MoCap-SPIN + PoseBERT                                                                                  Acceleration Error evaluation for MoCap-SPIN + PoseBERT model: 8.3,
Acceleration Error evaluation for MoCap-SPIN + PoseBERT model: 8.7,
MPJPE evaluation for MoCap-SPIN + PoseBERT model: 89.4,
MPJPE evaluation for MoCap-SPIN + PoseBERT model: 97.4,
MPVPE evaluation for MoCap-SPIN + PoseBERT model: 103.8,
PA-MPJPE evaluation for MoCap-SPIN + PoseBERT model: 52.9,
PA-MPJPE evaluation for MoCap-SPIN + PoseBERT model: 63.3
MacBERT-large                                                                                  Accuracy evaluation for MacBERT-large model: 62.9,
Macro F1 evaluation for MacBERT-large model: 85.6,
Micro F1 evaluation for MacBERT-large model: 62.4
En-BERT + TDA                                                                                  Accuracy evaluation for En-BERT + TDA model: 82.1%,
Accuracy evaluation for En-BERT + TDA model: 88.6,
MCC evaluation for En-BERT + TDA model: 0.565,
MCC evaluation for En-BERT + TDA model: 0.725
BERTSUMABS                                                                                  RG-1(%) evaluation for BERTSUMABS model: 23.05,
RG-2(%) evaluation for BERTSUMABS model: 9.48,
RG-L(%) evaluation for BERTSUMABS model: 18.07
SemBERT large                                                                                  EM evaluation for SemBERT large model: 80.9,
F1 evaluation for SemBERT large model: 83.6
VL-BERT                                                                                  ADD(S) AUC evaluation for VL-BERT model: 86.27
SMARTRoBERTa-LARGE                                                                                  % Dev Accuracy evaluation for SMARTRoBERTa-LARGE model: 57.1,
% Test Accuracy evaluation for SMARTRoBERTa-LARGE model: 57.1
TranX + BERT w/mined                                                                                  Accuracy evaluation for TranX + BERT w/mined model: 81.03,
BLEU Score evaluation for TranX + BERT w/mined model: 79.86,
BLEU evaluation for TranX + BERT w/mined model: 34.2,
Exact Match Accuracy evaluation for TranX + BERT w/mined model: 5.8
LightGBM + RoBERTa embedding                                                                                  AUROC evaluation for LightGBM + RoBERTa embedding model: 0.767,
AUROC evaluation for LightGBM + RoBERTa embedding model: 0.865,
AUROC evaluation for LightGBM + RoBERTa embedding model: 0.954
ALBERT (fine-tuned)                                                                                  Average (%) evaluation for ALBERT (fine-tuned) model: 27.1,
Humanities evaluation for ALBERT (fine-tuned) model: 27.2,
Other evaluation for ALBERT (fine-tuned) model: 27.9,
Parameters (Billions) evaluation for ALBERT (fine-tuned) model: 0.031,
STEM evaluation for ALBERT (fine-tuned) model: 27.7,
Social Sciences evaluation for ALBERT (fine-tuned) model: 25.7
BERT-Large                                                                                  D(BERT): F1 evaluation for BERT-Large model: 62.4,
D(BiDAF): F1 evaluation for BERT-Large model: 71.3,
D(RoBERTa): F1 evaluation for BERT-Large model: 54.4,
Matched evaluation for BERT-Large model: 88,
Mismatched evaluation for BERT-Large model: 88,
Overall: F1 evaluation for BERT-Large model: 62.7,
ROUGE-L evaluation for BERT-Large model: 49.98,
Structure Aware Intrinsic Dimension evaluation for BERT-Large model: 1037,
Structure Aware Intrinsic Dimension evaluation for BERT-Large model: 1200,
Unpaired Accuracy evaluation for BERT-Large model: 73.1%
SciBERT (Base Vocab)                                                                                  F1 evaluation for SciBERT (Base Vocab) model: 64.02,
F1 evaluation for SciBERT (Base Vocab) model: 65.24,
F1 evaluation for SciBERT (Base Vocab) model: 70.82,
F1 evaluation for SciBERT (Base Vocab) model: 73.7,
F1 evaluation for SciBERT (Base Vocab) model: 74.42,
F1 evaluation for SciBERT (Base Vocab) model: 75.77,
F1 evaluation for SciBERT (Base Vocab) model: 84.43,
F1 evaluation for SciBERT (Base Vocab) model: 86.81,
F1 evaluation for SciBERT (Base Vocab) model: 86.88,
F1 evaluation for SciBERT (Base Vocab) model: 88.11,
F1 evaluation for SciBERT (Base Vocab) model: 91.26,
F1 evaluation for SciBERT (Base Vocab) model: 92.32
mBert                                                                                  Accuracy evaluation for mBert model: 0.832,
F1 score evaluation for mBert model: 0.8365
RoBERTa+RegCCRF                                                                                  F1 evaluation for RoBERTa+RegCCRF model: 87.51
KG-BERTScore                                                                                  Score evaluation for KG-BERTScore model: 17.28
VideoBERT + S3D                                                                                  BLEU-3 evaluation for VideoBERT + S3D model: 7.59,
BLEU-4 evaluation for VideoBERT + S3D model: 4.33,
CIDEr evaluation for VideoBERT + S3D model: 0.55,
METEOR evaluation for VideoBERT + S3D model: 11.94,
ROUGE-L evaluation for VideoBERT + S3D model: 28.80
Deep ParsBERT                                                                                  Macro F1 evaluation for Deep ParsBERT model: 0.65,
Macro F1 evaluation for Deep ParsBERT model: 0.71
SBERT-NLI-base                                                                                  Spearman Correlation evaluation for SBERT-NLI-base model: 0.7291,
Spearman Correlation evaluation for SBERT-NLI-base model: 0.7703
Triaffine + BERT                                                                                  F1 evaluation for Triaffine + BERT model: 85.05,
F1 evaluation for Triaffine + BERT model: 86.82
ALBERT xlarge                                                                                  EM evaluation for ALBERT xlarge model: 83.1,
F1 evaluation for ALBERT xlarge model: 85.9
LTG-BERT-small 24M                                                                                  Accuracy evaluation for LTG-BERT-small 24M model: 53.7,
Accuracy evaluation for LTG-BERT-small 24M model: 77.6,
Matched evaluation for LTG-BERT-small 24M model: 78,
Mismatched evaluation for LTG-BERT-small 24M model: 78.8
SPAN-BERT                                                                                  Avg F1 evaluation for SPAN-BERT model: 65.74,
Laptop 2014 (F1) evaluation for SPAN-BERT model: 61.25,
Restaurant 2014 (F1) evaluation for SPAN-BERT model: 73.68,
Restaurant 2015 (F1) evaluation for SPAN-BERT model: 62.29
SciBERT (Beltagy et al., 2019)                                                                                  Dev Set (Acc-%) evaluation for SciBERT (Beltagy et al., 2019) model: 0.39,
Test Set (Acc-%) evaluation for SciBERT (Beltagy et al., 2019) model: 0.39
PARADE(BERT)                                                                                  P@20 evaluation for PARADE(BERT) model: 0.4486,
nDCG@20 evaluation for PARADE(BERT) model: 0.5252
BERT-Base-uncased-PruneOFA (85% unstruct sparse, QAT Int8)                                                                                  EM evaluation for BERT-Base-uncased-PruneOFA (85% unstruct sparse, QAT Int8) model: 80.84,
F1 evaluation for BERT-Base-uncased-PruneOFA (85% unstruct sparse, QAT Int8) model: 88.24,
Matched evaluation for BERT-Base-uncased-PruneOFA (85% unstruct sparse, QAT Int8) model: 81.4,
Mismatched evaluation for BERT-Base-uncased-PruneOFA (85% unstruct sparse, QAT Int8) model: 82.51
RoBERTa (no data aug)                                                                                  EM evaluation for RoBERTa (no data aug) model: 86.5,
F1 evaluation for RoBERTa (no data aug) model: 89.4
Entity-Aware BERT                                                                                  F1 evaluation for Entity-Aware BERT model: 89.0
BERT_large+ITPT                                                                                  Accuracy evaluation for BERT_large+ITPT model: 95.79,
Error evaluation for BERT_large+ITPT model: 1.81,
Error evaluation for BERT_large+ITPT model: 28.62
LAMBERT (75M)                                                                                  F1 evaluation for LAMBERT (75M) model: 80.42
PromCSE-RoBERTa-large                                                                                  Spearman Correlation evaluation for PromCSE-RoBERTa-large model: 0.7956,
Spearman Correlation evaluation for PromCSE-RoBERTa-large model: 0.8243,
Spearman Correlation evaluation for PromCSE-RoBERTa-large model: 0.8381,
Spearman Correlation evaluation for PromCSE-RoBERTa-large model: 0.8496,
Spearman Correlation evaluation for PromCSE-RoBERTa-large model: 0.8787,
Spearman Correlation evaluation for PromCSE-RoBERTa-large model: 0.8808,
Spearman Correlation evaluation for PromCSE-RoBERTa-large model: 0.8897,
avg ± std evaluation for PromCSE-RoBERTa-large model: 74.8± 1.0
RoBERTaGCN                                                                                  Accuracy evaluation for RoBERTaGCN model: 72.8,
Accuracy evaluation for RoBERTaGCN model: 89.5,
Accuracy evaluation for RoBERTaGCN model: 89.7,
Accuracy evaluation for RoBERTaGCN model: 98.2
ViLBERT - VQA                                                                                  DA VQA Score evaluation for ViLBERT - VQA model: 12.0,
MC Accuracy evaluation for ViLBERT - VQA model: 42.1
Mirror-RoBERTa-base (unsup.)                                                                                  Spearman Correlation evaluation for Mirror-RoBERTa-base (unsup.) model: 0.648,
Spearman Correlation evaluation for Mirror-RoBERTa-base (unsup.) model: 0.706,
Spearman Correlation evaluation for Mirror-RoBERTa-base (unsup.) model: 0.732,
Spearman Correlation evaluation for Mirror-RoBERTa-base (unsup.) model: 0.78,
Spearman Correlation evaluation for Mirror-RoBERTa-base (unsup.) model: 0.787,
Spearman Correlation evaluation for Mirror-RoBERTa-base (unsup.) model: 0.798,
Spearman Correlation evaluation for Mirror-RoBERTa-base (unsup.) model: 0.819
SAIS-RoBERTa-large                                                                                  F1 evaluation for SAIS-RoBERTa-large model: 65.11,
Ign F1 evaluation for SAIS-RoBERTa-large model: 63.44
SA-BERT+HCL                                                                                  MAP evaluation for SA-BERT+HCL model: 0.639,
MAP evaluation for SA-BERT+HCL model: 0.671,
MRR evaluation for SA-BERT+HCL model: 0.681,
MRR evaluation for SA-BERT+HCL model: 0.683,
P@1 evaluation for SA-BERT+HCL model: 0.503,
P@1 evaluation for SA-BERT+HCL model: 0.514,
R10@1 evaluation for SA-BERT+HCL model: 0.330,
R10@1 evaluation for SA-BERT+HCL model: 0.454,
R10@1 evaluation for SA-BERT+HCL model: 0.721,
R10@2 evaluation for SA-BERT+HCL model: 0.531,
R10@2 evaluation for SA-BERT+HCL model: 0.659,
R10@2 evaluation for SA-BERT+HCL model: 0.896,
R10@5 evaluation for SA-BERT+HCL model: 0.858,
R10@5 evaluation for SA-BERT+HCL model: 0.917,
R10@5 evaluation for SA-BERT+HCL model: 0.993
BERT-Base (single model)                                                                                  EM evaluation for BERT-Base (single model) model: 54.040,
F1 evaluation for BERT-Base (single model) model: 56.065
BERT-UMS+FGC                                                                                  R10@1 evaluation for BERT-UMS+FGC model: 0.886,
R10@2 evaluation for BERT-UMS+FGC model: 0.948,
R10@5 evaluation for BERT-UMS+FGC model: 0.990
PubMedBERT (uncased; abstracts)                                                                                  Accuracy evaluation for PubMedBERT (uncased; abstracts) model: 71.7,
F1 evaluation for PubMedBERT (uncased; abstracts) model: 82.32
ALUM (RoBERTa-LARGE)                                                                                  A1 evaluation for ALUM (RoBERTa-LARGE) model: 72.3,
A2 evaluation for ALUM (RoBERTa-LARGE) model: 52.1,
A3 evaluation for ALUM (RoBERTa-LARGE) model: 48.4
Li and Choi - BERT                                                                                  EM evaluation for Li and Choi - BERT model: 46.8,
F1 evaluation for Li and Choi - BERT model: 63.1
ALBERT large                                                                                  EM evaluation for ALBERT large model: 79.0,
F1 evaluation for ALBERT large model: 82.1
1-6 BertGCN                                                                                  Accuracy evaluation for 1-6 BertGCN model: 96.6
BERTwiki 340M (fine-tuned on WSCR)                                                                                  Accuracy evaluation for BERTwiki 340M (fine-tuned on WSCR) model: 72.5,
Accuracy evaluation for BERTwiki 340M (fine-tuned on WSCR) model: 74.7
JointBERT-CAE                                                                                  Accuracy evaluation for JointBERT-CAE model: 97.5,
F1 evaluation for JointBERT-CAE model: 0.961,
F1 evaluation for JointBERT-CAE model: 0.97,
Intent Accuracy evaluation for JointBERT-CAE model: 97.7,
Intent Accuracy evaluation for JointBERT-CAE model: 98.3,
Slot F1 Score evaluation for JointBERT-CAE model: 97.0,
Slot F1 evaluation for JointBERT-CAE model: 95.5
MWP-BERT                                                                                  Accuracy (5-fold) evaluation for MWP-BERT model: 82.4,
Accuracy (training-test) evaluation for MWP-BERT model: 84.7,
Answer Accuracy evaluation for MWP-BERT model: 76.6
Space-BERT                                                                                  Accuracy (2 classes) evaluation for Space-BERT model: 0.8110,
Accuracy (2 classes) evaluation for Space-BERT model: 0.8309,
F1 Macro evaluation for Space-BERT model: 0.8006,
F1 Macro evaluation for Space-BERT model: 0.8108
BERT-FP-LBL                                                                                  Accuracy (5-fold) evaluation for BERT-FP-LBL model: 0.927
Deep Biaffine + RoBERTa                                                                                  LAS evaluation for Deep Biaffine + RoBERTa model: 95.75,
UAS evaluation for Deep Biaffine + RoBERTa model: 97.29
CLIPBERT                                                                                  Accuracy evaluation for CLIPBERT model: 0.374
RoBERTa-base                                                                                  F1 (%) evaluation for RoBERTa-base model: 71.14,
F1 (%) evaluation for RoBERTa-base model: 72.18,
F1 Micro evaluation for RoBERTa-base model: 52.03,
HONEST evaluation for RoBERTa-base model: 2.38,
Test evaluation for RoBERTa-base model: 48.5
Retro-Reader on ALBERT (single model)                                                                                  EM evaluation for Retro-Reader on ALBERT (single model) model: 88.107,
F1 evaluation for Retro-Reader on ALBERT (single model) model: 91.419
HSI-BERT                                                                                  OA@15perclass evaluation for HSI-BERT model: 58.50±1.56,
OA@15perclass evaluation for HSI-BERT model: 75.31±1.59,
OA@15perclass evaluation for HSI-BERT model: 82.93±0.94
RoBERTa-pair-large                                                                                  F1 (%) evaluation for RoBERTa-pair-large model: 80.0
RoBERTa-Base                                                                                  ALL evaluation for RoBERTa-Base model: 61.3,
Emoji evaluation for RoBERTa-Base model: 30.9,
Emotion evaluation for RoBERTa-Base model: 76.1,
Hate evaluation for RoBERTa-Base model: 46.6,
Irony evaluation for RoBERTa-Base model: 59.7,
Offensive evaluation for RoBERTa-Base model: 79.5,
Sentiment evaluation for RoBERTa-Base model: 71.3,
Stance evaluation for RoBERTa-Base model: 68
LP-BERT                                                                                  Hits@1 evaluation for LP-BERT model: 0.223,
Hits@1 evaluation for LP-BERT model: 0.343,
Hits@10 evaluation for LP-BERT model: 0.490,
Hits@10 evaluation for LP-BERT model: 0.752,
Hits@10 evaluation for LP-BERT model: 1.000,
Hits@3 evaluation for LP-BERT model: 0.336,
Hits@3 evaluation for LP-BERT model: 0.563,
MR evaluation for LP-BERT model: 1.18,
MR evaluation for LP-BERT model: 154,
MR evaluation for LP-BERT model: 92,
MRR evaluation for LP-BERT model: 0.31,
MRR evaluation for LP-BERT model: 0.482
AEN-BERT                                                                                  Accuracy evaluation for AEN-BERT model: 74.71,
Laptop (Acc) evaluation for AEN-BERT model: 79.93,
Mean Acc (Restaurant + Laptop) evaluation for AEN-BERT model: 81.53,
Restaurant (Acc) evaluation for AEN-BERT model: 83.12
BERT-LARGE (Single+TriviaQA)                                                                                  EM evaluation for BERT-LARGE (Single+TriviaQA) model: 84.2,
F1 evaluation for BERT-LARGE (Single+TriviaQA) model: 91.1,
F1 evaluation for BERT-LARGE (Single+TriviaQA) model: 91.8
SemBERT                                                                                  % Test Accuracy evaluation for SemBERT model: 91.9,
% Train Accuracy evaluation for SemBERT model: 94.4,
Parameters evaluation for SemBERT model: 339m
NCBI_BERT(base) (P)                                                                                  F1 evaluation for NCBI_BERT(base) (P) model: 86.6,
F1 evaluation for NCBI_BERT(base) (P) model: 93.5
Sentence BERT                                                                                  Soft-F1 evaluation for Sentence BERT model: 0.31
Transition-based (+BERT + Efficient Training + Effective Encoding)                                                                                  Full MRP F1 evaluation for Transition-based (+BERT + Efficient Training + Effective Encoding) model: 81.7,
Full UCCA F1 evaluation for Transition-based (+BERT + Efficient Training + Effective Encoding) model: 66.7,
LPP MRP F1 evaluation for Transition-based (+BERT + Efficient Training + Effective Encoding) model: 82.6,
LPP UCCA F1 evaluation for Transition-based (+BERT + Efficient Training + Effective Encoding) model: 64.4
SciBERT cased (SciVocab, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus')                                                                                  F1 evaluation for SciBERT cased (SciVocab, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus') model: 90.69,
Precision evaluation for SciBERT cased (SciVocab, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus') model: 89,
Recall evaluation for SciBERT cased (SciVocab, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus') model: 92.54
pucpr/biobertpt-bio                                                                                  Micro F1 evaluation for pucpr/biobertpt-bio model: 0.602
BERT-SAWS                                                                                  F-Measure evaluation for BERT-SAWS model: 0.934
Siamese Bi-Encoder (RoBERTa)                                                                                  Recall@100 evaluation for Siamese Bi-Encoder (RoBERTa) model: 71.63,
Recall@200 evaluation for Siamese Bi-Encoder (RoBERTa) model: 78.38,
Recall@500 evaluation for Siamese Bi-Encoder (RoBERTa) model: 83.77
GEANet-SciBERT                                                                                  F1 evaluation for GEANet-SciBERT model: 60.06
CorefQA + SpanBERT-large                                                                                  Avg F1 evaluation for CorefQA + SpanBERT-large model: 83.1
GAZP + BERT                                                                                  interaction match accuracy evaluation for GAZP + BERT model: 23.5,
question match accuracy evaluation for GAZP + BERT model: 45.9
SIRE-BERT-base                                                                                  F1 evaluation for SIRE-BERT-base model: 62.05,
Ign F1 evaluation for SIRE-BERT-base model: 60.18
IMN-BERT                                                                                  Avg F1 evaluation for IMN-BERT model: 64.23,
Laptop 2014 (F1) evaluation for IMN-BERT model: 61.73,
Restaurant 2014 (F1) evaluation for IMN-BERT model: 70.72,
Restaurant 2015 (F1) evaluation for IMN-BERT model: 60.22
LM-CPPF RoBERTa-base                                                                                  Accuracy evaluation for LM-CPPF RoBERTa-base model: 14.1%,
Accuracy evaluation for LM-CPPF RoBERTa-base model: 54.9,
Accuracy evaluation for LM-CPPF RoBERTa-base model: 68.4,
Accuracy evaluation for LM-CPPF RoBERTa-base model: 70.2%,
Accuracy evaluation for LM-CPPF RoBERTa-base model: 93.2,
Accuracy evaluation for LM-CPPF RoBERTa-base model: 93.3
RoBERTa (LARGE)                                                                                  # Correct Groups evaluation for RoBERTa (LARGE) model: 29 ± 3,
# Solved Walls evaluation for RoBERTa (LARGE) model: 0 ± 0,
Adjusted Mutual Information (AMI) evaluation for RoBERTa (LARGE) model: 9.4 ± .4,
Adjusted Rand Index (ARI) evaluation for RoBERTa (LARGE) model:  8.4 ± .3,
Fowlkes Mallows Score (FMS) evaluation for RoBERTa (LARGE) model:  26.7 ± .2,
Wasserstein Distance (WD) evaluation for RoBERTa (LARGE) model: 88.4 ± .4
LSTM + BERT (concat)                                                                                  % Test Accuracy evaluation for LSTM + BERT (concat) model: 75.83
DV-ngrams-cosine with NB sub-sampling + RoBERTa.base                                                                                  Accuracy evaluation for DV-ngrams-cosine with NB sub-sampling + RoBERTa.base model: 95.94
GAIN-BERT                                                                                  F1 evaluation for GAIN-BERT model: 61.24,
Ign F1 evaluation for GAIN-BERT model: 59.00
Transformer with RoBERTa                                                                                  Accuracy evaluation for Transformer with RoBERTa model: 38.9,
Execution Accuracy evaluation for Transformer with RoBERTa model: 38.9
BERT + SCH attm                                                                                  Val Accuracy evaluation for BERT + SCH attm model: 91.422
DistilBERT-uncased-PruneOFA (85% unstruct sparse, QAT Int8)                                                                                  EM evaluation for DistilBERT-uncased-PruneOFA (85% unstruct sparse, QAT Int8) model: 77.03,
F1 evaluation for DistilBERT-uncased-PruneOFA (85% unstruct sparse, QAT Int8) model: 85.13,
Matched evaluation for DistilBERT-uncased-PruneOFA (85% unstruct sparse, QAT Int8) model: 80.66,
Mismatched evaluation for DistilBERT-uncased-PruneOFA (85% unstruct sparse, QAT Int8) model: 81.14
ViT-B/16 + BERT base                                                                                  Text Score evaluation for ViT-B/16 + BERT base model: 31.2
RoBERTa-Large Ensemble                                                                                  Accuracy evaluation for RoBERTa-Large Ensemble model: 85.5
BERT + TAE                                                                                  Accuracy evaluation for BERT + TAE model: 81.03,
BLEU evaluation for BERT + TAE model: 33.41
BERT+MTL                                                                                  Weighted-F1 evaluation for BERT+MTL model: 35.92,
Weighted-F1 evaluation for BERT+MTL model: 61.90
Joint BERT + CRF                                                                                  Accuracy evaluation for Joint BERT + CRF model: 97.9,
F1 evaluation for Joint BERT + CRF model: 0.96
BERT-based tracker                                                                                  Joint evaluation for BERT-based tracker model: 90.5,
Request evaluation for BERT-based tracker model: 97.6
SpERT.PL (with overlap and BioBERT)                                                                                  NER Macro F1 evaluation for SpERT.PL (with overlap and BioBERT) model: 91.17,
RE+ Macro F1 evaluation for SpERT.PL (with overlap and BioBERT) model: 82.03
BERT - 3 Layers                                                                                  EM evaluation for BERT - 3 Layers model: 77.7,
F1 evaluation for BERT - 3 Layers model: 85.8
BERT-large                                                                                  Accuracy (easy) evaluation for BERT-large model: 72.0,
Accuracy (hard) evaluation for BERT-large model: 32.3,
Accuracy evaluation for BERT-large model: 49.8,
F1 evaluation for BERT-large model: 76.9,
HONEST evaluation for BERT-large model: 3.33,
Spearman's Rho evaluation for BERT-large model: 37.6
SenseBERT-large 340M                                                                                  Accuracy evaluation for SenseBERT-large 340M model: 72.1
RACL-BERT                                                                                  Avg F1 evaluation for RACL-BERT model: 68.29,
F1 evaluation for RACL-BERT model: 63.4,
Laptop 2014 (F1) evaluation for RACL-BERT model: 63.4,
Restaurant 2014 (F1) evaluation for RACL-BERT model: 75.42,
Restaurant 2015 (F1) evaluation for RACL-BERT model: 66.05
ContextBERT                                                                                  Macro F1 (w/o Neutral) evaluation for ContextBERT model: 54.30,
Macro F1 evaluation for ContextBERT model: 59.79,
Weighted F1 (w/o Neutral) evaluation for ContextBERT model: 79.67,
Weighted F1 evaluation for ContextBERT model: 88.33
MixCon3D-PointBERT                                                                                  Accuracy (%) evaluation for MixCon3D-PointBERT model: 86.8,
OBJ_ONLY Accuracy(%) evaluation for MixCon3D-PointBERT model: 58.6
DCoM-Single-DistilBERT                                                                                  F1 score evaluation for DCoM-Single-DistilBERT model: 0.925
Multimodal(ViT+BERT, Input: Image + Headline) - Dot                                                                                  Accuracy evaluation for Multimodal(ViT+BERT, Input: Image + Headline) - Dot model: 0.8202
RoBERTa Baseline                                                                                  F1-score evaluation for RoBERTa Baseline model: 49.1,
Macro-F1 evaluation for RoBERTa Baseline model: 10.4
ALBERT-base                                                                                  F1 evaluation for ALBERT-base model: 52.2,
Precision evaluation for ALBERT-base model: 44.8,
Recall evaluation for ALBERT-base model: 62.7
VisualBERT                                                                                  Accuracy (%) evaluation for VisualBERT model: 46.5,
Accuracy (%) evaluation for VisualBERT model: 46.6,
Accuracy (%) evaluation for VisualBERT model: 47.8,
Accuracy (%) evaluation for VisualBERT model: 48.3,
Accuracy (%) evaluation for VisualBERT model: 48.8,
Accuracy (%) evaluation for VisualBERT model: 49.3,
Accuracy (%) evaluation for VisualBERT model: 49.7,
Accuracy (%) evaluation for VisualBERT model: 50.0,
Accuracy (Dev) evaluation for VisualBERT model: 67.4%,
Accuracy (Test-P) evaluation for VisualBERT model: 67%,
Accuracy (Test-U) evaluation for VisualBERT model: 67.3%,
Accuracy evaluation for VisualBERT model: 52.2,
Accuracy evaluation for VisualBERT model: 52.4,
Accuracy evaluation for VisualBERT model: 53.95,
Accuracy evaluation for VisualBERT model: 66.7,
Accuracy evaluation for VisualBERT model: 70.8,
Accuracy evaluation for VisualBERT model: 71.6,
Accuracy evaluation for VisualBERT model: 73.2,
Average Accuracy evaluation for VisualBERT model: 48.8,
Gap (West) evaluation for VisualBERT model: -10.42,
R@1 evaluation for VisualBERT model: 70.4,
R@1 evaluation for VisualBERT model: 71.33,
R@10 evaluation for VisualBERT model: 86.31,
R@10 evaluation for VisualBERT model: 86.51,
R@5 evaluation for VisualBERT model: 84.49,
R@5 evaluation for VisualBERT model: 84.98,
accuracy evaluation for VisualBERT model: 55.2,
average pairwise accuracy evaluation for VisualBERT model: 46.4,
overall evaluation for VisualBERT model: 71,
pairwise accuracy evaluation for VisualBERT model: 39.7,
pairwise accuracy evaluation for VisualBERT model: 44.4,
pairwise accuracy evaluation for VisualBERT model: 45.7,
pairwise accuracy evaluation for VisualBERT model: 47.6,
pairwise accuracy evaluation for VisualBERT model: 48.2,
pairwise accuracy evaluation for VisualBERT model: 48.5,
pairwise accuracy evaluation for VisualBERT model: 49.2,
pairwise accuracy evaluation for VisualBERT model: 49.5,
pairwise accuracy evaluation for VisualBERT model: 50.0
ECPE-MLL-bert                                                                                  F1 evaluation for ECPE-MLL-bert model: 74.52
FT-Bangla BERT Large                                                                                  F1 evaluation for FT-Bangla BERT Large model: 79
SimCSE-RoBERTalarge                                                                                  Spearman Correlation evaluation for SimCSE-RoBERTalarge model: 0.8195,
Spearman Correlation evaluation for SimCSE-RoBERTalarge model: 0.8236,
Spearman Correlation evaluation for SimCSE-RoBERTalarge model: 0.8393,
Spearman Correlation evaluation for SimCSE-RoBERTalarge model: 0.8666,
Spearman Correlation evaluation for SimCSE-RoBERTalarge model: 0.867
ruBERT                                                                                  Accuracy evaluation for ruBERT model: 74.3,
MCC evaluation for ruBERT model: 0.42
TANDA-RoBERTa (ASNQ, TREC-QA)                                                                                  MAP evaluation for TANDA-RoBERTa (ASNQ, TREC-QA) model: 0.943,
MRR evaluation for TANDA-RoBERTa (ASNQ, TREC-QA) model: 0.974
FinQANet (RoBERTa-large)                                                                                  Execution Accuracy evaluation for FinQANet (RoBERTa-large) model: 65.05,
Execution Accuracy evaluation for FinQANet (RoBERTa-large) model: 68.9,
Execution Accuracy evaluation for FinQANet (RoBERTa-large) model: 68.90,
Program Accuracy evaluation for FinQANet (RoBERTa-large) model: 63.52,
Program Accuracy evaluation for FinQANet (RoBERTa-large) model: 68.24
RoBERTa-base 125M                                                                                  Accuracy evaluation for RoBERTa-base 125M model: 56.3,
Accuracy evaluation for RoBERTa-base 125M model: 63
Two Branch Network (Text - Bert + Image - Nts-Net)                                                                                  Accuracy evaluation for Two Branch Network (Text - Bert + Image - Nts-Net) model: 96.81
ALBERT-base 11M                                                                                  Accuracy evaluation for ALBERT-base 11M model: 52.8,
Accuracy evaluation for ALBERT-base 11M model: 55.4
BERT-QA with Hard EM objective                                                                                  Rouge-L evaluation for BERT-QA with Hard EM objective model: 58.8
BERT large finetune UDA                                                                                  Accuracy evaluation for BERT large finetune UDA model: 62.88,
Accuracy evaluation for BERT large finetune UDA model: 95.8,
Accuracy evaluation for BERT large finetune UDA model: 96.5,
Error evaluation for BERT large finetune UDA model: 2.05,
Error evaluation for BERT large finetune UDA model: 32.08
CGM2IR-SciBERTbase                                                                                  F1 evaluation for CGM2IR-SciBERTbase model: 73.8,
F1 evaluation for CGM2IR-SciBERTbase model: 84.7
ZEN (Init with Chinese BERT)                                                                                  F1 evaluation for ZEN (Init with Chinese BERT) model: 95.25,
F1 evaluation for ZEN (Init with Chinese BERT) model: 98.35
RoBERTa-RF-T1 hybrid                                                                                  Accuracy (5-fold) evaluation for RoBERTa-RF-T1 hybrid model: 0.990
Character-BERT+RS                                                                                   Macro-F1 evaluation for Character-BERT+RS model: 65.90,
Accuracy evaluation for Character-BERT+RS model: 83.73,
Macro-F1 evaluation for Character-BERT+RS model: 65.9
RoBERTa-wwm-ext-base                                                                                  Accuracy evaluation for RoBERTa-wwm-ext-base model: 85.5
BERT-PCL                                                                                  F1-score evaluation for BERT-PCL model: 63.69,
Macro-F1 evaluation for BERT-PCL model: 43.28
KELM (finetuning RoBERTa-large based single model)                                                                                  EM evaluation for KELM (finetuning RoBERTa-large based single model) model: 89.1,
F1 evaluation for KELM (finetuning RoBERTa-large based single model) model: 89.6
Sw-BERT + H0M                                                                                  Accuracy evaluation for Sw-BERT + H0M model: 76.9,
MCC evaluation for Sw-BERT + H0M model: 0.542
SchizzoBioBERT                                                                                  Average F1 evaluation for SchizzoBioBERT model: 0.885,
Averaged Precision evaluation for SchizzoBioBERT model: 0.903
OutEffHop-Bert_base                                                                                  Perplexity evaluation for OutEffHop-Bert_base model: 6.209,
Perplexity evaluation for OutEffHop-Bert_base model: 6.295
ABSA-DeBERTa                                                                                  Laptop (Acc) evaluation for ABSA-DeBERTa model: 82,76,
Mean Acc (Restaurant + Laptop) evaluation for ABSA-DeBERTa model: 86,11,
Restaurant (Acc) evaluation for ABSA-DeBERTa model: 89,46
Unsupervised Bert                                                                                  Task 1 Accuracy: all evaluation for Unsupervised Bert model: 54.4,
Task 1 Accuracy: domain specific evaluation for Unsupervised Bert model: 60.6,
Task 1 Accuracy: general purpose evaluation for Unsupervised Bert model: 49.2,
Task 2 Accuracy: all evaluation for Unsupervised Bert model: 62.8,
Task 2 Accuracy: domain specific evaluation for Unsupervised Bert model: 69.1,
Task 2 Accuracy: general purpose evaluation for Unsupervised Bert model: 57.6,
Task 3 Accuracy: all evaluation for Unsupervised Bert model: 60.5,
Task 3 Accuracy: domain specific evaluation for Unsupervised Bert model: 67.9,
Task 3 Accuracy: general purpose evaluation for Unsupervised Bert model: 54.4
DistilBERT-uncased-PruneOFA (85% unstruct sparse)                                                                                  EM evaluation for DistilBERT-uncased-PruneOFA (85% unstruct sparse) model: 78.1,
F1 evaluation for DistilBERT-uncased-PruneOFA (85% unstruct sparse) model: 85.82,
Matched evaluation for DistilBERT-uncased-PruneOFA (85% unstruct sparse) model: 81.35,
Mismatched evaluation for DistilBERT-uncased-PruneOFA (85% unstruct sparse) model: 82.03
DistilBERT (BASE)                                                                                  # Correct Groups evaluation for DistilBERT (BASE) model: 49 ± 4,
# Solved Walls evaluation for DistilBERT (BASE) model: 0 ± 0,
Adjusted Mutual Information (AMI) evaluation for DistilBERT (BASE) model:  14.0 ± .3,
Adjusted Rand Index (ARI) evaluation for DistilBERT (BASE) model: 11.3 ± .3,
Fowlkes Mallows Score (FMS) evaluation for DistilBERT (BASE) model: 29.1 ± .2,
Wasserstein Distance (WD) evaluation for DistilBERT (BASE) model: 86.7 ± .6
Graph2Tree with RoBERTa                                                                                  Accuracy (%) evaluation for Graph2Tree with RoBERTa model: 88.7,
Accuracy evaluation for Graph2Tree with RoBERTa model: 43.8,
Execution Accuracy evaluation for Graph2Tree with RoBERTa model: 43.8,
Execution Accuracy evaluation for Graph2Tree with RoBERTa model: 82.2
BERT-large 345M (0-shot)                                                                                  Accuracy evaluation for BERT-large 345M (0-shot) model: 51.9
Graph-Bert                                                                                  Accuracy evaluation for Graph-Bert model: 71.2%,
Accuracy evaluation for Graph-Bert model: 79.3%,
Accuracy evaluation for Graph-Bert model: 84.3%
Retro-Reader on ALBERT (ensemble)                                                                                  EM evaluation for Retro-Reader on ALBERT (ensemble) model: 90.115,
F1 evaluation for Retro-Reader on ALBERT (ensemble) model: 92.580
BERT-Large 32k batch size with AdamW                                                                                  F1 evaluation for BERT-Large 32k batch size with AdamW model: 91.58
BERT + SVM (with Handcrafted Features)                                                                                  Accuracy (5-fold) evaluation for BERT + SVM (with Handcrafted Features) model: 0.838
ESIM + BERT (FarsTail, MultiNLI)                                                                                  % Test Accuracy evaluation for ESIM + BERT (FarsTail, MultiNLI) model: 74.62
FinBERT                                                                                  Accuracy evaluation for FinBERT model: 86,
F1 score evaluation for FinBERT model: 84,
MSE evaluation for FinBERT model: 0.07,
R^2 evaluation for FinBERT model: 0.55
ExDeBERTa 567M                                                                                  Accuracy evaluation for ExDeBERTa 567M model: 79.6,
Accuracy evaluation for ExDeBERTa 567M model: 83.6,
Accuracy evaluation for ExDeBERTa 567M model: 85.5,
Accuracy evaluation for ExDeBERTa 567M model: 87
TagBERT                                                                                  F1-score evaluation for TagBERT model: 46
E2GRE-RoBERTa-large                                                                                  F1 evaluation for E2GRE-RoBERTa-large model: 62.50,
Ign F1 evaluation for E2GRE-RoBERTa-large model: 60.30
BERT-FP                                                                                  MAP evaluation for BERT-FP model: 0.644,
MAP evaluation for BERT-FP model: 0.702,
MRR evaluation for BERT-FP model: 0.680,
MRR evaluation for BERT-FP model: 0.712,
NDCG@3 evaluation for BERT-FP model: 0.609,
NDCG@5 evaluation for BERT-FP model: 0.709,
P@1 evaluation for BERT-FP model: 0.512,
P@1 evaluation for BERT-FP model: 0.543,
R10@1 evaluation for BERT-FP model: 0.324,
R10@1 evaluation for BERT-FP model: 0.488,
R10@1 evaluation for BERT-FP model: 0.870,
R10@1 evaluation for BERT-FP model: 0.911,
R10@2 evaluation for BERT-FP model: 0.542,
R10@2 evaluation for BERT-FP model: 0.708,
R10@2 evaluation for BERT-FP model: 0.956,
R10@2 evaluation for BERT-FP model: 0.962,
R10@5 evaluation for BERT-FP model: 0.870,
R10@5 evaluation for BERT-FP model: 0.927,
R10@5 evaluation for BERT-FP model: 0.993,
R10@5 evaluation for BERT-FP model: 0.994,
Recall 10@1 evaluation for BERT-FP model: 0.259
Trans-Encoder-BERT-base-bi (unsup.)                                                                                  Spearman Correlation evaluation for Trans-Encoder-BERT-base-bi (unsup.) model: 0.7276,
Spearman Correlation evaluation for Trans-Encoder-BERT-base-bi (unsup.) model: 0.7509,
Spearman Correlation evaluation for Trans-Encoder-BERT-base-bi (unsup.) model: 0.779,
Spearman Correlation evaluation for Trans-Encoder-BERT-base-bi (unsup.) model: 0.8305,
Spearman Correlation evaluation for Trans-Encoder-BERT-base-bi (unsup.) model: 0.839,
Spearman Correlation evaluation for Trans-Encoder-BERT-base-bi (unsup.) model: 0.8508,
Spearman Correlation evaluation for Trans-Encoder-BERT-base-bi (unsup.) model: 0.851
RoBERTa + Mutation Data Augmentation                                                                                  F1-Score evaluation for RoBERTa + Mutation Data Augmentation model: 0.414
SimCSE-RoBERTa-large                                                                                  Spearman Correlation evaluation for SimCSE-RoBERTa-large model: 0.7746,
Spearman Correlation evaluation for SimCSE-RoBERTa-large model: 0.8727
Contextual DeBERTa-V3-Large + SSP                                                                                  MAP evaluation for Contextual DeBERTa-V3-Large + SSP model: 0.919,
MRR evaluation for Contextual DeBERTa-V3-Large + SSP model: 0.945
MatchSum (RoBERTa-base)                                                                                  ROUGE-1 evaluation for MatchSum (RoBERTa-base) model: 44.41,
ROUGE-2 evaluation for MatchSum (RoBERTa-base) model: 20.86,
ROUGE-L evaluation for MatchSum (RoBERTa-base) model: 40.55
RpBERT                                                                                  F1 evaluation for RpBERT model: 74.90,
F1 evaluation for RpBERT model: 87.80
SemBERT (ensemble)                                                                                  EM evaluation for SemBERT (ensemble) model: 86.166,
F1 evaluation for SemBERT (ensemble) model: 88.886
MyBert                                                                                  BLEX evaluation for MyBert model: 77.21
DESC+MOL+SciBERT                                                                                  F1 evaluation for DESC+MOL+SciBERT model: 0.8408,
Micro F1 evaluation for DESC+MOL+SciBERT model: 84.08
ReasonBERTR                                                                                  F1 Score evaluation for ReasonBERTR model: 41.3,
F1 evaluation for ReasonBERTR model: 45.5,
Joint F1 evaluation for ReasonBERTR model: 34.1
SenseBERT-base 110M                                                                                  Accuracy evaluation for SenseBERT-base 110M model: 67.5%,
Accuracy evaluation for SenseBERT-base 110M model: 70.3,
Accuracy evaluation for SenseBERT-base 110M model: 90.6%
BERT (ensemble)                                                                                  EM evaluation for BERT (ensemble) model: 87.433,
F1 evaluation for BERT (ensemble) model: 93.160
KELM (finetuning BERT-large based single model)                                                                                  Accuracy evaluation for KELM (finetuning BERT-large based single model) model: 78.0,
EM evaluation for KELM (finetuning BERT-large based single model) model: 27.2,
EM evaluation for KELM (finetuning BERT-large based single model) model: 76.2,
F1 evaluation for KELM (finetuning BERT-large based single model) model: 70.8,
F1 evaluation for KELM (finetuning BERT-large based single model) model: 76.7
ALBERTv2                                                                                  Accuracy evaluation for ALBERTv2 model: 86.02,
Accuracy evaluation for ALBERTv2 model: 97.62
LSTM + BERT-Base                                                                                  Accuracy evaluation for LSTM + BERT-Base model: 36.2
MotionBert (finetune)                                                                                  Accuracy (CS) evaluation for MotionBert (finetune) model: 93.0,
Accuracy (CV) evaluation for MotionBert (finetune) model: 97.2
MA-BERT                                                                                  IMDB (Acc) evaluation for MA-BERT model: 57.3,
Yelp 2013 (Acc) evaluation for MA-BERT model: 70.3,
Yelp 2014 (Acc) evaluation for MA-BERT model: 71.4
BERT-MRP                                                                                  AUROC evaluation for BERT-MRP model: 0.862,
Accuracy evaluation for BERT-MRP model: 0.704,
Macro F1 evaluation for BERT-MRP model: 0.699
SMART-BERT                                                                                  Accuracy evaluation for SMART-BERT model: 71.2%,
Dev Accuracy evaluation for SMART-BERT model: 71.2,
Dev Accuracy evaluation for SMART-BERT model: 87.7,
Dev Accuracy evaluation for SMART-BERT model: 91.5,
Dev Accuracy evaluation for SMART-BERT model: 91.7,
Dev Accuracy evaluation for SMART-BERT model: 93.0,
Dev F1 evaluation for SMART-BERT model: 88.5,
Dev F1 evaluation for SMART-BERT model: 91.3,
Dev Matched evaluation for SMART-BERT model: 85.6,
Dev Mismatched evaluation for SMART-BERT model: 86.0,
Dev Pearson Correlation evaluation for SMART-BERT model: 90.0,
Dev Spearman Correlation evaluation for SMART-BERT model: 89.4
DPL-BERT                                                                                  Laptop (Acc) evaluation for DPL-BERT model: 81.96,
Mean Acc (Restaurant + Laptop) evaluation for DPL-BERT model: 85.75,
Restaurant (Acc) evaluation for DPL-BERT model: 89.54
Legal-BERT                                                                                  CaseHOLD evaluation for Legal-BERT model: 75.1,
ECtHR Task A evaluation for Legal-BERT model: 71.2 / 64.6,
ECtHR Task B evaluation for Legal-BERT model: 88.0 / 77.2,
EUR-LEX evaluation for Legal-BERT model: 72.2 / 56.2,
F1(10-fold) evaluation for Legal-BERT model: 75.0,
F1(10-fold) evaluation for Legal-BERT model: 96.3,
LEDGAR evaluation for Legal-BERT model: 88.1 / 82.7,
Macro F1 (10-fold) evaluation for Legal-BERT model: 68.0,
SCOTUS evaluation for Legal-BERT model: 76.2 / 65.8,
UNFAIR-ToS evaluation for Legal-BERT model: 88.6 / 82.3
RoBERTa-large 355M                                                                                  Accuracy evaluation for RoBERTa-large 355M model: 54.9
GBERT/GELECTRA Ensemble                                                                                  F1 evaluation for GBERT/GELECTRA Ensemble model: 71.8
ALBERT base                                                                                  EM evaluation for ALBERT base model: 76.1,
F1 evaluation for ALBERT base model: 79.1
CamemBERT (subword masking)                                                                                  F1 evaluation for CamemBERT (subword masking) model: 87.93,
Precision evaluation for CamemBERT (subword masking) model: 88.35,
Recall evaluation for CamemBERT (subword masking) model: 87.46
BERT-HateXplain [Attn]                                                                                  AUROC evaluation for BERT-HateXplain [Attn] model: 0.851,
Accuracy evaluation for BERT-HateXplain [Attn] model: 0.698,
Macro F1 evaluation for BERT-HateXplain [Attn] model: 0.687
R-BERT                                                                                  F1 evaluation for R-BERT model: 69.4,
F1 evaluation for R-BERT model: 89.25
InfoBERT (single model)                                                                                  Accuracy evaluation for InfoBERT (single model) model: 0.4603
C-BERT (ESGNN + BERT)                                                                                  Accuracy evaluation for C-BERT (ESGNN + BERT) model: 98.28
BERTserini                                                                                  EM evaluation for BERTserini model: 38.6,
EM evaluation for BERTserini model: 50.2
RoBERTa Liu et al. (2019)                                                                                  Accuracy evaluation for RoBERTa Liu et al. (2019) model: 72.1
PFN (ALBERT XXL, average aggregation)                                                                                  NER Macro F1 evaluation for PFN (ALBERT XXL, average aggregation) model: 91.5,
RE+ Macro F1 evaluation for PFN (ALBERT XXL, average aggregation) model: 83.9
BioBERT (pre-trained on PubMed abstracts + PMC, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus')                                                                                  F1 evaluation for BioBERT (pre-trained on PubMed abstracts + PMC, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus') model: 93.38,
Precision evaluation for BioBERT (pre-trained on PubMed abstracts + PMC, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus') model: 92.98,
Recall evaluation for BioBERT (pre-trained on PubMed abstracts + PMC, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus') model: 93.85
BERT2RND                                                                                  BLEU-4 evaluation for BERT2RND model: 22.47
D-REX_RoBERTa                                                                                  F1 (v2) evaluation for D-REX_RoBERTa model: 67.2
VLN-BERT                                                                                  spl evaluation for VLN-BERT model: 0.57
RoBERTa-large 355M (fine-tuned)                                                                                  Accuracy evaluation for RoBERTa-large 355M (fine-tuned) model: 77.1
DiffCSE-BERT-base                                                                                  Spearman Correlation evaluation for DiffCSE-BERT-base model: 0.7228,
Spearman Correlation evaluation for DiffCSE-BERT-base model: 0.7647,
Spearman Correlation evaluation for DiffCSE-BERT-base model: 0.8054,
Spearman Correlation evaluation for DiffCSE-BERT-base model: 0.8390,
Spearman Correlation evaluation for DiffCSE-BERT-base model: 0.8443
CodeBERT (MLM)                                                                                  Smoothed BLEU-4 evaluation for CodeBERT (MLM) model: 13.59,
Smoothed BLEU-4 evaluation for CodeBERT (MLM) model: 15.48,
Smoothed BLEU-4 evaluation for CodeBERT (MLM) model: 15.55,
Smoothed BLEU-4 evaluation for CodeBERT (MLM) model: 21,
Smoothed BLEU-4 evaluation for CodeBERT (MLM) model: 26.79,
Smoothed BLEU-4 evaluation for CodeBERT (MLM) model: 7.95,
Smoothed BLEU-4 evaluation for CodeBERT (MLM) model: 8.51
BertSum                                                                                  Content F1 evaluation for BertSum model: 29.8,
Content F1 evaluation for BertSum model: 36.4,
ROUGE-1 evaluation for BertSum model: 35.91,
ROUGE-1 evaluation for BertSum model: 48.26,
ROUGE-2 evaluation for BertSum model: 13.9,
ROUGE-L evaluation for BertSum model: 34.82,
ROUGE-L evaluation for BertSum model: 44.02
Wiki+RoBERTa                                                                                  F1 evaluation for Wiki+RoBERTa model: 61.0,
F1@10 evaluation for Wiki+RoBERTa model: 19.2,
P@50K evaluation for Wiki+RoBERTa model: 98.5,
P@5K evaluation for Wiki+RoBERTa model: 100.0,
Precision evaluation for Wiki+RoBERTa model: 58.1,
Recall evaluation for Wiki+RoBERTa model: 64.2,
Recall evaluation for Wiki+RoBERTa model: 73.0
SA-BERT                                                                                  MAP evaluation for SA-BERT model: 0.619,
MRR evaluation for SA-BERT model: 0.659,
P@1 evaluation for SA-BERT model: 0.496,
R10@1 evaluation for SA-BERT model: 0.313,
R10@1 evaluation for SA-BERT model: 0.704,
R10@1 evaluation for SA-BERT model: 0.855,
R10@2 evaluation for SA-BERT model: 0.481,
R10@2 evaluation for SA-BERT model: 0.879,
R10@2 evaluation for SA-BERT model: 0.928,
R10@5 evaluation for SA-BERT model: 0.847,
R10@5 evaluation for SA-BERT model: 0.983,
R10@5 evaluation for SA-BERT model: 0.985,
R2@1 evaluation for SA-BERT model: 0.965
LSTM-CRF+ELMo+BERT+Flair                                                                                  F1 evaluation for LSTM-CRF+ELMo+BERT+Flair model: 93.38
Multimodal(ViT+BERT, Input: Image + Body)                                                                                  Accuracy evaluation for Multimodal(ViT+BERT, Input: Image + Body) model: 0.9249
TASE-BERT                                                                                  F1 evaluation for TASE-BERT model: 80.7
Ru-RoBERTa+TDA                                                                                  Accuracy evaluation for Ru-RoBERTa+TDA model: 85.7,
MCC evaluation for Ru-RoBERTa+TDA model: 0.594
Partially Fine-tuned HuBERT Large                                                                                  WA evaluation for Partially Fine-tuned HuBERT Large model: 0.796
BERT_LARGE_SQUAD_DOCVQA_FINETUNED_Baseline                                                                                  ANLS evaluation for BERT_LARGE_SQUAD_DOCVQA_FINETUNED_Baseline model: 0.665,
Accuracy evaluation for BERT_LARGE_SQUAD_DOCVQA_FINETUNED_Baseline model: 55.77
SRoBERTa-NLI-base                                                                                  Spearman Correlation evaluation for SRoBERTa-NLI-base model: 0.7446,
Spearman Correlation evaluation for SRoBERTa-NLI-base model: 0.7777
SEG-BERT                                                                                  Accuracy evaluation for SEG-BERT model: 53.4%,
Accuracy evaluation for SEG-BERT model: 68.86%,
Accuracy evaluation for SEG-BERT model: 77.09%,
Accuracy evaluation for SEG-BERT model: 77.2%,
Accuracy evaluation for SEG-BERT model: 78.42%,
Accuracy evaluation for SEG-BERT model: 90.85%
C-LMKE(bert-base)                                                                                  Hits@1 evaluation for C-LMKE(bert-base) model: 0.523,
Hits@10 evaluation for C-LMKE(bert-base) model: 0.789,
Hits@3 evaluation for C-LMKE(bert-base) model: 0.671,
MR evaluation for C-LMKE(bert-base) model: 79,
MRR evaluation for C-LMKE(bert-base) model: 0.619
NegBERT                                                                                  F1 evaluation for NegBERT model: 90.95,
F1 evaluation for NegBERT model: 91.24,
F1 evaluation for NegBERT model: 92.36,
F1 evaluation for NegBERT model: 92.94,
F1 evaluation for NegBERT model: 95.68
BERT-Winogrande 345M (fine-tuned)                                                                                  Accuracy evaluation for BERT-Winogrande 345M (fine-tuned) model: 64.9
TUCORE-GCN_RoBERTa                                                                                  F1 (v2) evaluation for TUCORE-GCN_RoBERTa model: 73.1,
F1c (v2) evaluation for TUCORE-GCN_RoBERTa model: 65.9,
Micro-F1 evaluation for TUCORE-GCN_RoBERTa model: 61.91,
Weighted-F1 evaluation for TUCORE-GCN_RoBERTa model: 39.24,
Weighted-F1 evaluation for TUCORE-GCN_RoBERTa model: 65.36
DRE-MIR-BERTbase                                                                                  F1 evaluation for DRE-MIR-BERTbase model: 63.15,
Ign F1 evaluation for DRE-MIR-BERTbase model: 61.03
ProtBert-BFD                                                                                  Q3 evaluation for ProtBert-BFD model: 0.76,
Q3 evaluation for ProtBert-BFD model: 0.83,
Q3 evaluation for ProtBert-BFD model: 0.84,
Q8 evaluation for ProtBert-BFD model: 0.65,
Q8 evaluation for ProtBert-BFD model: 0.7,
Q8 evaluation for ProtBert-BFD model: 0.73
BERT-Entity-Sim (local & global) AIDA-B                                                                                  Micro-F1 evaluation for BERT-Entity-Sim (local & global) AIDA-B model: 93.54
FlauBERT (large)                                                                                  Accuracy evaluation for FlauBERT (large) model: 83.4
U-MEM* + SpanBERT-large                                                                                  Avg F1 evaluation for U-MEM* + SpanBERT-large model: 79.6
FTTransformer + RoBERTa fintune                                                                                  AUROC evaluation for FTTransformer + RoBERTa fintune model: 0.96
ULIP + PointBERT                                                                                  Accuracy (%) evaluation for ULIP + PointBERT model: 60.4,
Overall Accuracy evaluation for ULIP + PointBERT model: 86.4,
Overall Accuracy evaluation for ULIP + PointBERT model: 94.1
BERT_CSlarge                                                                                  Accuracy evaluation for BERT_CSlarge model: 62.2
SSP-BERT + SCIJE                                                                                  Link & Rel F1 evaluation for SSP-BERT + SCIJE model: 57.4,
Link & Rel F1 evaluation for SSP-BERT + SCIJE model: 59.4,
Link F1 evaluation for SSP-BERT + SCIJE model: 73.0,
Link F1 evaluation for SSP-BERT + SCIJE model: 83.7
BERT Large                                                                                  Accuracy evaluation for BERT Large model: 55.5,
Accuracy evaluation for BERT Large model: 69.6,
Accuracy evaluation for BERT Large model: 93.1
CorefBERT                                                                                  F1 evaluation for CorefBERT model: 89.2
BERT (Input: Caption)                                                                                  Accuracy evaluation for BERT (Input: Caption) model: 0.7792
KG-BERT                                                                                  Hits@10 evaluation for KG-BERT model: 0.42,
Hits@10 evaluation for KG-BERT model: 0.524,
Hits@10 evaluation for KG-BERT model: 0.990,
MR evaluation for KG-BERT model: 1.47,
MR evaluation for KG-BERT model: 153,
MR evaluation for KG-BERT model: 97
RANK-CP-bert                                                                                  F1 evaluation for RANK-CP-bert model: 73.60
Trompt + RoBERTa embedding                                                                                  AUROC evaluation for Trompt + RoBERTa embedding model: 0.885
TANDA-RoBERTa (ASNQ, WikiQA)                                                                                  MAP evaluation for TANDA-RoBERTa (ASNQ, WikiQA) model: 0.920,
MRR evaluation for TANDA-RoBERTa (ASNQ, WikiQA) model: 0.933
BERTS                                                                                  F1 (v1) evaluation for BERTS model: 61.2,
F1c (v1) evaluation for BERTS model: 55.4
ruRoBERTa                                                                                  Accuracy evaluation for ruRoBERTa model: 79.34,
MCC evaluation for ruRoBERTa model: 0.53
CamemBERT-Base                                                                                  EM evaluation for CamemBERT-Base model: 78.4,
F1 evaluation for CamemBERT-Base model: 88.4
ViLBERT                                                                                  Accuracy (%) evaluation for ViLBERT model: 2.4,
Accuracy (%) evaluation for ViLBERT model: 49.9,
Accuracy (%) evaluation for ViLBERT model: 50.0,
Accuracy (%) evaluation for ViLBERT model: 50.3,
Accuracy (%) evaluation for ViLBERT model: 50.4,
Accuracy (%) evaluation for ViLBERT model: 50.6,
Accuracy (%) evaluation for ViLBERT model: 50.7,
Accuracy (%) evaluation for ViLBERT model: 51.8,
Accuracy (%) evaluation for ViLBERT model: 52.6,
Accuracy (%) evaluation for ViLBERT model: 55.9,
Accuracy evaluation for ViLBERT model: 59.99,
Accuracy evaluation for ViLBERT model: 70.55,
Average Accuracy evaluation for ViLBERT model: 51.3,
DA VQA Score evaluation for ViLBERT model: 25.9,
Gap (West) evaluation for ViLBERT model: -7.28,
MC Accuracy evaluation for ViLBERT model: 41.5,
MRR evaluation for ViLBERT model: 0.287,
Reasoning (Alg.) evaluation for ViLBERT model: 50.62,
Reasoning (Com.) evaluation for ViLBERT model: 75.60,
Reasoning (Cou.) evaluation for ViLBERT model: 71.05,
Reasoning (Est.) evaluation for ViLBERT model: 99.22,
Reasoning (Fra.) evaluation for ViLBERT model: 74.09,
Reasoning (Geo.) evaluation for ViLBERT model: 80.05,
Reasoning (Mea.) evaluation for ViLBERT model: 99.07,
Reasoning (Pat.) evaluation for ViLBERT model: 62.78,
Reasoning (Pro.) evaluation for ViLBERT model: 70.94,
Reasoning (Sce.) evaluation for ViLBERT model: 58.52,
Reasoning (Sen.) evaluation for ViLBERT model: 81.78,
Reasoning (Spa.) evaluation for ViLBERT model: 49.46,
Reasoning (Tim.) evaluation for ViLBERT model: 66.72,
Sub-tasks (Blank) evaluation for ViLBERT model: 77.08,
Sub-tasks (Img.) evaluation for ViLBERT model: 76.66,
Sub-tasks (Txt.) evaluation for ViLBERT model: 70.47,
average pairwise accuracy evaluation for ViLBERT model: 63.7,
pairwise accuracy evaluation for ViLBERT model: 47.2,
pairwise accuracy evaluation for ViLBERT model: 48.1,
pairwise accuracy evaluation for ViLBERT model: 57.2,
pairwise accuracy evaluation for ViLBERT model: 58.6,
pairwise accuracy evaluation for ViLBERT model: 61.2,
pairwise accuracy evaluation for ViLBERT model: 62.9,
pairwise accuracy evaluation for ViLBERT model: 66.5,
pairwise accuracy evaluation for ViLBERT model: 68.3,
pairwise accuracy evaluation for ViLBERT model: 70.7,
pairwise accuracy evaluation for ViLBERT model: 73.7,
pairwise accuracy evaluation for ViLBERT model: 86.9
RoBERTa-Base Joint MSPP Flexible                                                                                  Accuracy evaluation for RoBERTa-Base Joint MSPP Flexible model: 75.36
Text-only BERT                                                                                  Accuracy evaluation for Text-only BERT model: 35.33
BioBERT-MIMIC                                                                                  Accuracy evaluation for BioBERT-MIMIC model: 83.45
Bottom-up (DeBERTa)                                                                                  Standard Parseval (Full) evaluation for Bottom-up (DeBERTa) model: 44.4,
Standard Parseval (Full) evaluation for Bottom-up (DeBERTa) model: 55.4,
Standard Parseval (Nuclearity) evaluation for Bottom-up (DeBERTa) model: 60.0,
Standard Parseval (Nuclearity) evaluation for Bottom-up (DeBERTa) model: 68.0,
Standard Parseval (Relation) evaluation for Bottom-up (DeBERTa) model: 51.4,
Standard Parseval (Relation) evaluation for Bottom-up (DeBERTa) model: 57.3,
Standard Parseval (Span) evaluation for Bottom-up (DeBERTa) model: 77.8
Joint_RoBERTa                                                                                  F1 (v2) evaluation for Joint_RoBERTa model: 65.2
SpanBERT (single model)                                                                                  EM evaluation for SpanBERT (single model) model: 88.8,
F1 evaluation for SpanBERT (single model) model: 94.6,
Hardware Burden evaluation for SpanBERT (single model) model: 586G
BioBERT (large)                                                                                  Accuracy evaluation for BioBERT (large) model: 36.7
bert-large-uncased + APN                                                                                  BA evaluation for bert-large-uncased + APN model: 0.717,
DE evaluation for bert-large-uncased + APN model: 0.661,
PA evaluation for bert-large-uncased + APN model: 0.299
SpanBERT + Cluster Merging                                                                                  Avg F1 evaluation for SpanBERT + Cluster Merging model: 80.2
BERT pretrained on MIMIC-III                                                                                  Answer F1 evaluation for BERT pretrained on MIMIC-III model: 63.55
CamemBERT                                                                                  Accuracy evaluation for CamemBERT model: 81.2,
Exact Match Accuracy evaluation for CamemBERT model: 16.55,
Hamming Score evaluation for CamemBERT model: 36.24,
LAS evaluation for CamemBERT model: 81.37,
LAS evaluation for CamemBERT model: 92.47,
LAS evaluation for CamemBERT model: 92.9,
LAS evaluation for CamemBERT model: 94.39,
UAS evaluation for CamemBERT model: 86.05,
UAS evaluation for CamemBERT model: 94.82,
UAS evaluation for CamemBERT model: 95.21,
UAS evaluation for CamemBERT model: 95.56,
UPOS evaluation for CamemBERT model: 96.68,
UPOS evaluation for CamemBERT model: 97.63,
UPOS evaluation for CamemBERT model: 98.19,
UPOS evaluation for CamemBERT model: 99.21
CRF Parser + BERT                                                                                  F1 score evaluation for CRF Parser + BERT model: 91.55,
F1 score evaluation for CRF Parser + BERT model: 92.27,
F1 score evaluation for CRF Parser + BERT model: 95.69
BERT-Large-uncased-PruneOFA (90% unstruct sparse)                                                                                  EM evaluation for BERT-Large-uncased-PruneOFA (90% unstruct sparse) model: 83.35,
F1 evaluation for BERT-Large-uncased-PruneOFA (90% unstruct sparse) model: 90.2,
Matched evaluation for BERT-Large-uncased-PruneOFA (90% unstruct sparse) model: 83.74,
Mismatched evaluation for BERT-Large-uncased-PruneOFA (90% unstruct sparse) model: 84.2
PromptNER [RoBERTa-large]                                                                                  F1 evaluation for PromptNER [RoBERTa-large] model: 88.26,
F1 evaluation for PromptNER [RoBERTa-large] model: 88.72,
F1 evaluation for PromptNER [RoBERTa-large] model: 93.08
SSAN-RoBERTa-large                                                                                  F1 evaluation for SSAN-RoBERTa-large model: 61.42,
Ign F1 evaluation for SSAN-RoBERTa-large model: 59.47
Top-down (DeBERTa)                                                                                  Standard Parseval (Full) evaluation for Top-down (DeBERTa) model: 43.4,
Standard Parseval (Full) evaluation for Top-down (DeBERTa) model: 54.4,
Standard Parseval (Nuclearity) evaluation for Top-down (DeBERTa) model: 57.9,
Standard Parseval (Nuclearity) evaluation for Top-down (DeBERTa) model: 67.9,
Standard Parseval (Relation) evaluation for Top-down (DeBERTa) model: 50.0,
Standard Parseval (Relation) evaluation for Top-down (DeBERTa) model: 56.6,
Standard Parseval (Span) evaluation for Top-down (DeBERTa) model: 77.3,
Standard Parseval (Span) evaluation for Top-down (DeBERTa) model: 78.5
BERT-pair-NLI-B                                                                                  F1 score evaluation for BERT-pair-NLI-B model: 92.18,
Precision evaluation for BERT-pair-NLI-B model: 93.57,
Recall evaluation for BERT-pair-NLI-B model: 90.83
Triaffine + BioBERT                                                                                  F1 evaluation for Triaffine + BioBERT model: 81.23,
F1 evaluation for Triaffine + BioBERT model: 87.40
CalBERT                                                                                  Accuracy evaluation for CalBERT model: 79.4,
F1 evaluation for CalBERT model: 62,
Precision evaluation for CalBERT model: 61.8,
Recall evaluation for CalBERT model: 61.8
BERT (single model)                                                                                  Accuracy evaluation for BERT (single model) model: 0.3369,
EM evaluation for BERT (single model) model: 85.083,
F1 evaluation for BERT (single model) model: 91.835
BERT-large 340M (fine-tuned on WSCR)                                                                                  Accuracy evaluation for BERT-large 340M (fine-tuned on WSCR) model: 71.4,
Accuracy evaluation for BERT-large 340M (fine-tuned on WSCR) model: 71.9
Phraseformer(BERT, ExEm(w2v))                                                                                  F1 score evaluation for Phraseformer(BERT, ExEm(w2v)) model: 48.48,
F1 score evaluation for Phraseformer(BERT, ExEm(w2v)) model: 66.96,
F1 score evaluation for Phraseformer(BERT, ExEm(w2v)) model: 69.70
m-BERT augmented with Hindi QA                                                                                  EM(QE-PE) evaluation for m-BERT augmented with Hindi QA model: 64.29,
EM(QE-PH) evaluation for m-BERT augmented with Hindi QA model: 44.71,
EM(QH-PE) evaluation for m-BERT augmented with Hindi QA model: 41.01,
EM(QH-PH) evaluation for m-BERT augmented with Hindi QA model: 45.63,
F1 (QE-PE) evaluation for m-BERT augmented with Hindi QA model: 76.51,
F1 (QE-PH) evaluation for m-BERT augmented with Hindi QA model: 57.31,
F1(QH-PE) evaluation for m-BERT augmented with Hindi QA model: 51.04,
F1(QH-PH) evaluation for m-BERT augmented with Hindi QA model: 59.80
Early Fusion (Bert + InceptionV3)                                                                                  Accuracy (%) evaluation for Early Fusion (Bert + InceptionV3) model: 92.5
CM-BERT                                                                                  Accuracy evaluation for CM-BERT model: 84.5%,
F1 score evaluation for CM-BERT model: 84.5%
BERT+TDA                                                                                  Accuracy evaluation for BERT+TDA model: 88.2%,
MCC evaluation for BERT+TDA model: 0.726
EmoOne-RoBERTa                                                                                  Macro F1 evaluation for EmoOne-RoBERTa model: 55.84,
Micro-F1 evaluation for EmoOne-RoBERTa model: 61.67,
Weighted-F1 evaluation for EmoOne-RoBERTa model: 38,
Weighted-F1 evaluation for EmoOne-RoBERTa model: 66.49
SJRC (BERT-Large +SRL)                                                                                  % Test Accuracy evaluation for SJRC (BERT-Large +SRL) model: 91.3,
% Train Accuracy evaluation for SJRC (BERT-Large +SRL) model: 95.7,
Parameters evaluation for SJRC (BERT-Large +SRL) model: 308m
Bi-LSTM seq2seq: BERT + characters in 1 encoder                                                                                  F1 evaluation for Bi-LSTM seq2seq: BERT + characters in 1 encoder model: 88.3,
F1 evaluation for Bi-LSTM seq2seq: BERT + characters in 1 encoder model: 89.3
RoBERTaGen                                                                                  Accuracy (5-fold) evaluation for RoBERTaGen model: 76.6
RefVOS with BERT Pre-train                                                                                  Overall IoU evaluation for RefVOS with BERT Pre-train model: 54.17,
Overall IoU evaluation for RefVOS with BERT Pre-train model: 58.65,
Overall IoU evaluation for RefVOS with BERT Pre-train model: 63.19
RoBERTa-Large 355M (fine-tuned)                                                                                  Accuracy evaluation for RoBERTa-Large 355M (fine-tuned) model: 76.7
RoBERTa-ft 355M (fine-tuned)                                                                                  Accuracy evaluation for RoBERTa-ft 355M (fine-tuned) model: 86.4
MiniGPT-4-7B (BERTScore)                                                                                  Group Score evaluation for MiniGPT-4-7B (BERTScore) model: 2.75,
Image Score evaluation for MiniGPT-4-7B (BERTScore) model: 8.00,
Text Score evaluation for MiniGPT-4-7B (BERTScore) model: 14.00
FTTransformer + RoBERTa embedding                                                                                  AUROC evaluation for FTTransformer + RoBERTa embedding model: 0.936
RoBERTa-large-ST                                                                                  Macro F1 evaluation for RoBERTa-large-ST model: 80.7
HuBERT-B-LS960                                                                                  VoxCeleb (Dev) evaluation for HuBERT-B-LS960 model: 19.6,
VoxCeleb (Test) evaluation for HuBERT-B-LS960 model: 21.2,
VoxPopuli (Dev) evaluation for HuBERT-B-LS960 model: 18.6,
VoxPopuli (Test) evaluation for HuBERT-B-LS960 model: 19.1
spaCy-XLM-RoBERTa                                                                                  Labelled Attachment Score evaluation for spaCy-XLM-RoBERTa model: 79.39,
Unlabeled Attachment Score evaluation for spaCy-XLM-RoBERTa model: 83.82
BERT4Rec                                                                                  HR@10 (full corpus) evaluation for BERT4Rec model: 0.2816,
HR@10 (full corpus) evaluation for BERT4Rec model: 0.2843,
NDCG@10 (full corpus) evaluation for BERT4Rec model: 0.1537,
nDCG@10 (full corpus) evaluation for BERT4Rec model: 0.1703
AlephBERT-base                                                                                  F1 evaluation for AlephBERT-base model: 84.91
N-ary semi-markov + BERT                                                                                  F1 score evaluation for N-ary semi-markov + BERT model: 92.50
BERT-MultiNLI 340M (fine-tuned)                                                                                  Accuracy evaluation for BERT-MultiNLI 340M (fine-tuned) model: 80.4
RoBERTa-Large 355M                                                                                  Accuracy evaluation for RoBERTa-Large 355M model: 72.1,
Accuracy evaluation for RoBERTa-Large 355M model: 79.4,
Accuracy evaluation for RoBERTa-Large 355M model: 81.7
MAG-BERT (Text + Audio + Video)                                                                                  Accuracy (20 classes) evaluation for MAG-BERT (Text + Audio + Video) model: 72.65,
Accuracy (Binary) evaluation for MAG-BERT (Text + Audio + Video) model: 89.24
BERT + RegLER                                                                                  F1 evaluation for BERT + RegLER model: 58.9
NFC + BERT-large                                                                                  F1 score evaluation for NFC + BERT-large model: 95.92
Space-DistilBERT                                                                                  Accuracy (2 classes) evaluation for Space-DistilBERT model: 0.8322,
F1 Macro evaluation for Space-DistilBERT model: 0.8320
SemBERT (single model)                                                                                  EM evaluation for SemBERT (single model) model: 84.800,
F1 evaluation for SemBERT (single model) model: 87.864
PhoBERT                                                                                  F1 (%) evaluation for PhoBERT model: 94.5
DistilProtBert                                                                                  Q3 evaluation for DistilProtBert model: 0.72,
Q3 evaluation for DistilProtBert model: 0.79,
Q3 evaluation for DistilProtBert model: 0.81
Zero shot mBERT 3                                                                                  F1 evaluation for Zero shot mBERT 3 model: 72.44,
F1 evaluation for Zero shot mBERT 3 model: 76.53,
F1 evaluation for Zero shot mBERT 3 model: 83.35
Heinsen Routing + RoBERTa-large                                                                                  Accuracy (2 classes) evaluation for Heinsen Routing + RoBERTa-large model: 96.2,
Accuracy evaluation for Heinsen Routing + RoBERTa-large model: 96.0
BERT-Large-uncased-PruneOFA (90% unstruct sparse, QAT Int8)                                                                                  EM evaluation for BERT-Large-uncased-PruneOFA (90% unstruct sparse, QAT Int8) model: 83.22,
F1 evaluation for BERT-Large-uncased-PruneOFA (90% unstruct sparse, QAT Int8) model: 90.02,
Matched evaluation for BERT-Large-uncased-PruneOFA (90% unstruct sparse, QAT Int8) model: 83.47,
Mismatched evaluation for BERT-Large-uncased-PruneOFA (90% unstruct sparse, QAT Int8) model: 84.08
BERT+GloVe                                                                                  R@10 evaluation for BERT+GloVe model: 0.87,
R@15 evaluation for BERT+GloVe model: 0.92,
R@20 evaluation for BERT+GloVe model: 0.94,
R@5 evaluation for BERT+GloVe model: 0.76
BertForTokenClassification (Spark NLP)                                                                                  F1 evaluation for BertForTokenClassification (Spark NLP) model: 82.59,
F1 evaluation for BertForTokenClassification (Spark NLP) model: 87.83,
F1 evaluation for BertForTokenClassification (Spark NLP) model: 90.89,
F1 evaluation for BertForTokenClassification (Spark NLP) model: 91.65,
F1 evaluation for BertForTokenClassification (Spark NLP) model: 94.39
EditSQL + BERT                                                                                  interaction match accuracy evaluation for EditSQL + BERT model: 25.3,
question match accuracy evaluation for EditSQL + BERT model: 47.9
BERT-RP                                                                                  AUROC evaluation for BERT-RP model: 0.853,
Accuracy evaluation for BERT-RP model: 0.707,
Macro F1 evaluation for BERT-RP model: 0.693
SBERT-NLI-large                                                                                  Spearman Correlation evaluation for SBERT-NLI-large model: 0.7375,
Spearman Correlation evaluation for SBERT-NLI-large model: 0.7490000000000001,
Spearman Correlation evaluation for SBERT-NLI-large model: 0.7846,
Spearman Correlation evaluation for SBERT-NLI-large model: 0.79
ViLBERT 12-in-1                                                                                  Accuracy (%) evaluation for ViLBERT 12-in-1 model: 52.2,
Accuracy (%) evaluation for ViLBERT 12-in-1 model: 53.4,
Accuracy (%) evaluation for ViLBERT 12-in-1 model: 54.3,
Accuracy (%) evaluation for ViLBERT 12-in-1 model: 54.4,
Accuracy (%) evaluation for ViLBERT 12-in-1 model: 57.3,
Accuracy (%) evaluation for ViLBERT 12-in-1 model: 62.0,
Accuracy (%) evaluation for ViLBERT 12-in-1 model: 64.9,
Accuracy (%) evaluation for ViLBERT 12-in-1 model: 66.7,
Accuracy (%) evaluation for ViLBERT 12-in-1 model: 69.2,
Accuracy (%) evaluation for ViLBERT 12-in-1 model: 71.5,
Accuracy (%) evaluation for ViLBERT 12-in-1 model: 89.0,
Average Accuracy evaluation for ViLBERT 12-in-1 model: 63.2,
average pairwise accuracy evaluation for ViLBERT 12-in-1 model: 75.1,
pairwise accuracy evaluation for ViLBERT 12-in-1 model: 58.9,
pairwise accuracy evaluation for ViLBERT 12-in-1 model: 65.9,
pairwise accuracy evaluation for ViLBERT 12-in-1 model: 67.7,
pairwise accuracy evaluation for ViLBERT 12-in-1 model: 69.2,
pairwise accuracy evaluation for ViLBERT 12-in-1 model: 72.4,
pairwise accuracy evaluation for ViLBERT 12-in-1 model: 75.7,
pairwise accuracy evaluation for ViLBERT 12-in-1 model: 76.7,
pairwise accuracy evaluation for ViLBERT 12-in-1 model: 77.3,
pairwise accuracy evaluation for ViLBERT 12-in-1 model: 80.2,
pairwise accuracy evaluation for ViLBERT 12-in-1 model: 86.9,
pairwise accuracy evaluation for ViLBERT 12-in-1 model: 95.6
BERT-ITPT-FiT                                                                                  Accuracy (10 classes) evaluation for BERT-ITPT-FiT model: -,
Accuracy (2 classes) evaluation for BERT-ITPT-FiT model: 95.63,
Accuracy evaluation for BERT-ITPT-FiT model: 70.58%,
Accuracy evaluation for BERT-ITPT-FiT model: 77.62,
Accuracy evaluation for BERT-ITPT-FiT model: 98.07,
Accuracy evaluation for BERT-ITPT-FiT model: 98.08%,
Error evaluation for BERT-ITPT-FiT model: 0.68,
Error evaluation for BERT-ITPT-FiT model: 3.2,
Error evaluation for BERT-ITPT-FiT model: 4.8
BERT                                                                                   KIN evaluation for BERT model: 72.0,
 LUO evaluation for BERT model: 73.2,
1:1 Accuracy evaluation for BERT model: 20.6,
AMH evaluation for BERT model: 0,
Accuracy (10-fold) evaluation for BERT model: 68.0%,
Accuracy (5-fold) evaluation for BERT model: 0.857,
Accuracy evaluation for BERT model: 61.66,
Accuracy evaluation for BERT model: 70.5%,
Accuracy evaluation for BERT model: 72.79,
Accuracy evaluation for BERT model: 74.3%,
Accuracy evaluation for BERT model: 83.2,
Accuracy evaluation for BERT model: 86.94,
Accuracy evaluation for BERT model: 88.2,
Accuracy evaluation for BERT model: 91.37,
Accuracy evaluation for BERT model: 92.0872,
Accuracy evaluation for BERT model: 94.0,
Accuracy evaluation for BERT model: 98.171,
Accuracy evaluation for BERT model: 99.40,
Accuracy evaluation for BERT model: 99.96,
Alpha-Word accuracy evaluation for BERT model: 98.53,
Alpha-Word accuracy evaluation for BERT model: 98.63,
Alpha-Word accuracy evaluation for BERT model: 98.64,
Alpha-Word accuracy evaluation for BERT model: 98.88,
Alpha-Word accuracy evaluation for BERT model: 98.95,
Alpha-Word accuracy evaluation for BERT model: 99.22,
Alpha-Word accuracy evaluation for BERT model: 99.32,
Alpha-Word accuracy evaluation for BERT model: 99.41,
Alpha-Word accuracy evaluation for BERT model: 99.62,
Alpha-Word accuracy evaluation for BERT model: 99.66,
Alpha-Word accuracy evaluation for BERT model: 99.71,
Alpha-Word accuracy evaluation for BERT model: 99.73,
Average Accuracy evaluation for BERT model: 57.52,
Average F1 evaluation for BERT model: 46,
Average F1 evaluation for BERT model: 54.10,
Average F1 evaluation for BERT model: 66.0%,
Average Precision evaluation for BERT model: 54.18,
Average Recall evaluation for BERT model: 54.02,
Avg F1 evaluation for BERT model: 0.8408,
Avg F1 evaluation for BERT model: 0.8553,
Avg F1 evaluation for BERT model: 0.9079,
CaseHOLD evaluation for BERT model: 70.7,
Classification Accuracy evaluation for BERT model: 0.7664,
ECtHR Task A evaluation for BERT model: 71.4 / 64.0,
ECtHR Task B evaluation for BERT model: 87.6 / 77.8,
ENG  evaluation for BERT model: 92.9,
EUR-LEX evaluation for BERT model: 71.6 / 55.6,
Exact Match evaluation for BERT model: 66%,
F1 (%) evaluation for BERT model: 96.53,
F1 (%) evaluation for BERT model: 97.37,
F1 evaluation for BERT model: 0.731,
F1 evaluation for BERT model: 0.871585052,
F1 evaluation for BERT model: 0.929702403,
F1 evaluation for BERT model: 32.7,
F1 evaluation for BERT model: 53.2,
F1 evaluation for BERT model: 66.0,
F1 evaluation for BERT model: 66.83%,
F1 evaluation for BERT model: 89.4,
F1 evaluation for BERT model: 94.1,
F1(10-fold) evaluation for BERT model: 72.2,
F1(10-fold) evaluation for BERT model: 95.8,
F1-score evaluation for BERT model: 0.7883,
HAU evaluation for BERT model: 86.6,
Hits@1 evaluation for BERT model: 24.3,
IBO evaluation for BERT model: 83.5,
LEDGAR evaluation for BERT model: 87.7 / 82.2,
LUG evaluation for BERT model: 78.4,
MAP evaluation for BERT model: 0.591,
MAP evaluation for BERT model: 0.625,
MRR evaluation for BERT model: 0.633,
MRR evaluation for BERT model: 0.639,
MSE evaluation for BERT model: 0.046,
Macro F1 (10-fold) evaluation for BERT model: 61.3,
Macro F1 (w/o Neutral) evaluation for BERT model: 50.14,
Macro F1 evaluation for BERT model: 0.48,
Macro F1 evaluation for BERT model: 0.724,
Macro F1 evaluation for BERT model: 0.803,
Macro F1 evaluation for BERT model: 55.80,
NDCG@3 evaluation for BERT model: 0.625,
NDCG@5 evaluation for BERT model: 0.714,
Overall Accuracy evaluation for BERT model:  65.8,
P@1 evaluation for BERT model: 0.453,
P@1 evaluation for BERT model: 0.454,
PCM evaluation for BERT model: 87.0,
Pair-level 13-class Acc evaluation for BERT model: 39.73,
Pair-level 4-class Acc evaluation for BERT model: 58.13,
Pair-level 6-class Acc evaluation for BERT model: 42.33,
Params evaluation for BERT model: 110M,
Precision evaluation for BERT model: 56.1,
Precision evaluation for BERT model: 79.17,
R10@1 evaluation for BERT model: 0.280,
R10@1 evaluation for BERT model: 0.404,
R10@2 evaluation for BERT model: 0.470,
R10@2 evaluation for BERT model: 0.606,
R10@5 evaluation for BERT model: 0.828,
R10@5 evaluation for BERT model: 0.875,
ROC-AUC FAR evaluation for BERT model: 28.15,
ROC-AUC IID evaluation for BERT model: 84.54,
ROC-AUC NEAR evaluation for BERT model: 86.05,
ROC-AUC-ID (In-Distribution setup) evaluation for BERT model: 79.62,
ROUGE-1 evaluation for BERT model: 62.37,
ROUGE-2 evaluation for BERT model: 50.70,
ROUGE-L evaluation for BERT model: 59.40,
Recall evaluation for BERT model: 50.6,
SCOTUS evaluation for BERT model: 70.5 / 60.9,
SWA evaluation for BERT model: 83.3,
Session-level 13-class Acc evaluation for BERT model: 39.4,
Session-level 4-class Acc evaluation for BERT model: 47.1,
Session-level 6-class Acc evaluation for BERT model: 41.87,
Spearman Correlation evaluation for BERT model: 29.82,
Spearman Correlation evaluation for BERT model: 54.36,
UNFAIR-ToS evaluation for BERT model: 87.5 / 81.0,
V-Measure evaluation for BERT model: 30.12,
WOL evaluation for BERT model: 62.2,
Weighted F1 (w/o Neutral) evaluation for BERT model: 73.55,
Weighted F1 evaluation for BERT model: 84.83,
YOR evaluation for BERT model: 73.8
BERT - 6 Layers                                                                                  EM evaluation for BERT - 6 Layers model: 81.5,
F1 evaluation for BERT - 6 Layers model: 88.5
BERT+ASGen                                                                                  EM evaluation for BERT+ASGen model: 54.7,
F1 evaluation for BERT+ASGen model: 64.5
Multimodal(ViT+BERT, Input: Image + Abstract)                                                                                  Accuracy evaluation for Multimodal(ViT+BERT, Input: Image + Abstract) model: 0.8610
PubmedBERT + PURE (domain-adapted)                                                                                  Exact Match F1 ("Any Combination") evaluation for PubmedBERT + PURE (domain-adapted) model: 69.4,
Exact Match F1 ("Positive Combination") evaluation for PubmedBERT + PURE (domain-adapted) model: 61.8
cfilt/HiNER-original-xlm-roberta-large                                                                                  F1-score (Weighted) evaluation for cfilt/HiNER-original-xlm-roberta-large model: 88.78
BERT-FirstP                                                                                  nDCG@20 evaluation for BERT-FirstP model: 0.444
AristoRoBERTa + Graph Soft Counter                                                                                  Accuracy evaluation for AristoRoBERTa + Graph Soft Counter model: 87.4
DeepPavlov multilingual BERT                                                                                  EM evaluation for DeepPavlov multilingual BERT model: 64.35+-0.39,
F1 evaluation for DeepPavlov multilingual BERT model: 83.39+-0.08
DocBERT                                                                                  Accuracy evaluation for DocBERT model: 0.764
FLAT+BERT                                                                                  F1 evaluation for FLAT+BERT model: 68.55,
F1 evaluation for FLAT+BERT model: 81.82,
F1 evaluation for FLAT+BERT model: 95.86,
F1 evaluation for FLAT+BERT model: 96.09
RoBERTa (fine-tuned)                                                                                  Average (%) evaluation for RoBERTa (fine-tuned) model: 27.9,
Humanities evaluation for RoBERTa (fine-tuned) model: 27.9,
Other evaluation for RoBERTa (fine-tuned) model: 27.7,
Parameters (Billions) evaluation for RoBERTa (fine-tuned) model: 0.354,
STEM evaluation for RoBERTa (fine-tuned) model: 27.0,
Social Sciences evaluation for RoBERTa (fine-tuned) model: 28.8
Bottom-up (SpanBERT)                                                                                  Standard Parseval (Full) evaluation for Bottom-up (SpanBERT) model: 40.5,
Standard Parseval (Full) evaluation for Bottom-up (SpanBERT) model: 52.7,
Standard Parseval (Nuclearity) evaluation for Bottom-up (SpanBERT) model: 53.8,
Standard Parseval (Nuclearity) evaluation for Bottom-up (SpanBERT) model: 65.3,
Standard Parseval (Relation) evaluation for Bottom-up (SpanBERT) model: 46.0,
Standard Parseval (Relation) evaluation for Bottom-up (SpanBERT) model: 54.9,
Standard Parseval (Span) evaluation for Bottom-up (SpanBERT) model: 72.9
MobileBERT + 1bit-1dim model compression using DKM                                                                                  Accuracy evaluation for MobileBERT + 1bit-1dim model compression using DKM model: 63.17
BERT (pred POS/lemmas)                                                                                  Full F1 (Preps) evaluation for BERT (pred POS/lemmas) model: 71.6,
Function F1 (Preps) evaluation for BERT (pred POS/lemmas) model: 82.8,
Role F1 (Preps) evaluation for BERT (pred POS/lemmas) model: 72.4,
Tags (Full) Acc evaluation for BERT (pred POS/lemmas) model: 82.5
DeBERTa-Ensemble                                                                                  Accuracy evaluation for DeBERTa-Ensemble model: 77.5,
Accuracy evaluation for DeBERTa-Ensemble model: 98.4
HIBERT                                                                                  ROUGE-1 evaluation for HIBERT model: 42.37,
ROUGE-2 evaluation for HIBERT model: 19.95,
ROUGE-L evaluation for HIBERT model: 38.83
CorefRoBERTa-large                                                                                  F1 evaluation for CorefRoBERTa-large model: 60.25,
Ign F1 evaluation for CorefRoBERTa-large model: 57.90
BERT + SCH attn                                                                                  Val F1 Score evaluation for BERT + SCH attn model: 88.436
Point-BERT                                                                                  OBJ-BG (OA) evaluation for Point-BERT model: 87.43,
OBJ-ONLY (OA) evaluation for Point-BERT model: 88.12,
Overall Accuracy evaluation for Point-BERT model: 83.1,
Overall Accuracy evaluation for Point-BERT model: 91.0,
Overall Accuracy evaluation for Point-BERT model: 92.7,
Overall Accuracy evaluation for Point-BERT model: 93.8,
Overall Accuracy evaluation for Point-BERT model: 94.6,
Overall Accuracy evaluation for Point-BERT model: 96.3,
Standard Deviation evaluation for Point-BERT model: 2.7,
Standard Deviation evaluation for Point-BERT model: 3.1,
Standard Deviation evaluation for Point-BERT model: 5.1,
Standard Deviation evaluation for Point-BERT model: 5.4
BiBERT                                                                                  BLEU score evaluation for BiBERT model: 31.26,
BLEU score evaluation for BiBERT model: 34.94,
BLEU score evaluation for BiBERT model: 38.61,
Number of Params evaluation for BiBERT model: 73.8M
GTS with RoBERTa                                                                                  Accuracy (%) evaluation for GTS with RoBERTa model: 88.5,
Accuracy evaluation for GTS with RoBERTa model: 41.0,
Execution Accuracy evaluation for GTS with RoBERTa model: 41.0,
Execution Accuracy evaluation for GTS with RoBERTa model: 81.2
SqueezeBERT                                                                                  Accuracy evaluation for SqueezeBERT model: 46.5%,
Accuracy evaluation for SqueezeBERT model: 65.1,
Accuracy evaluation for SqueezeBERT model: 65.1%,
Accuracy evaluation for SqueezeBERT model: 73.2%,
Accuracy evaluation for SqueezeBERT model: 80.3%,
Accuracy evaluation for SqueezeBERT model: 87.8%,
Accuracy evaluation for SqueezeBERT model: 90.1%,
Accuracy evaluation for SqueezeBERT model: 91.4,
Matched evaluation for SqueezeBERT model: 82.0,
Mismatched evaluation for SqueezeBERT model: 81.1,
Number of Params evaluation for SqueezeBERT model: 51.1M
ALBERT xxlarge                                                                                  EM evaluation for ALBERT xxlarge model: 85.1,
F1 evaluation for ALBERT xxlarge model: 88.1
RoBERTa-Large + NPCRF (replicated by Adaseq)                                                                                  F1 evaluation for RoBERTa-Large + NPCRF (replicated by Adaseq) model: 47.3
DeBERTa                                                                                  Accuracy evaluation for DeBERTa model: 89.73,
Accuracy evaluation for DeBERTa model: 90.21,
Accuracy evaluation for DeBERTa model: 94.5,
Accuracy evaluation for DeBERTa model: 94.5%,
Accuracy evaluation for DeBERTa model: 94.78,
Accuracy evaluation for DeBERTa model: 95.1,
Accuracy evaluation for DeBERTa model: 98.451,
F1 evaluation for DeBERTa model: 95.1
RemBERT                                                                                  Accuracy evaluation for RemBERT model: 75.06,
MCC evaluation for RemBERT model: 0.44,
MCC evaluation for RemBERT model: 0.6
SignBERT+                                                                                  BLEU-4 evaluation for SignBERT+ model: 25.7,
Top-1 Accuracy evaluation for SignBERT+ model: 55.59,
Top-1 Accuracy evaluation for SignBERT+ model: 73.71,
Word Error Rate (WER) evaluation for SignBERT+ model: 19.9,
Word Error Rate (WER) evaluation for SignBERT+ model: 20
Edit-SQL+BERT                                                                                  interaction match accuracy evaluation for Edit-SQL+BERT model: 13.7,
question match accuracy evaluation for Edit-SQL+BERT model: 40.8
CRF2O + BERT                                                                                  F1 evaluation for CRF2O + BERT model: 85.45
NLI_RoBERTa                                                                                  F1 evaluation for NLI_RoBERTa model: 71.0
Partially Fine-tuned HuBERT                                                                                  Accuracy (%) evaluation for Partially Fine-tuned HuBERT model: 87.51,
F1 evaluation for Partially Fine-tuned HuBERT model: 0.753
BERTweet                                                                                  ALL evaluation for BERTweet model: 67.9,
AUROC evaluation for BERTweet model: 0.979,
Acc evaluation for BERTweet model: 90.1,
Acc evaluation for BERTweet model: 95.2,
Emoji evaluation for BERTweet model: 33.4,
Emotion evaluation for BERTweet model: 79.3,
F1 evaluation for BERTweet model: 52.1,
F1 evaluation for BERTweet model: 56.5,
GMB BNSP evaluation for BERTweet model: 0.9603,
GMB BPSN evaluation for BERTweet model: 0.8945,
GMB Subgroup evaluation for BERTweet model: 0.878,
Irony evaluation for BERTweet model: 82.1,
Macro F1 evaluation for BERTweet model: 0.3612,
Micro F1 evaluation for BERTweet model: 0.4928,
Offensive evaluation for BERTweet model: 79.5,
Precision evaluation for BERTweet model: 0.3363,
Recall evaluation for BERTweet model: 0.9216,
Sentiment evaluation for BERTweet model: 73.4,
Stance evaluation for BERTweet model: 71.2
SSAN-RoBERTa-large+Adaptation                                                                                  F1 evaluation for SSAN-RoBERTa-large+Adaptation model: 65.92,
Ign F1 evaluation for SSAN-RoBERTa-large+Adaptation model: 63.78
BERT-MaxP                                                                                  nDCG@20 evaluation for BERT-MaxP model: 0.469
BERT+AVG+MLP                                                                                  Accuracy of Sentiment evaluation for BERT+AVG+MLP model: 51.50,
Macro-F1 of Sentiment evaluation for BERT+AVG+MLP model: 48.02
BABERT                                                                                  F1 evaluation for BABERT model: 96.70,
F1 evaluation for BABERT model: 97.45,
F1 evaluation for BABERT model: 98.44
LinkBERT (large)                                                                                  Average F1 evaluation for LinkBERT (large) model: 81.0,
EM evaluation for LinkBERT (large) model: 87.45,
F1 evaluation for LinkBERT (large) model: 72.6,
F1 evaluation for LinkBERT (large) model: 78.2,
F1 evaluation for LinkBERT (large) model: 92.7
ResNet + RoBERTa finetune                                                                                  AUROC evaluation for ResNet + RoBERTa finetune model: 0.786,
AUROC evaluation for ResNet + RoBERTa finetune model: 0.97
kNN-BERT + POS (training corpus: WNGT)                                                                                  F1 evaluation for kNN-BERT + POS (training corpus: WNGT) model: 85.32
RECENT+SpanBERT                                                                                  F1 evaluation for RECENT+SpanBERT model: 75.2
SMART+BERT-BASE                                                                                  Accuracy evaluation for SMART+BERT-BASE model: 85.6,
Accuracy evaluation for SMART+BERT-BASE model: 93
IndicBERT Large                                                                                  Accuracy evaluation for IndicBERT Large model: 77.54
XLM-RoBERTa-Large                                                                                  EM evaluation for XLM-RoBERTa-Large model: 79.0,
F1 evaluation for XLM-RoBERTa-Large model: 89.5
DeBERTa-V3-Large + ALL                                                                                  MAP evaluation for DeBERTa-V3-Large + ALL model: 0.909,
MRR evaluation for DeBERTa-V3-Large + ALL model: 0.920
Stack-Propagation (+BERT)                                                                                  Accuracy evaluation for Stack-Propagation (+BERT) model: 97.50,
F1 evaluation for Stack-Propagation (+BERT) model: 96.10,
Intent Accuracy evaluation for Stack-Propagation (+BERT) model: 99.0,
Slot F1 Score evaluation for Stack-Propagation (+BERT) model: 97.00
Top-down (RoBERTa)                                                                                  Standard Parseval (Full) evaluation for Top-down (RoBERTa) model: 41.5,
Standard Parseval (Full) evaluation for Top-down (RoBERTa) model: 53.8,
Standard Parseval (Nuclearity) evaluation for Top-down (RoBERTa) model: 56.1,
Standard Parseval (Nuclearity) evaluation for Top-down (RoBERTa) model: 66.6,
Standard Parseval (Relation) evaluation for Top-down (RoBERTa) model: 48.7,
Standard Parseval (Relation) evaluation for Top-down (RoBERTa) model: 55.8,
Standard Parseval (Span) evaluation for Top-down (RoBERTa) model: 75.7,
Standard Parseval (Span) evaluation for Top-down (RoBERTa) model: 77.3
BERT-Base-uncased-PruneOFA (85% unstruct sparse)                                                                                  EM evaluation for BERT-Base-uncased-PruneOFA (85% unstruct sparse) model: 81.1,
F1 evaluation for BERT-Base-uncased-PruneOFA (85% unstruct sparse) model: 88.42,
Matched evaluation for BERT-Base-uncased-PruneOFA (85% unstruct sparse) model: 82.71,
Mismatched evaluation for BERT-Base-uncased-PruneOFA (85% unstruct sparse) model: 83.67
BERT_base+ITPT                                                                                  Accuracy evaluation for BERT_base+ITPT model: 95.63,
Error evaluation for BERT_base+ITPT model: 1.92,
Error evaluation for BERT_base+ITPT model: 29.42
BERT-VFT                                                                                  R10@1 evaluation for BERT-VFT model: 0.855,
R10@2 evaluation for BERT-VFT model: 0.928,
R10@5 evaluation for BERT-VFT model: 0.985
BERT+Aspect-based approaches                                                                                  F1 evaluation for BERT+Aspect-based approaches model: 0.737
SpERT.PL (without overlap and BioBERT)                                                                                  NER Macro F1 evaluation for SpERT.PL (without overlap and BioBERT) model: 91.14,
RE+ Macro F1 evaluation for SpERT.PL (without overlap and BioBERT) model: 82.39
BERT [Attn]                                                                                  AUROC evaluation for BERT [Attn] model: 0.843,
Accuracy evaluation for BERT [Attn] model: 0.69,
Macro F1 evaluation for BERT [Attn] model: 0.674
BERT-Base uncased (fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus')                                                                                  F1 evaluation for BERT-Base uncased (fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus') model: 89.16,
Precision evaluation for BERT-Base uncased (fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus') model: 89.31,
Recall evaluation for BERT-Base uncased (fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus') model: 89.12
CharacterBERT (base, medical)                                                                                  Accuracy evaluation for CharacterBERT (base, medical) model: 84.95,
Exact Span F1 evaluation for CharacterBERT (base, medical) model: 89.24,
Micro F1 evaluation for CharacterBERT (base, medical) model: 73.44,
Micro F1 evaluation for CharacterBERT (base, medical) model: 80.38
NLI_DeBERTa                                                                                  F1 (1% Few-Shot) evaluation for NLI_DeBERTa model: 63.7,
F1 (10% Few-Shot) evaluation for NLI_DeBERTa model: 67.9,
F1 (5% Few-Shot) evaluation for NLI_DeBERTa model: 69.0,
F1 (Zero-Shot) evaluation for NLI_DeBERTa model: 62.8,
F1 evaluation for NLI_DeBERTa model: 73.9
ParsBERT+PCapsNet +SA+Title+ Auxiliary                                                                                  F-Measure evaluation for ParsBERT+PCapsNet +SA+Title+ Auxiliary model: 0.947
BERT-SPC                                                                                  Accuracy evaluation for BERT-SPC model: 73.55,
Laptop (Acc) evaluation for BERT-SPC model: 78.99,
Mean Acc (Restaurant + Laptop) evaluation for BERT-SPC model: 81.73,
Restaurant (Acc) evaluation for BERT-SPC model: 84.46
CRF Parser + RoBERTa                                                                                  F1 score evaluation for CRF Parser + RoBERTa model: 96.32
TUCORE-GCN_BERT                                                                                  F1 (v2) evaluation for TUCORE-GCN_BERT model: 65.5,
F1c (v2) evaluation for TUCORE-GCN_BERT model: 60.2,
Micro-F1 evaluation for TUCORE-GCN_BERT model: 58.34,
Weighted-F1 evaluation for TUCORE-GCN_BERT model: 36.01,
Weighted-F1 evaluation for TUCORE-GCN_BERT model: 62.47
RoBERTa BCE                                                                                  AUROC evaluation for RoBERTa BCE model: 0.9813,
GMB BNSP evaluation for RoBERTa BCE model: 0.9616,
GMB BPSN evaluation for RoBERTa BCE model: 0.8901,
GMB Subgroup evaluation for RoBERTa BCE model: 0.88,
Macro F1 evaluation for RoBERTa BCE model: 0.4749,
Micro F1 evaluation for RoBERTa BCE model: 0.5359,
Precision evaluation for RoBERTa BCE model: 0.3836,
Recall evaluation for RoBERTa BCE model: 0.8891
DistilBERT                                                                                  AUROC evaluation for DistilBERT model: 0.9804,
Accuracy evaluation for DistilBERT model: 44.4%,
Accuracy evaluation for DistilBERT model: 49.1%,
Accuracy evaluation for DistilBERT model: 62.9%,
Accuracy evaluation for DistilBERT model: 85.31,
Accuracy evaluation for DistilBERT model: 89.2%,
Accuracy evaluation for DistilBERT model: 89.69,
Accuracy evaluation for DistilBERT model: 90.2%,
Accuracy evaluation for DistilBERT model: 91.3,
Accuracy evaluation for DistilBERT model: 92.82,
Accuracy evaluation for DistilBERT model: 93.4,
Accuracy evaluation for DistilBERT model: 97.981,
Accuracy evaluation for DistilBERT model: 99.96,
EM evaluation for DistilBERT model: 77.7,
F1 evaluation for DistilBERT model: 0.082,
F1 evaluation for DistilBERT model: 85.8,
F1 evaluation for DistilBERT model: 89,
F1 evaluation for DistilBERT model: 91,
F1 evaluation for DistilBERT model: 93.5,
GMB BNSP evaluation for DistilBERT model: 0.9644,
GMB BPSN evaluation for DistilBERT model: 0.874,
GMB Subgroup evaluation for DistilBERT model: 0.8762,
Macro F1 evaluation for DistilBERT model: 0.3879,
Micro F1 evaluation for DistilBERT model: 0.5115,
Pearson Correlation evaluation for DistilBERT model: 0.907,
Precision evaluation for DistilBERT model: 0.3572,
Recall evaluation for DistilBERT model: 0.9001,
Rouge-L evaluation for DistilBERT model: 0.097
DeBERTa + RoBERTa + XLNet                                                                                  F0.5 evaluation for DeBERTa + RoBERTa + XLNet model: 76.05
Camembert-Base-SquadFR-Fquad-Piaf                                                                                  EM evaluation for Camembert-Base-SquadFR-Fquad-Piaf model: 77.0,
F1 evaluation for Camembert-Base-SquadFR-Fquad-Piaf model: 88.4
MotionBERT-HybrIK                                                                                  MPJPE evaluation for MotionBERT-HybrIK model: 68.8,
MPVPE evaluation for MotionBERT-HybrIK model: 79.4,
PA-MPJPE evaluation for MotionBERT-HybrIK model: 40.6
DeepPavlov RuBERT                                                                                  EM evaluation for DeepPavlov RuBERT model: 66.30+-0.24,
F1 evaluation for DeepPavlov RuBERT model: 84.60+-0.11
Bert Chinese                                                                                  Accuray evaluation for Bert Chinese model: 59.11,
F1-score evaluation for Bert Chinese model: 58.99,
Precision evaluation for Bert Chinese model: 59.07,
Recall evaluation for Bert Chinese model: 59.20
BERTLARGE                                                                                  Accuracy evaluation for BERTLARGE model: 65.1,
Accuracy evaluation for BERTLARGE model: 78.3,
Score evaluation for BERTLARGE model: 62.0
LSA+DeBERTa-V3-Large                                                                                  Laptop (Acc) evaluation for LSA+DeBERTa-V3-Large model: 86.21,
Mean Acc (Restaurant + Laptop) evaluation for LSA+DeBERTa-V3-Large model: 88.27,
Restaurant (Acc) evaluation for LSA+DeBERTa-V3-Large model: 90.33
Albert Lan et al. (2020) (ensemble)                                                                                  Accuracy evaluation for Albert Lan et al. (2020) (ensemble) model: 76.5
NL2SQL-BERT                                                                                  Accuracy evaluation for NL2SQL-BERT model: 89
Joint BERT                                                                                  Accuracy evaluation for Joint BERT model: 97.5,
F1 evaluation for Joint BERT model: 0.961
Eider-RoBERTa-large                                                                                  F1 evaluation for Eider-RoBERTa-large model: 64.79,
Ign F1 evaluation for Eider-RoBERTa-large model: 62.85
ColBERT                                                                                  nDCG@10 evaluation for ColBERT model: 0.305,
nDCG@10 evaluation for ColBERT model: 0.401,
nDCG@10 evaluation for ColBERT model: 0.524,
nDCG@10 evaluation for ColBERT model: 0.671,
nDCG@10 evaluation for ColBERT model: 0.677
Syn-LSTM + BERT (wo doc-context)                                                                                  F1 evaluation for Syn-LSTM + BERT (wo doc-context) model: 90.85
Ours + ResNext101 BERT                                                                                  Average accuracy of 3 splits evaluation for Ours + ResNext101 BERT model: 84.53
SSAN-BERT-base                                                                                  F1 evaluation for SSAN-BERT-base model: 58.16,
Ign F1 evaluation for SSAN-BERT-base model: 55.84
VLC-BERT                                                                                  Accuracy evaluation for VLC-BERT model: 43.1,
DA VQA Score evaluation for VLC-BERT model: 38.05
BERT-Base + CLR + LSTM                                                                                  Accuracy evaluation for BERT-Base + CLR + LSTM model: 97.30
R²SQL + BERT                                                                                  interaction match accuracy evaluation for R²SQL + BERT model: 17.0,
question match accuracy evaluation for R²SQL + BERT model: 46.8
EFL (Entailment as Few-shot Learner) + RoBERTa-large                                                                                  % Test Accuracy evaluation for EFL (Entailment as Few-shot Learner) + RoBERTa-large model: 93.1,
% Train Accuracy evaluation for EFL (Entailment as Few-shot Learner) + RoBERTa-large model: ?,
Parameters evaluation for EFL (Entailment as Few-shot Learner) + RoBERTa-large model: 355m
RoBERTa-Base Joint + MSPP                                                                                  MAP evaluation for RoBERTa-Base Joint + MSPP model: 0.911,
MRR evaluation for RoBERTa-Base Joint + MSPP model: 0.952
HIN-BERT-base                                                                                  F1 evaluation for HIN-BERT-base model: 55.60,
Ign F1 evaluation for HIN-BERT-base model: 53.70
Bottom-up (BERT)                                                                                  Standard Parseval (Full) evaluation for Bottom-up (BERT) model: 32.9,
Standard Parseval (Full) evaluation for Bottom-up (BERT) model: 46.0,
Standard Parseval (Nuclearity) evaluation for Bottom-up (BERT) model: 46.3,
Standard Parseval (Nuclearity) evaluation for Bottom-up (BERT) model: 57.8,
Standard Parseval (Relation) evaluation for Bottom-up (BERT) model: 39.5,
Standard Parseval (Relation) evaluation for Bottom-up (BERT) model: 47.8,
Standard Parseval (Span) evaluation for Bottom-up (BERT) model: 66.6,
Standard Parseval (Span) evaluation for Bottom-up (BERT) model: 68.3
BERT-large(single model)                                                                                  EM evaluation for BERT-large(single model) model: 24.1,
F1 evaluation for BERT-large(single model) model: 70.0
CRF2o + BERT                                                                                  F1 evaluation for CRF2o + BERT model: 87.66,
F1 evaluation for CRF2o + BERT model: 89.03
Roberta base                                                                                  Accuracy evaluation for Roberta base model: 90.06,
Parameters evaluation for Roberta base model: 125M
ATLOP-RoBERTa-large                                                                                  F1 evaluation for ATLOP-RoBERTa-large model: 63.40,
Ign F1 evaluation for ATLOP-RoBERTa-large model: 61.39
REX-VisualBert                                                                                  BLEU-4 evaluation for REX-VisualBert model: 54.59,
CIDEr evaluation for REX-VisualBert model: 464.20,
GQA-test evaluation for REX-VisualBert model: 57.77,
GQA-val evaluation for REX-VisualBert model: 66.16,
Grounding evaluation for REX-VisualBert model: 67.95,
METEOR evaluation for REX-VisualBert model: 39.22,
ROUGE-L evaluation for REX-VisualBert model: 78.56,
SPICE evaluation for REX-VisualBert model: 46.80
BERT-SocialIQA 340M                                                                                  Accuracy evaluation for BERT-SocialIQA 340M model: 72.5,
Accuracy evaluation for BERT-SocialIQA 340M model: 83.4
GraphCodeBERT                                                                                  Average Accuracy evaluation for GraphCodeBERT model: 62.51,
Average F1 evaluation for GraphCodeBERT model: 60.57,
Average Precision evaluation for GraphCodeBERT model: 60.06,
Average Recall evaluation for GraphCodeBERT model: 61.08,
Go evaluation for GraphCodeBERT model: 84.1,
JS evaluation for GraphCodeBERT model: 71.1,
Java evaluation for GraphCodeBERT model: 75.7,
Overall evaluation for GraphCodeBERT model: 77.4,
PHP evaluation for GraphCodeBERT model: 72.5 ,
Python evaluation for GraphCodeBERT model: 87.9,
Ruby evaluation for GraphCodeBERT model: 73.2
Transformer + Pre-train with Pseudo Data (+BERT)                                                                                  F0.5 evaluation for Transformer + Pre-train with Pseudo Data (+BERT) model: 65.2,
F0.5 evaluation for Transformer + Pre-train with Pseudo Data (+BERT) model: 69.8
BioBERT-CRF                                                                                  F1 evaluation for BioBERT-CRF model: 88.7
BERTjoint                                                                                  F1 evaluation for BERTjoint model: 64.7
HRLCE + BERT                                                                                  Micro-F1 evaluation for HRLCE + BERT model: 0.7709
RefVOS with BERT + MLM Loss                                                                                  Overall IoU evaluation for RefVOS with BERT + MLM Loss model: 49.73
RoBERTa-base 125M (fine-tuned)                                                                                  Average (%) evaluation for RoBERTa-base 125M (fine-tuned) model: 27.9
DeBERTaV3-large+KEAR                                                                                  Accuracy evaluation for DeBERTaV3-large+KEAR model: 91.2
TagRec(BERT+USE)                                                                                  R@10 evaluation for TagRec(BERT+USE) model: 0.92,
R@15 evaluation for TagRec(BERT+USE) model: 0.95,
R@20 evaluation for TagRec(BERT+USE) model: 0.96,
R@5 evaluation for TagRec(BERT+USE) model: 0.86
GCDT + BERT-L                                                                                  F1 evaluation for GCDT + BERT-L model: 93.47
RuBERT-RuSentiment                                                                                  Weighted F1 evaluation for RuBERT-RuSentiment model: 75.71
Mubert                                                                                  FAD VGG evaluation for Mubert model: 9.6
DRE-MIR-SciBERT                                                                                  F1 evaluation for DRE-MIR-SciBERT model: 76.6,
F1 evaluation for DRE-MIR-SciBERT model: 86.4
2 Step: Factor Graph Attention + VD-Bert                                                                                  MRR (x 100) evaluation for 2 Step: Factor Graph Attention + VD-Bert model: 69.92,
Mean evaluation for 2 Step: Factor Graph Attention + VD-Bert model: 3.84,
NDCG (x 100) evaluation for 2 Step: Factor Graph Attention + VD-Bert model: 72.83,
R@1 evaluation for 2 Step: Factor Graph Attention + VD-Bert model: 58.3,
R@10 evaluation for 2 Step: Factor Graph Attention + VD-Bert model: 89.6,
R@5 evaluation for 2 Step: Factor Graph Attention + VD-Bert model: 81.55
BERT-LSTM-base                                                                                  F1 evaluation for BERT-LSTM-base model: 67.8
VisualBERT base                                                                                  Group Score evaluation for VisualBERT base model: 1.50,
Image Score evaluation for VisualBERT base model: 2.50,
Text Score evaluation for VisualBERT base model: 15.50
BERT-base                                                                                   Strict Detection (Pr.) evaluation for BERT-base model: 81.83,
 Strict Detection (Re.) evaluation for BERT-base model: 79.56,
Accuracy (2 classes) evaluation for BERT-base model: 0.6588,
Accuracy (2 classes) evaluation for BERT-base model: 0.8220,
Accuracy evaluation for BERT-base model: 84.7,
F1 Macro evaluation for BERT-base model: 0.6555,
F1 Macro evaluation for BERT-base model: 0.7484,
F1 evaluation for BERT-base model: 53.22,
F1 evaluation for BERT-base model: 73.9,
HONEST evaluation for BERT-base model: 1.19,
Ign F1 evaluation for BERT-base model: 56.17,
Relaxed Detection (F1) evaluation for BERT-base model: 90.08,
Relaxed Detection (Pr.) evaluation for BERT-base model: 91.37,
Relaxed Detection (Re.) evaluation for BERT-base model: 88.84,
Strict Detection (F1) evaluation for BERT-base model: 80.67,
Test evaluation for BERT-base model: 47.3,
Type evaluation for BERT-base model: 82.00
Top-down (SpanBERT)                                                                                  Standard Parseval (Full) evaluation for Top-down (SpanBERT) model: 36.7,
Standard Parseval (Full) evaluation for Top-down (SpanBERT) model: 52.2,
Standard Parseval (Nuclearity) evaluation for Top-down (SpanBERT) model: 54.5,
Standard Parseval (Nuclearity) evaluation for Top-down (SpanBERT) model: 65.4,
Standard Parseval (Relation) evaluation for Top-down (SpanBERT) model: 42.7,
Standard Parseval (Relation) evaluation for Top-down (SpanBERT) model: 54.5,
Standard Parseval (Span) evaluation for Top-down (SpanBERT) model: 73.7,
Standard Parseval (Span) evaluation for Top-down (SpanBERT) model: 76.5
BERTSUMEXT                                                                                  RG-1(%) evaluation for BERTSUMEXT model: 20.94,
RG-2(%) evaluation for BERTSUMEXT model: 4.98,
RG-L(%) evaluation for BERTSUMEXT model: 14.48
Single layer bilstm distilled from BERT                                                                                  Accuracy evaluation for Single layer bilstm distilled from BERT model: 90.7
MAG-BERT*                                                                                  Acc-2 evaluation for MAG-BERT* model: 82.37,
Acc-7 evaluation for MAG-BERT* model: 43.62,
Corr evaluation for MAG-BERT* model: 0.781,
F1 evaluation for MAG-BERT* model: 82.5,
MAE evaluation for MAG-BERT* model: 0.727
BERT-Base                                                                                  Direct Intrinsic Dimension evaluation for BERT-Base model: 1861,
Direct Intrinsic Dimension evaluation for BERT-Base model: 9295,
Structure Aware Intrinsic Dimension evaluation for BERT-Base model: 1608,
Structure Aware Intrinsic Dimension evaluation for BERT-Base model: 8030
ProBERT                                                                                  Bias (F/M) evaluation for ProBERT model: 0.97,
Feminine F1 (F) evaluation for ProBERT model: 91.1,
Masculine F1 (M) evaluation for ProBERT model: 94.0,
Overall F1 evaluation for ProBERT model: 92.5
MobileBERT + 2bit-1dim model compression using DKM                                                                                  Accuracy evaluation for MobileBERT + 2bit-1dim model compression using DKM model: 82.13
ASA + RoBERTa                                                                                  Accuracy evaluation for ASA + RoBERTa model: 69.2,
Accuracy evaluation for ASA + RoBERTa model: 93.6%,
Accuracy evaluation for ASA + RoBERTa model: 96.3,
F1 evaluation for ASA + RoBERTa model: 57.3,
F1 evaluation for ASA + RoBERTa model: 73.7,
Matched evaluation for ASA + RoBERTa model: 88,
Spearman Correlation evaluation for ASA + RoBERTa model: 0.892
AutoBERT-Zero (Large)                                                                                  Accuracy evaluation for AutoBERT-Zero (Large) model: 90.7%,
Number of Params evaluation for AutoBERT-Zero (Large) model: 318M
MRN+BERT                                                                                  F1 evaluation for MRN+BERT model: 61.74,
Ign F1 evaluation for MRN+BERT model: 59.52
NPRM-BERT                                                                                  Accuracy (5-fold) evaluation for NPRM-BERT model: 0.979
Attach-Juxtapose Parser + BERT                                                                                  F1 score evaluation for Attach-Juxtapose Parser + BERT model: 93.52
Approach0+ColBERT (reranking)                                                                                  P@10 evaluation for Approach0+ColBERT (reranking) model: 0.276
BERT + BiLSTM                                                                                  F evaluation for BERT + BiLSTM model: 46.8,
P evaluation for BERT + BiLSTM model: 44.3,
R evaluation for BERT + BiLSTM model: 49.6,
VI evaluation for BERT + BiLSTM model: 93.3
BERT + EE                                                                                  F1 evaluation for BERT + EE model: 76.61
SciBERT cased (SciVocab, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus')                                                                                  F1 evaluation for SciBERT cased (SciVocab, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus') model: 89.3,
Precision evaluation for SciBERT cased (SciVocab, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus') model: 87.31,
Recall evaluation for SciBERT cased (SciVocab, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus') model: 91.53
ResNet + RoBERTa embedding                                                                                  AUROC evaluation for ResNet + RoBERTa embedding model: 0.882,
AUROC evaluation for ResNet + RoBERTa embedding model: 0.934
SimCSE-RoBERTa-base                                                                                  Spearman Correlation evaluation for SimCSE-RoBERTa-base model: 0.7016,
Spearman Correlation evaluation for SimCSE-RoBERTa-base model: 0.8136
SDGCN-BERT                                                                                  Laptop (Acc) evaluation for SDGCN-BERT model: 81.35,
Mean Acc (Restaurant + Laptop) evaluation for SDGCN-BERT model: 82.46,
Restaurant (Acc) evaluation for SDGCN-BERT model: 83.57
Roberta-DeductReasoner                                                                                  Accuracy (%) evaluation for Roberta-DeductReasoner model: 92,
Accuracy (5-fold) evaluation for Roberta-DeductReasoner model: 83,
Answer Accuracy evaluation for Roberta-DeductReasoner model: 78.6,
Execution Accuracy evaluation for Roberta-DeductReasoner model: 47.3
RoBERTa-large 355M (MLP quantized vector-wise, fine-tuned)                                                                                  Accuracy evaluation for RoBERTa-large 355M (MLP quantized vector-wise, fine-tuned) model: 68.6%,
Accuracy evaluation for RoBERTa-large 355M (MLP quantized vector-wise, fine-tuned) model: 85.4%,
Accuracy evaluation for RoBERTa-large 355M (MLP quantized vector-wise, fine-tuned) model: 91.0%,
Accuracy evaluation for RoBERTa-large 355M (MLP quantized vector-wise, fine-tuned) model: 94.7%,
Accuracy evaluation for RoBERTa-large 355M (MLP quantized vector-wise, fine-tuned) model: 96.4,
Matched evaluation for RoBERTa-large 355M (MLP quantized vector-wise, fine-tuned) model: 90.2,
Pearson Correlation evaluation for RoBERTa-large 355M (MLP quantized vector-wise, fine-tuned) model: 0.919
RoBERTa-large                                                                                  Accuracy (easy) evaluation for RoBERTa-large model: 75.5,
Accuracy (hard) evaluation for RoBERTa-large model: 40.0,
Accuracy evaluation for RoBERTa-large model: 55.6,
Accuracy evaluation for RoBERTa-large model: 96.54,
C evaluation for RoBERTa-large model: 34.5,
EM evaluation for RoBERTa-large model: 51.1,
F1 evaluation for RoBERTa-large model: 75.2,
HONEST evaluation for RoBERTa-large model: 2.62,
Macro F1 evaluation for RoBERTa-large model: 70.9
RoBERTa-large Tagger + LIQUID (Ensemble)                                                                                  Exact F1 evaluation for RoBERTa-large Tagger + LIQUID (Ensemble) model: 73.1
HeterGSAN+Reconstruction+BERT-base                                                                                  F1 evaluation for HeterGSAN+Reconstruction+BERT-base model: 59.45,
Ign F1 evaluation for HeterGSAN+Reconstruction+BERT-base model: 57.12
Triaffine + ALBERT                                                                                  F1 evaluation for Triaffine + ALBERT model: 87.27,
F1 evaluation for Triaffine + ALBERT model: 88.56,
F1 evaluation for Triaffine + ALBERT model: 88.83
BERT-CRF                                                                                  F1 evaluation for BERT-CRF model: 76.58,
F1 evaluation for BERT-CRF model: 77.04,
F1 evaluation for BERT-CRF model: 84.5,
F1 evaluation for BERT-CRF model: 86,
F1 evaluation for BERT-CRF model: 89.65,
F1 evaluation for BERT-CRF model: 93.6
BERT-MRC                                                                                  F1 evaluation for BERT-MRC model: 82.11,
F1 evaluation for BERT-MRC model: 85.98,
F1 evaluation for BERT-MRC model: 86.88,
F1 evaluation for BERT-MRC model: 91.11,
F1 evaluation for BERT-MRC model: 93.04,
F1 evaluation for BERT-MRC model: 95.75
Ensemble multilingual BERT model                                                                                  F1 score evaluation for Ensemble multilingual BERT model model: 0.95924
BERT + VAE                                                                                  F1 - macro evaluation for BERT + VAE model: 86.79,
F1 Macro evaluation for BERT + VAE model: 79.03,
F1 Macro evaluation for BERT + VAE model: 92.32
BERT-base finetune (single model)                                                                                  In-domain evaluation for BERT-base finetune (single model) model: 79.8,
Out-of-domain evaluation for BERT-base finetune (single model) model: 74.1,
Overall evaluation for BERT-base finetune (single model) model: 78.1
SAISORE+CR+ET-SciBERT                                                                                  F1 evaluation for SAISORE+CR+ET-SciBERT model: 79,
F1 evaluation for SAISORE+CR+ET-SciBERT model: 87.1
BERT + Small Training                                                                                  MRR evaluation for BERT + Small Training model: 0.359
RoBERTa-large 355M + EFL + UCA                                                                                  Accuracy evaluation for RoBERTa-large 355M + EFL + UCA model: 87.2%
CaseLaw-BERT                                                                                  CaseHOLD evaluation for CaseLaw-BERT model: 75.6,
ECtHR Task A evaluation for CaseLaw-BERT model: 71.2 / 64.2,
ECtHR Task B evaluation for CaseLaw-BERT model: 88.0 / 77.5,
EUR-LEX evaluation for CaseLaw-BERT model: 71.0 / 55.9,
LEDGAR evaluation for CaseLaw-BERT model: 88.0 / 82.3,
SCOTUS evaluation for CaseLaw-BERT model: 76.4 / 66.2,
UNFAIR-ToS evaluation for CaseLaw-BERT model: 88.3 / 81.0
DeBERTa                                                                                   CaseHOLD evaluation for DeBERTa  model: 72.1,
ECtHR Task A evaluation for DeBERTa  model: 69.1 / 61.2,
ECtHR Task B evaluation for DeBERTa  model: 87.4 / 77.3,
EUR-LEX evaluation for DeBERTa  model: 72.3 / 57.2,
LEDGAR evaluation for DeBERTa  model: 87.9 / 82.0,
SCOTUS evaluation for DeBERTa  model: 70.0 / 60.0,
UNFAIR-ToS evaluation for DeBERTa  model: 87.2 / 78.8
BERT-Japanese                                                                                  Exact Match evaluation for BERT-Japanese model: 63.38,
F1 evaluation for BERT-Japanese model: 78.92
Sequence tagging + token-level transformations + two-stage fine-tuning (+BERT, RoBERTa, XLNet)                                                                                  F0.5 evaluation for Sequence tagging + token-level transformations + two-stage fine-tuning (+BERT, RoBERTa, XLNet) model: 66.5,
Precision evaluation for Sequence tagging + token-level transformations + two-stage fine-tuning (+BERT, RoBERTa, XLNet) model: 78.2,
Recall evaluation for Sequence tagging + token-level transformations + two-stage fine-tuning (+BERT, RoBERTa, XLNet) model: 41.5
SciDeBERTa (CS)                                                                                  F1 evaluation for SciDeBERTa (CS) model: 71.1
BFCR + SpanBERT                                                                                  CoNLL F1 evaluation for BFCR + SpanBERT model: 50.4
BERT+DP                                                                                  AVG evaluation for BERT+DP model: 37.0,
AVG evaluation for BERT+DP model: 53.6,
F-BC evaluation for BERT+DP model: 64.0,
F-Score evaluation for BERT+DP model: 71.3,
F_NMI evaluation for BERT+DP model: 21.4,
V-Measure evaluation for BERT+DP model: 40.4
BERT FT(Microblog)                                                                                  MAP evaluation for BERT FT(Microblog) model: 0.3278,
P@20 evaluation for BERT FT(Microblog) model: 0.4287
RoBERTa (ensemble)                                                                                  Accuracy evaluation for RoBERTa (ensemble) model: 67.8%,
Accuracy evaluation for RoBERTa (ensemble) model: 88.2%,
Accuracy evaluation for RoBERTa (ensemble) model: 89,
Accuracy evaluation for RoBERTa (ensemble) model: 92.3%,
Accuracy evaluation for RoBERTa (ensemble) model: 96.7,
Accuracy evaluation for RoBERTa (ensemble) model: 98.9%,
Mismatched evaluation for RoBERTa (ensemble) model: 90.2
En-BERT (pre-trained) + TDA                                                                                  MCC evaluation for En-BERT (pre-trained) + TDA model: 0.420
DV-ngrams-cosine + RoBERTa.base                                                                                  Accuracy evaluation for DV-ngrams-cosine + RoBERTa.base model: 95.92
BERT$^{c}$                                                                                  Accuracy (%) evaluation for BERT$^{c}$ model: 66.32,
Accuracy of Agreeableness evaluation for BERT$^{c}$ model: 80.98,
Accuracy of Conscientiousness evaluation for BERT$^{c}$ model: 63.35,
Accuracy of Extraversion evaluation for BERT$^{c}$ model: 78.08,
Accuracy of Neurotism evaluation for BERT$^{c}$ model: 55.29,
Accuracy of Openness evaluation for BERT$^{c}$ model: 53.90,
Macro-F1 evaluation for BERT$^{c}$ model: 72.69
TransE-RoBERTa                                                                                  Test MRR evaluation for TransE-RoBERTa model: 0.6288,
Validation MRR evaluation for TransE-RoBERTa model: 0.6039
AlephBERTGimmel-base MTL                                                                                  F1 evaluation for AlephBERTGimmel-base MTL model: 80.39
DrBERT                                                                                  Exact Match Accuracy evaluation for DrBERT model: 15.32,
Hamming Score evaluation for DrBERT model: 37.37
SsciBERT                                                                                  F1 evaluation for SsciBERT model: 66.1
CRF2o + RoBERTa                                                                                  F1 evaluation for CRF2o + RoBERTa model: 88.32,
F1 evaluation for CRF2o + RoBERTa model: 89.54
BERT (linear projection)                                                                                  SemEval 2007 evaluation for BERT (linear projection) model: 68.1,
SemEval 2013 evaluation for BERT (linear projection) model: 71.1,
SemEval 2015 evaluation for BERT (linear projection) model: 76.2,
Senseval 2 evaluation for BERT (linear projection) model: 75.5,
Senseval 3 evaluation for BERT (linear projection) model: 73.6
VOD (BioLinkBERT)                                                                                  Accuracy evaluation for VOD (BioLinkBERT) model: 55.0,
Dev Set (Acc-%) evaluation for VOD (BioLinkBERT) model: 0.583,
Test Set (Acc-%) evaluation for VOD (BioLinkBERT) model: 0.629
GlossBERT                                                                                  SemEval 2007 evaluation for GlossBERT model: 72.5,
SemEval 2013 evaluation for GlossBERT model: 76.1,
SemEval 2015 evaluation for GlossBERT model: 80.4,
Senseval 2 evaluation for GlossBERT model: 77.7,
Senseval 3 evaluation for GlossBERT model: 75.2
RoBERTa_large - (Separated Context-Response)                                                                                  F1 evaluation for RoBERTa_large - (Separated Context-Response) model: 0.716
BERT-large 345M                                                                                  Accuracy evaluation for BERT-large 345M model: 55.6
FinQANet (BERT-large)                                                                                  Execution Accuracy evaluation for FinQANet (BERT-large) model: 57.43,
Program Accuracy evaluation for FinQANet (BERT-large) model: 55.52
SBERT-STSb-base                                                                                  Spearman Correlation evaluation for SBERT-STSb-base model: 0.8479
ELASTIC (RoBERTa-large)                                                                                  Answer Accuracy evaluation for ELASTIC (RoBERTa-large) model: 83.00,
Execution Accuracy evaluation for ELASTIC (RoBERTa-large) model: 68.96,
Program Accuracy evaluation for ELASTIC (RoBERTa-large) model: 65.21
SMART_BERT (single model)                                                                                  Accuracy evaluation for SMART_BERT (single model) model: 0.3029
ELC-BERT-base 98M (zero init)                                                                                  Accuracy evaluation for ELC-BERT-base 98M (zero init) model: 63,
Matched evaluation for ELC-BERT-base 98M (zero init) model: 84.4,
Mismatched evaluation for ELC-BERT-base 98M (zero init) model: 84.5
RoBERTa-Base + SSP                                                                                  MAP evaluation for RoBERTa-Base + SSP model: 0.887,
MRR evaluation for RoBERTa-Base + SSP model: 0.899
JointBERT                                                                                  F1 (%) evaluation for JointBERT model: 75.83,
F1 (%) evaluation for JointBERT model: 77.55,
F1 (%) evaluation for JointBERT model: 83.44,
F1 (%) evaluation for JointBERT model: 97.09,
F1 (%) evaluation for JointBERT model: 97.49
ATHENA (roberta-large)                                                                                  Accuracy (%) evaluation for ATHENA (roberta-large) model: 93,
Accuracy (training-test) evaluation for ATHENA (roberta-large) model: 86.5,
Execution Accuracy evaluation for ATHENA (roberta-large) model: 54.8,
Execution Accuracy evaluation for ATHENA (roberta-large) model: 67.8,
Execution Accuracy evaluation for ATHENA (roberta-large) model: 91
BERT-pair-QA-M                                                                                  Aspect evaluation for BERT-pair-QA-M model: 86.4,
Sentiment evaluation for BERT-pair-QA-M model: 93.6
BERT-BASE                                                                                  F1 evaluation for BERT-BASE model: 92.4
Eider-BERT-base                                                                                  F1 evaluation for Eider-BERT-base model: 62.47,
Ign F1 evaluation for Eider-BERT-base model: 60.42
ViLBERT - OK-VQA                                                                                  DA VQA Score evaluation for ViLBERT - OK-VQA model: 9.2,
MC Accuracy evaluation for ViLBERT - OK-VQA model: 34.1
BERT2BERT                                                                                  BLEU-4 evaluation for BERT2BERT model: 21.26
BERT-Base 110M                                                                                  Accuracy evaluation for BERT-Base 110M model: 40.5
KnowBert-W+W                                                                                  F1 evaluation for KnowBert-W+W model: 71.5,
F1 evaluation for KnowBert-W+W model: 89.1
BERT (BASE)                                                                                   Wasserstein Distance (WD) evaluation for BERT (BASE) model: 89.5 ± .4,
# Correct Groups evaluation for BERT (BASE) model: 22 ± 2,
# Solved Walls evaluation for BERT (BASE) model: 0 ± 0,
Adjusted Mutual Information (AMI) evaluation for BERT (BASE) model: 8.1 ± .4,
Adjusted Rand Index (ARI) evaluation for BERT (BASE) model: 6.4 ± .3,
Fowlkes Mallows Score (FMS) evaluation for BERT (BASE) model: 25.1 ± .2
AristoRoBERTa + QA-GNN                                                                                  Accuracy evaluation for AristoRoBERTa + QA-GNN model: 82.8
M-BERT+Flair+Word+Char                                                                                  F1 evaluation for M-BERT+Flair+Word+Char model: 72.8,
F1 evaluation for M-BERT+Flair+Word+Char model: 72.9,
F1 evaluation for M-BERT+Flair+Word+Char model: 74.3
RoBERTa_large (Context-Response)                                                                                  F1 evaluation for RoBERTa_large (Context-Response) model: 0.772
BERT+CONCEPT FILTER                                                                                  NDCG evaluation for BERT+CONCEPT FILTER model: 0.25
Airbert                                                                                  error evaluation for Airbert model: 2.58,
length evaluation for Airbert model: 686.54,
oracle success evaluation for Airbert model: 0.99,
spl evaluation for Airbert model: 0.01,
success evaluation for Airbert model: 0.78
BERT (query + URL)                                                                                  F1-score evaluation for BERT (query + URL) model: 0.774,
Precision evaluation for BERT (query + URL) model: 0.789,
Recall evaluation for BERT (query + URL) model: 0.764
RefVOS with BERT + MLM loss                                                                                  Overall IoU evaluation for RefVOS with BERT + MLM loss model: 36.17,
Overall IoU evaluation for RefVOS with BERT + MLM loss model: 44.71,
Overall IoU evaluation for RefVOS with BERT + MLM loss model: 59.45
DRAGON + BioLinkBERT                                                                                  Accuracy evaluation for DRAGON + BioLinkBERT model: 47.5
Trans-Encoder-RoBERTa-large-bi (unsup.)                                                                                  Spearman Correlation evaluation for Trans-Encoder-RoBERTa-large-bi (unsup.) model: 0.8176,
Spearman Correlation evaluation for Trans-Encoder-RoBERTa-large-bi (unsup.) model: 0.8655
Tran-BERT-MS-ML-R                                                                                  Quadratic Weighted Kappa evaluation for Tran-BERT-MS-ML-R model: 0.791
TANDA DeBERTa-V3-Large + ALL                                                                                  MAP evaluation for TANDA DeBERTa-V3-Large + ALL model: 0.954,
MRR evaluation for TANDA DeBERTa-V3-Large + ALL model: 0.984
AV-HuBERT Large                                                                                  Word Error Rate (WER) evaluation for AV-HuBERT Large model: 1.3,
Word Error Rate (WER) evaluation for AV-HuBERT Large model: 1.4,
Word Error Rate (WER) evaluation for AV-HuBERT Large model: 26.9
CiteBERT                                                                                  Avg. evaluation for CiteBERT model: 58.8
NeRoBERTa                                                                                  ROUGE-1 evaluation for NeRoBERTa model: 43.86,
ROUGE-2 evaluation for NeRoBERTa model: 20.64,
ROUGE-L evaluation for NeRoBERTa model: 40.20
SciBERT                                                                                  Avg. evaluation for SciBERT model: 59.6,
F1 evaluation for SciBERT model: 70.98,
F1 evaluation for SciBERT model: 84.9,
F1 evaluation for SciBERT model: 86.32
CFER-BERT-base                                                                                  F1 evaluation for CFER-BERT-base model: 59.82,
Ign F1 evaluation for CFER-BERT-base model: 57.89
BERTbase-flow (NLI)                                                                                  Spearman Correlation evaluation for BERTbase-flow (NLI) model: 0.6544
U-MEM* + SpanBERT                                                                                  F1 evaluation for U-MEM* + SpanBERT model: 79.6
BERT-pair-QA-B                                                                                  Accuracy (3-way) evaluation for BERT-pair-QA-B model: 89.9,
Accuracy (4-way) evaluation for BERT-pair-QA-B model: 85.9,
Aspect evaluation for BERT-pair-QA-B model: 87.9,
Binary Accuracy evaluation for BERT-pair-QA-B model: 95.6,
Sentiment evaluation for BERT-pair-QA-B model: 93.3
BioBERT Base                                                                                  AUROC evaluation for BioBERT Base model: 71.59,
AUROC evaluation for BioBERT Base model: 82.55,
AUROC evaluation for BioBERT Base model: 82.81,
AUROC evaluation for BioBERT Base model: 86.36
DeBERTa-1.5B                                                                                  Accuracy evaluation for DeBERTa-1.5B model: 76.4,
Accuracy evaluation for DeBERTa-1.5B model: 90.4,
Accuracy evaluation for DeBERTa-1.5B model: 93.2%,
Accuracy evaluation for DeBERTa-1.5B model: 95.9,
Accuracy evaluation for DeBERTa-1.5B model: 96.8,
Accuracy evaluation for DeBERTa-1.5B model: 97.2,
EM evaluation for DeBERTa-1.5B model: 63.7,
EM evaluation for DeBERTa-1.5B model: 94.1,
F1 evaluation for DeBERTa-1.5B model: 88.2,
F1 evaluation for DeBERTa-1.5B model: 94.5,
F1 evaluation for DeBERTa-1.5B model: 94.9
CodeBERT(MLM)                                                                                  Go evaluation for CodeBERT(MLM) model: 83.31,
Go evaluation for CodeBERT(MLM) model: 90.79,
JS evaluation for CodeBERT(MLM) model: 81.77,
JS evaluation for CodeBERT(MLM) model: 86.4,
Java evaluation for CodeBERT(MLM) model: 80.63,
Java evaluation for CodeBERT(MLM) model: 90.46,
PHP evaluation for CodeBERT(MLM) model: 85.05,
PHP evaluation for CodeBERT(MLM) model: 88.21,
Python evaluation for CodeBERT(MLM) model: 82.2,
Python evaluation for CodeBERT(MLM) model: 87.21,
Ruby evaluation for CodeBERT(MLM) model: 80.17,
Ruby evaluation for CodeBERT(MLM) model: 86.84
BERT+Calculator (ensemble)                                                                                  F1 evaluation for BERT+Calculator (ensemble) model: 81.78
BERT-fused NMT                                                                                  BLEU evaluation for BERT-fused NMT model: 36.02,
BLEU evaluation for BERT-fused NMT model: 38.27,
BLEU score evaluation for BERT-fused NMT model: 30.75,
BLEU score evaluation for BERT-fused NMT model: 43.78
MUPPET Roberta Large                                                                                  Accuracy evaluation for MUPPET Roberta Large model: 79.2,
Accuracy evaluation for MUPPET Roberta Large model: 86.4,
Accuracy evaluation for MUPPET Roberta Large model: 87.5,
Accuracy evaluation for MUPPET Roberta Large model: 92.8%,
Accuracy evaluation for MUPPET Roberta Large model: 97.4
RuBERT                                                                                  F1 evaluation for RuBERT model: 84.6,
Weighted F1 evaluation for RuBERT model: 72.63
pucpr/biobertpt-all                                                                                  Micro F1 evaluation for pucpr/biobertpt-all model: 0.604
RoBERTa-large with LlamBERT                                                                                  Accuracy evaluation for RoBERTa-large with LlamBERT model: 96.68
BERTSUM+Transformer                                                                                  ROUGE-1 evaluation for BERTSUM+Transformer model: 43.25,
ROUGE-2 evaluation for BERTSUM+Transformer model: 20.24,
ROUGE-L evaluation for BERTSUM+Transformer model: 39.63
BERT-SumP                                                                                  nDCG@20 evaluation for BERT-SumP model: 0.467
AOM mBERT                                                                                  F1 evaluation for AOM mBERT model: 0.8549
kNN-BERT + POS (training corpus: SemCor)                                                                                  F1 evaluation for kNN-BERT + POS (training corpus: SemCor) model: 63.17
BERT+sent2vec                                                                                   R@10 evaluation for BERT+sent2vec  model: 0.89,
R@15 evaluation for BERT+sent2vec  model: 0.93,
R@20 evaluation for BERT+sent2vec  model: 0.95,
R@5 evaluation for BERT+sent2vec  model: 0.79
RoBERTa WWM Ext (News)                                                                                  Accuray evaluation for RoBERTa WWM Ext (News) model: 61.34,
F1-score evaluation for RoBERTa WWM Ext (News) model: 61.48,
Precision evaluation for RoBERTa WWM Ext (News) model: 61.97,
Recall evaluation for RoBERTa WWM Ext (News) model: 61.32
DateBERT                                                                                   Strict Detection (Pr.) evaluation for DateBERT model: 82.72,
 Strict Detection (Re.) evaluation for DateBERT model: 85.79,
Relaxed Detection (F1) evaluation for DateBERT model: 92.60,
Relaxed Detection (Pr.) evaluation for DateBERT model: 90.95,
Relaxed Detection (Re.) evaluation for DateBERT model: 94.35,
Strict Detection (F1) evaluation for DateBERT model:  84.21,
Type evaluation for DateBERT model: 86.21
RoBERTa (single model)                                                                                  Accuracy evaluation for RoBERTa (single model) model: 0.5021,
EM evaluation for RoBERTa (single model) model: 86.820,
F1 evaluation for RoBERTa (single model) model: 89.795
Megatron-BERT                                                                                  Accuracy (High) evaluation for Megatron-BERT model: 88.6,
Accuracy (Middle) evaluation for Megatron-BERT model: 91.8,
Accuracy evaluation for Megatron-BERT model: 89.5
RoBERTaMarian                                                                                  Accuracy evaluation for RoBERTaMarian model: 77.95,
BLEU Score evaluation for RoBERTaMarian model: 88.91,
BLEU evaluation for RoBERTaMarian model: 35.74,
Exact Match Accuracy evaluation for RoBERTaMarian model: 13.8
Approach0+ColBERT (fusion)                                                                                  MAP evaluation for Approach0+ColBERT (fusion) model: 0.215,
NDCG evaluation for Approach0+ColBERT (fusion) model: 0.447,
P@10 evaluation for Approach0+ColBERT (fusion) model: 0.252,
bpref evaluation for Approach0+ColBERT (fusion) model: 0.202
BERT-BiLSTM-context                                                                                  F1 evaluation for BERT-BiLSTM-context model: 48.02,
F1 evaluation for BERT-BiLSTM-context model: 69.84
Megatron-BERT (ensemble)                                                                                  Accuracy (High) evaluation for Megatron-BERT (ensemble) model: 90.0,
Accuracy (Middle) evaluation for Megatron-BERT (ensemble) model: 93.1,
Accuracy evaluation for Megatron-BERT (ensemble) model: 90.9
PFN (ALBERT XXL, no aggregation)                                                                                  NER Macro F1 evaluation for PFN (ALBERT XXL, no aggregation) model: 91.3,
RE+ Macro F1 evaluation for PFN (ALBERT XXL, no aggregation) model: 83.2
Pyramid + BERT                                                                                  F1 evaluation for Pyramid + BERT model: 79.19
SimCSE-BERT-base                                                                                  Spearman Correlation evaluation for SimCSE-BERT-base model: 0.8241
ReasonBERTB                                                                                  F1 evaluation for ReasonBERTB model: 37.2,
F1-Score evaluation for ReasonBERTB model: 22.4
mBERT                                                                                  % Test Accuracy evaluation for mBERT model: 83.38,
AVG evaluation for mBERT model: 59.6,
Accuracy evaluation for mBERT model: 91.23,
F1 evaluation for mBERT model: 69.56,
F1 evaluation for mBERT model: 74.96,
F1 evaluation for mBERT model: 77.57,
MCC evaluation for mBERT model: 0.15,
MCC evaluation for mBERT model: 0.36,
Question Answering evaluation for mBERT model: 53.8,
Sentence Retrieval evaluation for mBERT model: 47.7,
Sentence-pair Classification evaluation for mBERT model: 73.7,
Structured Prediction evaluation for mBERT model: 66.3
BERT-DPR 345M (0-shot)                                                                                  Accuracy evaluation for BERT-DPR 345M (0-shot) model: 51
Transformer + Pre-train with Pseudo Data + BERT                                                                                  GLEU evaluation for Transformer + Pre-train with Pseudo Data + BERT model: 62.0
VL-BERTBASE                                                                                  Accuracy evaluation for VL-BERTBASE model: 55.2,
Accuracy evaluation for VL-BERTBASE model: 71.16,
Accuracy evaluation for VL-BERTBASE model: 73.8,
Accuracy evaluation for VL-BERTBASE model: 74.4
G-DAUG-Combo + RoBERTa-Large                                                                                  Accuracy evaluation for G-DAUG-Combo + RoBERTa-Large model: 71.4,
Accuracy evaluation for G-DAUG-Combo + RoBERTa-Large model: 84.0
Bottom-up (RoBERTa)                                                                                  Standard Parseval (Full) evaluation for Bottom-up (RoBERTa) model: 41.4,
Standard Parseval (Full) evaluation for Bottom-up (RoBERTa) model: 53.7,
Standard Parseval (Nuclearity) evaluation for Bottom-up (RoBERTa) model: 55.5,
Standard Parseval (Nuclearity) evaluation for Bottom-up (RoBERTa) model: 66.5,
Standard Parseval (Relation) evaluation for Bottom-up (RoBERTa) model: 47.9,
Standard Parseval (Relation) evaluation for Bottom-up (RoBERTa) model: 55.4,
Standard Parseval (Span) evaluation for Bottom-up (RoBERTa) model: 73.2,
Standard Parseval (Span) evaluation for Bottom-up (RoBERTa) model: 76.1
CamemBERT-Large                                                                                  EM evaluation for CamemBERT-Large model: 82.1,
F1 evaluation for CamemBERT-Large model: 92.2
SMARTRoBERTa                                                                                  Accuracy evaluation for SMARTRoBERTa model: 92.0%,
Dev Accuracy evaluation for SMARTRoBERTa model: 89.2,
Dev Accuracy evaluation for SMARTRoBERTa model: 92.0,
Dev Accuracy evaluation for SMARTRoBERTa model: 95.6,
Dev Accuracy evaluation for SMARTRoBERTa model: 96.9,
Dev F1 evaluation for SMARTRoBERTa model: 92.1,
Dev Matched evaluation for SMARTRoBERTa model: 91.1,
Dev Mismatched evaluation for SMARTRoBERTa model: 91.3,
Dev Pearson Correlation evaluation for SMARTRoBERTa model: 92.8,
Dev Spearman Correlation evaluation for SMARTRoBERTa model: 92.6
VideoBERT (cross modal)                                                                                  Object Top 5 Accuracy evaluation for VideoBERT (cross modal) model: 33.7,
Object Top-1 Accuracy evaluation for VideoBERT (cross modal) model: 13.1,
Verb Top-1 Accuracy evaluation for VideoBERT (cross modal) model: 3.2,
Verb Top-5 Accuracy evaluation for VideoBERT (cross modal) model: 43.3
BERT large                                                                                  Accuracy evaluation for BERT large model: 65.83,
Accuracy evaluation for BERT large model: 95.49,
Accuracy evaluation for BERT large model: 97.37,
Error evaluation for BERT large model: 0.68,
Error evaluation for BERT large model: 1.89,
Error evaluation for BERT large model: 29.32
BioBERT                                                                                  Avg. evaluation for BioBERT model: 58.8,
F1 evaluation for BioBERT model: 0.8088,
F1 evaluation for BioBERT model: 76.46,
F1 evaluation for BioBERT model: 77.59,
F1 evaluation for BioBERT model: 89.71,
Micro F1 evaluation for BioBERT model: 80.88
RoBERTa+TDA                                                                                  Accuracy evaluation for RoBERTa+TDA model: 87.3%,
MCC evaluation for RoBERTa+TDA model: 0.695
BERTlarge (MIMIC)                                                                                  Exact Span F1 evaluation for BERTlarge (MIMIC) model: 90.25
BERT-LARGE (Ensemble+TriviaQA)                                                                                  EM evaluation for BERT-LARGE (Ensemble+TriviaQA) model: 86.2,
EM evaluation for BERT-LARGE (Ensemble+TriviaQA) model: 87.4,
F1 evaluation for BERT-LARGE (Ensemble+TriviaQA) model: 92.2,
F1 evaluation for BERT-LARGE (Ensemble+TriviaQA) model: 93.2
BERT large UDA                                                                                  Error evaluation for BERT large UDA model: 1.09
Noise-robust Co-regularization + BERT-large                                                                                  F1 evaluation for Noise-robust Co-regularization + BERT-large model: 73.0,
F1 evaluation for Noise-robust Co-regularization + BERT-large model: 94.04
ELECTRA and ALBERT                                                                                  Test evaluation for ELECTRA and ALBERT model: 71.0
PromCSE-RoBERTa-large (0.355B)                                                                                  Spearman Correlation evaluation for PromCSE-RoBERTa-large (0.355B) model: 0.7956,
Spearman Correlation evaluation for PromCSE-RoBERTa-large (0.355B) model: 0.8243,
Spearman Correlation evaluation for PromCSE-RoBERTa-large (0.355B) model: 0.8381,
Spearman Correlation evaluation for PromCSE-RoBERTa-large (0.355B) model: 0.8496,
Spearman Correlation evaluation for PromCSE-RoBERTa-large (0.355B) model: 0.8787,
Spearman Correlation evaluation for PromCSE-RoBERTa-large (0.355B) model: 0.8808,
Spearman Correlation evaluation for PromCSE-RoBERTa-large (0.355B) model: 0.8897,
avg ± std evaluation for PromCSE-RoBERTa-large (0.355B) model: 74.8± 1.0
RoBERTa-large 354M                                                                                  Accuracy evaluation for RoBERTa-large 354M model: 73.9
BERT-large 340M (fine-tuned)                                                                                  Accuracy evaluation for BERT-large 340M (fine-tuned) model: 64.5,
Accuracy evaluation for BERT-large 340M (fine-tuned) model: 66.8
superconductors-Scibert                                                                                  F1 evaluation for superconductors-Scibert model: 77.03,
Precision evaluation for superconductors-Scibert model: 73.69,
Recall evaluation for superconductors-Scibert model: 80.69
SA-BERT+BERT-FP                                                                                  MAP evaluation for SA-BERT+BERT-FP model: 0.701,
MRR evaluation for SA-BERT+BERT-FP model: 0.715,
NDCG@3 evaluation for SA-BERT+BERT-FP model: 0.674,
NDCG@5 evaluation for SA-BERT+BERT-FP model: 0.753,
P@1 evaluation for SA-BERT+BERT-FP model: 0.555,
R10@1 evaluation for SA-BERT+BERT-FP model: 0.497,
R10@2 evaluation for SA-BERT+BERT-FP model: 0.685,
R10@5 evaluation for SA-BERT+BERT-FP model: 0.931
RoBERTa Large                                                                                  Macro F1 evaluation for RoBERTa Large model: 77.06,
Neg. F1 evaluation for RoBERTa Large model: 87.89,
Pos. F1 evaluation for RoBERTa Large model: 66.23
PairGCN-BERT                                                                                  F1 evaluation for PairGCN-BERT model: 72.02
Sequence Labeling with edits using BERT, Faster inference                                                                                  F0.5 evaluation for Sequence Labeling with edits using BERT, Faster inference model: 61.2
Q-BERT (Shen et al., 2020)                                                                                  Accuracy evaluation for Q-BERT (Shen et al., 2020) model: 65.1,
Accuracy evaluation for Q-BERT (Shen et al., 2020) model: 84.7,
Accuracy evaluation for Q-BERT (Shen et al., 2020) model: 88.2,
Accuracy evaluation for Q-BERT (Shen et al., 2020) model: 93.0,
Accuracy evaluation for Q-BERT (Shen et al., 2020) model: 94.8,
Matched evaluation for Q-BERT (Shen et al., 2020) model: 87.8,
Pearson Correlation evaluation for Q-BERT (Shen et al., 2020) model: 0.911
EE + BERT-large                                                                                  Avg F1 evaluation for EE + BERT-large model: 76.61
Heinsen Routing + RoBERTa Large                                                                                  Accuracy evaluation for Heinsen Routing + RoBERTa Large model: 59.8,
Accuracy evaluation for Heinsen Routing + RoBERTa Large model: 96.2
CorefBERT-base                                                                                  F1 evaluation for CorefBERT-base model: 56.96,
Ign F1 evaluation for CorefBERT-base model: 54.54
VL-BERTLARGE                                                                                  Accuracy evaluation for VL-BERTLARGE model: 58.9,
Accuracy evaluation for VL-BERTLARGE model: 59.7,
Accuracy evaluation for VL-BERTLARGE model: 71.79,
Accuracy evaluation for VL-BERTLARGE model: 75.5,
Accuracy evaluation for VL-BERTLARGE model: 75.8,
Accuracy evaluation for VL-BERTLARGE model: 77.9,
Accuracy evaluation for VL-BERTLARGE model: 78.4,
overall evaluation for VL-BERTLARGE model: 72.2
TinyBERT (M=6;d'=768;d'i=3072)                                                                                  Accuracy evaluation for TinyBERT (M=6;d'=768;d'i=3072) model: 86.3,
Matched evaluation for TinyBERT (M=6;d'=768;d'i=3072) model: 84.5,
Mismatched evaluation for TinyBERT (M=6;d'=768;d'i=3072) model: 84.5
BioBERT (Lee et al.,2020)                                                                                  Dev Set (Acc-%) evaluation for BioBERT (Lee et al.,2020) model: 0.38,
Test Set (Acc-%) evaluation for BioBERT (Lee et al.,2020) model: 0.37
ALBERTxxlarge+DUMA(ensemble)                                                                                  Accuracy (High) evaluation for ALBERTxxlarge+DUMA(ensemble) model: 92.6,
Accuracy (Middle) evaluation for ALBERTxxlarge+DUMA(ensemble) model: 88.7,
Accuracy evaluation for ALBERTxxlarge+DUMA(ensemble) model: 89.8
Trans-Encoder-RoBERTa-large-cross (unsup.)                                                                                  Spearman Correlation evaluation for Trans-Encoder-RoBERTa-large-cross (unsup.) model: 0.7163,
Spearman Correlation evaluation for Trans-Encoder-RoBERTa-large-cross (unsup.) model: 0.7828,
Spearman Correlation evaluation for Trans-Encoder-RoBERTa-large-cross (unsup.) model: 0.8194,
Spearman Correlation evaluation for Trans-Encoder-RoBERTa-large-cross (unsup.) model: 0.8503,
Spearman Correlation evaluation for Trans-Encoder-RoBERTa-large-cross (unsup.) model: 0.867,
Spearman Correlation evaluation for Trans-Encoder-RoBERTa-large-cross (unsup.) model: 0.8831,
Spearman Correlation evaluation for Trans-Encoder-RoBERTa-large-cross (unsup.) model: 0.8863
BERT-Base cased (fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus')                                                                                  F1 evaluation for BERT-Base cased (fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus') model: 84.21,
Precision evaluation for BERT-Base cased (fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus') model: 83.36,
Recall evaluation for BERT-Base cased (fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus') model: 85.2
deberta-v3-base+tasksource                                                                                  Accuracy evaluation for deberta-v3-base+tasksource model: 87.15%
ALBERT (ensemble model)                                                                                  EM evaluation for ALBERT (ensemble model) model: 89.731,
F1 evaluation for ALBERT (ensemble model) model: 92.215
ChemBERTa-2 Fine-tuned                                                                                  AUC evaluation for ChemBERTa-2 Fine-tuned model: 0.793
BERTWiki-WSCR                                                                                  Accuracy evaluation for BERTWiki-WSCR model: 71.9,
Score evaluation for BERTWiki-WSCR model: 72.2
AutoBERT-Zero (Base)                                                                                  Accuracy evaluation for AutoBERT-Zero (Base) model: 90.5%,
Number of Params evaluation for AutoBERT-Zero (Base) model: 104M
RoBERTa-SupCon                                                                                  F1 (%) evaluation for RoBERTa-SupCon model: 57.23,
F1 (%) evaluation for RoBERTa-SupCon model: 79.28,
F1 (%) evaluation for RoBERTa-SupCon model: 79.99,
F1 (%) evaluation for RoBERTa-SupCon model: 94.29,
F1 (%) evaluation for RoBERTa-SupCon model: 95.21,
F1 (%) evaluation for RoBERTa-SupCon model: 98.33,
F1 Micro evaluation for RoBERTa-SupCon model: 88.63
DeBERTa-V3-Large + SSP                                                                                  MAP evaluation for DeBERTa-V3-Large + SSP model: 0.743,
MAP evaluation for DeBERTa-V3-Large + SSP model: 0.923,
MRR evaluation for DeBERTa-V3-Large + SSP model: 0.800,
MRR evaluation for DeBERTa-V3-Large + SSP model: 0.946
CorefBERT-large                                                                                  F1 evaluation for CorefBERT-large model: 58.83,
Ign F1 evaluation for CorefBERT-large model: 56.40
Multilingual BERT                                                                                  F1-score evaluation for Multilingual BERT model: 0.75
ColBERT model                                                                                  F1-score evaluation for ColBERT model model: 0.982
Seq2Rel (w/PubMedBERT)                                                                                  Exact Match F1 ("Any Combination") evaluation for Seq2Rel (w/PubMedBERT) model: 71.1,
Exact Match F1 ("Positive Combination") evaluation for Seq2Rel (w/PubMedBERT) model: 66.7
RoBERTa Large (translate test)                                                                                  Accuracy evaluation for RoBERTa Large (translate test) model: 76.05
RoBERTa-Winogrande-ft 355M (fine-tuned)                                                                                  Accuracy evaluation for RoBERTa-Winogrande-ft 355M (fine-tuned) model: 90.6
seq2seq+BERT+Flair                                                                                  F1 evaluation for seq2seq+BERT+Flair model: 78.31,
F1 evaluation for seq2seq+BERT+Flair model: 84.33,
F1 evaluation for seq2seq+BERT+Flair model: 84.40,
Multi-Task Supervision evaluation for seq2seq+BERT+Flair model: n
Fine-tuned HuBERT Large                                                                                  EER evaluation for Fine-tuned HuBERT Large model: 2.36
BioBERT (base)                                                                                  Accuracy evaluation for BioBERT (base) model: 34.1
RoBERTa WWM Ext (News+Factors)                                                                                  Accuray evaluation for RoBERTa WWM Ext (News+Factors) model: 62.49,
F1-score evaluation for RoBERTa WWM Ext (News+Factors) model: 62.54,
Precision evaluation for RoBERTa WWM Ext (News+Factors) model: 62.59,
Recall evaluation for RoBERTa WWM Ext (News+Factors) model: 62.51
BioLinkBert                                                                                  Macro F1 evaluation for BioLinkBert model: 0.77
RoBERTa+HyKAS Ma et al. (2019)                                                                                  Accuracy evaluation for RoBERTa+HyKAS Ma et al. (2019) model: 73.2
SciBERT uncased (SciVocab, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus')                                                                                  F1 evaluation for SciBERT uncased (SciVocab, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus') model: 89.3,
Precision evaluation for SciBERT uncased (SciVocab, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus') model: 87.99,
Recall evaluation for SciBERT uncased (SciVocab, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus') model: 90.78
Dual-Channel mBERT                                                                                  F1-score (Weighted) evaluation for Dual-Channel mBERT model: 0.650
BERTwiki 340M (fine-tuned on half of WSCR)                                                                                  Accuracy evaluation for BERTwiki 340M (fine-tuned on half of WSCR) model: 70.3
UDPipe 2.0 + mBERT + FLAIR                                                                                  LAS evaluation for UDPipe 2.0 + mBERT + FLAIR model: 84.60,
UAS evaluation for UDPipe 2.0 + mBERT + FLAIR model: 87.64
SciDeBERTa v2                                                                                  F1 evaluation for SciDeBERTa v2 model: 72.4
ATLOP-BERT-base                                                                                  F1 evaluation for ATLOP-BERT-base model: 61.30,
Ign F1 evaluation for ATLOP-BERT-base model: 59.31
ALBERT (Ensemble)                                                                                  Accuracy evaluation for ALBERT (Ensemble) model: 91.4
Custom Legal-BERT                                                                                  F1(10-fold) evaluation for Custom Legal-BERT model: 78.7,
F1(10-fold) evaluation for Custom Legal-BERT model: 97.4,
Macro F1 (10-fold) evaluation for Custom Legal-BERT model: 69.5
ATHENA (roberta-base)                                                                                  Accuracy (%) evaluation for ATHENA (roberta-base) model: 92.2,
Accuracy (training-test) evaluation for ATHENA (roberta-base) model: 84.4,
Execution Accuracy evaluation for ATHENA (roberta-base) model: 45.6,
Execution Accuracy evaluation for ATHENA (roberta-base) model: 52.5,
Execution Accuracy evaluation for ATHENA (roberta-base) model: 86.4
BERTWSCR                                                                                  Accuracy evaluation for BERTWSCR model: 70.5,
Score evaluation for BERTWSCR model: 70.3
TinyBERT (M=6;d' =768;d'i=3072)                                                                                  Accuracy evaluation for TinyBERT (M=6;d' =768;d'i=3072) model: 54,
EM evaluation for TinyBERT (M=6;d' =768;d'i=3072) model: 69.9,
EM evaluation for TinyBERT (M=6;d' =768;d'i=3072) model: 79.7,
F1 evaluation for TinyBERT (M=6;d' =768;d'i=3072) model: 73.4,
F1 evaluation for TinyBERT (M=6;d' =768;d'i=3072) model: 87.5
Phraseformer(BERT, Node2vec)                                                                                  F1 score evaluation for Phraseformer(BERT, Node2vec) model: 47.46,
F1 score evaluation for Phraseformer(BERT, Node2vec) model: 65.94,
F1 score evaluation for Phraseformer(BERT, Node2vec) model: 68.68
RoBERTa (Large)                                                                                  A1 evaluation for RoBERTa (Large) model: 72.4,
A2 evaluation for RoBERTa (Large) model: 49.8,
A3 evaluation for RoBERTa (Large) model: 44.4
MAPO + TABERTLarge (K = 3)                                                                                  Accuracy (Dev) evaluation for MAPO + TABERTLarge (K = 3) model: 52.2,
Accuracy (Test) evaluation for MAPO + TABERTLarge (K = 3) model: 51.8,
Exact Match Accuracy (Dev) evaluation for MAPO + TABERTLarge (K = 3) model: 64.5
w2v-BERT XXL                                                                                  Word Error Rate (WER) evaluation for w2v-BERT XXL model: 1.4,
Word Error Rate (WER) evaluation for w2v-BERT XXL model: 2.5
BERT-Base-uncased-PruneOFA (90% unstruct sparse)                                                                                  EM evaluation for BERT-Base-uncased-PruneOFA (90% unstruct sparse) model: 79.83,
F1 evaluation for BERT-Base-uncased-PruneOFA (90% unstruct sparse) model: 87.25,
Matched evaluation for BERT-Base-uncased-PruneOFA (90% unstruct sparse) model: 81.45,
Mismatched evaluation for BERT-Base-uncased-PruneOFA (90% unstruct sparse) model: 82.43
Hierarchical + BERT                                                                                  F1 evaluation for Hierarchical + BERT model: 90.30,
F1 evaluation for Hierarchical + BERT model: 93.37
Mirror-BERT-base (unsup.)                                                                                  Spearman Correlation evaluation for Mirror-BERT-base (unsup.) model: 0.674,
Spearman Correlation evaluation for Mirror-BERT-base (unsup.) model: 0.703,
Spearman Correlation evaluation for Mirror-BERT-base (unsup.) model: 0.713,
Spearman Correlation evaluation for Mirror-BERT-base (unsup.) model: 0.743,
Spearman Correlation evaluation for Mirror-BERT-base (unsup.) model: 0.764,
Spearman Correlation evaluation for Mirror-BERT-base (unsup.) model: 0.796,
Spearman Correlation evaluation for Mirror-BERT-base (unsup.) model: 0.814
Two-tower Bi-Encoder (RoBERTa)                                                                                  Recall@100 evaluation for Two-tower Bi-Encoder (RoBERTa) model: 74.78,
Recall@200 evaluation for Two-tower Bi-Encoder (RoBERTa) model: 78.04,
Recall@500 evaluation for Two-tower Bi-Encoder (RoBERTa) model: 83.39
DistilBERT 66M                                                                                  Accuracy evaluation for DistilBERT 66M model: 44.4,
Accuracy evaluation for DistilBERT 66M model: 49.1%,
Accuracy evaluation for DistilBERT 66M model: 62.9%,
Accuracy evaluation for DistilBERT 66M model: 89.2%,
Accuracy evaluation for DistilBERT 66M model: 90.2%,
Accuracy evaluation for DistilBERT 66M model: 91.3,
Accuracy evaluation for DistilBERT 66M model: 92.82,
F1 evaluation for DistilBERT 66M model: 85.8,
Pearson Correlation evaluation for DistilBERT 66M model: 0.907
SAPar + BERT                                                                                  F1 score evaluation for SAPar + BERT model: 92.66
Zero-shot-BERT-SORT                                                                                  1:1 Accuracy evaluation for Zero-shot-BERT-SORT model: +55%
SpERT.PL (SciBERT)                                                                                  Cross Sentence evaluation for SpERT.PL (SciBERT) model: No,
Entity F1 evaluation for SpERT.PL (SciBERT) model: 70.53,
Relation F1 evaluation for SpERT.PL (SciBERT) model: 51.25
BERT, AC Pretraining                                                                                  Accuracy (%) evaluation for BERT, AC Pretraining model: 99.4
BERTje                                                                                  Accuracy evaluation for BERTje model: 93%
Head-Driven Phrase Structure Grammar Parsing (Joint) + BERT                                                                                  F1 score evaluation for Head-Driven Phrase Structure Grammar Parsing (Joint) + BERT model: 95.84
En-BERT + TDA + PCA                                                                                  Accuracy evaluation for En-BERT + TDA + PCA model: 88.6%
IS-BERT-NLI                                                                                  Spearman Correlation evaluation for IS-BERT-NLI model: 0.5677,
Spearman Correlation evaluation for IS-BERT-NLI model: 0.6121,
Spearman Correlation evaluation for IS-BERT-NLI model: 0.6425,
Spearman Correlation evaluation for IS-BERT-NLI model: 0.6921,
Spearman Correlation evaluation for IS-BERT-NLI model: 0.6924,
Spearman Correlation evaluation for IS-BERT-NLI model: 0.7016,
Spearman Correlation evaluation for IS-BERT-NLI model: 0.7523
BERT-E2E-ABSA                                                                                  F1 evaluation for BERT-E2E-ABSA model: 61.12
MobileBERT                                                                                  Accuracy evaluation for MobileBERT model: 88.8%,
Number of Params evaluation for MobileBERT model: 25.3M
SemEHR+WS (rules+BlueBERT) with tuning number of training data                                                                                  F1 evaluation for SemEHR+WS (rules+BlueBERT) with tuning number of training data model: 0.711,
F1 evaluation for SemEHR+WS (rules+BlueBERT) with tuning number of training data model: 0.861,
F1 evaluation for SemEHR+WS (rules+BlueBERT) with tuning number of training data model: 0.907
SpanBERT-large                                                                                  F1 evaluation for SpanBERT-large model: 70.8
DG-SpanBERT-large                                                                                  F1 evaluation for DG-SpanBERT-large model: 71.5
BERTScore                                                                                  Score evaluation for BERTScore model: 10.47
Ru-BERT+TDA                                                                                  Accuracy evaluation for Ru-BERT+TDA model: 80.1,
MCC evaluation for Ru-BERT+TDA model: 0.478
RoBERTa-DPR 355M (0-shot)                                                                                  Accuracy evaluation for RoBERTa-DPR 355M (0-shot) model: 58.9
ActBERT-revised                                                                                  CIDEr evaluation for ActBERT-revised model: 74.71,
ROUGE-L evaluation for ActBERT-revised model: 28.15,
SPICE evaluation for ActBERT-revised model: 19.52
BERT-single-large                                                                                  F1 (%) evaluation for BERT-single-large model: 78.8
BERT (none)                                                                                  Full F1 (Preps) evaluation for BERT (none) model: 70.9,
Function F1 (Preps) evaluation for BERT (none) model: 81.0,
Role F1 (Preps) evaluation for BERT (none) model: 71.9,
Tags (Full) Acc evaluation for BERT (none) model: 82.0
SciBert (Finetune)                                                                                  F1 evaluation for SciBert (Finetune) model: 83.64
DeBERTa (large)                                                                                  Accuracy evaluation for DeBERTa (large) model: 69.5,
Accuracy evaluation for DeBERTa (large) model: 92.3%,
Accuracy evaluation for DeBERTa (large) model: 92.5,
Accuracy evaluation for DeBERTa (large) model: 95.3%,
Accuracy evaluation for DeBERTa (large) model: 96.5,
Matched evaluation for DeBERTa (large) model: 91.1,
Mismatched evaluation for DeBERTa (large) model: 91.1
DPR+RoBERTa-base-crossencoder-reranker                                                                                  Precision@100 evaluation for DPR+RoBERTa-base-crossencoder-reranker model: 88.03,
Precision@20 evaluation for DPR+RoBERTa-base-crossencoder-reranker model: 84.46
BERT-base 110M (fine-tuned on WSCR)                                                                                  Accuracy evaluation for BERT-base 110M (fine-tuned on WSCR) model: 62.3,
Accuracy evaluation for BERT-base 110M (fine-tuned on WSCR) model: 70.5
N-ary semi-markov + BERT-large                                                                                  F1 score evaluation for N-ary semi-markov + BERT-large model: 95.92
OUT2 + IN3 + USE + BERT                                                                                  Micro-F1 evaluation for OUT2 + IN3 + USE + BERT model: 0.7731
DistilBERT-base                                                                                  HONEST evaluation for DistilBERT-base model: 1.90
Li and Choi - RoBERTa                                                                                  EM evaluation for Li and Choi - RoBERTa model: 53.5,
F1 evaluation for Li and Choi - RoBERTa model: 69.6
SciBERT (mean pooling / no preprocessing)                                                                                  Entity F1 (partial) evaluation for SciBERT (mean pooling / no preprocessing) model: 41.21,
Relation F1 evaluation for SciBERT (mean pooling / no preprocessing) model: 32.28
It-BERT (pre-trained) + TDA                                                                                  Accuracy evaluation for It-BERT (pre-trained) + TDA model: 89.2,
MCC evaluation for It-BERT (pre-trained) + TDA model: 0.478
RoBERTa-Winogrande 355M (fine-tuned)                                                                                  Accuracy evaluation for RoBERTa-Winogrande 355M (fine-tuned) model: 79.1,
Accuracy evaluation for RoBERTa-Winogrande 355M (fine-tuned) model: 84.4
BERT+DK                                                                                  Accuracy evaluation for BERT+DK model: 0.591,
Macro-F1 evaluation for BERT+DK model: 0.549,
Micro-F1 evaluation for BERT+DK model: 0.713
AlephBERT-base MultiTask                                                                                  F1 evaluation for AlephBERT-base MultiTask model: 79.15
ParsBERT                                                                                  % Test Accuracy evaluation for ParsBERT model: 82.99
RoBERTa-large + G-DAug-Inf                                                                                  Accuracy evaluation for RoBERTa-large + G-DAug-Inf model: 80
BERT-base 110M                                                                                  Accuracy evaluation for BERT-base 110M model: 53.1,
Accuracy evaluation for BERT-base 110M model: 56.5
OpenShape-PointBERT                                                                                  Accuracy (%) evaluation for OpenShape-PointBERT model: 85.3,
Need 3D Data? evaluation for OpenShape-PointBERT model: Yes
k-RoBERTa (parallel)                                                                                  MSE evaluation for k-RoBERTa (parallel) model: 0.05,
R^2 evaluation for k-RoBERTa (parallel) model: 0.71
TinyBERT                                                                                  Accuracy evaluation for TinyBERT model: 43.3%,
Accuracy evaluation for TinyBERT model: 62.9%,
Accuracy evaluation for TinyBERT model: 86.4%,
Accuracy evaluation for TinyBERT model: 87.7%,
Accuracy evaluation for TinyBERT model: 92.6,
F1 evaluation for TinyBERT model: 71.3,
Matched evaluation for TinyBERT model: 82.5,
Mismatched evaluation for TinyBERT model: 81.8,
Pearson Correlation evaluation for TinyBERT model: 0.799
Phraseformer(BERT, DeepWalk)                                                                                  F1 score evaluation for Phraseformer(BERT, DeepWalk) model: 47.22,
F1 score evaluation for Phraseformer(BERT, DeepWalk) model: 65.70,
F1 score evaluation for Phraseformer(BERT, DeepWalk) model: 68.44
ChemBERTa-2 (MTR-77M)                                                                                  Molecules (M) evaluation for ChemBERTa-2 (MTR-77M) model: 77,
RMSE evaluation for ChemBERTa-2 (MTR-77M) model: 0.798,
RMSE evaluation for ChemBERTa-2 (MTR-77M) model: 0.889,
RMSE evaluation for ChemBERTa-2 (MTR-77M) model: 1.363,
RMSE evaluation for ChemBERTa-2 (MTR-77M) model: 48.515,
ROC-AUC evaluation for ChemBERTa-2 (MTR-77M) model: 56.3,
ROC-AUC evaluation for ChemBERTa-2 (MTR-77M) model: 72.8,
ROC-AUC evaluation for ChemBERTa-2 (MTR-77M) model: 79.9
Wiki+RoBERTa                                                                                   F1 evaluation for Wiki+RoBERTa  model: 63.2,
F1@10 evaluation for Wiki+RoBERTa  model: 9.4,
P@50K evaluation for Wiki+RoBERTa  model:  96.5,
P@5K evaluation for Wiki+RoBERTa  model: 99.0,
Precision evaluation for Wiki+RoBERTa  model: 60.9,
Recall evaluation for Wiki+RoBERTa  model: 64.5,
Recall evaluation for Wiki+RoBERTa  model: 65.6
ImageBERT                                                                                  Image-to-text R@1 evaluation for ImageBERT model: 44.0,
Image-to-text R@1 evaluation for ImageBERT model: 70.7,
Image-to-text R@10 evaluation for ImageBERT model: 80.4,
Image-to-text R@10 evaluation for ImageBERT model: 94.0,
Image-to-text R@5 evaluation for ImageBERT model: 71.2,
Image-to-text R@5 evaluation for ImageBERT model: 90.2,
Text-to-image R@1 evaluation for ImageBERT model: 32.3,
Text-to-image R@1 evaluation for ImageBERT model: 54.3,
Text-to-image R@10 evaluation for ImageBERT model: 70.2,
Text-to-image R@10 evaluation for ImageBERT model: 87.5,
Text-to-image R@5 evaluation for ImageBERT model: 59.0,
Text-to-image R@5 evaluation for ImageBERT model: 79.6
BERTwwm + SQuAD 2                                                                                  F1 evaluation for BERTwwm + SQuAD 2 model: 68.2
g2pM (BERT)                                                                                  Accuracy evaluation for g2pM (BERT) model: 97.85
RoBERTa-large + self-explaining layer                                                                                  % Test Accuracy evaluation for RoBERTa-large + self-explaining layer model: 92.3,
% Train Accuracy evaluation for RoBERTa-large + self-explaining layer model: ?,
Parameters evaluation for RoBERTa-large + self-explaining layer model: 355m+
G2GT SpanBERT-large reduced                                                                                  F1 evaluation for G2GT SpanBERT-large reduced model: 80.5
Second-best learning and decoding + BERT                                                                                  F1 evaluation for Second-best learning and decoding + BERT model: 77.05,
F1 evaluation for Second-best learning and decoding + BERT model: 83.99,
F1 evaluation for Second-best learning and decoding + BERT model: 84.97
TANDA-DeBERTa-V3-Large + ALL                                                                                  MAP evaluation for TANDA-DeBERTa-V3-Large + ALL model: 0.927,
MRR evaluation for TANDA-DeBERTa-V3-Large + ALL model: 0.939
XLM RoBERTa                                                                                  GMB BPSN evaluation for XLM RoBERTa model: 0.8859,
Micro F1 evaluation for XLM RoBERTa model: 0.468,
Precision evaluation for XLM RoBERTa model: 0.3135,
Recall evaluation for XLM RoBERTa model: 0.923
Top-down (BERT)                                                                                  Standard Parseval (Full) evaluation for Top-down (BERT) model: 30.9,
Standard Parseval (Full) evaluation for Top-down (BERT) model: 46.6,
Standard Parseval (Nuclearity) evaluation for Top-down (BERT) model: 44.6,
Standard Parseval (Nuclearity) evaluation for Top-down (BERT) model: 59.1,
Standard Parseval (Relation) evaluation for Top-down (BERT) model: 37.6,
Standard Parseval (Relation) evaluation for Top-down (BERT) model: 48.3,
Standard Parseval (Span) evaluation for Top-down (BERT) model: 65.3,
Standard Parseval (Span) evaluation for Top-down (BERT) model: 69.8
TinyBERT-6 67M                                                                                  Accuracy evaluation for TinyBERT-6 67M model: 54,
Accuracy evaluation for TinyBERT-6 67M model: 66%,
Accuracy evaluation for TinyBERT-6 67M model: 86.3,
Accuracy evaluation for TinyBERT-6 67M model: 87.3%,
Accuracy evaluation for TinyBERT-6 67M model: 90.4%,
Accuracy evaluation for TinyBERT-6 67M model: 93.1,
EM evaluation for TinyBERT-6 67M model: 69.9,
EM evaluation for TinyBERT-6 67M model: 79.7,
F1 evaluation for TinyBERT-6 67M model: 73.4,
F1 evaluation for TinyBERT-6 67M model: 87.5,
Matched evaluation for TinyBERT-6 67M model: 84.5,
Matched evaluation for TinyBERT-6 67M model: 84.6,
Mismatched evaluation for TinyBERT-6 67M model: 83.2,
Mismatched evaluation for TinyBERT-6 67M model: 84.5
24hBERT                                                                                  Accuracy evaluation for 24hBERT model: 57.1,
Accuracy evaluation for 24hBERT model: 57.7%,
Accuracy evaluation for 24hBERT model: 70.7,
Accuracy evaluation for 24hBERT model: 87.5%,
Accuracy evaluation for 24hBERT model: 90.6,
Accuracy evaluation for 24hBERT model: 93.0,
Matched evaluation for 24hBERT model: 84.4,
Mismatched evaluation for 24hBERT model: 83.8,
Pearson Correlation evaluation for 24hBERT model: 0.820
CharacterBERT (base, medical, ensemble)                                                                                  Pearson Correlation evaluation for CharacterBERT (base, medical, ensemble) model: 85.62
CodeBERT (RTD)                                                                                  Smoothed BLEU-4 evaluation for CodeBERT (RTD) model: 12.72,
Smoothed BLEU-4 evaluation for CodeBERT (RTD) model: 15.03,
Smoothed BLEU-4 evaluation for CodeBERT (RTD) model: 20.25,
Smoothed BLEU-4 evaluation for CodeBERT (RTD) model: 26.02,
Smoothed BLEU-4 evaluation for CodeBERT (RTD) model: 8.73
TinyBERT-4 14.5M                                                                                  Accuracy evaluation for TinyBERT-4 14.5M model: 43.3%,
Accuracy evaluation for TinyBERT-4 14.5M model: 62.9%,
Accuracy evaluation for TinyBERT-4 14.5M model: 86.4%,
Accuracy evaluation for TinyBERT-4 14.5M model: 87.7%,
Accuracy evaluation for TinyBERT-4 14.5M model: 92.6,
Matched evaluation for TinyBERT-4 14.5M model: 82.5,
Mismatched evaluation for TinyBERT-4 14.5M model: 81.8,
Pearson Correlation evaluation for TinyBERT-4 14.5M model: 0.799
Uni-Enc+BERT-FP                                                                                  MAP evaluation for Uni-Enc+BERT-FP model: 0.648,
MRR evaluation for Uni-Enc+BERT-FP model: 0.688,
P@1 evaluation for Uni-Enc+BERT-FP model: 0.518,
R10@1 evaluation for Uni-Enc+BERT-FP model: 0.327,
R10@1 evaluation for Uni-Enc+BERT-FP model: 0.916,
R10@2 evaluation for Uni-Enc+BERT-FP model: 0.557,
R10@2 evaluation for Uni-Enc+BERT-FP model: 0.965,
R10@5 evaluation for Uni-Enc+BERT-FP model: 0.865,
R10@5 evaluation for Uni-Enc+BERT-FP model: 0.994
DialogueRNN-BERT                                                                                  Macro F1 (w/o Neutral) evaluation for DialogueRNN-BERT model: 52.15,
Macro F1 evaluation for DialogueRNN-BERT model: 57.10,
Weighted F1 (w/o Neutral) evaluation for DialogueRNN-BERT model: 75.50,
Weighted F1 evaluation for DialogueRNN-BERT model: 83.41
DocBert [DOCBERT]                                                                                  Accuracy evaluation for DocBert [DOCBERT] model: 91.95
BERT-Attention                                                                                  Top-1 Recall evaluation for BERT-Attention model: 13.8,
Top-3 Recall evaluation for BERT-Attention model: 40.6,
Top-5 Recall evaluation for BERT-Attention model: 61.2
DUAL+BERT-base                                                                                  F1 evaluation for DUAL+BERT-base model: 57.74
R2+1D-BERT                                                                                  3-fold Accuracy evaluation for R2+1D-BERT model: 98.69,
Average accuracy of 3 splits evaluation for R2+1D-BERT model: 85.10
LSTM Seq2Seq with RoBERTa                                                                                  Accuracy evaluation for LSTM Seq2Seq with RoBERTa model: 40.3,
Execution Accuracy evaluation for LSTM Seq2Seq with RoBERTa model: 40.3,
Execution Accuracy evaluation for LSTM Seq2Seq with RoBERTa model: 76.9
BioLinkBERT (340 M)                                                                                  Accuracy evaluation for BioLinkBERT (340 M) model: 45.1
CapsNet-BERT-DR                                                                                  Acc evaluation for CapsNet-BERT-DR model: 82.970
ViHealthBERT                                                                                  F1 (%) evaluation for ViHealthBERT model: 96.7
ComplEx-RoBERTa                                                                                  Test MRR evaluation for ComplEx-RoBERTa model: 0.7186,
Validation MRR evaluation for ComplEx-RoBERTa model: 0.7052
ActBERT                                                                                  Frame accuracy evaluation for ActBERT model: 57.0
ALBERT                                                                                  Accuracy evaluation for ALBERT model: 69.1%,
Accuracy evaluation for ALBERT model: 89.2%,
Accuracy evaluation for ALBERT model: 90.5%,
Accuracy evaluation for ALBERT model: 91.8,
Accuracy evaluation for ALBERT model: 91.8%,
Accuracy evaluation for ALBERT model: 93.0,
Accuracy evaluation for ALBERT model: 93.4%,
Accuracy evaluation for ALBERT model: 97.1,
Accuracy evaluation for ALBERT model: 99.2%,
F1 evaluation for ALBERT model: 93.0,
F1 score evaluation for ALBERT model: 77.1,
Matched evaluation for ALBERT model: 91.3,
Pearson Correlation evaluation for ALBERT model: 0.925
RoBERTa+DualCL                                                                                  Accuracy evaluation for RoBERTa+DualCL model: 94.91,
Accuracy evaluation for RoBERTa+DualCL model: 97.34,
Error evaluation for RoBERTa+DualCL model: 2.60
SuPar-BERTweet                                                                                  Labelled Attachment Score evaluation for SuPar-BERTweet model: 83.4,
Unlabeled Attachment Score evaluation for SuPar-BERTweet model: 87.2 
Vanilla BERT                                                                                  P@20 evaluation for Vanilla BERT model: 0.4042,
nDCG@20 evaluation for Vanilla BERT model: 0.4541
SciBERT (active learning)                                                                                  Exact Span F1 evaluation for SciBERT (active learning) model: 66.4
elEmBERT-V1                                                                                  AUC evaluation for elEmBERT-V1 model: 0.778,
AUC evaluation for elEmBERT-V1 model: 0.856,
AUC evaluation for elEmBERT-V1 model: 0.905,
AUC evaluation for elEmBERT-V1 model: 0.961
BERT-Base uncased (fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus')                                                                                  F1 evaluation for BERT-Base uncased (fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus') model: 86.8,
Precision evaluation for BERT-Base uncased (fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus') model: 85.76,
Recall evaluation for BERT-Base uncased (fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus') model: 88.15
Trans-Encoder-RoBERTa-base-cross (unsup.)                                                                                  Spearman Correlation evaluation for Trans-Encoder-RoBERTa-base-cross (unsup.) model: 0.7637,
Spearman Correlation evaluation for Trans-Encoder-RoBERTa-base-cross (unsup.) model: 0.7903,
Spearman Correlation evaluation for Trans-Encoder-RoBERTa-base-cross (unsup.) model: 0.8377,
Spearman Correlation evaluation for Trans-Encoder-RoBERTa-base-cross (unsup.) model: 0.8465,
Spearman Correlation evaluation for Trans-Encoder-RoBERTa-base-cross (unsup.) model: 0.8577
BFCR + SpanBERT + Transfer Learning                                                                                  CoNLL F1 evaluation for BFCR + SpanBERT + Transfer Learning model: 61.4
Dense-CCNet-BERTbase                                                                                  F1 evaluation for Dense-CCNet-BERTbase model: 62.55,
Ign F1 evaluation for Dense-CCNet-BERTbase model: 60.46
caw-coref + RoBERTa                                                                                  F1 evaluation for caw-coref + RoBERTa model: 81.6
CorefDRE- BERT                                                                                  F1 evaluation for CorefDRE- BERT model: 60.82,
Ign F1 evaluation for CorefDRE- BERT model: 60.78
TaBERT                                                                                  Weighted-F1 evaluation for TaBERT model: 97.2
bert-base                                                                                  Micro F1 evaluation for bert-base model: 73.2,
P@5 evaluation for bert-base model: 68.7,
RP@5 evaluation for bert-base model: 79.6,
nDCG@5 evaluation for bert-base model: 82.3
Bi-LSTM-CRF + Flair Embeddings + CamemBERT (oscar−138gb−base) Embeddings                                                                                  Weighted Average F1-score evaluation for Bi-LSTM-CRF + Flair Embeddings + CamemBERT (oscar−138gb−base) Embeddings model: 97.98
Qbert Rainbow+SEER                                                                                  Score evaluation for Qbert Rainbow+SEER model: 4123.5
RoBERTa-large+Self-Explaining                                                                                  % Test Accuracy evaluation for RoBERTa-large+Self-Explaining model: 92.3,
Accuracy evaluation for RoBERTa-large+Self-Explaining model: 59.1,
Parameters evaluation for RoBERTa-large+Self-Explaining model: 340
CompactBioBERT                                                                                  F1 evaluation for CompactBioBERT model: 79.88,
F1 evaluation for CompactBioBERT model: 85.38,
F1 evaluation for CompactBioBERT model: 86.71,
F1 evaluation for CompactBioBERT model: 88.67,
F1 evaluation for CompactBioBERT model: 94.31
DeBERTa-pair-large                                                                                  F1 (%) evaluation for DeBERTa-pair-large model: 80.9
BioBERT (pre-trained on PubMed abstracts + PMC, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus')                                                                                  F1 evaluation for BioBERT (pre-trained on PubMed abstracts + PMC, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus') model: 89.75,
Precision evaluation for BioBERT (pre-trained on PubMed abstracts + PMC, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus') model: 88.93,
Recall evaluation for BioBERT (pre-trained on PubMed abstracts + PMC, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, original corpus') model: 90.76
ViLBERT base                                                                                  Group Score evaluation for ViLBERT base model: 4.75,
Image Score evaluation for ViLBERT base model: 7.25,
Text Score evaluation for ViLBERT base model: 23.75
RoBERTa-DPR 355M                                                                                  Accuracy evaluation for RoBERTa-DPR 355M model: 83.1
BERTMarian                                                                                  Accuracy evaluation for BERTMarian model: 76.68,
BLEU Score evaluation for BERTMarian model: 56.55,
BLEU evaluation for BERTMarian model: 32.46,
Exact Match Accuracy evaluation for BERTMarian model: 12.40
BERT (Input: Body)                                                                                  Accuracy evaluation for BERT (Input: Body) model: 0.9203
DeBERTalarge                                                                                  Accuracy evaluation for DeBERTalarge model: 86.8,
EM evaluation for DeBERTalarge model: 88.0,
F1 evaluation for DeBERTalarge model: 90.7,
Test evaluation for DeBERTalarge model: 90.8
Phraseformer(BERT, ExEm(ft))                                                                                  F1 score evaluation for Phraseformer(BERT, ExEm(ft)) model: 48.65,
F1 score evaluation for Phraseformer(BERT, ExEm(ft)) model: 67.13,
F1 score evaluation for Phraseformer(BERT, ExEm(ft)) model: 69.87
BERT (large)                                                                                  Average F1 evaluation for BERT (large) model: 78.5,
ICAT Score evaluation for BERT (large) model: 69.89
ELC-BERT-small 24M                                                                                  Accuracy evaluation for ELC-BERT-small 24M model: 55.4,
Accuracy evaluation for ELC-BERT-small 24M model: 76.1,
Matched evaluation for ELC-BERT-small 24M model: 79.2,
Mismatched evaluation for ELC-BERT-small 24M model: 79.9
SimCSE-BERT-sup                                                                                  Accuracy evaluation for SimCSE-BERT-sup model: 67.32,
Spearman Correlation evaluation for SimCSE-BERT-sup model: 23.31,
Spearman Correlation evaluation for SimCSE-BERT-sup model: 79.12,
V-Measure evaluation for SimCSE-BERT-sup model: 33.43
BERT-IL Finetuned                                                                                  Restaurant (Acc) evaluation for BERT-IL Finetuned model: 86.20,
Restaurant (Acc) evaluation for BERT-IL Finetuned model: 88.70
RoBERTa Base                                                                                  Exact Span F1 evaluation for RoBERTa Base model: 32.63,
F1 evaluation for RoBERTa Base model: 75.45,
F1(Neg) evaluation for RoBERTa Base model: 85.85,
F1(Pos) evaluation for RoBERTa Base model: 58.17,
Macro F1 evaluation for RoBERTa Base model: 76.51,
Neg. F1 evaluation for RoBERTa Base model: 88.74,
Pos. F1 evaluation for RoBERTa Base model: 64.28
Ensemble ALBERT                                                                                  F1 evaluation for Ensemble ALBERT model: 90.123
UMS_BERT+                                                                                  MAP evaluation for UMS_BERT+ model: 0.625,
MRR evaluation for UMS_BERT+ model: 0.664,
P@1 evaluation for UMS_BERT+ model: 0.499,
R10@1 evaluation for UMS_BERT+ model: 0.318,
R10@1 evaluation for UMS_BERT+ model: 0.762,
R10@1 evaluation for UMS_BERT+ model: 0.875,
R10@2 evaluation for UMS_BERT+ model: 0.482,
R10@2 evaluation for UMS_BERT+ model: 0.905,
R10@2 evaluation for UMS_BERT+ model: 0.942,
R10@5 evaluation for UMS_BERT+ model: 0.858,
R10@5 evaluation for UMS_BERT+ model: 0.986,
R10@5 evaluation for UMS_BERT+ model: 0.988
Sieve-based+SapBERT                                                                                  F1-score (strict) evaluation for Sieve-based+SapBERT model: 0.8275
S-BERT                                                                                  Fact-F1 evaluation for S-BERT model: 13.65
RoBERTa-large 355M (0-shot)                                                                                  Accuracy evaluation for RoBERTa-large 355M (0-shot) model: 50
PubMedBERT-CRF                                                                                  F1 evaluation for PubMedBERT-CRF model: 89.3
BERT (LARGE)                                                                                   Wasserstein Distance (WD) evaluation for BERT (LARGE) model: 88.3 ± .5,
# Correct Groups evaluation for BERT (LARGE) model: 33 ± 2,
# Solved Walls evaluation for BERT (LARGE) model: 0 ± 0,
Adjusted Mutual Information (AMI) evaluation for BERT (LARGE) model: 10.3 ± .3,
Adjusted Rand Index (ARI) evaluation for BERT (LARGE) model: 8.2 ± .3,
Fowlkes Mallows Score (FMS) evaluation for BERT (LARGE) model:  26.5 ± .2
kNN-BERT                                                                                  F1 evaluation for kNN-BERT model: 60.94,
F1 evaluation for kNN-BERT model: 76.52,
F1 evaluation for kNN-BERT model: 80.12,
F1 evaluation for kNN-BERT model: 81.20
BERT-based Ensembles                                                                                  F1 score evaluation for BERT-based Ensembles model: 0.558
BERT (nearest neighbour)                                                                                  SemEval 2007 evaluation for BERT (nearest neighbour) model: 63.3,
SemEval 2013 evaluation for BERT (nearest neighbour) model: 69.2,
SemEval 2015 evaluation for BERT (nearest neighbour) model: 74.4,
Senseval 2 evaluation for BERT (nearest neighbour) model: 73.8,
Senseval 3 evaluation for BERT (nearest neighbour) model: 71.6
DialogueCRN+RoBERTa                                                                                  Accuracy evaluation for DialogueCRN+RoBERTa model: 66.93,
Accuracy evaluation for DialogueCRN+RoBERTa model: 67.39,
Micro-F1 evaluation for DialogueCRN+RoBERTa model: 41.04,
Weighted-F1 evaluation for DialogueCRN+RoBERTa model: 38.79,
Weighted-F1 evaluation for DialogueCRN+RoBERTa model: 65.77,
Weighted-F1 evaluation for DialogueCRN+RoBERTa model: 67.53
RoBERTa-Twitter                                                                                  ALL evaluation for RoBERTa-Twitter model: 61.0,
Emoji evaluation for RoBERTa-Twitter model: 29.3,
Emotion evaluation for RoBERTa-Twitter model: 72.0,
Hate evaluation for RoBERTa-Twitter model: 49.9,
Irony evaluation for RoBERTa-Twitter model: 65.4,
Offensive evaluation for RoBERTa-Twitter model: 77.1,
Sentiment evaluation for RoBERTa-Twitter model: 69.1,
Stance evaluation for RoBERTa-Twitter model: 66.7
R-BERT-CNN                                                                                  F1 (micro) evaluation for R-BERT-CNN model: 55.67
AristoRoBERTa                                                                                  Accuracy evaluation for AristoRoBERTa model: 77.8
HySPA (ours) w/ RoBERTa                                                                                  Relation F1 evaluation for HySPA (ours) w/ RoBERTa model: 68.2
RoBERTa.base                                                                                  Accuracy evaluation for RoBERTa.base model: 95.79
MotionBERT (Finetune)                                                                                  2D detector evaluation for MotionBERT (Finetune) model: SH,
Accuracy evaluation for MotionBERT (Finetune) model: 67.4%,
Average MPJPE (mm) evaluation for MotionBERT (Finetune) model: 16.9,
Average MPJPE (mm) evaluation for MotionBERT (Finetune) model: 37.5,
Frames Needed evaluation for MotionBERT (Finetune) model: 243,
MPJPE evaluation for MotionBERT (Finetune) model: 76.9,
MPVPE evaluation for MotionBERT (Finetune) model: 88.1,
Multi-View or Monocular evaluation for MotionBERT (Finetune) model: Monocular,
Need Ground Truth 2D Pose evaluation for MotionBERT (Finetune) model: No,
PA-MPJPE evaluation for MotionBERT (Finetune) model: 47.2,
Use Video Sequence evaluation for MotionBERT (Finetune) model: Yes,
Using 2D ground-truth joints evaluation for MotionBERT (Finetune) model: Yes
PtrNet Decoder (BERT-base)                                                                                  F1 (strict) evaluation for PtrNet Decoder (BERT-base) model: 89.6,
F1 evaluation for PtrNet Decoder (BERT-base) model: 89.6
LTG-BERT-base 98M                                                                                  Accuracy evaluation for LTG-BERT-base 98M model: 54.7,
Accuracy evaluation for LTG-BERT-base 98M model: 82.7,
Matched evaluation for LTG-BERT-base 98M model: 83,
Mismatched evaluation for LTG-BERT-base 98M model: 83.4
SciBERT-ATLOPBASE                                                                                  F1 evaluation for SciBERT-ATLOPBASE model: 69.4,
F1 evaluation for SciBERT-ATLOPBASE model: 83.9
BertSumExtAbs                                                                                  ROUGE-1 evaluation for BertSumExtAbs model: 38.81,
ROUGE-1 evaluation for BertSumExtAbs model: 42.13,
ROUGE-2 evaluation for BertSumExtAbs model: 16.50,
ROUGE-2 evaluation for BertSumExtAbs model: 19.6,
ROUGE-3 evaluation for BertSumExtAbs model: 31.27,
ROUGE-L evaluation for BertSumExtAbs model: 39.18
BERTSQG                                                                                  BLEU-4 evaluation for BERTSQG model: 22.17
BERT-Base + LSTM                                                                                  Accuracy evaluation for BERT-Base + LSTM model: 96.60
BERT-SparseLT + CONTaiNER                                                                                  10 way 1~2 shot evaluation for BERT-SparseLT + CONTaiNER model: 52.75,
10 way 5~10 shot evaluation for BERT-SparseLT + CONTaiNER model: 62.43,
5 way 1~2 shot evaluation for BERT-SparseLT + CONTaiNER model: 57.14,
5 way 5~10 shot evaluation for BERT-SparseLT + CONTaiNER model: 66.17
BERT-large 340M                                                                                  Accuracy evaluation for BERT-large 340M model: 61.4,
Accuracy evaluation for BERT-large 340M model: 62.0,
Accuracy evaluation for BERT-large 340M model: 65.1,
Accuracy evaluation for BERT-large 340M model: 65.5,
Accuracy evaluation for BERT-large 340M model: 67,
Accuracy evaluation for BERT-large 340M model: 70.1%,
Accuracy evaluation for BERT-large 340M model: 78.3,
Accuracy evaluation for BERT-large 340M model: 80.8
Roberta-large                                                                                  1:1 Accuracy evaluation for Roberta-large model: 78.65
DeBERTa-Large 304M                                                                                  Accuracy evaluation for DeBERTa-Large 304M model: 80.2,
Accuracy evaluation for DeBERTa-Large 304M model: 87.4,
Accuracy evaluation for DeBERTa-Large 304M model: 94.7
SignBERT                                                                                  Top-1 Accuracy evaluation for SignBERT model: 83.30
BERT + Keep Learning                                                                                  1 in 10 R@1 evaluation for BERT + Keep Learning model: 82.4
wl-coref + RoBERTa                                                                                  Avg F1 evaluation for wl-coref + RoBERTa model: 81.0,
F1 evaluation for wl-coref + RoBERTa model: 81
RATSQL + BERT                                                                                  Accuracy evaluation for RATSQL + BERT model: 65.6
BERT-base 110M + MAS                                                                                  Accuracy evaluation for BERT-base 110M + MAS model: 60.3,
Accuracy evaluation for BERT-base 110M + MAS model: 68.3
BERT-ext + abs + RL + rerank                                                                                  ROUGE-1 evaluation for BERT-ext + abs + RL + rerank model: 41.90,
ROUGE-2 evaluation for BERT-ext + abs + RL + rerank model: 19.08,
ROUGE-L evaluation for BERT-ext + abs + RL + rerank model: 39.64
BioM-BERT                                                                                  F1 evaluation for BioM-BERT model: 80.0
BABERT-LE                                                                                  F1 evaluation for BABERT-LE model: 96.84,
F1 evaluation for BABERT-LE model: 97.56,
F1 evaluation for BABERT-LE model: 98.63
Skeleton-Aware BERT                                                                                  F1 evaluation for Skeleton-Aware BERT model: 90.36
Constituent Tree Parsing (+BERT)                                                                                  English-20K (open) F1 evaluation for Constituent Tree Parsing (+BERT) model: 76.7,
English-Wiki (open) F1 evaluation for Constituent Tree Parsing (+BERT) model: 80.5
DocuNet-SciBERTbase                                                                                  F1 evaluation for DocuNet-SciBERTbase model: 76.3,
F1 evaluation for DocuNet-SciBERTbase model: 85.3
ViT-B/16 + BERT base + ViLEM                                                                                  Text Score evaluation for ViT-B/16 + BERT base + ViLEM model: 36.5
BERT + Dep-GCN                                                                                  F1 evaluation for BERT + Dep-GCN model: 48.71
AraBERTv1                                                                                  Accuracy evaluation for AraBERTv1 model: 86.7,
Accuracy evaluation for AraBERTv1 model: 93.8,
Accuracy evaluation for AraBERTv1 model: 96.1
BERT-hLSTMs                                                                                  CIDEr evaluation for BERT-hLSTMs model: 8.37
LLaVA-7B (BERTScore)                                                                                  Group Score evaluation for LLaVA-7B (BERTScore) model: 2.25,
Image Score evaluation for LLaVA-7B (BERTScore) model: 5.25,
Text Score evaluation for LLaVA-7B (BERTScore) model: 13.50
HuBERT-B-LS960 (e2e approach)                                                                                  F1 (%) evaluation for HuBERT-B-LS960 (e2e approach) model: 48.0,
F1 (%) evaluation for HuBERT-B-LS960 (e2e approach) model: 49.8,
Recall (%)	 evaluation for HuBERT-B-LS960 (e2e approach) model: 47.5,
Text model evaluation for HuBERT-B-LS960 (e2e approach) model: -,
label-F1 (%) evaluation for HuBERT-B-LS960 (e2e approach) model: 62.9
CodeBERT                                                                                  Accuracy (C#→Java) evaluation for CodeBERT model: 58,
Accuracy (Java→C#) evaluation for CodeBERT model: 59,
Accuracy (medium) evaluation for CodeBERT model: 0.052,
Accuracy (small) evaluation for CodeBERT model: 0.164,
Accuracy evaluation for CodeBERT model: 47.8,
Average Accuracy evaluation for CodeBERT model: 61.72,
Average F1 evaluation for CodeBERT model: 59.57,
Average Precision evaluation for CodeBERT model: 59.34,
Average Recall evaluation for CodeBERT model: 59.80,
BLEU (C#→Java) evaluation for CodeBERT model: 72.14,
BLEU (Java→C#) evaluation for CodeBERT model: 79.92,
BLEU (medium) evaluation for CodeBERT model: 91.07,
BLEU (small) evaluation for CodeBERT model: 77.42,
BLEU-4 evaluation for CodeBERT model: 91.70,
CodeBLEU (C#→Java) evaluation for CodeBERT model: 79.41,
CodeBLEU (Java→C#) evaluation for CodeBERT model: 85.1,
CodeBLEU (medium) evaluation for CodeBERT model: 87.52,
CodeBLEU (small) evaluation for CodeBERT model: 75.58,
Exact Match Accuracy evaluation for CodeBERT model: 89.75,
F1 evaluation for CodeBERT model: 58.95,
Go evaluation for CodeBERT model: 69.3,
JS evaluation for CodeBERT model: 74.8,
Java evaluation for CodeBERT model: 86.8,
MRR evaluation for CodeBERT model: 27.19,
Overall evaluation for CodeBERT model: 76.0,
PHP evaluation for CodeBERT model: 70.6,
Python evaluation for CodeBERT model: 84.0,
Ruby evaluation for CodeBERT model: 70.6
BERT-ext + RL                                                                                  ROUGE-1 evaluation for BERT-ext + RL model: 42.76,
ROUGE-2 evaluation for BERT-ext + RL model: 19.87,
ROUGE-L evaluation for BERT-ext + RL model: 39.11
Wav2Seq (from HuBERT-large)                                                                                  F1 (%) evaluation for Wav2Seq (from HuBERT-large) model: 65.4
PubMedBERT+MLP+CRF                                                                                  F1-score (strict) evaluation for PubMedBERT+MLP+CRF model: 0.8454,
F1-score (strict) evaluation for PubMedBERT+MLP+CRF model: 0.8731
BERT$^{s}$                                                                                  Accuracy (%) evaluation for BERT$^{s}$ model: 67.23,
Accuracy of Agreeableness evaluation for BERT$^{s}$ model: 85.76,
Accuracy of Conscientiousness evaluation for BERT$^{s}$ model: 63.60,
Accuracy of Extraversion evaluation for BERT$^{s}$ model: 78.08,
Accuracy of Neurotism evaluation for BERT$^{s}$ model: 50.75,
Accuracy of Openness evaluation for BERT$^{s}$ model: 57.93,
Macro-F1 evaluation for BERT$^{s}$ model: 72.93
SciBERT uncased (SciVocab, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus')                                                                                  F1 evaluation for SciBERT uncased (SciVocab, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus') model: 91.51,
Precision evaluation for SciBERT uncased (SciVocab, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus') model: 91.3,
Recall evaluation for SciBERT uncased (SciVocab, fine-tuned on 'Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus') model: 91.79
ClinicalBERT                                                                                  Exact Span F1 evaluation for ClinicalBERT model: 87.4
RoBERTa                                                                                  Accuracy (High) evaluation for RoBERTa model: 81.3,
Accuracy (Middle) evaluation for RoBERTa model: 86.5,
Accuracy evaluation for RoBERTa model: 0.779,
Accuracy evaluation for RoBERTa model: 67.8%,
Accuracy evaluation for RoBERTa model: 83.2,
Accuracy evaluation for RoBERTa model: 88.2%,
Accuracy evaluation for RoBERTa model: 89%,
Accuracy evaluation for RoBERTa model: 89.42,
Accuracy evaluation for RoBERTa model: 90.2%,
Accuracy evaluation for RoBERTa model: 92.3%,
Accuracy evaluation for RoBERTa model: 95.3,
Accuracy evaluation for RoBERTa model: 96.7,
Accuracy evaluation for RoBERTa model: 98.9%,
Accuracy evaluation for RoBERTa model: 99.76,
Average Accuracy evaluation for RoBERTa model: 59.84,
Average F1 evaluation for RoBERTa model: 57.54,
Average Precision evaluation for RoBERTa model: 57.45,
Average Recall evaluation for RoBERTa model: 57.62,
CaseHOLD evaluation for RoBERTa model: 71.7,
ECtHR Task A evaluation for RoBERTa model: 69.5 / 60.7,
ECtHR Task B evaluation for RoBERTa model: 87.2 / 77.3,
EUR-LEX evaluation for RoBERTa model: 71.8 / 57.5,
F1 evaluation for RoBERTa model: 71.3,
F1 evaluation for RoBERTa model: 88.7,
F1 evaluation for RoBERTa model: 91.59,
F1 evaluation for RoBERTa model: 95.3,
F1-score (Weighted) evaluation for RoBERTa model: 78.13,
Hits@1 evaluation for RoBERTa model: 22.5,
LEDGAR evaluation for RoBERTa model: 87.9 / 82.1,
Matched evaluation for RoBERTa model: 90.8,
Mismatched evaluation for RoBERTa model: 90.2,
Pearson Correlation evaluation for RoBERTa model: 0.922,
ROUGE-1 evaluation for RoBERTa model: 63.22,
ROUGE-2 evaluation for RoBERTa model: 51.34,
ROUGE-L evaluation for RoBERTa model: 60.26,
SCOTUS evaluation for RoBERTa model: 70.8 / 61.2,
Smoothed BLEU-4 evaluation for RoBERTa model: 13.2,
Smoothed BLEU-4 evaluation for RoBERTa model: 14.52,
Smoothed BLEU-4 evaluation for RoBERTa model: 14.92,
Smoothed BLEU-4 evaluation for RoBERTa model: 19.9,
Smoothed BLEU-4 evaluation for RoBERTa model: 26.09,
Smoothed BLEU-4 evaluation for RoBERTa model: 5.72,
Smoothed BLEU-4 evaluation for RoBERTa model: 7.26,
Test evaluation for RoBERTa model: 89.9,
UNFAIR-ToS evaluation for RoBERTa model: 87.7 / 81.5,
Weighted Average F1-score evaluation for RoBERTa model: 0.93
DocuNet-RoBERTa-large                                                                                  F1 evaluation for DocuNet-RoBERTa-large model: 64.55,
Ign F1 evaluation for DocuNet-RoBERTa-large model: 62.4

914 Rows. -- 11729 msec.
