Result of /data/leuven/370/vsc37064/new_queries_named_graph_openml/query_15.txt:
OpenLink Virtuoso Interactive SQL (Virtuoso)
Version 07.20.3240 as of Mar 11 2025
Type HELP; for help and EXIT; to exit.
Connected to OpenLink Virtuoso
Driver: 07.20.3240 OpenLink Virtuoso ODBC Driver
datasetName                                                                       description
LONG VARCHAR                                                                      LONG VARCHAR
_______________________________________________________________________________

BNG(audiology,1000,1)                                                                                  
BNG(audiology,1000,10)                                                                                  
BNG(audiology,1000,5)                                                                                  
BNG(audiology,5000,1)                                                                                  
BNG(audiology,5000,10)                                                                                  
BNG(audiology,5000,5)                                                                                  
BNG(satellite_image)                                                                                  
BNG(satimage)                                                                                  
FIFA20-Players-Dataset-with-Stats--Images                                                                                  About Dataset This data set includes15K Fifa20 Players with 15+ features and their images , including their position, age, and Country, and many more. It can be used for learning Statistics, Performing Data Analysis, and Data Visualization using various libraries like Seaborn, Pandas-Bokeh, and Plotly. It can be used to plot various Plots to understand the comparison between various features. References Sofifa.com
RelevantImagesDatasetTEST                                                                                  Testing dataset
The-Social-Dilemma-Tweets---Text-Classification                                                                                  Context The Social Dilemma, a documentary-drama hybrid explores the dangerous human impact of social networking, with tech experts sounding the alarm on their own creations as the tech experts sound the alarm on the dangerous human impact of social networking. Initial release: January 2020 Director: Jeff Orlowski Producer: Larissa Rhodes Music director: Mark A. Crawford Screenplay: Jeff Orlowski, Vickie Curtis, Davis Coombe Content This dataset brings you the twitter responses made with the TheSocialDilemma hashtag after watching the eye-opening documentary 'The Social Dilemma' released in an OTT platform(Netflix) on September 9th, 2020. The dataset was extracted using TwitterAPI, consisting of nearly 10,526 tweets from twitter users all over the globe! No Columns Descriptions 1 user_name The name of the user, as theyve defined it. 2 user_location The user-defined location for this accounts profile. 3 user_description The user-defined UTF-8 string describing their account. 4 user_created Time and date, when the account was created. 5 user_followers The number of followers an account currently has. 6 user_friends The number of friends an account currently has. 7 user_favourites The number of favorites a account currently has 8 user_verified When true, indicates that the user has a verified account 9 date UTC time and date when the Tweet was created 10 text The actual UTF-8 text of the Tweet 11 hashtags All the other hashtags posted in the tweet along with TheSocialDilemma 12 source Utility used to post the Tweet, Tweets from the Twitter website have a source value - web 13 is_retweet Indicates whether this Tweet has been Retweeted by the authenticating user. 14 Sentiment(Target variable) Indicates the sentiment of the tweet, consists of three categories: Positive, neutral, and negative Inspiration You can use this data to dive into the subjects that use this hashtag, look to the geographical distribution, evaluate sentiments, looks to trends.
audiology                                                                                  **Author**: Professor Jergen at Baylor College of Medicine **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Audiology+(Standardized)) **Please cite**: Bareiss, E. Ray, & Porter, Bruce (1987). Protos: An Exemplar-Based Learning Apprentice. In the Proceedings of the 4th International Workshop on Machine Learning, 12-23, Irvine, CA: Morgan Kaufmann **Audiology Database** This database is a standardized version of the original audiology database (see audiology.* in this directory). The non-standard set of attributes have been converted to a standard set of attributes according to the rules that follow. * Each property that appears anywhere in the original .data or .test file has been represented as a separate attribute in this file. * A property such as age_gt_60 is represented as a boolean attribute with values f and t. * In most cases, a property of the form x(y) is represented as a discrete attribute x() whose possible values are the various y's; air() is an example. There are two exceptions: ** when only one value of y appears anywhere, e.g. static(normal). In this case, x_y appears as a boolean attribute. ** when one case can have two or more values of x, e.g. history(..). All possible values of history are treated as separate boolean attributes. * Since boolean attributes only appear as positive conditions, each boolean attribute is assumed to be false unless noted as true. The value of multi-value discrete attributes taken as unknown ('?') unless a value is specified. * The original case identifications, p1 to p200 in the .data file and t1 to t26 in the .test file, have been added as a unique identifier attribute. [Note: in the original .data file, p165 has a repeated specification of o_ar_c(normal); p166 has repeated specification of speech(normal) and conflicting values air(moderate) and air(mild). No other problems with the original data were noted.] ### Attribute Information: age_gt_60: f, t. air(): mild,moderate,severe,normal,profound. airBoneGap: f, t. ar_c(): normal,elevated,absent. ar_u(): normal,absent,elevated. bone(): mild,moderate,normal,unmeasured. boneAbnormal: f, t. bser(): normal,degraded. history_buzzing: f, t. history_dizziness: f, t. history_fluctuating: f, t. history_fullness: f, t. history_heredity: f, t. history_nausea: f, t. history_noise: f, t. history_recruitment: f, t. history_ringing: f, t. history_roaring: f, t. history_vomiting: f, t. late_wave_poor: f, t. m_at_2k: f, t. m_cond_lt_1k: f, t. m_gt_1k: f, t. m_m_gt_2k: f, t. m_m_sn: f, t. m_m_sn_gt_1k: f, t. m_m_sn_gt_2k: f, t. m_m_sn_gt_500: f, t. m_p_sn_gt_2k: f, t. m_s_gt_500: f, t. m_s_sn: f, t. m_s_sn_gt_1k: f, t. m_s_sn_gt_2k: f, t. m_s_sn_gt_3k: f, t. m_s_sn_gt_4k: f, t. m_sn_2_3k: f, t. m_sn_gt_1k: f, t. m_sn_gt_2k: f, t. m_sn_gt_3k: f, t. m_sn_gt_4k: f, t. m_sn_gt_500: f, t. m_sn_gt_6k: f, t. m_sn_lt_1k: f, t. m_sn_lt_2k: f, t. m_sn_lt_3k: f, t. middle_wave_poor: f, t. mod_gt_4k: f, t. mod_mixed: f, t. mod_s_mixed: f, t. mod_s_sn_gt_500: f, t. mod_sn: f, t. mod_sn_gt_1k: f, t. mod_sn_gt_2k: f, t. mod_sn_gt_3k: f, t. mod_sn_gt_4k: f, t. mod_sn_gt_500: f, t. notch_4k: f, t. notch_at_4k: f, t. o_ar_c(): normal,elevated,absent. o_ar_u(): normal,absent,elevated. s_sn_gt_1k: f, t. s_sn_gt_2k: f, t. s_sn_gt_4k: f, t. speech(): normal,good,very_good,very_poor,poor,unmeasured. static_normal: f, t. tymp(): a,as,b,ad,c. viith_nerve_signs: f, t. wave_V_delayed: f, t. waveform_ItoV_prolonged: f, t. indentifier (unique for each instance) class: cochlear_unknown,mixed_cochlear_age_fixation,poss_central mixed_cochlear_age_otitis_media,mixed_poss_noise_om, cochlear_age,normal_ear,cochlear_poss_noise,cochlear_age_and_noise, acoustic_neuroma,mixed_cochlear_unk_ser_om,conductive_discontinuity, retrocochlear_unknown,conductive_fixation,bells_palsy, cochlear_noise_and_heredity,mixed_cochlear_unk_fixation, otitis_media,possible_menieres,possible_brainstem_disorder, cochlear_age_plus_poss_menieres,mixed_cochlear_age_s_om, mixed_cochlear_unk_discontinuity,mixed_poss_central_om
audiology                                                                                  **Author**: **Source**: Unknown - Date unknown **Please cite**: Binarized version of the original data set (see version 1). The multi-class target feature is converted to a two-class nominal target feature by re-labeling the majority class as positive ('P') and all others as negative ('N'). Originally converted by Quan Sun.
image                                                                                  Multi-label dataset. The image benchmark dataset consists of 2000 natural scene images. Zhou and Zhang (2007) extracted 135 features for each image and made it publicly available as processed image dataset. Each observation can be associated with different label sets, where all possible labels are {desert, mountains, sea, sunset, trees}. About 22% of the images belong to more than one class. However, images belonging to three classes or more are very rare.
image                                                                                  Multi-label dataset. The image benchmark dataset consists of 2000 natural scene images. Zhou and Zhang (2007) extracted 135 features for each image and made it publicly available as processed image dataset. Each observation can be associated with different label sets, where all possible labels are {desert, mountains, sea, sunset, trees}. About 22% of the images belong to more than one class. However, images belonging to three classes or more are very rare.
image                                                                                  Multi-label dataset. The image benchmark dataset consists of 2000 natural scene images. Zhou and Zhang (2007) extracted 135 features for each image and made it publicly available as processed image dataset. Each observation can be associated with different label sets, where all possible labels are {desert, mountains, sea, sunset, trees}. About 22% of the images belong to more than one class. However, images belonging to three classes or more are very rare.
one-hundred-plants-texture                                                                                  **Author**: James Cope, Thibaut Beghin, Paolo Remagnino, Sarah Barman. **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/One-hundred+plant+species+leaves+data+set) - 2010 **Please cite**: Charles Mallah, James Cope, James Orwell. Plant Leaf Classification Using Probabilistic Integration of Shape, Texture and Margin Features. Signal Processing, Pattern Recognition and Applications, in press. 2013. ### Description One-hundred plant species leaves dataset (Class = Texture). ### Sources ``` (a) Original owners of colour Leaves Samples: James Cope, Thibaut Beghin, Paolo Remagnino, Sarah Barman. The colour images are not included. The Leaves were collected in the Royal Botanic Gardens, Kew, UK. email: james.cope@kingston.ac.uk (b) This dataset consists of work carried out by James Cope, Charles Mallah, and James Orwell. Donor of database Charles Mallah: charles.mallah@kingston.ac.uk; James Cope: james.cope@kingston.ac.uk ``` ### Dataset Information The original data directory contains the binary images (masks) of the leaf samples (colour images not included). There are three features for each image: Shape, Margin and Texture. For each feature, a 64 element vector is given per leaf sample. These vectors are taken as a contiguous descriptor (for shape) or histograms (for texture and margin). So, there are three different files, one for each feature problem: * 'data_Sha_64.txt' -> prediction based on shape * 'data_Tex_64.txt' -> prediction based on texture [**dataset provided here**] * 'data_Mar_64.txt' -> prediction based on margin Each row has a 64-element feature vector followed by the Class label. There is a total of 1600 samples with 16 samples per leaf class (100 classes), and no missing values. ### Attributes Information Three 64 element feature vectors per sample. ### Relevant Papers Charles Mallah, James Cope, James Orwell. Plant Leaf Classification Using Probabilistic Integration of Shape, Texture and Margin Features. Signal Processing, Pattern Recognition and Applications, in press. J. Cope, P. Remagnino, S. Barman, and P. Wilkin. Plant texture classification using gabor co-occurrences. Advances in Visual Computing, pages 699-677, 2010. T. Beghin, J. Cope, P. Remagnino, and S. Barman. Shape and texture based plant leaf classification. In: Advanced Concepts for Intelligent Vision Systems, pages 345-353. Springer, 2010.
satellite_image                                                                                  **Author**: **Source**: Unknown - 1993 **Please cite**: Source: Ashwin Srinivasan Department of Statistics and Data Modeling University of Strathclyde Glasgow Scotland UK ross '@' uk.ac.turing The original Landsat data for this database was generated from data purchased from NASA by the Australian Centre for Remote Sensing, and used for research at: The Centre for Remote Sensing University of New South Wales Kensington, PO Box 1 NSW 2033 Australia. The sample database was generated taking a small section (82 rows and 100 columns) from the original data. The binary values were converted to their present ASCII form by Ashwin Srinivasan. The classification for each pixel was performed on the basis of an actual site visit by Ms. Karen Hall, when working for Professor John A. Richards, at the Centre for Remote Sensing at the University of New South Wales, Australia. Conversion to 3x3 neighbourhoods and splitting into test and training sets was done by Alistair Sutherland. Data Set Information: The database consists of the multi-spectral values of pixels in 3x3 neighbourhoods in a satellite image, and the classification associated with the central pixel in each neighbourhood. The aim is to predict this classification, given the multi-spectral values. In the sample database, the class of a pixel is coded as a number. The Landsat satellite data is one of the many sources of information available for a scene. The interpretation of a scene by integrating spatial data of diverse types and resolutions including multispectral and radar data, maps indicating topography, land use etc. is expected to assume significant importance with the onset of an era characterised by integrative approaches to remote sensing (for example, NASA's Earth Observing System commencing this decade). Existing statistical methods are ill-equipped for handling such diverse data types. Note that this is not true for Landsat MSS data considered in isolation (as in this sample database). This data satisfies the important requirements of being numerical and at a single resolution, and standard maximum-likelihood classification performs very well. Consequently, for this data, it should be interesting to compare the performance of other methods against the statistical approach. One frame of Landsat MSS imagery consists of four digital images of the same scene in different spectral bands. Two of these are in the visible region (corresponding approximately to green and red regions of the visible spectrum) and two are in the (near) infra-red. Each pixel is a 8-bit binary word, with 0 corresponding to black and 255 to white. The spatial resolution of a pixel is about 80m x 80m. Each image contains 2340 x 3380 such pixels. The database is a (tiny) sub-area of a scene, consisting of 82 x 100 pixels. Each line of data corresponds to a 3x3 square neighbourhood of pixels completely contained within the 82x100 sub-area. Each line contains the pixel values in the four spectral bands (converted to ASCII) of each of the 9 pixels in the 3x3 neighbourhood and a number indicating the classification label of the central pixel. The number is a code for the following classes: Number Class 1 red soil 2 cotton crop 3 grey soil 4 damp grey soil 5 soil with vegetation stubble 6 mixture class (all types present) 7 very damp grey soil NB. There are no examples with class 6 in this dataset. The data is given in random order and certain lines of data have been removed so you cannot reconstruct the original image from this dataset. In each line of data the four spectral values for the top-left pixel are given first followed by the four spectral values for the top-middle pixel and then those for the top-right pixel, and so on with the pixels read out in sequence left-to-right and top-to-bottom. Thus, the four spectral values for the central pixel are given by attributes 17,18,19 and 20. If you like you can use only these four attributes, while ignoring the others. This avoids the problem which arises when a 3x3 neighbourhood straddles a boundary. Attribute Information: The attributes are numerical, in the range 0 to 255. UCI: http://archive.ics.uci.edu/ml/datasets/Statlog+(Landsat+Satellite)
satimage                                                                                  **Author**: Ashwin Srinivasan, Department of Statistics and Data Modeling, University of Strathclyde **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Statlog+(Landsat+Satellite)) - 1993 **Please cite**: [UCI](https://archive.ics.uci.edu/ml/citation_policy.html) The database consists of the multi-spectral values of pixels in 3x3 neighbourhoods in a satellite image, and the classification associated with the central pixel in each neighbourhood. The aim is to predict this classification, given the multi-spectral values. In the sample database, the class of a pixel is coded as a number. One frame of Landsat MSS imagery consists of four digital images of the same scene in different spectral bands. Two of these are in the visible region (corresponding approximately to green and red regions of the visible spectrum) and two are in the (near) infra-red. Each pixel is a 8-bit binary word, with 0 corresponding to black and 255 to white. The spatial resolution of a pixel is about 80m x 80m. Each image contains 2340 x 3380 such pixels. The database is a (tiny) sub-area of a scene, consisting of 82 x 100 pixels. Each line of data corresponds to a 3x3 square neighbourhood of pixels completely contained within the 82x100 sub-area. Each line contains the pixel values in the four spectral bands (converted to ASCII) of each of the 9 pixels in the 3x3 neighbourhood and a number indicating the classification label of the central pixel. Each pixel is categorized as one of the following classes: 1 red soil 2 cotton crop 3 grey soil 4 damp grey soil 5 soil with vegetation stubble 6 mixture class (all types present) 7 very damp grey soil NB. There are no examples with class 6 in this dataset. The data is given in random order and certain lines of data have been removed so you cannot reconstruct the original image from this dataset. ### Attribute information There are 36 predictive attributes (= 4 spectral bands x 9 pixels in neighborhood). In each line of data the four spectral values for the top-left pixel are given first followed by the four spectral values for the top-middle pixel and then those for the top-right pixel, and so on with the pixels read out in sequence left-to-right and top-to-bottom. Thus, the four spectral values for the central pixel are given by attributes 17,18,19 and 20. If you like you can use only these four attributes, while ignoring the others. This avoids the problem which arises when a 3x3 neighbourhood straddles a boundary. In this version, the pixel values 0..255 are normalized around 0. **Note: it is unclear why the attributes are named Aattr - Fattr in this version, since there are only 4 bands and 9 pixels, naming them A1, B1, C1, D1, A2, B2, C2, D2, ... would have made more sense.**
satimage                                                                                  
texture                                                                                  **Author**: Laboratory of Image Processing and Pattern Recognition (INPG-LTIRF), Grenoble - France. **Source**: [ELENA project](https://www.elen.ucl.ac.be/neural-nets/Research/Projects/ELENA/databases/REAL/texture/) **Please cite**: None ####1. Summary This database was generated by the Laboratory of Image Processing and Pattern Recognition (INPG-LTIRF) in the development of the Esprit project ELENA No. 6891 and the Esprit working group ATHOS No. 6620. ``` (a) Original source: P. Brodatz 'Textures: A Photographic Album for Artists and Designers', Dover Publications,Inc.,New York, 1966. (b) Creation: Laboratory of Image Processing and Pattern Recognition Institut National Polytechnique de Grenoble INPG Laboratoire de Traitement d'Image et de Reconnaissance de Formes LTIRF Av. Felix Viallet, 46 F-38031 Grenoble Cedex France (c) Contact: Dr. A. Guerin-Dugue, INPG-LTIRF, guerin@tirf.inpg.fr ``` ####2. Past Usage: This database has a private usage at the TIRF laboratory. It has been created in order to study the textures discrimination with high order statistics. ``` A.Guerin-Dugue, C. Aviles-Cruz, 'High Order Statistics from Natural Textured Images', In ATHOS workshop on System Identification and High Order Statistics, Sophia-Antipolis, France, September 1993. Guerin-Dugue, A. and others, Deliverable R3-B4-P - Task B4: Benchmarks, Technical report, Elena-NervesII 'Enhanced Learning for Evolutive Neural Architecture', ESPRIT-Basic Research Project Number 6891, June 1995. ``` ####3. Relevant Information: The aim is to distinguish between 11 different textures (Grass lawn, Pressed calf leather, Handmade paper, Raffia looped to a high pile, Cotton canvas, ...), each pattern (pixel) being characterised by 40 attributes built by the estimation of fourth order modified moments in four orientations: 0, 45, 90 and 135 degrees. A statistical method based on the extraction of fourth order moments for the characterization of natural micro-textures was developed called 'fourth order modified moments' (mm4) [Guerin93], this method measures the deviation from first-order Gauss-Markov process, for each texture. The features were estimated in four directions to take into account the possible orientations of the textures (0, 45, 90 and 135 degrees). Only correlation between the current pixel, the first neighbourhood and the second neighbourhood are taken into account. This small neighbourhood is adapted to the fine grain property of the textures. The data set contains 11 classes of 500 instances and each class refers to a type of texture in the Brodatz album. The database dimension is 40 plus one for the class label. The 40 attributes were build respectively by the estimation of the following fourth order modified moments in four orientations: 0, 45, 90 and 135 degrees: mm4(000), mm4(001), mm4(002), mm4(011), mm4(012), mm4(022), mm4(111), mm4(112), mm4(122) and mm4(222). !! Patterns are always sorted by class and are presented in the increasing order of their class label in each dataset relative to the texture database (texture.dat, texture_CR.dat, texture_PCA.dat, texture_DFA.dat) ####4. Class: The class label is a code for the following classes: ``` Class Class label 2 Grass lawn (D09) 3 Pressed calf leather (D24) 4 Handmade paper (D57) 6 Raffia looped to a high pile: (D84) 7 Cotton canvas (D77) 8 Pigskin (D92) 9 Beach sand: (D28) 10 Beach sand (D29) 12 Oriental straw cloth (D53) 13 Oriental straw cloth (D78) 14 Oriental grass fiber cloth (D79) ``` ####5. Summary Statistics: Table here below provides for each attribute of the database the dynamic (Min and Max values), the mean value and the standard deviation. ``` Attribute Min Max Mean Standard deviation 1 -1.4495 0.7741 -1.0983 0.2034 2 -1.2004 0.3297 -0.5867 0.2055 3 -1.3099 0.3441 -0.5838 0.3135 4 -1.1104 0.5878 -0.4046 0.2302 5 -1.0534 0.4387 -0.3307 0.2360 6 -1.0029 0.4515 -0.2422 0.2225 7 -1.2076 0.5246 -0.6026 0.2003 8 -1.0799 0.3980 -0.4322 0.2210 9 -1.0570 0.4369 -0.3317 0.2361 10 -1.2580 0.3546 -0.5978 0.3268 11 -1.4495 0.7741 -1.0983 0.2034 12 -1.0831 0.3715 -0.5929 0.2056 13 -1.1194 0.6347 -0.4019 0.3368 14 -1.0182 0.1573 -0.6270 0.1390 15 -0.9435 0.1642 -0.4482 0.1952 16 -0.9944 0.0357 -0.5763 0.1587 17 -1.1722 0.0201 -0.7331 0.1955 18 -1.0174 0.1155 -0.4919 0.2335 19 -1.0044 0.0833 -0.4727 0.2257 20 -1.1800 0.4392 -0.4831 0.3484 21 -1.4495 0.7741 -1.0983 0.2034 22 -1.2275 0.5963 -0.7363 0.2220 23 -1.3412 0.4464 -0.7771 0.3290 24 -1.1774 0.6882 -0.5770 0.2646 25 -1.1369 0.4098 -0.5085 0.2538 26 -1.1099 0.3725 -0.4038 0.2515 27 -1.2393 0.6120 -0.7279 0.2278 28 -1.1540 0.4221 -0.5863 0.2446 29 -1.1323 0.3916 -0.5090 0.2526 30 -1.4224 0.4718 -0.7708 0.3264 31 -1.4495 0.7741 -1.0983 0.2034 32 -1.1789 0.5647 -0.6463 0.1890 33 -1.1473 0.6755 -0.4919 0.3304 34 -1.1228 0.3132 -0.6435 0.1441 35 -1.0145 0.3396 -0.4918 0.1922 36 -1.0298 0.1560 -0.5934 0.1704 37 -1.2534 0.0899 -0.7795 0.1641 38 -1.0966 0.1944 -0.5541 0.2111 39 -1.0765 0.2019 -0.5230 0.2015 40 -1.2155 0.4647 -0.5677 0.3091 ``` The dynamic of the attributes is in [-1.45 - 0.775]. The database resulting from the centering and reduction by attribute of the Texture database is on the ftp server in the `REAL/texture/texture_CR.dat.Z' file. ####6. Confusion matrix. The following confusion matrix of the k_NN classifier was obtained with a Leave_One_Out error counting method on the texture_CR.dat database. k was set to 1 in order to reach the minimum mean error rate : 1.0 +/- 0.8%. ``` Class 2 3 4 6 7 8 9 10 12 13 14 2 97.0 1.0 0.4 0.0 0.0 0.0 1.6 0.0 0.0 0.0 0.0 3 0.2 99.0 0.0 0.0 0.0 0.0 0.4 0.0 0.0 0.0 0.4 4 1.0 0.0 98.8 0.0 0.0 0.0 0.2 0.0 0.0 0.0 0.0 6 0.0 0.0 0.0 99.4 0.0 0.0 0.0 0.6 0.0 0.0 0.0 7 0.0 0.0 0.0 0.0 100.0 0.0 0.0 0.0 0.0 0.0 0.0 8 0.0 0.0 0.0 0.0 0.0 98.6 0.0 1.4 0.0 0.0 0.0 9 0.4 0.0 0.2 0.0 0.0 0.2 98.8 0.4 0.0 0.0 0.0 10 0.0 0.0 0.0 0.0 0.0 1.4 0.0 98.6 0.0 0.0 0.0 12 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 100.0 0.0 0.0 13 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 99.8 0.2 14 0.0 0.4 0.0 0.0 0.0 0.4 0.0 0.0 0.2 0.0 99.0 ``` 7. Result of the Principal Component Analysis: The Principal Components Analysis is a very classical method in pattern recognition [Duda73]. PCA reduces the sample dimension in a linear way for the best representation in lower dimensions keeping the maximum of inertia. The best axe for the representation is however not necessary the best axe for the discrimination. After PCA, features are selected according to the percentage of initial inertia which is covered by the different axes and the number of features is determined according to the percentage of initial inertia to keep for the classification process. This selection method has been applied on the texture_CR database. When quasi-linear correlations exists between some initial features, these redundant dimensions are removed by PCA and this preprocessing is then recommended. In this case, before a PCA, the determinant of the data covariance matrix is near zero; this database is thus badly conditioned for all process which use this information (the quadratic classifier for example). The following file is available for the texture database: ''texture_PCA.dat.Z'', it is the projection of the ''texture_CR'' database on its principal components (sorted in a decreasing order of the related inertia percentage; so, if you desire to work on the database projected on its x first principal components you only have to keep the x first attributes of the texture_PCA.dat database and the class labels (last attribute)). Table here below provides the inertia percentages associated to the eigenvalues corresponding to the principal component axis sorted in the decreasing order of the associated inertia percentage. 99.85 percent of the total database inertia will remain if the 20 first principal components are kept. ``` Eigen Value Inertia Cumulated value percentage inertia 1 30.267500000 75.6687000000 75.6687000000 2 3.6512500000 9.1281300000 84.7969000000 3 2.2937000000 5.7342400000 90.5311000000 4 1.7039700000 4.2599300000 94.7910000000 5 0.6716540000 1.6791300000 96.4702000000 6 0.5015290000 1.2538200000 97.7240000000 7 0.1922830000 0.4807070000 98.2047000000 8 0.1561070000 0.3902670000 98.5950000000 9 0.1099570000 0.2748920000 98.8699000000 10 0.0890891000 0.2227230000 99.0926000000 11 0.0656016000 0.1640040000 99.2566000000 12 0.0489988000 0.1224970000 99.3791000000 13 0.0433819000 0.1084550000 99.4875000000 14 0.0345022000 0.0862554000 99.5738000000 15 0.0299203000 0.0748007000 99.6486000000 16 0.0248857000 0.0622141000 99.7108000000 17 0.0167901000 0.0419752000 99.7528000000 18 0.0161633000 0.0404083000 99.7932000000 19 0.0128898000 0.0322246000 99.8254000000 20 0.0113884000 0.0284710000 99.8539000000 21 0.0078481400 0.0196204000 99.8735000000 22 0.0071527800 0.0178820000 99.8914000000 23 0.0067661400 0.0169153000 99.9083000000 24 0.0053149500 0.0132874000 99.9216000000 25 0.0051102600 0.0127757000 99.9344000000 26 0.0047116600 0.0117792000 99.9461000000 27 0.0036193700 0.0090484300 99.9552000000 28 0.0033222000 0.0083054900 99.9635000000 29 0.0030722400 0.0076806100 99.9712000000 30 0.0026373300 0.0065933300 99.9778000000 31 0.0020996800 0.0052492000 99.9830000000 32 0.0019376500 0.0048441200 99.9879000000 33 0.0015642300 0.0039105700 99.9918000000 34 0.0009679080 0.0024197700 99.9942000000 35 0.0009578000 0.0023945000 99.9966000000 36 0.0007379780 0.0018449400 99.9984000000 37 0.0006280250 0.0015700600 100.000000000 38 0.0000000040 0.0000000099 100.000000000 39 0.0000000001 0.0000000003 100.000000000 40 0.0000000008 0.0000000019 100.000000000 ``` This matrix can be found in the texture_EV.dat file. The Discriminant Factorial Analysis (DFA) can be applied to a learning database where each learning sample belongs to a particular class [Duda73]. The number of discriminant features selected by DFA is fixed in function of the number of classes (c) and of the number of input dimensions (d); this number is equal to the minimum between d and c-1. In the usual case where d is greater than c, the output dimension is fixed equal to the number of classes minus one and the discriminant axes are selected in order to maximize the between-variance and to minimize the within-variance of the classes. The discrimination power (ratio of the projected between-variance over the projected within-variance) is not the same for each discriminant axis: this ratio decreases for each axis. So for a problem with many classes, this preprocessing will not be always efficient as the last output features will not be so discriminant. This analysis uses the information of the inverse of the global covariance matrix, so the covariance matrix must be well conditioned (for example, a preliminary PCA must be applied to remove the linearly correlated dimensions). The Discriminant Factorial Analysis (DFA) has been applied on the 18 first principal components of the texture_PCA database (thus by keeping only the 18 first attributes of these databases before to apply the DFA preprocessing) in order to build the texture_DFA.dat.Z database file, having 10 dimensions (the texture database having 11 classes). In the case of the texture database, experiments shown that a DFA preprocessing is very useful and most of the time improved the classifiers performances. [Duda73] Duda, R.O. and Hart, P.E.,Pattern Classification and Scene Analysis, John Wiley & Sons, 1973.

21 Rows. -- 2954 msec.
