Result of /data/leuven/370/vsc37064/new_queries_named_graph_predicated_grouped/query_15.txt:
OpenLink Virtuoso Interactive SQL (Virtuoso)
Version 07.20.3240 as of Mar 11 2025
Type HELP; for help and EXIT; to exit.
Connected to OpenLink Virtuoso
Driver: 07.20.3240 OpenLink Virtuoso ODBC Driver
datasetName                                                                       description
LONG VARCHAR                                                                      LONG VARCHAR
_______________________________________________________________________________

'20 Presidential Debate Sept 29th Audio Transcript                                                                                  
'HCMC_AIC_Image_Resize_Rename_02                                                                                  
'In-the-Wild' Audio Deepfake Data                                                                                  We present a dataset of audio deepfakes (and corresponding benign audio) for a set of politicians and other public figures, collected from publicly available sources such as social networks and video streaming platforms. For n = 58 celebrities and politicians, we collect both bona-fide and spoofed audio. In total, we collect 20.8 hours of bona-fide and 17.2 hours of spoofed audio. On average, there are 23 minutes of bona-fide and 18 minutes of spoofed audio per speaker.
(10)Dataset Text Document Classification                                                                                  ### Context This dataset is a collection newsgroup documents. The 10 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering. ### Content There is file (class.txt) that contains a reference to the document_id number and the newsgroup it is associated with. There are also 10 files that contain all of the documents, one document per newsgroup. In this dataset, duplicate messages have been removed and the original messages only contain 'From' and 'Subject' headers (1000 messages total). Each new message in the bundled file begins with these four headers: The Newsgroup and Document_id can be referenced against class.txt Organization Each newsgroup file in the bundle represents a single newsgroup Each message in a file is the text of some newsgroup document that was posted to that newsgroup. This is a list of the 10 newsgroups: business entertainment food graphics historical medical politics space sport technologie ### Acknowledgements Ken Lang is credited by the source for collecting this data. The source of the data files is here: http://qwone.com/~jason/20Newsgroups/ ### Inspiration This dataset text can be used to classify text documents
(BlockShuffle) imagenet-vgg-verydeep-19.mat                                                                                  
(Database) Cracks images in mortar coating                                                                                  
(Pokémon) Pokemon Images via PokéAPI v2                                                                                  This dataset was put together as I wanted to train a model for Pokémon sprites, so I figured I would just utilize PokéAPI and put it on Kaggle to make it easier for me and everyone. PokéAPI: https://pokeapi.co/docs/v2 The API was used to get a list of all Pokémon and their front-facing sprite URLs, valid URLs were then scraped and put together in the dataset present. (If this is not allowed, please let me know and I will be more than happy to take it down)
(Resized) aerial-imagery-for-roof-segmentation                                                                                  ### Context https://www.airs-dataset.com/ &gt; AIRS (Aerial Imagery for Roof Segmentation) is a public dataset that aims at benchmarking the algorithms of roof segmentation from very-high-resolution aerial imagery. The main features of AIRS can be summarized as: &gt; &gt; - 457km2 coverage of orthorectified aerial images with over 220,000 buildings &gt; - Very high spatial resolution of imagery (0.075m) &gt; - Refined ground truths that strictly align with roof outlines ### Citation ``` @article{chen2019, title={Aerial imagery for roof segmentation: A large-scale dataset towards automatic mapping of buildings}, author={Chen, Qi and Wang, Lei and Wu, Yifan and Wu, Guangming and Guo, Zhiling and Waslander, Steven L}, journal={ISPRS Journal of Photogrammetry and Remote Sensing}, volume={147}, pages={42--55}, year={2019}, publisher={Elsevier} } ```
../input/Image                                                                                  
0-50_train_image                                                                                  
0-9up_audio_embedding                                                                                  
0.007Ball.text                                                                                  
01_Retinanet50d_imagenet_300epoch_421                                                                                  
02_Retinanet101d_imagenet_200epoch_432                                                                                  
03_Retinanet101d_imagenet_350epoch_434                                                                                  
0517image                                                                                  
0522_dacon_user_data_suhwa_image                                                                                  
05_CRCNN101d_imagenet_350epoch_457                                                                                  
05agu_firststage_option_b_BIGimage                                                                                  
08_U_of_Montreal_COVID-19_Image_Data_Collection                                                                                  
098uhnkiu789o2k3ejrifiuo3-images                                                                                  
1 Million CAPTCHA images                                                                                  
1,200 Images & OCR Annotations RUS                                                                                  # Images & OCR Annotations RUS - 1,200 Images with annotations The dataset consists of 1,200 paired images with Russian text and detailed OCR annotations. This dataset is designed to assist with text recognition tasks, particularly for the Russian language. The images are categorized into two types: graphics and products, and are captured in two lighting conditions: daylight and nightlight. # Get the Dataset ###This is just an example of the labeling. Leave a request on **[https://trainingdata.pro/data-market](https://trainingdata.pro/data-market/text-from-the-covers-and-labels-of-goods?utm_source=kaggle&utm_medium=cpc&utm_campaign=1200-images-and-ocr-annotations-rus)** to discuss your requirements, learn about the price and buy the dataset ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2F03da3a06bf2152cab5420be2870d96a4%2F20221209_141641.jpg?generation=1682585904560888&alt=media) # Dataset Structure ### - 1200 pairs of images and annotations ### - Images: - Types: Graphics (outdoor), Products (indoor) - Lighting conditions: Daylight, Nightlight ### - Annotations: - Bounding-box labeling for each word - Cyrillic text with punctuation marks and symbols included # Contents - `images` folder: containing all the image files - `annotations` folder: containing all the annotation files in XML-format - `info.csv`: a summary file with the number of images in each category # Image Details ### 1. Graphics: - Names of organizations, posters, billboards, stickers, announcements, and banners - Most often taken outdoors ### 2. Products: - Food, cosmetics, medicines, personal hygiene items, book and video game covers - Taken indoors # Lighting Conditions ### 1. Daylight: - Taken in daylight or in home lighting ### 2. Nightlight: - Taken at night or in a dark room # Annotation Details - Bounding-box labeling for each word - OCR for cyrillic text with punctuation marks and symbols included # Applications: This dataset can be used for various text recognition tasks, such as OCR systems development, natural language processing, and computer vision applications, specifically tailored for the Russian language. *keywords: ocr annotations dataset, consumer goods dataset, off-the-shelf dataset, computer vision dataset, retail dataset, sales dataset, image dataset, retail images dataset, shopping ocr dataset, image-to-text dataset, text detection dataset*
1,995 People Face Images Data (Asian race)                                                                                  Description: 1,995 People Face Images Data (Asian race). For each subject, more than 20 images per person with frontal face were collected. This data can be used for face recognition and other tasks. Data size: 1,995 people, more than 20 images per person with frontal face Race distribution: Asian people
1.5_sec_splitted_chicken_distress_audio                                                                                  
1.5_sec_splitted_chicken_egg_laying_audio                                                                                  
1.5_sec_splitted_chicken_feeding_audio                                                                                  
1.5_sec_splitted_chicken_heat_distress_audio                                                                                  
10 Big Cats of the Wild - Image Classification                                                                                  Images were gathered from Google searches and downloaded using app 'download all images' . I highly recommend this app as it is very fast and returns a zip file with the images which you can then unzip to a specific directory. I have developed a custom set of tools to create datasets. The first tool used creates a dataset framework in a specified directory I call Datasets. It inputs the name of the new dataset and creates a directory with that name and within that directory creates 4 subdirectories train, test, valid and storage. The storage directory is where the unzipped downloaded images are placed. Downloaded images can be a crazy mix of ungodly file names and image formats. I wrote a python program called order_by_size. It operates on the downloaded images, within the storage directory, It removes files with extensions that are not jpg, png, or bmp and deletes files that are below a user specified image size. Then it renames the files sequentially using 'zeros' padding and converts them to jpg format, and orders the files so that the first file is the largest image size, 2nd file is the next largest and so on. For the images in your dataset you want to start with images that are large. Later these images will be cropped to a region of interest and you want these cropped images to be large and have sufficient pixel count so that features can be extracted by your classification model. Now that the files are sequentially ordered and have jpg extensions I use another program called duplicate delete. This program uses file hashing to detect duplicate images and deletes any duplicates. This prevents having images in common between the train, test and validation images when the files are partitioned. Now when you do a Google search you will get a lot of what you want and also a lot of junk. I wrote another python program called review_images that sequentially shows each of the images in the storage directory and you can elect to delete or keep the image if it is the correct type of image you want. This then eliminates unwanted images from the storage directory. Then comes the hard part. If you want to build a high quality dataset you should crop your images so that the resulting image has a high ratio of pixels in the region of interest to the total number of pixels. For that I use paint shop pro version 9. If you examine the dataset images you will see that in most cases the image of the cat takes up at least 50% of the pixels in the image. After all that is done I use the order_by_size program again with different parameters which converts all the images to a specified size. For this dataset I used 224 X 224 X3 as the image size. Now we have a uniform ordered and properly pruned set of images for a specific class like tigers for example. I wrote another python program called make_class, it inputs the new class name (tiger for example) and creates a new class sub directory in the train, test and valid directories. Then it partitions the images in the storage directory into train images, test images and validation images and stores them in the class directory of the train, test and valid directories. Finally I wrote another python program that creates a dataset csv file. To make a high quality dataset takes a lot of work but the tools I have generated helps to reduce the work load.
10+ M. Beatport Tracks / ALL IMAGES                                                                                  To be used in conjunction with beatport database which can be downloaded here: * **[https://www.kaggle.com/datasets/mcfurland/10-m-beatport-tracks-spotify-audio-features](url)** &nbsp; Images scraped directly from **[https://beatport.com/](url)** &nbsp; Waveform Images coming soon. *** ### **How to use:** - Each folder in this dataset are referenced by their respective csv files in Kaggle dataset titled **[10+ M. Beatport Tracks / Spotify Audio Features](https://www.kaggle.com/datasets/mcfurland/10-m-beatport-tracks-spotify-audio-features)** - These files are **bp_artist_media.csv**, **bp_label_media.csv**, **bp_release_media.csv**, **bp_track_media.csv** - The values in column `{type}_img_id` correspond to an image and are named as such. - The values in column `{type}_id` can then be used to link that image to parent values. *** ### **Considerations:** - All images have been reduced to size 224x224 specifically for use in deep learning and as a result, quality has been degraded from their original form. - Images have been reduced in size while keeping their original aspect ratio to avoid loss of data. White boarders have been added when necessary to maintain consistent 224x224 sizing. - Releases, Labels, and Artists may have more than 1 associated image. Although Beatport does not display more than 1 at any given time, historical images are stored in these media tables. This is done by design. To use the most up to date image, use most current updated_on column when grouping by `{type}_id` - Tracks can only have 1 associated waveform image at any given time. - Stock images are when an image has not been provided to Beatport to display. - `artist_img_id` = 5539565 - `label_img_id` = 5539566 - `release_img_id` = 5245821 - some images (despite appearing to be referenced in the csv files) might not exist. These images have broken links to an image and cannot be downloaded nor are they viewable on Beatport *** Full image files without size reduction and degraded quality are available upon request.
10+ M. Beatport Tracks / Spotify Audio Features                                                                                  ### All Spotify data retrieved through API. ### Replicate my database by visiting my [Github](https://github.com/mcfurland/everybeat) repo &nbsp; **Download images relating to release, label & artist at** [https://www.kaggle.com/datasets/mcfurland/10-m-beatport-tracks-all-images](url) - Beatport tables prefixed with **`bp_`** - Spotify tables prefixed with **`sp_`** - Match Spotify Audio features to Beatport tracks by joining the two dataframes/tables on **`isrc`** - **Any feedback on my repo or dataset so that I can improve would be greatly appreciated.** *** Spotify typically caps # of API calls per 24 hours to 10k. To maximize number of requests I have used the following workflow: **1) Run my reverse_isrc Scrapy spider found in my [Github](https://github.com/mcfurland/everybeat).** - selects isrc from database - group by distinct release_id, order by count (descending) in each group - this gives us the releases with the most tracks in each group - send POST requests to https://www.isrcfinder.com/reverse-lookup/ using that isrc to find the associated spotify album id - write data to separate database table **2) Send API calls to Spotify using Spotipy (endpoint album)** - this gets all tracks associated with release/albums from step 1 - write data to database **3) Run isrc Scrapy spider.** - Uses all of the spotify track id from step 2 - sends POST requests to https://www.isrcfinder.com to retrieve that track's isrc **4) Send API calls to Spotify using Spotipy (endpoint audio features)** - select all distinct isrc from step 3 that match an isrc from beatport database. - request audio features using track_id from those matched isrc in batches of 50. **5) Repeat steps 2-4 until initial releases from step 1 have been completed.** &nbsp; **Considerations:** - Free VPN was used through Windscribe. - 6 separate email accounts were used to create 6 Spotify developer accounts. - When creating a new account make sure you are using an incognito browser with VPN enabled so that accounts don't get linked - Write down the VPN location you plan on using for each account and use that location each time. - Never be signed in to 2 accounts simultaneously - Before switching to a new account, delete cache file created when running Spotipy *** **Context:** Electronic music, with its broad range of genres and styles, plays an essential role in today's musical culture. Classifying these genres, however, is a nuanced and often subjective endeavor. This dataset, 10+ M. Beatport Tracks / Spotify Audio Features, is assembled to support machine learning and deep learning efforts in tackling electronic music genre classification. *** **Sources:** The dataset comprises over 10 million songs and related data such as artists, labels, and releases, structured in a typical relational format. This data has been carefully scraped from Beatport, a renowned online music store specializing in electronic music. The project, utilizing tools such as **Postgres**, **Scrapy**, **Apache Airflow**, and **Docker**, has resulted in creating a self-maintaining database to ensure data quality. *** **Inspiration:** The driving force behind this dataset comes from a love of electronic music and a commitment to expanding access to this type of data. By offering comprehensive information on electronic tracks, releases, artists, and labels, this dataset serves as a starting point for those who share a passion for electronic music and wish to embark on a journey of data exploration and innovation. *** **What's Next:** The dataset represents merely the initial phase of an ongoing endeavor to deepen the connection between electronic music and deep learning. Upcoming enhancements include the integration of waveforms, spectrogram images, and visual content related to artists, releases, and labels. These additions are designed to provide richer features for deep learning models in music classification. The inclusion of Spotify audio features, combined with the potential for cross-database linking via ISRC with platforms like Musicbrainz and Discogs, will further enrich the dataset. Further feature engineering will take place to provide accessibility to a wide swath of data, designed to allow others to jump straight into data exploration rather than data collection.
10.74k_detect_ai_generated_text                                                                                  essays_a, and b provide 15 prompts, others only 7 prompts. ```python essays_a : 750 essays_b : 750 essays_c : 840 essays_d : 8.4k ```
100 Image Bunga Dataset                                                                                  Tugas Kecerdasan Buatan - Nadar Alimusa Ramadhani (202210370311327) - Teguh Tri Saputra (202210370311322) 100 Image Flower Dataset Kami akan melakukan perbandingan klasifikasi Image dari dataset kami dan dataset rujukan menggunakan CNN. Perbandingan tersebut akan menunjukkan apabila ada perbedaannya atau tidak dari dataset yang dirujuk.
100 Meters Alt. Normal+Segmented Images (Unreal)                                                                                  ### Context I created the dataset myself using Unreal engine and Airsim platforms. I collected data by flying a drone over a town envoriment in Airsim. Normal and Segmented Images were collected with Unreal and Airsim. The pictures were collected with Procedural Eco System only 100 meters from the top down camera. There are 3 folders, approximately 1000 each, normal, segmented and pixel labeled
100 Popular memes images                                                                                  
100 Sports Image Classification                                                                                  ### Context Please upvote if you find this dataset of use. - Thank you Images were gathered from internet searches. The images were scanned with a duplicate image detector program I wrote. Any duplicate images were removed to prevent bleed through of images between the train, test and valid data sets. All images were then resized to 224 X224 X 3 and converted to jpg format. A csv file is included that for each image file contains the relative path to the image file, the image file class label and the dataset (train, test or valid) that the image file resides in. This is a clean dataset. If you build a good model you should achieve at least 95% accuracy on the test set. If you build a very good model for example using transfer learning you should be able to achieve 98%+ on test set accuracy. If you find this data set useful please upvote. Thanks ### Content Collection of sports images covering 73 different sports.. Images are 224,224,3 jpg format. Data is separated into train, test and valid directories. Additional a csv file is includes for those that wish to use it to create there own train, test and validation datasets. . ### Inspiration Wanted to build a clean data set that was easy to use and had no bad images or duplication between the train, test and validation data sets. Provides a good data set to test your models on. Design for straight forward application of keras preprocessing functions like ImageDataenerator.flow_from_directory or is you use the csv file ImageDataGenerator.flow_from_dataframe
100 Sports Image Classification                                                                                  
100+ tracks of Textured Grooves                                                                                  106 tracks of various genres provided for the purposes of machine learning. Textured grooves work can be found on https://soundcloud.com/textured-grooves and other platforms.
1000 Dog vs Cat from Google Images                                                                                  
1000 Fundus images with 39 categories                                                                                  All these 1000 fundus images which belong to 39classes are come from the Joint Shantou International Eye Centre (JSIEC), Shantou city, Guangdong province ,China. These images are a small part of total 209,494 fundus images to be used for training validating and testing our deep learning platform. The copyright of these images belongs to JSIEC, and can be freely used for any purpose.
10000 images of 7 class Animal                                                                                  
10000_ face_image_Dataset_GAN                                                                                  
10000_flickr_images                                                                                  
1000audiosamples                                                                                  
10015 Skin Cancer Images                                                                                  
100k_with_6M_faiss_context                                                                                  
1024 Cars Images                                                                                  
1024x1024 tiled images                                                                                  
1024x1024-images                                                                                  
102flowers_image_dataset                                                                                  
102ftext                                                                                  
103k-universal-images-illustrations                                                                                  
105 Images of 10 Actors at Different Ages                                                                                  
105,941 Images Natural Scenes OCR Data of 12 Languages                                                                                  Description: 105,941 Images Natural Scenes OCR Data of 12 Languages. The data covers 12 languages (6 Asian languages, 6 European languages), multiple natural scenes, multiple photographic angles. For annotation, line-level quadrilateral bounding box annotation and transcription for the texts were annotated in the data. The data can be used for tasks such as OCR of multi-language. Data size: 105,941 images, including Asian language family: Japanese 9,997 images, Korean 10,231 images, Indonesian 7,591 images, Malay 5,650 images, Vietnamese 8,822 images, Thai 9,645 images; European language family: French 10,015 images, German 7,213 images, Italian 8,824 images, Portuguese 7,754 images, Russian 10,376 images and Spanish 9,823 images Collecting environment: including shop plaque, stop board, poster, ticket, road sign, comic, cover picture, prompt/reminder, warning, packing instruction, menu, building sign, etc.
107 - Dataset for Cat Images For SSD                                                                                  From MS COCO dataset. Cat images with annotations.
109,818 Word Dictionary in .text Format                                                                                  ### Dictionary for your App It is 5, pipe '|' separated columns: Word | Pronunciation | Second pronunciations (if any ) | Part of Speech | Definition. I'm assuming this is the Webster's Unabridged Dictionary you can find here: [gutenberg.org](https://www.gutenberg.org/ebooks/29765) The file I used to make this was located here: https://www.kaggle.com/prabhaskumarpsk/dictionary
10K rewritten texts dataset/LLM Prompt Recovery                                                                                  **About 10000 rewritten texts using Gemma 7b-it, the original texts from column 'Support' in file train.csv from dataset [SciQ (Scientific Question Answering)](https://www.kaggle.com/datasets/thedevastator/sciq-a-dataset-for-science-question-answering)** **if you find it useful, upvote it**
10_Class_Image Classification                                                                                  
10_augmented_audio                                                                                  
10classaudiodataset                                                                                  
11 Common Nut Types For Image Classification                                                                                  Background: Nuts are nutrient-dense foods that contribute to healthier eating. Food image datasets enable artificial intelligence (AI) powered diet-tracking apps to help people monitor daily eating patterns. Aim: This study aimed to create an image dataset of commonly consumed nut types and use it to build an AI computer vision model to automate nut type classification tasks. Methods: iPhone 11 was used to take photos of 11 nut types—almond, brazil nut, cashew, chestnut, hazelnut, macadamia, peanut, pecan, pine nut, pistachio, and walnut. The dataset contains 2,200 images, 200 per nut type. The dataset was randomly split into the training (60% or 1,320 images), validation (20% or 440 images), and test sets (20% or 440 images). A neural network model was constructed and trained using transfer learning and other computer vision techniques—data augmentation, mixup, normalization, label smoothing, and learning rate optimization. Results: The trained neural network model correctly predicted 338 out of 440 images (40 per nut type) in the validation set, achieving 99.55% accuracy. Moreover, the model classified the 440 images in the test set with 100% accuracy. Conclusion: This study built a nut image dataset and used it to train a neural network model to classify images by nut type. The model achieved near-perfect accuracy on the validation and test sets, demonstrating the feasibility of automating nut type classification using smartphone photos. Being made open-source, the dataset and model can assist the development of diet-tracking apps that facilitate users’ adoption and adherence to a healthy diet. Please cite the following peer-reviewed publication if you use this dataset: An R, Perez-Cruet J, Wang J. We got nuts! Use deep neural networks to classify images of common edible nuts. 2022. Nutrition and Health. In press.
11/2/23 + 11/10/23 Images (Untreated KPC)                                                                                  
11223images                                                                                  
11224images                                                                                  
11960 images with 117 age groups                                                                                  Total input images: 11960 Total labels: 117 [age] is an integer from 0 to 116, indicating the age [gender] is either 0 (male) or 1 (female) [race] is an integer from 0 to 4, denoting White, Black, Asian, Indian, and Others (like Hispanic, Latino, Middle Eastern). [date&time] is in the format of yyyymmddHHMMSSFFF, showing the date and time an image was collected to UTKFace.
11aug_unetserenet_imagenetmeanstd                                                                                  
12,160 images Artificially Masked for MFAI & MFGI                                                                                  In our research work, augmentation method such as 'Artificial masking is used for masked face age identification (MFAI) and masked face gender identification (MFGI). The original image dataset was created by ''Sanjaya Subedi' and is accessible on Kaggle, which includes people belonging to different classes of age and gender groups ranging from (0-116), without face masks and augmentation. In our study work, sixteen (16) classes were chosen out of a total of original classes as developed by ''Sanjaya Subedi'. Out of the total 20,000 original images of the different classes, we have created a dataset consists of 12,160 images by artificially masking people's faces with masks and then used in our study work for training and performance evaluation. Artificial face masking is important in the areas of computer vision and facial identification because it addresses privacy, security, and research challenges. It is used to protect people's privacy in images, providing identity in applications such as surveillance and social media sharing. It also improves security by masking facial features to prevent unauthorized entry. In research, artificial masking broadens datasets while maintaining privacy, allowing for the development and evaluation of facial identification techniques with real-world significance. High-quality datasets are critical for training models, evaluating performance, preventing bias, and guaranteeing algorithm generalization, all of which contribute considerably to the progress and ethical deployment of face identification technology.
12-07-2022-norotation_above_Imagesourcing3                                                                                  
12-class image dataset of autonomous cars                                                                                  This set of images is made up of 936 images, distributed in training and testing. The folders with names annotations_prepped refer to the real, segmented photos, that is, with each pixel encoded in one of 12 classes. The actual photos are in the folders prefixed with images_prepped: `annotations_prepped_train`: Training set, in which each pixel of each image is encoded in 12 classes. `annotations_prepped_test`: Test set, in which each pixel of each image is encoded in 12 classes. `images_prepped_train`: Set of actual training images `images_prepped_test`: Set of actual test images
1200 Images per class Skin Lesion size 224X224                                                                                  
120h-spanish-speech-text                                                                                  
12306 captcha image                                                                                  
123456789_xsq_images                                                                                  
1244 images of hot and cold water meters                                                                                  ### Source - [toloka.ai](https://toloka.ai/datasets) This dataset, collected by Roman Kucev from TrainingData.ru, contains 1244 images of hot and cold water meters as well as their readings and coordinates of the displays showing those readings. Each image contains exactly one water meter. The archive also includes the pictures of the results of segmentation with the masks and collages. Toloka was used for photo capturing, segmentation, and recognizing the readings.
125*125 pixels Size Satellite Images                                                                                  
13,000 Screen Capture Images + How to Get More                                                                                  [**Source code for utility I made to download the images**](https://github.com/DataSnaek/Prntsc_Scraper) [**Post about how it was done and the implications**](https://mitchelljolly.com/prntsc/) ## Story This is a set of 13,000 images from the site https://prnt.sc/. It is a site that enables users to easily upload images, either through the web interface, or, most commonly, through the downloadable screen cap tool which enables easy selection and uploading of an area of your screen. As you can see on their homepage, at the point of posting this, they have almost a billion images uploaded. The amount of information in there will be incredible, it’s an information enthusiast dream. Around 2 years ago I discovered this, and I thought it was interesting to mass download these images with a tool I created, but I was manually looking at every single image. As I became more interested in machine learning, I figured experimenting with the 20,000 or so images I had downloaded at the time from the site would be interesting, especially since because of the nature of the site and its ease of access, it gets used for a few very specific purposes which is very useful for image categorisation. Video games are an extremely popular use, pictures of people, animations, screenshots of chats and the most popular; debugging or technical help. Anyone in this field knows people are not particularly conscious of where they put information. I’m sure you can imagine some of the interesting nuggets of info in here just waiting to be found. I was able to find a fair amount just using a retrained Inception CNN and some OCR. I manually categorised just over 5,600 images into 6 categories: 1. Animations 2. Games 3. Objects 4. People 5. Text 6. A very specific kind of animated game character uploaded frequently enough to deserve its own category I was able to achieve around 85% accuracy for categorisation with the rest of the image set (I have 1,000,000 images downloaded from the site total) using just those manually categorised 5,609 images. ## Image Collection The way an image is assigned its link on their site is what made it easy to scan and scrape images from their site. The URL codes are generated sequentially out of a combination letters and numbers of length 6, i.e. prnt.sc/jkl123 prnt.sc/jkl124 prnt.sc/jkl125 prnt.sc/jkl126 Would represent images uploaded one after another. This is of course very easy to write a script to scrape and collect images from. So far I have collected 1,000,000 images in total, and I am now making as many of them as I could available here, but of course you can imagine how easy it would be to collect massive amounts of images from this site. My images were collected over the last 2 years, although the vast majority were collected within the last 4 months. ## Copyright The LightShot terms of service, including their stance on copyright: https://app.prntscr.com/en/privacy.html ## Inspiration This image set is interesting because it represents a balance between a completely random and chaotic set of images and a very structured set of images, due to the fact that there is a great amount of variance image to image, but essentially every image can be categorised into a set of very specific purposes that the site is used for. Because of this, it is good for learning and testing out image processing models. It is essentially an information gatherers gold mine. There are 1 billion screenshots uploaded by everyday users to be collected and processed on this site, all available sequentially. I’m sure you can imagine the kinds of easily accessible (to a machine learning enthusiast) information waiting to be collected on this site.
13-class-leaf-images                                                                                  
130k Images (128x128) - Universal Image Embeddings                                                                                  **Introduction** This is my scraped, collected, and curated dataset for the Google Universal Image Embedding competition resized to **128x128**. It contains 130k+ images in total and below provides a count for each class - **Data Count** | apparel | artwork | cars | dishes | furniture | illustrations | landmark | meme | packaged | storefronts | toys | | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | | 32,226| 4,957 | 8,144 | 5,831 | 10,488 | 3,347 | 33,063 | 3,301 | 23,382 | 5,387 | 2,402 | **Data Source** 1. Apparel - Deep Fashion Dataset 2. Artwork - Google Scrapped 3. Cars - Stanford Cars Dataset 4. Dishes - Google Scrapped 5. Furniture - Google Scrapped 6. Illustrations - Google Scrapped 7. Landmark - Google Landmark Dataset 8. Meme - Google Scrapped 9. Packaged - Holosecta, Grozi 3.2k, Freiburg Groceries, SKU110K 10. Storefronts - Google Scrapped 11. Toys - Google Scrapped
130k Images (512x512) - Universal Image Embeddings                                                                                  **Introduction** This is my scraped, collected, and curated dataset for the Google Universal Image Embedding competition resized to **512x512**. It contains 130k+ images in total and below provides a count for each class - **Data Count** | apparel | artwork | cars | dishes | furniture | illustrations | landmark | meme | packaged | storefronts | toys | | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | | 32,226| 4,957 | 8,144 | 5,831 | 10,488 | 3,347 | 33,063 | 3,301 | 23,382 | 5,387 | 2,402 | **Data Source** 1. Apparel - Deep Fashion Dataset 2. Artwork - Google Scrapped 3. Cars - Stanford Cars Dataset 4. Dishes - Google Scrapped 5. Furniture - Google Scrapped 6. Illustrations - Google Scrapped 7. Landmark - Google Landmark Dataset 8. Meme - Google Scrapped 9. Packaged - Holosecta, Grozi 3.2k, Freiburg Groceries, SKU110K 10. Storefronts - Google Scrapped 11. Toys - Google Scrapped
130k-universal-images-storefronts                                                                                  
130k_universal_images_apparel                                                                                  
130k_universal_images_artwork                                                                                  
130k_universal_images_cars                                                                                  
130k_universal_images_dishes                                                                                  
130k_universal_images_furniture                                                                                  
13k Human mammogram image                                                                                  
14 Types of Abnormal Images & Videos Data                                                                                  **General Information** 8,643 Images - 14 Types of Abnormal Images & Videos Data. The data includes indoor scenes (library, craft store, etc.) and outdoor scenes (road, building, square, railway station, etc.). The data diversity includes multiple scenes, 14 types of abnormal videos & images data, different light conditions, different image resolutions. The data can be used for tasks such as image deblurring and image denoising. **Content** 8,643 images, 14 types **Acknowledgements** Original location: https://bit.ly/3GDAHOb
14 Types of Abnormal Images & Videos Data                                                                                  **Description** The data includes indoor scenes (library, craft store, etc.) and outdoor scenes (road, building, square, railway station, etc.). The data diversity includes multiple scenes, 14 types of abnormal videos & images data, different light conditions, different image resolutions. The data can be used for tasks such as image deblurring and image denoising. For more details, please visit: https://www.nexdata.ai/datasets/1043?source=Kaggle **Specifications** Data size 8,643 images, 14 types Collecting environment indoor scenes (library, craft store, etc.), outdoor scenes (road, building, square, railway station, etc.) Data diversity including multiple scenes, 14 types of abnormal videos & images data, different light conditions, different image resolutions Device cellphone, camera Data format the video data format is .mp4, the image data format is .jpg or .png Collecting content video data includes Dynamic Blur, Blocking Artifact; image data includes Abnormal Saturation, Abnormal Contrast Ratio, Resolution Reduction, Blurry Image, Blurry Scene, Noise Image, Noise-like Image, Old Image, Black Frame, Color-bar Noise, Streak Shaped Noise and Lens Occlusion Accuracy according to the Collection content, the collecting accuracy is over 97% **Get the Dataset** This is just an example of the data. To access more sample data or request the price, contact us at info@nexdata.ai
140 images dataset                                                                                  
141K_Products_Images                                                                                  
143 Different Dog Breeds image Classification                                                                                  This is an image classification dataset consisting of images of dogs. The dataset size is approximately 3.5 GB. It contains images of various dog breeds, totaling 143 different categories. Each category consists of 90-100 images.
14april_withconcataudio                                                                                  
14audiofiles                                                                                  
14audiofiles                                                                                  
15 textures raw data                                                                                  
150 Famous Movie Catchphrases with Context                                                                                  ### Context A small dataset of 150 popular catchphrases from classic movies we all know. ### Content Inside the dataset you will find the catchphrases along with the movie from which the catchphrases originats and the context in which the phrase was used. ### Acknowledgements The credit for collecting and picking the top 150 catchphrases goes to geektyrant.com.
150 Northern mockingbird labeled image                                                                                  The Northern Mockingbird Image Classification Dataset is a collection of images of northern mockingbirds in various poses and environments. The dataset includes 150 images of northern mockingbirds. The images are of high resolution and have been pre-processed to ensure consistency in size and quality. The dataset is well-suited for training and evaluating machine learning models for image classification tasks. Additionally, this dataset can be used for research on computer vision, object detection, and image segmentation, specifically in the field of ornithology. The dataset also includes some images of Northern mockingbirds in flight which can be used to train models for bird flight detection and tracking.
157 Blog Texts in Turkish Language                                                                                  ### Context This dataset is taken from http://www.kemik.yildiz.edu.tr/data/File/ruh_hali.zip. The dataset uploaded for global access. ### Content The dataset contains 105 Turkish review texts in 4 classes. There are raw_texts and texts_in_arff folders that contain 4 classes, each folder classified with the topic name. Classes Turkish to English: karisik -&gt; mixed neseli -&gt; cheerful sinirli -&gt; angry uzgun -&gt; sad ### Acknowledgements Many thanks to [YTÜ Kemik NLP Group](http://www.kemik.yildiz.edu.tr). ### Reference “TÜRKÇE METİNLERİN SINIFLANDIRILMASINDA METİN TEMSİL YÖNTEMLERİNİN PERFORMANS KARŞILAŞTIRILMASI”, M.Fatih AMASYALI, Sümeyra BALCI, Esra Nur VARLI, Emrah METE ,EMO Bilimsel Dergi, Cilt 2, Sayı 4, 95-104, 2012.
15K Chest X-Ray Images (COVID-19)                                                                                  
15aralik-segm-smp-training-imagenet-fullaug                                                                                  
15textures                                                                                  
15textures test input                                                                                  
168,200 audio: RUS Speech Recognition                                                                                  # Russian Cities Speech Recognition - 168,200 recordings The dataset contains audio recordings of 8,411 unique Russian city and street names, pronounced by native Russian speakers. A total of 1,200 participants are involved in the project, with each object being pronounced by 20 different individuals. *Participants may appear multiple times within the dataset, but no individual pronounces more than 1,000 objects.* # Get the Dataset ## This is just an example of the data Leave a request on **[https://trainingdata.pro/data-market](https://trainingdata.pro/data-market/russian-language-speech-recognition?utm_source=kaggle&utm_medium=cpc&utm_campaign=it-spectrum-dataset)** to discuss your requirements, learn about the price and buy the dataset # Dataset Structure - **Audio Recordings**: 168,220 WAV files, 16kHz sample rate, 16 bits per sample - **Metadata**: XLSX file containing transcriptions, speaker identifiers, age, and gender # Annotation Details: - **Transcription**: Text representation of the city or street name in Russian - **Speaker Identifier**: Unique identifier for the person who pronounced the object - **Age**: Age of the speaker in years, with an average age of 38 and a standard deviation of 11 years - **Gender**: Gender of the speaker (male or female), with 30% of recordings by males and 70% by females # Applications: This dataset can be used for: - speech recognition systems for Russian language - text-to-speech synthesis for Russian place names - enhancing voice assistants' capabilities in understanding and pronouncing Russian place names *keyword: speech-to-text dataset, speech recognition dataset, audio dataset, automatic speech recognition dataset, voice dataset, human speech recognition, audio recording of human voices, speech recognition russian, stt/asr*
16mayis_04_intermContext                                                                                  
16mayis_contextMapping_04                                                                                  
19sept_fpneffib6_200epochsimagenet                                                                                  
1image                                                                                  
1k Pharmaceutical Pill Image Dataset                                                                                  ### Context 1K dataset of speckled pharmaceutical pills. Using a CNN to extract features and create binary hash code, these pills can be retrieved from a mobile device for remote identification. Every pill can be tracked using a mobile phone app.![Mobile pill identification app][1] ### Content 1 K pharmaceutical pills jpeg images that have been convoluted by: rotations, grey scale, noise, non-pill ### Acknowledgements Special thanks for Funding and support of Microsoft - Paul DeBaun and NWCadence- Steve Borg ### Inspiration The Pill Crisis in America 1) Fake Fentanyl - killing young people 2) Opioid Abuse - killing all ages of people 3) Fake Online Drugs - killing unknown numbers 4) Non-Compliance - killing older people - Non-Compliance up to 90% of diabetics don't take their meds enough to benefit - Up to 75% of hypertensive patients do not adhere to their medicine - Less than 27% depressed patients adhere to their medication - 41-59% of mentally ill take their meds infrequently or not at all - 33% of patients with schizophrenia don’t take their medicine at all [1]: http://www.jellirolls.com/trumed/images/TruScan-app.jpg
1k_images_1/8                                                                                  
1trainimage                                                                                  
2 us pptpm CPCTexts                                                                                  
2-Digit Image Dataset                                                                                  Image of two-digit numbers (00-99) Train data: original images, total 100 Test data: 4 augmented images for each number, total 400 This dataset was created here. https://www.kaggle.com/code/stpeteishii/2-digit-image-dataset-generator
2.5d_images_v1                                                                                  
2.5d_images_v2                                                                                  
2.8m Text Data                                                                                  
20+ Skin Disease Directories with Face Images                                                                                  
20,000 Leagues Under the Sea - Full Text                                                                                  
200 Bird Species with 11,788 Images                                                                                  ### Details Caltech-UCSD Birds-200-2011 (CUB-200-2011) is an extended version of the CUB-200 dataset, with roughly double the number of images per class and new part location annotations. For detailed information about the dataset, please see the technical report linked below. [Click Here to browse the Dataset](http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/browse/index.html) - Number of categories: 200 - Number of images: 11,788 - Annotations per image: 15 Part Locations, 312 Binary Attributes, 1 Bounding Box Directory Information --------------------- - images/ The images organized in subdirectories based on species. See IMAGES AND CLASS LABELS section below for more info. - parts/ 15 part locations per image. See PART LOCATIONS section below for more info. - attributes/ 322 binary attribute labels from MTurk workers. See ATTRIBUTE LABELS section below for more info. ### IMAGES AND CLASS LABELS: Images are contained in the directory images/, with 200 subdirectories (one for each bird species) **List of image files (images.txt)** The list of image file names is contained in the file images.txt, with each line corresponding to one image: `
200-bird-species-with-11788-images                                                                                  
2000 Ultrasound Images for PCOS Disease Detection                                                                                  
2000 images                                                                                  
2000 images new                                                                                  
200K SHORT TEXTS FOR HUMOR DETECTION                                                                                  Dataset Paper : https://arxiv.org/abs/2004.12765 Title : Colbert: Using bert sentence embedding for humor detection Author: Annamoradnejad, Issa and Zoghi, Gohar journal: arXiv preprint arXiv:2004.12765
200Ma_image                                                                                  
200_Identities_14_Images/Identity                                                                                  
200image_per_each_class                                                                                  
200k Images Dataset | 2000 Classes                                                                                  
200k Short Texts for Humor Detection                                                                                  Automatic humor detection has interesting use cases in modern technologies, such as chatbots and personal assistants. For the task of humor detection, we created a new dataset for humor detection consisting of 200k formal short texts (100k positive, 100k negative). Part of this paper: https://arxiv.org/abs/2004.12765
200k images of birds (iNaturalist)                                                                                  ## iNaturalist 2021 Birds Data This is a subset of the iNaturalist 2021 birds data. There is 1486 species, and 100k images. Each species has around 140 images. I created this dataset because the previous version of this dataset was not large enough. That dataset had 50 images per class. This dataset adds an extra 100 images to each class.
200k(120*120)images                                                                                  
2016 U.S. Presidential Campaign Texts and Polls                                                                                  ### Context This is an aggregate of the data I studied for my thesis titled, 'Data Mining in Presidential Debates and Speeches: How Campaign Rhetoric Shaped Voter Opinion in the 2016 U.S. Presidential Race'. The goal of my thesis was to use NLP techniques to understand how Donald Trump’s rhetoric impacted the opinions of various voter groups throughout his campaign. Here is a summary of my findings: 1. Trump’s words were typically more common in an American English corpus and more extreme on both ends of the sentiment spectrum 2. Trump not only used rhetorical devices for persuasion but also adeptly coupled these devices with the right talking points based on the composition of his audience 3. Precise execution of the above strategy garnered him an unexpectedly large number of votes from the white female and Hispanic demographics I hope that others can use this dataset to answer questions of their own about the 2016 presidential campaign. ### Content Collection of data from the 2016 U.S. Presidential Election Campaign containing: 1. Transcripts of the three presidential debates, divided into separate Trump and Clinton text files 2. Transcripts of Trump's 64 speeches delivered after the RNC and Clinton's 35 speeches delivered after the DNC 3. Transcripts of select speeches delivered by candidates during the primary campaigns 4. USC Dornsife/LA Times Presidential Election Poll, with daily breakdown by voter groups 5. Five Thirty Eight Election Poll, containing daily data from numerous pollsters ### Acknowledgements Debate and speech texts scraped from the American Presidency Project website.
2019 Kaggle Survey Challenge Misc Images and Data                                                                                  ### Context The images in this dataset are used as part of my submission to the 2019 Kaggle ML & DS Survey challenge. ### Content It contains various images from external sources (refer to each image for full details).
2021MCM_C_Images_Global_ID                                                                                  
2022 Russia Ukraine War, Losses, Oryx + Images                                                                                  **Data will be updated weekly** The dataset describes russian and Ukrainian Equipment Losses During The 2022 Russian Invasion Of Ukraine. The dataset was created based on [Oryx](https://www.oryxspioenkop.com) by scraping [Ukrainian losses](https://www.oryxspioenkop.com/2022/02/attack-on-europe-documenting-ukrainian.html) and [russian losses](https://www.oryxspioenkop.com/2022/02/attack-on-europe-documenting-equipment.html) pages. This list only includes destroyed vehicles and equipment of which photo or videographic evidence is available. Therefore, the amount of equipment destroyed is significantly higher than recorded. You can find numbers here [2022 Ukraine Russia War Dataset](https://www.kaggle.com/datasets/piterfm/2022-ukraine-russian-war). ### Images data of Equipment Losses Images data include pictures of Equipment Losses. More than 7k (5 Gb) pictures of destroyed equipment you can find here. Data has been split into different folders by country and type of equipment. You can find the folder structure and some picture examples in [Data Overview Notebok](https://www.kaggle.com/code/piterfm/data-overview). ### Tabular data of Equipment Losses Tabular data includes Equipment Losses, Equipment Models, Countries that produce Equipment, the Number of Equipment Losses, and types of Losses (abandoned, damaged, destroyed, captured, etc.). You can find a basic overview of data in [Data Overview Notebok](https://www.kaggle.com/code/piterfm/data-overview). **Main Columns** - `equipment` - `model` - `sub_model` - `manufacturer` - `losses_total` ### Tracking Equipment - Tanks, - Armoured Fighting Vehicles, - Infantry Fighting Vehicles, - Armoured Personnel Carriers, - Mine-Resistant Ambush Protected, - Infantry Mobility Vehicles, - Command Posts And Communications Stations, - Engineering Vehicles And Equipment, - Heavy Mortars, - Towed Artillery, - Self-Propelled Artillery, - Multiple Rocket Launchers, - Anti-Aircraft Guns, - Self-Propelled Anti-Aircraft Guns, - Surface-To-Air Missile Systems, - Radars, - Jammers And Deception Systems, - Aircraft, - Helicopters, - Unmanned Aerial Vehicles, - Naval Ships, - Logistics Trains, - Trucks, - Vehicles and Jeeps ### Dataset History 2022-11-27 - dataset has been updated (after 277 days of the War). 2022-10-09 - dataset has been updated (after 228 days of the War). 2022-09-18 - dataset has been updated (after 207 days of the War). 2022-09-04 - dataset has been updated (after 193 days of the War). 2022-08-14 - dataset has been updated (after 172 days of the War). 2022-07-31 - dataset has been updated (after 158 days of the War). 2022-07-17 - dataset has been updated (after 144 days of the War). 2022-07-03 - dataset has been updated (after 120 days of the War). 2022-06-19 - dataset has been updated (after 116 days of the War). 2022-06-12 - dataset has been updated (after 109 days of the War). 2022-06-05 - dataset has been updated (after 102 days of the War). 2022-05-29 - dataset has been updated (after 95 days of the War). 2022-05-15 - dataset has been updated; pictures of destroyed equipment have been uploaded (after 81 days of the War). 2022-05-08 - dataset has been updated (after 74 days of the War). 2022-04-30 - dataset has been created (after 66 days of the War).
2023 Samsung AI Challenge Image Quality Assessment                                                                                   Reference : [Here](https://dacon.io/competitions/official/236134/data) [배경] 'Image Quality Assessment & Captioning (화질 평가/캡셔닝)'을 주제로, 카메라로 촬영된 영상의 화질에 대한 정량 평가 점수를 예측하고 그 평가 결과를 자연어로 상세하게 표현하는 알고리즘을 개발합니다. 화질 평가는 모두가 동의하는 절대적인 기준이 없고, 영상의 선명도, 노이즈 정도, 색감, 선호도 등 다양한 인지 화질 요소를 종합적으로 고려 해야 하는 Challenging 한 문제입니다. 다양한 인지 화질 요소에 대한 평가를 단일 점수로 나타낼 순 있으나 많은 의미가 생략되게 됩니다. 따라서, 새로운 화질 평가 연구의 한 방향으로 자연어로 상세히 영상의 화질을 설명할 수 있는 기술이 필요합니다. 이 기술은 향후 스마트폰 카메라에서 개인별, 상황별, 국가 별로 특성화되어 사용자에게 최고의 화질을 제공할 수 있는 AI 영상 처리 기술 개발에 활용될 예정입니다. [주제] 카메라 영상 화질 정량 평가 및 자연어 정성 평가를 동시 생성하는 알고리즘 개발 [설명] 사진 입력에 대해서 정량적 인지 화질 점수와 자연어 기반 정성 평가 캡셔닝을 생성하는 AI 모델을 개발 Input : 사진 Output : 자연어 기반 정성 평가 캡셔닝(Text output, 영어)과 정량적 인지 화질 점수(0~10, float)
2032 Sec Reports Text, Closing Price                                                                                  
20bn image                                                                                  
20fold_38k_from_270k_context_th0.8                                                                                  
20k+ Multi-Class Crop Disease Images                                                                                  The Crop Disease Image Dataset is a comprehensive collection of images depicting various diseases affecting major crops including wheat, maize, cotton, sugarcane, and rice. This dataset serves as a valuable resource for researchers, developers, and practitioners in the agricultural domain, facilitating the study, diagnosis, and mitigation of crop diseases. **Contents:** The dataset contains a diverse range of crop disease images, meticulously curated from multiple sources to ensure completeness and relevance. It encompasses images of common and rare diseases afflicting each crop, captured at different stages of development and severity. **Data Sources:** The images were sourced from various channels, including: Publicly available repositories such as Kaggle, providing foundational datasets for analysis. Web scraping of reputable agricultural websites and research databases to gather additional images, augmenting the dataset's diversity and breadth. Manual collection efforts, where field visits were conducted to document localized disease instances in collaboration with farmers and agricultural experts. **Key Features:** Crop Coverage: The dataset covers five major crops crucial for global food security: wheat, maize, cotton, sugarcane, and rice. Disease Diversity: It includes images of a wide array of crop diseases, ranging from fungal infections and bacterial blights to viral diseases and nutrient deficiencies. Geographical Relevance: The dataset captures region-specific instances of crop diseases, providing insights into localized agricultural challenges. Image Variations: Images depict diseases at different stages of development and severity, facilitating research on disease progression and symptom identification. High Quality: Rigorous quality checks were conducted to ensure image authenticity, clarity, and relevance, enhancing the dataset's utility for research and analysis. Potential Applications: Disease Diagnosis:Researchers and practitioners can use the dataset to develop machine learning models and diagnostic tools for automated disease detection in crops. Solution Development: The dataset can aid in the development of precision agriculture technologies and disease management strategies to enhance crop health and productivity. Educational Resources: It serves as a valuable educational resource for students and educators studying plant pathology and agricultural sciences. License: The dataset is released under an open license, allowing for unrestricted use and distribution for research and non-commercial purposes. Users are encouraged to attribute the dataset appropriately and contribute any improvements or extensions back to the community. **Conclusion:** The Crop Disease Image Dataset represents a collaborative effort to compile a comprehensive repository of crop disease images, aimed at advancing agricultural research, innovation, and sustainability. By leveraging this dataset, stakeholders can address pressing challenges in crop health and contribute to global food security initiatives.
21march_tfeffib2ns_diffaudio                                                                                  
220k-GPT4Vision Image Captions                                                                                  _____ # 220k-GPT4Vision Image Captions ### 220k-GPT4Vision Image Captions By laion (From Huggingface) [[source]](https://huggingface.co/datasets/laion/220k-GPT4Vision-captions-from-LIVIS) _____ ### About this dataset &gt; The dataset titled laion/220k-GPT4Vision-captions-from-LIVIS is a comprehensive collection of image captions specifically curated to support the capabilities of GPT-4 Vision. This dataset aims to provide detailed and factual descriptions for a vast array of images, empowering users with a better understanding of the visual content they encounter. With an extensive volume of data comprising image URLs and corresponding captions, this dataset serves as a valuable resource for training GPT-4 Vision in accurately describing diverse visual content. By utilizing this dataset, developers, researchers, and enthusiasts can enhance their models' ability to generate accurate and informative captions for images. This high-quality caption dataset has been thoughtfully designed to cater specifically to the training needs of GPT-4 Vision, enabling it to analyze and describe images with improved precision and contextuality ### Research Ideas &gt; - Image Captioning: This dataset can be used for developing and training models that automatically generate detailed and factual captions for a given image. It can be used to enhance the accessibility of visual content. &gt; - Visual Content Analysis: By analyzing the captions provided in this dataset, researchers and developers can gain insights into the visual features, objects, actions, and scenes depicted in images. This can be valuable for tasks such as object recognition, scene understanding, and image classification. &gt; - Cross-Modal Retrieval: The dataset can be utilized for cross-modal retrieval tasks where the goal is to retrieve relevant images based on a given query text or vice versa. By associating textual descriptions with corresponding images, it becomes possible to build more effective retrieval systems that bridge the gap between different modalities (text and image) ### Acknowledgements &gt; If you use this dataset in your research, please credit the original authors. &gt; [Data Source](https://huggingface.co/datasets/laion/220k-GPT4Vision-captions-from-LIVIS) &gt; &gt; ### License &gt; &gt; &gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)** &gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/). ### Columns **File: train.csv** | Column name | Description | |:--------------|:---------------------------------------------------------------------------------------------------------------------------------------------| | **url** | This column contains the URLs of the images for which captions are provided. Each URL points to an actual image that can be accessed online. | ### Acknowledgements &gt; If you use this dataset in your research, please credit the original authors. &gt; If you use this dataset in your research, please credit [laion (From Huggingface)](https://huggingface.co/datasets/laion/220k-GPT4Vision-captions-from-LIVIS).
224x224 images 1/20                                                                                  
224x224 images 10/20                                                                                  
224x224 images 11/20                                                                                  
224x224 images 12/20                                                                                  
224x224 images 13/20                                                                                  
224x224 images 14/20                                                                                  
224x224 images 15/20                                                                                  
224x224 images 16/20                                                                                  
224x224 images 17/20                                                                                  
224x224 images 18/20                                                                                  
224x224 images 19/20                                                                                  
224x224 images 2/20                                                                                  
224x224 images 20/20                                                                                  
224x224 images 3/20                                                                                  
224x224 images 4/20                                                                                  
224x224 images 5/20                                                                                  
224x224 images 6/20                                                                                  
224x224 images 7/20                                                                                  
224x224 images 8/20                                                                                  
224x224 images 9/20                                                                                  
22sept_uneteffib6_200epochsimagenet                                                                                  
23 Pairs of Identical Twins Face Image Data                                                                                  Description： 23 Pairs of Identical Twins Face Image Data. The collecting scenes includes indoor and outdoor scenes. The subjects are Chinese males and females. The data diversity inlcudes multiple face angles, multiple face postures, close-up of eyes, multiple light conditions and multiple age groups. This dataset can be used for tasks such as twins' face recognition. Data size： 23 pairs, each person in a pair of identical twins has 40 images (20 indoor images, 20 outdoor images) Population distribution： race distribution: Asian (Chinese); gender distribution: male 9 pairs, female 14 pairs; age distribution: 12 pairs under 18 years old, 10 pairs aged from 18 to 40, 1 pairs over 40 years old
23 Pet Breeds Image Classification                                                                                  ### **Content** * This Dataset contains images of **dogs** and **cats**. * That is 23 of the **most common** breeds; 15 breeds of dogs and 8 breeds of cats. * Each of the classes has **170 images** (except the 'mumbai cat' class) and is separated by **folders**. * Almost all of the images are in **.jpg** format, a few in **.jpeg** format, and only a couple in **.png** format. * Most of the images have been cleaned, but further cleaning may be needed to fit to your individual models. ### **Inspiration** * The dataset is highly inspired by the Oxford Dataset that has 37 breeds of cats and dogs. * This one contains the most common breeds of cats and dogs around the world. * I really wanted to create my very own dataset, for my custom deep learning model.
23206 Urdu nastaleeq ligatures Images                                                                                  
23eylul_wording_text_nocorrection_stratified                                                                                  
240x240 images 1/20                                                                                  
240x240 images 10/20                                                                                  
240x240 images 11/20                                                                                  
240x240 images 12/20                                                                                  
240x240 images 13/20                                                                                  
240x240 images 14/20                                                                                  
240x240 images 15/20                                                                                  
240x240 images 16/20                                                                                  
240x240 images 17/20                                                                                  
240x240 images 18/20                                                                                  
240x240 images 19/20                                                                                  
240x240 images 2/20                                                                                  
240x240 images 20/20                                                                                  
240x240 images 3/20                                                                                  
240x240 images 4/20                                                                                  
240x240 images 5/20                                                                                  
240x240 images 6/20                                                                                  
240x240 images 7/20                                                                                  
240x240 images 8/20                                                                                  
240x240 images 9/20                                                                                  
25 Indian Bird species with 22.6k images                                                                                  This is a dataset of ~925 images of 25 common bird of India. The challenge here is to create a model that can accurately identify these bids. Checkout the sample notebook for inspiration. I used a pretrained VGG16 model and applied transfer learning to train the model and achieved ~92% accuracy using it Source: https://media.ebird.org/
25 Indonesian Spices Image Data                                                                                  
25,000 Cat Images                                                                                  Dive into a purrfectly curated collection of 25,000 cat images! From fluffy kittens to majestic elders, our dataset captures the charm and diversity of our feline friends. Ideal for AI research, pet lovers, and artists alike, explore the world of whiskers with '25,000 Cat Images.'
2500 CrossFade (MixUp) Augmented Images                                                                                  ## Names and labels of the images are in the trainA.csv The kernel with the technique you can find here: [CrossFade Augmented Images](https://www.kaggle.com/frankmollard/crossfade-augmented-imgs) ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4306089%2F30537ff06723d228fee8d3a5278093a2%2Fheader.jpg?generation=1608061986766222&alt=media)
256*256 pixels size of Satellite Images                                                                                  
256-images-ab-70k                                                                                  
256Images                                                                                  
256_2_effi5_imagenet                                                                                  
256_2_effi6_imagenet                                                                                  
256x256 Melanoma TFRecords 70k Images                                                                                  Discussion about this dataset is [CNN Input Size Explained][1]. Image size in this dataset is 256x256. The other contents are the same as the original one [512x512 Melanoma TFRecords 70k Images][2] made by [Chris Deotte][3]. JPEG quality of the image is 100%. 256x256 images fit into Kaggle's run time disk size limitation 5 GB. The features in TFRecords of training set are as follows: ``` feature = { 'image': _bytes_feature(feature0), 'image_name': _bytes_feature(feature1), 'patient_id': _int64_feature(feature2), 'sex': _int64_feature(feature3), 'age_approx': _int64_feature(feature4), 'anatom_site_general_challenge': _int64_feature(feature5), 'source': _int64_feature(feature6), 'target': _int64_feature(feature7) } ``` The test TFRecords do not have `target` nor `source`. The notebook to create this dataset is [Create Various Sizes of External Data (Version 5)][4]. The datasets for the other image size are: - [768x768 Melanoma TFRecords 70k Images][5] - [512x512 Melanoma TFRecords 70k Images][2] by [Chris Deotte][3] - [256x256 Melanoma TFRecords 70k Images][6] [1]: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/160147 [2]: https://www.kaggle.com/cdeotte/512x512-melanoma-tfrecords-70k-images [3]: https://www.kaggle.com/cdeotte [4]: https://www.kaggle.com/tt195361/create-various-sizes-of-external-data?scriptVersionId=37601937 [5]: https://www.kaggle.com/tt195361/768x768-melanoma-tfrecords-70k-images [6]: https://www.kaggle.com/tt195361/256x256-melanoma-tfrecords-70k-images
256x256 images EDGE_ENHANCE                                                                                  ### Context To use notebook on [HuBMAP ](https://www.kaggle.com/c/hubmap-kidney-segmentation). ### Content I apply ImageFilter.EDGE_ENHANCE to [256x256-images](https://www.kaggle.com/iafoss/256x256-images). ### Related info mean: [0.65527532, 0.49901106, 0.69247992] std: [0.25565283, 0.31975344, 0.21533712] ### Reference https://pillow.readthedocs.io/en/stable/reference/ImageFilter.html
256x256images                                                                                  
256x256text2imagedeepfloydstage2                                                                                  
25_images_cd_70k                                                                                  
260x260 images 1/20                                                                                  
260x260 images 10/20                                                                                  
260x260 images 11/20                                                                                  
260x260 images 12/20                                                                                  
260x260 images 13/20                                                                                  
260x260 images 14/20                                                                                  
260x260 images 15/20                                                                                  
260x260 images 16/20                                                                                  
260x260 images 17/20                                                                                  
260x260 images 18/20                                                                                  
260x260 images 19/20                                                                                  
260x260 images 2/20                                                                                  
260x260 images 20/20                                                                                  
260x260 images 3/20                                                                                  
260x260 images 4/20                                                                                  
260x260 images 5/20                                                                                  
260x260 images 6/20                                                                                  
260x260 images 7/20                                                                                  
260x260 images 8/20                                                                                  
260x260 images 9/20                                                                                  
26textOPENAIgenerated                                                                                  
2D ECG Grayscale Images                                                                                  The scripts for this Arrhythmia Detection model using 2D CNN Project can be found here: https://github.com/GeoLek/Arrhythmia-detection-using-2D-CNN All images were first resized (downscaled) to 224x224 dimensions and then were converted to grayscale. Then we augmented the images to balance the imbalanced class images. We classified the following signals: &lt; N = Normal beat. This type is used for normal heartbeats. S = Supraventricular ectopic beat. This type of beat originates from the atria (the upper chambers of the heart) but outside the sinoatrial node (the natural pacemaker of the heart), causing an irregular heartbeat. V = Ventricular ectopic beat. This beat originates from the ventricles (the lower chambers of the heart) and is considered a premature beat. F = Fusion of ventricular and normal beat. A fusion beat occurs when a normal beat and an ectopic beat occur at the same time, causing a hybrid beat that has characteristics of both. Q = Unclassifiable beat. This label is used for beats that cannot be clearly classified into any of the other categories due to various reasons, such as poor signal quality or atypical patterns. The dataset was taken from this Kaggle repo, provided by Hari Mohan Rai: https://www.kaggle.com/datasets/erhmrai/ecg-image-data
2D brain image                                                                                  
2D images subset0                                                                                  
2D-FACT - Fake Image Classification Dataset                                                                                  The paper from which this dataset is from can be found on my personal website: https://ericji150.github.io Please site the paper if you are to use this dataset. E. Ji, B. Dong, B. Samanthula, N. Zhou. '2D-FACT: Dual-Domain Fake Image Detection Against Textto-Image Generative Models'. MIT Undergraduate Research Technology Conference (URTC 2023).
2D-ID images by 3t2FTS to classify HGG/LGG samples                                                                                  The dataset constitutes the 2D-ID images obtained by the 3t2FTS approach to categorize HGG/LGG samples of the BRATS 2017/2018 dataset. The 3t2FTS algorithm is explained in the first citation below which is based on first-order statistics to transform the 3D tumor information into 2D space (2D-ID images). The following four citations are mandatory to utilize the dataset that is formed by the usage of the 3t2FTS approach and BRATS 2017/2018 dataset: Usage of 3t2FTS Approach: https://www.mdpi.com/2504-4990/5/2/22 1-Hajmohamad, A.; Koyuncu, H. 3t2FTS: A novel feature transform strategy to classify 3D MRI voxels and its application on HGG/LGG classification. Mach. Learn. Knowl. Extr. 2023, 5, 359-383. Usage of BRATS 2017/2018 Dataset: 2-Menze, B.H.; Jakab, A.; Bauer, S.; Kalpathy-Cramer, J.; Farahani, K.; Kirby, J.; Burren, Y.; Porz, N.; Slotboom, J.; Wiest, R.; et al. The multimodal brain tumor image segmentation benchmark (BRATS). IEEE Trans. Med. Imaging 2014, 34, 1993–2024. 3-Bakas, S.; Akbari, H.; Sotiras, A.; Bilello, M.; Rozycki, M.; Kirby, J.S.; Freymann, J.B.; Farahani, K.; Davatzikos, C. Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features. Sci. Dat. 2017, 4, 170117. 4-Bakas, S.; Reyes, M.; Jakab, A.; Bauer, S.; Rempfler, M.; Crimi, A.; Shinohara, R.T.; Berger, C.; Ha, S.M.; Rozycki, M.; et al. Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge. arXiv 2018, arXiv:1811.02629. The following citations are declared on the following website of BRATS 2017/2018 Dataset: https://www.med.upenn.edu/sbia/brats2018/data.html Optional Citations: 5-S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, et al., 'Segmentation Labels and Radiomic Features for the Pre-operative Scans of the TCGA-GBM collection', The Cancer Imaging Archive, 2017. DOI: 10.7937/K9/TCIA.2017.KLXWJJ1Q 6-S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, et al., 'Segmentation Labels and Radiomic Features for the Pre-operative Scans of the TCGA-LGG collection', The Cancer Imaging Archive, 2017. DOI: 10.7937/K9/TCIA.2017.GJQ7R0EF
2D-ID images by 3t2FTS-v2 to classify HGG/LGG data                                                                                  The dataset constitutes the 2D-ID images obtained by the 3t2FTS-v2 approach to categorize HGG/LGG samples of the BRATS 2017/2018 dataset. The 3t2FTS-v2 algorithm is explained in the first citation below which is based on FOS, GLRLM, and z-score normalization to transform the 3D tumor information into 2D space (2D-ID images). The following four citations are mandatory to utilize the dataset that is formed by the usage of the 3t2FTS approach and BRATS 2017/2018 dataset: Usage of 3t2FTS-v2 Approach: https://www.mdpi.com/2306-5354/10/6/629 1-Koyuncu, H.; Barstuğan, M. A New Breakpoint to Classify 3D Voxels in MRI: A Space Transform Strategy with 3t2FTS-v2 and Its Application for ResNet50-Based Categorization of Brain Tumors. Bioengineering 2023, 10, 629. Usage of BRATS 2017/2018 Dataset: 2-Menze, B.H.; Jakab, A.; Bauer, S.; Kalpathy-Cramer, J.; Farahani, K.; Kirby, J.; Burren, Y.; Porz, N.; Slotboom, J.; Wiest, R.; et al. The multimodal brain tumor image segmentation benchmark (BRATS). IEEE Trans. Med. Imaging 2014, 34, 1993–2024. 3-Bakas, S.; Akbari, H.; Sotiras, A.; Bilello, M.; Rozycki, M.; Kirby, J.S.; Freymann, J.B.; Farahani, K.; Davatzikos, C. Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features. Sci. Dat. 2017, 4, 170117. 4-Bakas, S.; Reyes, M.; Jakab, A.; Bauer, S.; Rempfler, M.; Crimi, A.; Shinohara, R.T.; Berger, C.; Ha, S.M.; Rozycki, M.; et al. Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge. arXiv 2018, arXiv:1811.02629. The following citations are declared on the following website of BRATS 2017/2018 Dataset: https://www.med.upenn.edu/sbia/brats2018/data.html Optional Citations: 5-S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, et al., 'Segmentation Labels and Radiomic Features for the Pre-operative Scans of the TCGA-GBM collection', The Cancer Imaging Archive, 2017. DOI: 10.7937/K9/TCIA.2017.KLXWJJ1Q 6-S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, et al., 'Segmentation Labels and Radiomic Features for the Pre-operative Scans of the TCGA-LGG collection', The Cancer Imaging Archive, 2017. DOI: 10.7937/K9/TCIA.2017.GJQ7R0EF
2_US_PPTPM_alberta_xxlarge_V2_CusModel_CPCTexts_V1                                                                                  
2_US_PPTPM_berta-for-patent_CPCTexts_V1                                                                                  
2_US_PPTPM_berta-for-patent_CPCTexts_V2                                                                                  
2_US_PPTPM_berta-for-patent_CPCTexts_V3                                                                                  
2_US_PPTPM_berta-for-patent_CPCTexts_V4                                                                                  
2_US_PPTPM_deberta v3 large_CPCTexts_V1                                                                                  
2_US_PPTPM_deberta v3 large_CPCTexts_V2                                                                                  
2_US_PPTPM_deberta v3 large_CPCTexts_V3                                                                                  
2_US_PPTPM_deberta v3 large_CPCTexts_V4                                                                                  
2_US_PPTPM_deberta v3 large_CPCTexts_V5                                                                                  
2_US_PPTPM_deberta v3 large_CPCTexts_V6                                                                                  
2_US_PPTPM_deberta v3 large_CusModel_CPCTexts_V1                                                                                  
2_US_PPTPM_deberta v3 large_CusModel_CPCTexts_V2                                                                                  
2_US_PPTPM_ernie_large_V2_CusModel_CPCTexts_V1                                                                                  
2_US_PPTPM_ernie_large_V2_CusModel_CPCTexts_V2                                                                                  
2_US_PPTPM_funnel_large_CusModel_CPCTexts_V1                                                                                  
2_US_PPTPM_funnel_large_CusModel_CPCTexts_V2                                                                                  
2_US_PPTPM_roberta_large_CusModel_CPCTexts_V1                                                                                  
2_US_PPTPM_roberta_large_CusModel_CPCTexts_V2                                                                                  
2d fetal ultrasound images                                                                                  
2d floor plan dataset with text descriptions new                                                                                  2d floorplan images dataset by - @inproceedings{leng-etal-2023-tell2design, title = '{T}ell2{D}esign: A Dataset for Language-Guided Floor Plan Generation', author = 'Leng, Sicong and Zhou, Yang and Dupty, Mohammed Haroon and Lee, Wee Sun and Joyce, Sam and Lu, Wei', booktitle = 'Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)', publisher = 'Association for Computational Linguistics', url = 'https://aclanthology.org/2023.acl-long.820' }
3 diff images final                                                                                  
3 diff images final sem resize                                                                                  
3 sec Spectrogram images                                                                                  
3 x Image super resolution                                                                                  This dataset can be used to train simple image super resolution models. Dataset contains 4 folders 1. highres, high resolution images used for training (expected outputs) 3500, 510x510 rgb images 2. lowres, low resolution images used for training (inputs) 3500, 170x170 rgb images 3. highres valid, high resolution images used for validation 1000, 510 x 510 rgb images 4. lowres valid, low resolution images used for validation 1000, 170 x 170 rgb images
3,340 images are Artificially Masked                                                                                  In our research work, augmentation methods such as 'Artificial masking, 180-degree horizontal flipping, and color augmentation' are used. The original image dataset was created by ''Rajesh Kumar' and is accessible on Kaggle, which includes people belonging to 100 different classes without face masks and augmentation. In our study work, ten (10) classes were chosen at random from a total of 100 classes as mentioned above. The original 1,297 images of the 10 classes are increased to 3,340 images using augmentation methods and artificially masking people's faces with masks and then used in our study work for training and performance evaluation. Artificial face masking is important in the areas of computer vision and facial recognition because it addresses privacy, security, and research challenges. It is used to protect people's privacy in images, providing identity in applications such as surveillance and social media sharing. It also improves security by masking facial features to prevent unauthorized entry. In research, artificial masking broadens datasets while maintaining privacy, allowing for the development and evaluation of facial recognition techniques with real-world significance. High-quality datasets are critical for training models, evaluating performance, preventing bias, and guaranteeing algorithm generalization, all of which contribute considerably to the progress and ethical deployment of face recognition technology.
30 Musical Instruments -Image Classification                                                                                  
30 Types of Balls Updated- Image Classification                                                                                  ### Context A good data set for notebook developpers to use various augmentation techniques to build a high F1 score model ### Content This is a data set of images of various types of balls. There are 24 different types (classes) . train set includes 2860 files in 24 subdirectories. The training set is not balanced having a different number of images for each class. This was done intentionally so notebook developers could test methods to deal with unbalanced data. The data set is however of high quality where the region of interest (ROI), the ball typically comprises about 50% of the pixels. ### Acknowledgements We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research. ### Inspiration Your data will be in front of the world's largest data science community. What questions do you want to see answered?
300 images of squares, circles, and triangles                                                                                  #Context There are a lot of different types of shapes and it is important to be able to differentiate between them. #Content I drew 100 triangles, 100 squares and 100 circles in processing. each png image is 28x28 px the images are in 3 folders labeled squares, circles and triangles pretty straight forward #Acknowledgements Banner image by [@rawpixel from Unsplash][1]. #Inspiration Is that image a triangle or a square or a circle? [1]: https://unsplash.com/photos/JbDomeNrdOs
3000 Rewritten texts - Prompt recovery Challenge                                                                                  Generated data for Prompt recovery challenge. 3000 examples for now, will share more soon. 500 Prompts created using ChatGPT(4) - Different initial prompts were used to get some variety in creativity, text processing, or professional settings. To have more variety in the input text I used Wikipedia first paragraphs as the original texts. [https://huggingface.co/datasets/abokbot/wikipedia-first-paragraph](https://huggingface.co/datasets/abokbot/wikipedia-first-paragraph) Model used to generate is `gemma-7b-it-quant` The prompts file is shared for convinience, along with sharpened cosine similarity scores of each prompt with the baseline 'Improve the text' If you find the dataset useful, please upvote.
3000 SDXL-lightning Images                                                                                  
3000_ImageData_for_crack_detection                                                                                  
300_images_stocks                                                                                  
300x300 images 1/20                                                                                  
300x300 images 10/20                                                                                  
300x300 images 11/20                                                                                  
300x300 images 12/20                                                                                  
300x300 images 13/20                                                                                  
300x300 images 14/20                                                                                  
300x300 images 15/20                                                                                  
300x300 images 16/20                                                                                  
300x300 images 17/20                                                                                  
300x300 images 18/20                                                                                  
300x300 images 19/20                                                                                  
300x300 images 2/20                                                                                  
300x300 images 20/20                                                                                  
300x300 images 3/20                                                                                  
300x300 images 4/20                                                                                  
300x300 images 5/20                                                                                  
300x300 images 6/20                                                                                  
300x300 images 7/20                                                                                  
300x300 images 8/20                                                                                  
300x300 images 9/20                                                                                  
31329 fake images from thispersondoesnotexist                                                                                  
32patientimage                                                                                  
33 Car Image Classification                                                                                  **33 Car Models Image Dataset** - This dataset is composed of 33 different car models currently merchandising in South Korea. - Hyundai Motor, Kia Motor, Genesis Motor, Ssangyong Motor, Renault korea Motors - Data was collected by selenium from google image. - You can classify images by Convolutional Neural Networks.
35 snake breed image dataset                                                                                  
35k_stylegan1_images_rm_background                                                                                  
38-Cloud: Cloud Segmentation in Satellite Images                                                                                  ### Context This dataset contains **38 Landsat 8 scene images** and their manually extracted pixel-level ground truths for cloud detection. ### Content The entire images of these scenes are cropped into multiple 384*384 patches to be proper for deep learning-based semantic segmentation algorithms. There are 8400 patches for training and 9201 patches for testing. Each patch has 4 corresponding spectral channels which are Red (band 4), Green (band 3), Blue (band 2), and Near Infrared (band 5). Unlike other computer vision images, these channels are not combined. Instead, they are in their corresponding directories. ### Acknowledgements The 38-Cloud dataset is introduced in [1], yet it is a modification of the dataset in [2]. All the data has been prepared in the Laboratory for Robotics Vision (LRV), School of Engineering Science at Simon Fraser University, Burnaby, BC, Canada. ### Inspiration More information about the dataset can be found [here](https://github.com/SorourMo/38-Cloud-A-Cloud-Segmentation-Dataset). [1] S. Mohajerani, T. A. Krammer and P. Saeedi, 'A Cloud Detection Algorithm for Remote Sensing Images Using Fully Convolutional Neural Networks,' 2018 IEEE 20th International Workshop on Multimedia Signal Processing (MMSP), Vancouver, BC, 2018, pp. 1-5. doi: 10.1109/MMSP.2018.8547095 URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8547095&isnumber=8547039 [2] S. Mohajerani and P. Saeedi. 'Cloud-Net: An End-to-end Cloud Detection Algorithm for Landsat 8 Imagery'. (forthcoming) 2019. to appear at IEEE International Geoscience and Remote Sensing Symposium (IGARSS). URL: https://arxiv.org/pdf/1901.10077.pdf
3800 image dataset                                                                                  This dataset has about 3800 images, actually almost 3900, Please use it wisely and do not occasionally use it for actions that violate state or individual guidelines in each region, You are allowed to use this data according to personal needs such as experiments and so on, it is prohibited to use it for commercial purposes, I am not responsible if someone in business or a company with commercial purposes experiences problems with copyright claims due to the copyright of all these images not mine, the images are collected from Pinterest pins that I have saved on my boards for the last 6 years, You can contact me at email zardaniraper37@gmail.com if you need more photos for testing purposes, actually I don't want to admit it, but this is the reality, I am a 22 year old child from Indonesia, I only went to school until grade 3 of elementary school, As a non-educated person, it is not easy for me to become a machine learning developer, but that is not a problem, I learn from the internet and now I am equal to the average student, but this right also makes me move less freely, for the last 7 years. I am in the programming field only using Termux and an Android cellphone, there is absolutely no support either in terms of education or finances, if you have something to share, I hope you can spare even just 1 U$D to my paypal address dydani496@gmail.com, I am asking for help and donations to buy equipment needed in the programming field, especially computers, Thank you
384x384 Melanoma TFRecords 70k Images                                                                                  Discussion about this dataset is [CNN Input Size Explained][1]. Image size in this dataset is 384x384. The other contents are the same as the original one [512x512 Melanoma TFRecords 70k Images][2] made by [Chris Deotte][3]. JPEG quality of the image is 98%. This comes from the Kaggle's run time disk size limitation 5 GB. The features in TFRecords of training set are as follows: ``` feature = { 'image': _bytes_feature(feature0), 'image_name': _bytes_feature(feature1), 'patient_id': _int64_feature(feature2), 'sex': _int64_feature(feature3), 'age_approx': _int64_feature(feature4), 'anatom_site_general_challenge': _int64_feature(feature5), 'source': _int64_feature(feature6), 'target': _int64_feature(feature7) } ``` The test TFRecords do not have `target` nor `source`. The notebook to create this dataset is [Create Various Sizes of External Data (Version 3)][4]. The datasets for the other image size are: - [768x768 Melanoma TFRecords 70k Images][5] - [512x512 Melanoma TFRecords 70k Images][2] by [Chris Deotte][3] - [256x256 Melanoma TFRecords 70k Images][6] [1]: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/160147 [2]: https://www.kaggle.com/cdeotte/512x512-melanoma-tfrecords-70k-images [3]: https://www.kaggle.com/cdeotte [4]: https://www.kaggle.com/tt195361/create-various-sizes-of-external-data?scriptVersionId=37586157 [5]: https://www.kaggle.com/tt195361/768x768-melanoma-tfrecords-70k-images [6]: https://www.kaggle.com/tt195361/256x256-melanoma-tfrecords-70k-images
3900+ MARVEL Comic Cover & 900+ Characters Images                                                                                  **3900+ MARVEL Comic Cover & 900+ Characters Images** contains 3900+ MARVEL's Comic Cover Images and 900+ MARVEL's Comic Characters Images. **Comic Cover Images are named as name of comic, year of release.** **NAVIGATE INTO DATA EXPLORER AS GUIDED IN GIVEN IMAGE TO EXPLORE DATASET.** ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F15053304%2F91f4e24da0089d5dcbb1edbef39e42b4%2FScreenshot%202023-07-24%20151611.png?generation=1690192035978595&alt=media) **Note: Thumbnail and header image for this dataset is generated using AI.** **Marvel Entertainment is sole owner of this data.**
3991 Fashion Images Dataset with 15 labels                                                                                  This dataset has 15 labels with 3991 colored images. Each label has the different number of images and sizes. The labels are: Bag, Blazer, Cap, Dress, Jewellery, Outwear, Pants, Scarf, Shoes, Shorts, Skirt, Sunglasses, T-Shirt, Top, Vallet.
3D MRI-Ultrasound Brain Images                                                                                  The data contains MRI images and their corresponding ultrasound images. These images are one-to-one 3D volumes. The region of volume acquired by the MRI image is the same as the ultrasound. This image can be used for supervised learning techniques for medical image generation.
3D brain image                                                                                  
3D chest Image                                                                                  
3D human mesh with image renders circular                                                                                  
3D print error images after data enhancement                                                                                  The data in this dataset was collected by Yiqi Tang. The initial data came from the Internet, and then through manual data filtering, blurred, distorted and low resolution images were removed. Randomly divide into training and testing sets in a ratio of approximately 4:1. Afterwards, the training and testing sets were subjected to data augmentation such as stretching, inversion, and brightness adjustment. Finally, I used three data processing methods: 1. Gray scale 2. Edge extraction - low threshold 3. Edge extraction - high threshold
3d-ct-images                                                                                  
3rd-ml-month-car-image-aspect-ratio-crop-dataset                                                                                  
40 images of tma                                                                                  
40k-data-with-context-v2                                                                                  There is a discussion about this dataset [here][1]. Note that we need to filter and preprocess this dataset before using it. Some questions only have 2, 3, or 4 answer choices. Some questions are written as 'complete the statement' and some questions require images. It is probably best to remove the samples requiring images and remove the samples written as statements. There are various ways to use the questions with 2, 3, or 4 answer choices. Please read discussion post for ideas. There is a starter notebook [here][2] demonstrating how to best use this new dataset. Enjoy! [1]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/440908 [2]: https://www.kaggle.com/code/cdeotte/how-to-use-40k-dataset
42000 Turkish News Texts in 13 Classes                                                                                  ### Context This dataset is taken from http://www.kemik.yildiz.edu.tr/data/File/42bin_haber.rar. The dataset uploaded for global access. ### Content The dataset contains 42000 Turkish news texts in 13 classes. There is a news folder that contains 13 classes, each folder classified with the topic name. Classes Turkish to English: - dunya -&gt; world - ekonomi -&gt; economics - genel -&gt; general - guncel -&gt; current news - kultur-sanat -&gt; art and culture - magazin -&gt; magazine - planet -&gt; global - saglik -&gt; health - siyaset -&gt; politics - spor -&gt; sport - teknoloji -&gt; technology - turkiye -&gt; Turkey - yasam -&gt; life ### Acknowledgements Many thanks to [YTÜ Kemik NLP Group](http://www.kemik.yildiz.edu.tr). ### Reference 'Kişisel Gazete”, Oğuz Yıldırım, Fatih Atık, Yıldız Teknik Üniversitesi, Bilgisayar Mühendisliği Bölümü, Bitirme Projesi, 2013.
443 test images                                                                                  
443 train images                                                                                  
443-test-images                                                                                  
444-train-images                                                                                  
45-flip-image                                                                                  
4500images                                                                                  
460729 20x20 images of 56 handwritten characters                                                                                  ### Context A wonky combination of 3 datasets and some manually created data. ### Content 460729 20x20 images of 56 handwritten characters. Files is images listed sequentially 400 (20x20) bytes for the pixels, 1 byte for the labels. Dataset has been somewhat manually filtered, unlike with NIST you shouldn't find an E labelled as an e. ### Acknowledgements Data combined from: - NIST special database 19: https://www.nist.gov/srd/nist-special-database-19 - 'Handwritten math symbols dataset' by Xai Nano: https://www.kaggle.com/xainano/handwrittenmathsymbols/ - HASYv2: https://github.com/MartinThoma/HASY ### Inspiration A dataset simply created for my dissertation. Use at your own risk.
475_nose_images                                                                                  
480k_human_vs_llm_text                                                                                  
494 Fox labeled image dataset                                                                                  The Fox Image Classification Dataset is a collection of images of foxes in various poses and environments. The dataset includes 494 images of foxes. The images are of high resolution and have been pre-processed to ensure consistency in size and quality. The dataset is well-suited for training and evaluating machine learning models for image classification tasks. Additionally, this dataset can be used for research on computer vision, object detection, and image segmentation.
4K Retail Tshirt Images                                                                                  
4Q & BiModal OpenL3 Audio Embeddings                                                                                  The dataset involves deep audio embeddings that were generated using OpenL3 audio library. The OpenL3 library is a transfer learning modelling library that uses L3-Net model. The audio used was collected from http://mir.dei.uc.pt/downloads.html that involves 4Q audio emotion dataset and Bi-modal (audio and lyrics) emotion dataset. The configurations used in the network are as follows: `input_repr, content_type, embedding_size = 'mel128', 'music', 512` `model_kapre = openl3.models.load_audio_embedding_model(input_repr, content_type, embedding_size)`
4Q Audio Emotion Dataset (Russell)                                                                                  ### Context A new 4-quadrant audio emotion dataset. It contains 900 audio clips, annotated into 4 quadrants, according to Russell's model. The dataset consists of: - 900 ~30 second clips gathered from AllMusic API - The files are organized in 4 folders (Q1 to Q4) - Two metadata csv files with annotations and extra metadata *You can refer README file inside Audio and Features folder for more information.* ### Acknowledgements Source: http://mir.dei.uc.pt/downloads.html If you use it, please&nbsp;**cite the following article(s)**:&nbsp; [PDF](http://mir.dei.uc.pt/pdf/Journals/MOODetector/TAFFC_2018_Panda.pdf)&nbsp;Panda R., Malheiro R. & Paiva R. P. (2018). '*Novel audio features for music emotion recognition*'. IEEE Transactions on Affective Computing (IEEE early access). DOI: 10.1109/TAFFC.2018.2820691. [PDF](http://mir.dei.uc.pt/pdf/Conferences/MOODetector/ISMIR_2018_Panda.pdf)Panda R., Malheiro R., Paiva R. P. (2018). '*Musical Texture and Expressivity Features for Music Emotion Recognition*'. 19th International Society for Music Information Retrieval Conference -- ISMIR 2018, Paris, France.
4class_1k_images                                                                                  
4classimages                                                                                  
5,011 Images – Human Frontal face Data (Male)                                                                                  Description： 5,011 Images – Human Frontal face Data (Male). The data diversity includes multiple scenes, multiple ages and multiple races. This dataset includes 2,004 Caucasians , 3,007 Asians. This dataset can be used for tasks such as face detection, race detection, age detection, beard category classification. Data size： 5,011 people, one image per person Race distribution： 2,004 Caucasians , 3,007 Asians
5-class weather status image classification                                                                                  ## About I wanted to collect real fresh outdoors images with fire classes in a part of Misk Foundation Data Science Immersive project. With MS Bing API I collected and cleaned up to 1500 images for all classes. Further, I collected data from four kaggle datasets, their credits are below. Check my GitHub repo for my work https://github.com/ammar-faifi/Weather_Status_Predictor_From_Images Check the report here https://ammar-faifi.github.io/Weather_Status_Predictor_From_Images/ Online predictor here https://dsi-weather-predictor.herokuapp.com ## Data Summary | Class | Folder | Images Count | --- | --- | --- | | Sunny | sunny | 6702 | | Cloudy | cloudy | 6274 | | Foggy | foggy | 1261 | | Rainy | rainy | 1927 | | Snowy | snowy | 1875 | | Total | Nan | 18039 | ## Sources 1 - Manually from Bing API 2 - https://www.kaggle.com/datasets/jagadeesh23/weather-classification 3 - https://www.kaggle.com/datasets/polavr/twoclass-weather-classification 4 - https://www.kaggle.com/datasets/jehanbhathena/weather-dataset 5 - https://www.kaggle.com/datasets/pratik2901/multiclass-weather-dataset
50 Types of Car Parts -Image Classification                                                                                  This is a dataset of images of 50 types of car parts. It includes a train set, a test set and a validation set. There are 50 classes of car parts. The train set is not balanced. Class Ignition Coil has the most training images = 200. Class Leaf Spring has the least images = 110. The validation and test sets each have 5 images for each of the 50 classes. Images are of dimensions 224 X 224 X3 in jpg format. All images are originals there are no augmented images in the dataset.
500 Images of 10 Endangered & 10 Common Animals                                                                                  Thanks to **[iNaturalist](https://www.inaturalist.org/)**, we're able to collect the images of 500 animals that we divide into two classes namely Endangered and Not Endangered. This dataset contains 250 Endangered and 250 Not Endangered animals images, with each classes having 10 animals.
5000 Indian Cuisines Dataset(with images)                                                                                  ### Context The data was created to build a deep learning-based image to recipe model which can provide ingredients and recipe of a dish once the image is uploaded. ### Content Columns: name image_url description cuisine course diet prep_time ingredients instructions image_available The CSV data was scraped from https://www.archanaskitchen.com/ and the images were downloaded using an image downloader python code. You can find both the codes below: https://github.com/campusx-official/image-2-recipe-data-collector ### Acknowledgements https://archanaskitchen.com/ ### Inspiration You can build a recommender system using CSV data.
5000-5fold-ImageZip                                                                                  
50000 Labeled Captcha Images for DeepLearning                                                                                  
50K Celebrity Faces Image Dataset                                                                                  This dataset is a curated subset of the [CelebFaces Attributes (CelebA) Dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html), handpicked for deep learning tasks such as image synthesis and facial recognition. It includes **50,000 celebrity face images** from diverse identities, covering a wide range of poses, backgrounds, and facial attributes. These images are suitable for experimenting with GANs, facial recognition models, and other machine learning tasks related to face analysis. This dataset is perfect for hobbyists, researchers, and machine learning practitioners looking to experiment with a manageable yet diverse collection of celebrity face images.
50k+ Sneaker Images                                                                                  
50w-Chinese-dialogue-text                                                                                  
512 sized PNG images SeamCarving for Mayo STRIP AI                                                                                  Dataset For Mayo STRIP AI Use seam-carving to resize tif files to 512x512 png files. Because of imbalanced data, i upscaled LAA images twice. CE(547) + LAA(207*3 = 621) = 1168 image files in my dataset. For using flow_from_directory easy, I made 0(LAA) and 1(CE) directories.
512_Images_ab_70k                                                                                  
512x512 Melanoma TFRecords 70k Images                                                                                  Discussion about this dataset is [here][1] These TFRecords contain Kaggle's Melanoma Classification competition's 30k Train and 10k Test images plus 30k External images with Meta Data. They have the following features: feature = { 'image': _bytes_feature, 'image_name': _bytes_feature, 'patient_id': _int64_feature, 'sex': _int64_feature, 'age_approx': _int64_feature, 'anatom_site_general_challenge': _int64_feature, 'source': _int64_feature, 'target': _int64_feature } The test TFRecords do not have `target` nor `source` [1]: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/156245
512x512 test images                                                                                  
512x512_LITS_train_images_masks                                                                                  
512x512_train_images_jpeg_50k                                                                                  
540 Images Of Popular Graph Theory Graphs                                                                                  ### Content This dataset contains 540 images of popular graphs from the world of graph theory. Many different types of graphs from various graph families and complexities and even more hidden stories and questions. ### Inspiration There are various tasks when researching less trivial graphs and usually very computationally expensive, especially when dealing with higher-order graphs. Can we use our state of the art computer vision pipeline and algorithms to extract insight from graph images? Insight such as smaller and simpler parts like the number of vertices and edges to more difficult questions like clique sizes and graph radius and paths.
569-Images-Masks-Water                                                                                  
59K OCR snippets from 95K annotated Twitter images                                                                                  # **Dataset of paper: '*Enriching social analytics with latent Twitter image information*'** In this paper, we propose a framework that uses latent information from Twitter images by employing the Google Cloud Vision API platform aiming at enriching social analytics with semantics and textual information. Our study reveals that user-generated content, linked data as well as hidden concepts and textual information from social images can be highly considered for enriching social analytics. Therefore, we publish our annotated dataset for further use and evaluation from our research community. # Dataset overview ## Full dataset An overview of the dataset is provided in the table below: | Entity | Value | | --- | --- | | Images (Occurrences) | 94,745 (95,946) | | Unique Vision API Labels | 7,098 | | Labels per Image | 7.8 | | Images with text | 58,899 (62,2%) | | Unique OCR terms | 309,014 | | OCR terms per Image | 13.6 | | Twitter accounts | 411 | | Twitter accounts with Labels | 197 | | Twitter account Labels | 23 | | Labels per Twitter Account | 1.87 | ## Politics subset The first use case of our research was on (Greek and foreign) political Twitter accounts. The following table presents an overview of this particular subset: | Entity | Value | | --- | --- | | Images (Occurrences) | 39,499 (40,585) | | Unique Vision API Labels | 4,059 | | Labels per Image | 7.6 | | Images with text | 26,953 (68.2%) | | Unique OCR terms | 188,077 | | OCR terms per Image | 16.9 | | Twitter accounts | 130 | | Twitter accounts Labels | 12 | | Images per Twitter Account | 312.2 | ## Celebrities subset The second use case of our research was on celebrities' Twitter accounts. The following table presents an overview of this particular subset: | Entity | Value | | --- | --- | | Images (Occurrences) | 22,331 (22,393) | | Unique Vision API Labels | 3,790 | | Labels per Image | 8.2 | | Images with text | 13,138 (58.8%) | | Unique OCR terms | 46,243 | | OCR terms per Image | 6.8 | | Twitter accounts | 54 | | Twitter accounts Labels | 10 | | Images per Twitter Account | 414.7 | ## Press subset The third use case of our research was on Twitter accounts representing press (paper or digital) agencies. The following table presents an overview of this particular subset: | Entity | Value | | --- | --- | | Images (Occurrences) | 4,978 (4,979) | | Unique Vision API Labels | 2,772 | | Labels per Image | 7.6 | | Images with text | 3,577 (71.8%) | | Unique OCR terms | 90,623 | | OCR terms per Image | 71 | | Twitter accounts | 10 | | Twitter accounts Labels | 1 | | Images per Twitter Account | 497.9 | # Applications This dataset can prove a valuable source to a series of applications and tasks, such as: - training purposes of image multi-label classification models, - automatic generation of description for images displayed in browsers for aiding visually impaired users, - improving the efficiency of recommendation systems, - enriching social analytics with semantics and textual information, - identification of latent textual patterns in Twitter accounts of similar or overlapping communities, - evaluation of correlation between image context and OCR textual content. #Acknowledgements In case you find our research and dataset useful, please consider citing the following: **Publication A**: G. Razis, G. Theofilou, and I. Anagnostopoulos, “Enriching social analytics with latent Twitter image information”, 15th International Workshop on Semantic and Social Media Adaptation and Personalization (SMAP), Zakynthos, Greece, 2020, pp. 1-7. DOI: https://doi.org/10.1109/SMAP49528.2020.9248464 **Publication B**: G. Razis, G. Theofilou, and I. Anagnostopoulos, “Latent Twitter Image Information for Social Analytics”, Information Applications, Social and Semantic Trends: Tools and Applications, **Under Review**. DOI: TBA **Dataset**: G. Razis, G. Theofilou, and I. Anagnostopoulos, “42K OCR snippets from 67K annotated Twitter images”, Kaggle, DOI: https://www.doi.org/10.34740/kaggle/ds/732777.
5cap_single_image                                                                                  
6 Human Emotions for image classification                                                                                  Dataset to classify people sentiments using CNN and DL techniques. This dataset was collected from many websites.
6 Human Emotions for image classification                                                                                  Dataset to classify people sentiments using CNN and DL techniques. This dataset was collected from many websites. The dataset contains 2 categories happy and sad and soon will be updated to contain other categories.
60,000+ Images of Cars                                                                                  ### Context This dataset was obtained from: https://github.com/nicolas-gervais/predicting-car-price-from-scraped-data/tree/master/picture-scraper # Welcome to _The Car Connection Picture Dataset! It scrapes 297,000 pictures, of which around 198,000 unique URLs. Many of these are interior images, which are useless. You should have around 60,000 pictures in the end. ![img](https://user-images.githubusercontent.com/46652050/71590299-ebd23f00-2af5-11ea-916f-f19ff6fad04a.jpg)
6000+ Store Items Images Classified By Color                                                                                  Here is a novel dataset for image classification. 6239 images are classified into 12 categories based on their color. I was unable to find a similar dataset for color identification. In addition, I have included an unlabeled test directory, which can be used for competitions.
608x608px particle image data set for YOLO                                                                                  
60_CRCNN50d_JW0419R_200epoch_Imagenet_457                                                                                  
60k-data-with-context-v2                                                                                  This dataset can be used to train an Open Book model for Kaggle's LLM Science Exam competition. This dataset was generated by searching and concatenating all publicly shared datasets on Sept 1 2023. The `context` column was generated using Mgoksu's notebook [here][21] with `NUM_TITLES=5` and `NUM_SENTENCES=20` The `source` column indicates where the dataset originated. Below are the sources: source = 1 & 2 * Radek's 6.5k dataset. Discussion [here][4] annd [here][5], dataset [here][6]. source = 3 & 4 * Radek's 15k + 5.9k. Discussion [here][7] and [here][8], dataset [here][9] source = 5 & 6 * Radek's 6k + 6k. Discussion [here][10] and [here][11], dataset [here][12] source = 7 * Leonid's 1k. Discussion [here][13], dataset [here][14] source = 8 * Gigkpeaeums 3k. Discussion [here][15], dataset [here][16] source = 9 * Anil 3.4k. Discussion [here][17], dataset [here][18] source = 10, 11, 12 * Mgoksu 13k. Discussion [here][19], dataset [here][20] [4]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/425941 [5]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/426174 [6]: https://www.kaggle.com/datasets/radek1/additional-train-data-for-llm-science-exam [7]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/432316 [8]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/431786 [9]: https://www.kaggle.com/datasets/radek1/15k-high-quality-examples [10]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/432607 [11]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/432621 [12]: https://www.kaggle.com/datasets/radek1/sci-or-not-sci-hypthesis-testing-pack [13]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/424627 [14]: https://www.kaggle.com/datasets/leonidkulyk/wikipedia-stem-1k [15]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/430480 [16]: https://www.kaggle.com/datasets/mozattt/llm-science-3k-data [17]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/430988 [18]: https://www.kaggle.com/datasets/nlztrk/eduqg-dataset-llm-science-exam-format-34k [19]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/432182 [20]: https://www.kaggle.com/datasets/mgoksu/llm-science-exam-sciq-dataset [21]: https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model
60k_context                                                                                  
64x48image                                                                                  
6992 Meme Images Dataset with Labels                                                                                  Explore the sentiments behind internet memes with this diverse dataset of 6992 meme images. The set is aimed at tasks like sentiment classification and majority voting using any six classifiers of your choice (three for images, three for text) from the sklearn library. Outputs should include confusion matrix, accuracy, recall, precision, and F1-measure, providing a comprehensive overview of classifier performance. Ideal for those interested in multimodal data, social media analysis, NLP, image/text classification, text mining, machine learning, deep learning, and sentiment analysis.
7 Celebrity Images                                                                                  
7 Type of Nepali Medicine strip Images                                                                                  My dataset comprises images of seven different types of medicines that are popular in Nepal and Asia. These images capture both the front and back views of the medicine strips under varying lighting conditions and resolutions.
7 topic data for text classification                                                                                  ### Context This data set was collected to design an AI system that will try to predict the topic of the text that is passed by the user. ### Content This data set was collected from school text books of 11th and 12th grade and other open sources , the data was extracted from PDF files through python code. **Note : Data set needs to be cleaned for stopwords , punctuations , numbers and special characters before using as per needs.** ### Acknowledgements Thanks to NCRT website and other open sources
70 Dog Breeds-Image Data Set                                                                                  
70 images: actors with different hairstyles                                                                                  
7000 Dress Style Image [Resized 640 * 640]                                                                                  The following pre-processing was applied to each image: * Auto-orientation of pixel data (with EXIF-orientation stripping) * Resize to 640x640 (Stretch) The following augmentation was applied to create 3 versions of each source image: * 50% probability of horizontal flip * Random rotation of between -10 and +10 degrees * Random Gaussian blur of between 0 and 1.5 pixels * Salt and pepper noise was applied to 3 percent of pixels
768 image data whaless                                                                                  
768 image data whaless ssssssssss                                                                                  
768 images whale Dataset                                                                                  
768x768 Melanoma TFRecords 70k Images                                                                                  Discussion about this dataset is [CNN Input Size Explained][1]. Image size in this dataset is 768x768. The other contents are the same as the original one [512x512 Melanoma TFRecords 70k Images][2] made by [Chris Deotte][3]. JPEG quality of the image is 85%. This comes from the Kaggle's run time disk size limitation 5 GB. The features in TFRecords of training set are as follows: ``` feature = { 'image': _bytes_feature(feature0), 'image_name': _bytes_feature(feature1), 'patient_id': _int64_feature(feature2), 'sex': _int64_feature(feature3), 'age_approx': _int64_feature(feature4), 'anatom_site_general_challenge': _int64_feature(feature5), 'source': _int64_feature(feature6), 'target': _int64_feature(feature7) } ``` The test TFRecords do not have `target` nor `source`. The notebook to create this dataset is [Create Various Sizes of External Data (Version 4)][4]. The datasets for the other image size are: - [512x512 Melanoma TFRecords 70k Images][2] by [Chris Deotte][3] - [384x384 Melanoma TFRecords 70k Images][5] - [256x256 Melanoma TFRecords 70k Images][6] [1]: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/160147 [2]: https://www.kaggle.com/cdeotte/512x512-melanoma-tfrecords-70k-images [3]: https://www.kaggle.com/cdeotte [4]: https://www.kaggle.com/tt195361/create-various-sizes-of-external-data?scriptVersionId=37586599 [5]: https://www.kaggle.com/tt195361/384x384-melanoma-tfrecords-70k-images [6]: https://www.kaggle.com/tt195361/256x256-melanoma-tfrecords-70k-images
7krealimagesonlydataset                                                                                  
7sec_audio2022                                                                                  
8 kinds of image classification                                                                                  This dataset consists of around 35K images, it has 8 categories of photos which are seas, streets, buildings, glaciers, mountains, forests, cats, and dogs. All images shape is (150 , 150 , 3 ) , I collect them from many sources on the internet.
8+ M. Spotify Tracks, Genre, Audio Features                                                                                  
800+ labeled Pokemon Images                                                                                  
88,000+ Images of Cars                                                                                  The 88,000+ Images of Cars dataset is a comprehensive collection of high-quality images of cars for use in image generation and object detection tasks. This dataset includes a wide variety of car images, including sedans, SUVs, sports cars, trucks, and more.
8aug_testwithwholeimage                                                                                  
8chan Image Embeddings                                                                                  Online Image-boards always have been a wealth of resources, either be conversation, images, discussions and etc. Though there are obvious reasons why these boards are restricted from clear-net search engines regardless their data is still extremely valuable. In this dataset, I release high-quality image embeddings from all the images that I scrapped from 8chan website. 8chan is a notorious Image board, a bad brother of clear-net 4chan. It is only accessible through Tor browser and circuits. I have scrapped the site's all boards and all of its threads to extract images and later created embeddings through OpenAI's CLIP model. I am going to share the full-(raw) image dataset on Huggingface. Please feel free to check that later :)
92_retinanet50d_ImageNet_12epoch_340                                                                                  
92_retinanet50d_ImageNet_LR0006_342epoch_407                                                                                  
95 images of each of 1281 Pokemon                                                                                  
95-Cloud: Cloud Segmentation on Satellite Images                                                                                  ### Context Detection of clouds is an important step in many remote sensing applications that are based on optical imagery. 95-Cloud dataset is an extensive dataset for this task to help researchers to evaluate their deep learning-based cloud segmentation models. ### Content 95-Cloud dataset is an extension of our previous 38-Cloud dataset. 95-Cloud has 57 more Landsat 8 scenes for 'training' which are uploaded here. The rest of the training scene and the test scenes can be downloaded from [here](https://www.kaggle.com/sorour/38cloud-cloud-segmentation-in-satellite-images/download). More information about the dataset can be found at: https://github.com/SorourMo/95-Cloud-An-Extension-to-38-Cloud-Dataset https://github.com/SorourMo/38-Cloud-A-Cloud-Segmentation-Dataset https://github.com/SorourMo/Cloud-Net-A-semantic-segmentation-CNN-for-cloud-detection ### Acknowledgements This dataset has been prepared by Laboratory for Robotics Vision (LRV) at School of Engineering Science, Simon Fraser University, Vancouver, Canada.
99k-data-with-context-v2                                                                                  Discussion describing this dataset is [here][1] [1]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/444202
9_30_output_bert_lgb_large_text_first_pt_512                                                                                  
9_classes_noisy_image_dataset                                                                                  ### Context https://ieee-dataport.org/documents/cnn-based-noise-classification-and-denoising-images Paper: https://ieeexplore.ieee.org/document/8929277
A Benchmark Data for Turkish Text Categorization                                                                                  ### Context The data set is taken from kemik group http://www.kemik.yildiz.edu.tr/ The data are pre-processed for the text categorization, collocations are found, character set is corrected, and so forth. We named TTC4900 by mimicking the name convention of TTC 3600 dataset shared by the study http://journals.sagepub.com/doi/abs/10.1177/0165551515620551 If you use the dataset please insert web site as footnote and cite the paper as: Yildirim S. (2020) Comparing Deep Neural Networks to Traditional Models for Sentiment Analysis in Turkish Language. In: Agarwal B., Nayak R., Mittal N., Patnaik S. (eds) Deep Learning-Based Approaches for Sentiment Analysis. Algorithms for Intelligent Systems. Springer, Singapore https://doi.org/10.1007/978-981-15-1216-2_12 ### Content Each row represents the documents, and the categories ### Acknowledgements Thanks to kemik group, ### Inspiration Text Categorization, BOW, LSI, Deep Learning Approaches are the techniques to be applied
A Collection of Dental X-ray Images for Analysis                                                                                  Dental imaging plays a pivotal role in diagnosing oral health conditions, guiding treatment plans, and enhancing patient care. This meticulously curated dataset features a diverse collection of orthopantomogram (OPG) teeth X-ray images, comprising 70 high-quality samples sourced from local hospitals and trusted acquaintances. With annotations provided, this dataset is primed for training and testing machine learning models for dental image analysis tasks such as tooth type classification, anomaly detection, and more. Inspired by the burgeoning field of computer vision and driven by the need for improved diagnostic tools in dentistry, this dataset aims to empower researchers, practitioners, and enthusiasts to explore innovative solutions for enhancing oral healthcare outcomes
A Curated List of Image Deblurring Datasets                                                                                  Given a blurred image, image deblurring aims to produce a clear, high-quality image that accurately represents the original scene. Blurring can be caused by various factors such as camera shake, fast motion, out-of-focus objects, etc. making it a particularly challenging computer vision problem. This has led to the recent development of a large spectrum of deblurring models and unique datasets. Despite the rapid advancement in image deblurring, the process of finding and pre-processing a number of datasets for training and testing purposes has been both time exhaustive and unnecessarily complicated for both experts and non-experts alike. Moreover, there is a serious lack of ready-to-use domain-specific datasets such as face and text deblurring datasets. To this end, the following card contains a curated list of ready-to-use image deblurring datasets for training and testing various deblurring models. Additionally, we have created an extensive, highly customizable python package for single image deblurring called [DBlur](https://pypi.org/project/dblur/) that can be used to train and test various SOTA models on the given datasets just with 2-3 lines of code. Following is a list of the datasets that are currently provided: - **GoPro**: The GoPro dataset for deblurring consists of 3,214 blurred images with a size of 1,280×720 that are divided into 2,103 training images and 1,111 test images. - **HIDE**: HIDE is a motion-blurred dataset that includes 2025 blurred images for testing. It mainly focus on pedestrians and street scenes. - **RealBlur**: The RealBlur testing dataset consists of two subsets. The first is RealBlur-J, consisting of 1900 camera JPEG outputs. The second is RealBlur-R, consisting of 1900 RAW images. The RAW images are generated by using white balance, demosaicking, and denoising operations. - **CelebA**: A face deblurring dataset created using the CelebA dataset which consists of 2 000 000 training images, 1299 validation images, and 1300 testing images. The blurred images were created using the blurred kernels provided by [Shent et al. 2018](https://arxiv.org/pdf/1803.03345.pdf) - **Helen**: A face deblurring dataset created using the Helen dataset which consists of 2 000 training images, 155 validation images, and 155 testing images. The blurred images were created using the blurred kernels provided by [Shent et al. 2018](https://arxiv.org/pdf/1803.03345.pdf) - **Wider-Face**: A face deblurring dataset created using the Wider-Face dataset which consists of 4080 training images, 567 validation images, and 567 testing images. The blurred images were created using the blurred kernels provided by [Shent et al. 2018](https://arxiv.org/pdf/1803.03345.pdf) - **TextOCR**: A text deblurring dataset created using the TextOCR dataset which consists of 5000 training images, 500 validation images, and 500 testing images. The blurred images were created using the blurred kernels provided by [Shent et al. 2018](https://arxiv.org/pdf/1803.03345.pdf)
A Database of Leaf Images: from Mendeley Data                                                                                  
A Dataset of Multispectral Potato Plants Images                                                                                  The dataset contains aerial agricultural images of a potato field with manual labels of healthy and stressed plant regions. The images were collected with a Parrot Sequoia multispectral camera carried by a 3DR Solo drone flying at an altitude of 3 meters. The dataset consists of RGB images with a resolution of 750×750 pixels, and spectral monochrome red, green, red-edge, and near-infrared images with a resolution of 416×416 pixels, and XML files with annotated bounding boxes of healthy and stressed potato crop.
A big Text and Vocabs                                                                                  
A surprisingly difficult image Dataset [Heroquest]                                                                                  ### Context I would like to write a quest scraper. A Tool that takes a look at an image of a Heroquest quest map and can derive all symbols with their positions correctly; turning the 'dead' image once again into an editable quest file. I already have a simulation tool running which can predict the deadliness of a quest layout - for that I need accurate recognition - and on Heroscribe.org a great java-based tool for editing quest files can be downloaded. In ideal case, my tool can take an image and output the Heroscribe format. That's a task for later. Today, we just want to do the recognition. I took around 100 Maps from the ancient game Heroquest, cut them down to single square images and used them as training data set for a neural net. The incredible imbalance in the data set made that I artificially generated 100 more maps, to boost the underrepresented symbol appearances. All of the maps have been made in Heroscribe (downloadable at Heroscribe.org) and exported as png; like that they have the same size. ![EU format Heroquest map](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1711994%2F9050fb998965fcf24ef4b76d4c9fe4d7%2F11-BastionofChaos_EU.png?generation=1570256920345210&alt=media) Now I have 25 thousand snippets of Heroquest Quest Maps, in two cut out factors (78 and 34 pixel). In each sample, there can be one or more of the following things: Monsters, Furniture, Doors, and rooms. For each snippet, the position information is already preserved in the data set: It was taken during the cropping process. You know where you cut the image right now, so why not keeping that information right away? In the easiest case, there is just one symbol in a square. In some cases there are two or three of them at the same time; like there can be one or more door, one monster, and the square itself is discolored because the room is a special room. So here we have do recognize several symbols at the same time. The first (roughly half) of the dataset contains real data from real maps, in the second half I've made up data to fill gaps in the data coverage. ### The Y-Labels Y-Data is provided together with the X-Data in the pickle file; in three columns. One for single-square-items and furniture; one for doors and one for rooms. If there were too many items in one square, or sometimes when I was tired from labelling all the data, it could happen that I was putting a label in the wrong column or even put the wrong label. After many checks, I am quite sure that there is only a low percentage of wrong labels anymore; but still there can be labels in a column where they don't belong. I tried to train a resnet to recognize the Y-Data given and it was surprisingly difficult. Then I was separating the Y-Data into items that usually appear only one time per square, so that I could train three nets on three different item types. The remaining difficulty comes from multi-square-symbols that appear in any of four 90° rotations; from which only one square is presented to the net in the 'center image' data set. A probable way forward could be to train one net for single-square-symbol recognition and to separate out all the multi-square-symbols and let them be recognized in the bigger 78x78 snippet. Here, see for yourself: The 'center pic' of a table: It is difficult to recognize anything here. ![Table, small cutout](https://i.imgur.com/yCP4pF9.png) And the same square in the 'pic' cutout: ![Table, big cutout](https://i.imgur.com/9a9scVN.png) 'center pic' of a Treasure Chest: Sufficient to recognize it; easily! ![Treasure Chest, small cutout](https://i.imgur.com/KjX1QUV.png) Big cutout of the same Treasure Chest: Distracting details in the surrounding. ![Treasure Chest, big cutout](https://i.imgur.com/OPBlWHV.png) For each symbol, I also extracted the two main colors. There are maps in the EU format, which are completely black and white (see above picture). The other half of the maps is in US format: Monsters are green, furniture is dark red, traps and trapped furniture have a orange or turquoise background instead of white; Hero symbols are bright red. ![US format Heroquest map](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1711994%2F3900bd109f86618a48e619ec00ce892d%2F11-BastionofChaos_US.png?generation=1570257035192555&alt=media) The symbols in the data set are black and white, all of them. The columns 'min_color' and 'max_color' preserve the color information. I planned to give it as an auxiliary input to the neural net, but didn't yet get round to do it. The color information can be distracting, too: In the US map format, sometimes otherwise normal furniture symbols are marked with trap colors when they thought about some special event for it. ### Target acceptance rates Those are quite easy images on one side. Noiseless, size-fixed, no skew or zoom coming from photography... I even bootstrapped my data set by using K-Means to bulk-label some images. Yes, K-Means. It is easy to classify this data beyond the 95% recognition. So what's the catch? First of all, the number of classes. It's not a single-class recognition problem; in this data set we have around 100 classes. That's more difficult for a neural net than just saying 'Yes' or 'No. Then the number itself: one Map contains between 20 and 100 symbols, lets assume 50 as good average. 95% good recognition means... there are two or three errors per map. In the end you would invest more time searching for the errors than you would spend if you remake it from scratch; worse, the errors could slip by and end up being used. So, **95% is not good enough.** 98% means, there is one error per map. 99% means, there is one error in two maps. 99.5% means, one error in four maps. 99.9 is one error in ten maps. That would be ok, finally. I am electrical engineer. In my profession, **the acceptable has to be topped by half before you can sell**. For any usable quest scraper, I estimate 99.95% is the 'sellable' acceptance rate. Given the noise-free dataset and the clear lines of symbols, this should be easy. Or is it? :-) Neural nets are a completely new topic for me. Either I am just a beginner who doesn't understand a thing, or there are too many errors in the manually constructed training data set, or I was not creative enough. Also, I was training on my laptop, which wasn't strong enough to do more than a couple of epochs - after 150 epochs normally the weekend was over. However, the acceptance rate was going into saturation long before. After the 10th, latest 20th epoch nothing was happening anymore; I was in overfit, and yet the recognition on unseen snippets was poor to say the least. ### The Challenge Today is 5th of October. I offer a bottle of Languedoc Wine (From a site that I know to make very good wine!) to the person who can solve this dataset without overfit to 99.95% until 31.06.2020! If you're living in a country that is not allowing wine to be imported or is too far away, I sure find some other way of saying thank you. Help! :-) ### Content, technical description The dataset contains a pickle of a pandas dataframe. The columns are: 'min_color', 'max_color': Each symbol has two colors. Taking the 22-pixel center of the small image, I extracted the minimum and maximum color that have been found there. In EU Maps, those are always black and white. In US maps, those actually contain information. 'filename': The image was saved on my personal ssd somewhere and this is the path. This may help you extracting a feature, but I doubt it. 'x', 'y' : These are map positions. Square (1, 1) is the top left position on the heroquest map. The typical height is 19 squares, typical width is 26 squares, but there are exceptions. 'pic': The big picture, sometimes containing too many details (see above). Used as X for training, this would be of shape (n_samples, 78, 78, 1) 'center_pic': The small picture, better for small symbols; but it's sometimes difficult to recognize symbols that are bigger than one square. If it is used as X for training, it's shape is (n_samples, 34, 34, 1) 'real_Y': column containing str labels or 0 mostly for Furniture, Monsters and Secret Doors 'door_Y': column containing str labels or 0 mostly for Doors, if they are not secret. Rarely there is more than one door on a square - in this case it contains the label of only one of the doors. 'room_y': column containing the str labels or 0 for a non-standard room. Those are in the quest picture underneath any other symbol and therefore quite difficult to find; often impossible to see in the 'center pic'. ### Acknowledgements Thanks to Stephen Baker to have invented this great board game in 1989
A to Z Flowers - Features & Images                                                                                  ### Content A list of 223 flowers with features such as Plant Type, Flower Color, Blooming Season, Plant Height, Soil / Water / Sun Needs & Overal Maintenance. Also included is an Image of the flower. ### Acknowledgements A to Z Flowers
A1_Clf_ImageData_Tensor                                                                                  
A2 + bonus images                                                                                  
AADB_ImageDatabase                                                                                  
ABAW5_audio2                                                                                  
ABCImages                                                                                  
ABSA-Dataset_text_delimiter_based                                                                                  
ACCA Textbooks and Revision Notes                                                                                  
ACL ARC citation contexts with DBLP ID                                                                                  
ACL Anthology Corpus with Full Text                                                                                  [![License](https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/) This repository provides full-text and metadata to the ACL anthology collection (80k articles/posters as of September 2022) also including .pdf files and grobid extractions of the pdfs. ## How is this different from what ACL anthology provides and what already exists? - We provide pdfs, full-text, references and other details extracted by grobid from the PDFs while [ACL Anthology](https://aclanthology.org/anthology+abstracts.bib.gz) only provides abstracts. - There exists a similar corpus call [ACL Anthology Network](https://clair.eecs.umich.edu/aan/about.php) but is now showing its age with just 23k papers from Dec 2016. ---- The goal is to keep this corpus updated and provide a comprehensive repository of the full ACL collection. This repository provides data for `80,013` ACL articles/posters - 1. 📖 All PDFs in ACL anthology : **size 45G** [download here](https://drive.google.com/file/d/1OGHyJrkaVpbrdbmxsDotG-tI3LiKyxuC/view?usp=sharing) 2. 🎓 All bib files in ACL anthology with abstracts : **size 172M** [download here](https://drive.google.com/file/d/1dJ-iE85moBv3iYG2LhRLT6KQyVkmllBg/view?usp=sharing) 3. 🏷️ Raw grobid extraction results on all the ACL anthology pdfs which includes full text and references : **size 3.6G** [download here](https://drive.google.com/file/d/1xC-K6__W3FCalIDBlDROeN4d4xh0IVry/view?usp=sharing) 4. 💾 Dataframe with extracted metadata (table below with details) and full text of the collection for analysis : **size 489M** [download here](https://drive.google.com/file/d/1CFCzNGlTls0H-Zcaem4Hg_ETj4ebhcDO/view?usp=sharing) | **Column name** | **Description** | | :----------------: | :---------------------------: | | `acl_id` | unique ACL id | | `abstract` | abstract extracted by GROBID | | `full_text` | full text extracted by GROBID | | `corpus_paper_id` | Semantic Scholar ID | | `pdf_hash` | sha1 hash of the pdf | | `numcitedby` | number of citations from S2 | | `url` | link of publication | | `publisher` | - | | `address` | Address of conference | | `year` | - | | `month` | - | | `booktitle` | - | | `author` | list of authors | | `title` | title of paper | | `pages` | - | | `doi` | - | | `number` | - | | `volume` | - | | `journal` | - | | `editor` | - | | `isbn` | - | ```python >>> import pandas as pd >>> df = pd.read_parquet('acl-publication-info.74k.parquet') >>> df acl_id abstract full_text corpus_paper_id pdf_hash ... number volume journal editor isbn 0 O02-2002 There is a need to measure word similarity whe... There is a need to measure word similarity whe... 18022704 0b09178ac8d17a92f16140365363d8df88c757d0 ... None None None None None 1 L02-1310 8220988 8d5e31610bc82c2abc86bc20ceba684c97e66024 ... None None None None None 2 R13-1042 Thread disentanglement is the task of separati... Thread disentanglement is the task of separati... 16703040 3eb736b17a5acb583b9a9bd99837427753632cdb ... None None None None None 3 W05-0819 In this paper, we describe a word alignment al... In this paper, we describe a word alignment al... 1215281 b20450f67116e59d1348fc472cfc09f96e348f55 ... None None None None None 4 L02-1309 18078432 011e943b64a78dadc3440674419821ee080f0de3 ... None None None None None ... ... ... ... ... ... ... ... ... ... ... ... 73280 P99-1002 This paper describes recent progress and the a... This paper describes recent progress and the a... 715160 ab17a01f142124744c6ae425f8a23011366ec3ee ... None None None None None 73281 P00-1009 We present an LFG-DOP parser which uses fragme... We present an LFG-DOP parser which uses fragme... 1356246 ad005b3fd0c867667118482227e31d9378229751 ... None None None None None 73282 P99-1056 The processes through which readers evoke ment... The processes through which readers evoke ment... 7277828 924cf7a4836ebfc20ee094c30e61b949be049fb6 ... None None None None None 73283 P99-1051 This paper examines the extent to which verb d... This paper examines the extent to which verb d... 1829043 6b1f6f28ee36de69e8afac39461ee1158cd4d49a ... None None None None None 73284 P00-1013 Spoken dialogue managers have benefited from u... Spoken dialogue managers have benefited from u... 10903652 483c818c09e39d9da47103fbf2da8aaa7acacf01 ... None None None None None [73285 rows x 21 columns] ``` The provided ACL id is consistent with S2 API as well - [https://api.semanticscholar.org/graph/v1/paper/ACL:P83-1025](https://api.semanticscholar.org/graph/v1/paper/ACL:P83-1025) The API can be used to fetch more information for each paper in the corpus. --- ## Text generation on Huggingface We fine-tuned the distilgpt2 model from huggingface using the full-text from this corpus. The model is trained for generation task. Text Generation Demo : <https://huggingface.co/shaurya0512/distilgpt2-finetune-acl22> Example: ```python >>> from transformers import AutoTokenizer, AutoModelForCausalLM >>> tokenizer = AutoTokenizer.from_pretrained('shaurya0512/distilgpt2-finetune-acl22') >>> model = AutoModelForCausalLM.from_pretrained('shaurya0512/distilgpt2-finetune-acl22') >>> >>> input_context = 'We introduce a new language representation' >>> input_ids = tokenizer.encode(input_context, return_tensors='pt') # encode input context >>> outputs = model.generate( ... input_ids=input_ids, max_length=128, temperature=0.7, repetition_penalty=1.2 ... ) # generate sequences >>> print(f'Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}') ``` ```text Generated: We introduce a new language representation for the task of sentiment classification. We propose an approach to learn representations from unlabeled data, which is based on supervised learning and can be applied in many applications such as machine translation (MT) or information retrieval systems where labeled text has been used by humans with limited training time but no supervision available at all. Our method achieves state-oftheart results using only one dataset per domain compared to other approaches that use multiple datasets simultaneously, including BERTScore(Devlin et al., 2019; Liu & Lapata, 2020b ) ; RoBERTa+LSTM + L2SRC - ``` ### TODO 1. ~~Link the acl corpus to semantic scholar(S2), sources like S2ORC~~ 2. Extract figures and captions from the ACL corpus using pdffigures - [scientific-figure-captioning](https://github.com/billchen0/scientific-figure-captioning) 3. Have a release schedule to keep the corpus updated. 4. ACL citation graph 5. ~~Enhance metadata with bib file mapping - include authors~~ 6. ~~Add citation counts for papers~~ 7. Use [ForeCite](https://github.com/allenai/ForeCite) to extract impactful keywords from the corpus 8. Link datasets using [paperswithcode](https://github.com/paperswithcode/paperswithcode-data)? - don't know how useful this is 9. Have some stats about the data - [linguistic-diversity](http://stats.aclrollingreview.org/submissions/linguistic-diversity/); [geo-diversity](http://stats.aclrollingreview.org/submissions/geo-diversity/); if possible [explorer](http://stats.aclrollingreview.org/submissions/explorer/) We are hoping that this corpus can be helpful for analysis relevant to the ACL community. **Please cite/star 🌟 this page if you use this corpus** ## Citing the ACL Anthology Corpus If you use this corpus in your research please use the following BibTeX entry: @Misc{acl_anthology_corpus, author = {Shaurya Rohatgi}, title = {ACL Anthology Corpus with Full Text}, howpublished = {Github}, year = {2022}, url = {https://github.com/shauryr/ACL-anthology-corpus} } [<img src='https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&logo=buy-me-a-coffee&logoColor=black'>](https://www.buymeacoffee.com/shauryrG) <!-- If you are feeling generous buy me a ☕ --> ## Acknowledgements We thank Semantic Scholar for providing access to the citation related data in this corpus. ## License ACL anthology corpus is released under the [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/). By using this corpus, you are agreeing to its usage terms.
ADNI Brain MRI 2 Images with FastSurfer QuickSeg                                                                                  This dataset contains FastSurfer QuickSegmentation preprocessed data for 2 subjects. The raw dataset has been collected from the ADNI dataset. The tool, FastSurfer has been used on my local machine. Segmentation for each subject needed 4 to 5 minutes approximately.
AI Cat and Dog Images DALL·E Mini                                                                                  The dataset contains dog and cat images generated by DALL·E Mini, an AI model that draws images from any prompt. The task for this dataset is binary classification All images are scaled 256x256 I was able to achieve some pretty interesting results messing around with different prompts: *** ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F10590800%2Ffcac2958da1bdb16f4fa3fb2e75100a1%2Fcat34.jpeg?generation=1661467903628673&alt=media)
AI Generated Barcode Images/Masks Dataset                                                                                  This dataset comprises approximately 16,000 AI-generated UPCA barcodes and their corresponding barcode line masks. The barcodes were generated using Stable Diffusion with Controlnet. Although they are AI-generated, each barcode possesses a scannable value. About 89% of the images can be accurately read using zxingcpp without any image processing, and zxingcpp can read all the associated masks (if processed via zxingcpp.read_barcode(255-img, try_rotate=True)). The Excel file contains the corresponding values for each barcode, with the file names listed alongside their associated UPCA numbers. The purpose of this dataset is to simulate the conditions of a barcode captured in a photograph, sometimes under less than ideal conditions while including a mask that represents the truth values of what the barcode would appear like under perfect conditions, without any blur, damage, or distortions. ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F11925310%2Fea847d2718265dc21c7df5973c2ef1cd%2Ftransition.gif?generation=1688917610073576&alt=media)
AI Generated Images vs Real Images                                                                                  **The dataset is a captivating ensemble of images sourced from two distinct channels: web scraping and AI-generated content. The content covers many subjects; however, special emphasis was placed on these topics: people, animals, portraits, scenery, and psychedelics.** **Key Features:** Web-Scraped Images: These images are harvested from various online sources across the web. Ranging from landscapes, paintings, psychedelic trips, and portraits, the web-scraped images offer a glimpse into the vast spectrum of digital imagery available online. **Projects and Applications:** Image Classification and Recognition: Researchers and developers can leverage the dataset to train machine learning models for image classification and recognition tasks. By incorporating both web-scraped and AI-generated images, models can learn to identify and categorize objects, scenes, and concepts across diverse domains with greater accuracy and generalization. Artistic Exploration and Creative Synthesis: Artists, designers, and creative enthusiasts can draw inspiration from the dataset to explore new avenues of artistic expression and experimentation. They can use AI-generated imagery as a canvas for artistic reinterpretation, blending traditional techniques with computational aesthetics to produce captivating artworks and multimedia installations. Data Visualization and Exploratory Analysis: Data scientists and researchers can analyze the dataset to uncover insights into visual trends, patterns, and correlations. Have fun!
AI Image Similarity Challenge: Descriptor Track                                                                                  You will receive a reference set of 1 million images and a query set of 50,000 images. Some of the query images are derived from images in the reference set, and the rest are not. For this Descriptor Track, your task is to compute image descriptors (embeddings) for both the 50,000 query and 1 million reference images. Your descriptors will be floating-point vectors of up to 256 dimensions.
AI Image- Human Image Training Data For Sniffusion                                                                                  
AI TEXT Viz                                                                                  This Data Notebook will help to text visualization to understand large documents
AI Text Detection Dataset                                                                                  
AI Text-2-Image Tweets (+ Sentiment labels)                                                                                  ### Attribution Dataset thumbnail by [OpenAI](https://openai.com/dall-e-2/). ### Context These tweets are about the most popular AI-as-a-service solutions on the Internet that generate images using the text prompt. The dataset was collected using the Twitter API and Python script by hashtags #dalle2 #glide #imagen #stablediffusion and related keywords. ### Content Each record contains username data, tweet stats, text + attachments data, and **sentiment analysis label**. ``` ,fullname,username,body,comments,retweet,quote,heart,attachment_type,ai_service,sentiment 0,Yusuke Hayashi（林祐輔）,@hayashiyus,3D digital art created to explain deep learning. #dalle #dalle2,0,0,0,1,image,dalle2,neutral 1,Surreal Googling,@SurrealGoogling,'#dalle2 selfie generated using # #stablediffusion, style #Midjourney - Clearly this is the real Dall E 2 - Version 2',0,0,0,0,image,dalle2,neutral 2,aiGrampy,@AiGrampy,PyPiPy gonna let it fly! #digitalart #aiart #dalle #dalle2 #midjourney,0,0,0,1,image,dalle2,positive 3,Cento Avaria,@AvariaCento,'Primeras pruebas con DALL·E 2: People traveling around a mountain with a cat and a dog, Dalí art #DALLE2 #DALLE #ComputerVision #AI #DeepLearning #GAN #test',0,2,0,0,image,dalle2,neutral 4,Celestial Colour Maker,@Mrunknownfc1,The Hand Of Fear (1976) #DoctorWho #DrWho #Dalle2,1,2,0,14,image,dalle2,neutral ``` ### Inspiration Using text mining techniques, you can analyze the popularity and demand of each text-2-image service. Also, the dataset will give insights into which service has the most positive feedback.
AI Vs Human Text                                                                                  Around 500K essays are available in this dataset, both created by AI and written by Human. I have gathered the data from multiple sources, added them together and removed the duplicates
AI generated Retinal Image Dataset                                                                                  This dataset contains glaucomatous and normal retinal images generated using the OpenAI API. Glaucomatous images show eyes that have been diagnosed with glaucoma and have a cup-to-disc ratio greater than 0.5, while normal images show eyes without any signs of glaucoma or other eye diseases. The images are high-quality JPEG files with a size of 1024x1024 pixels, and have been collected and curated to ensure accuracy and consistency. Additionally, it's worth noting that the retinal images in this dataset have been generated using the OpenAI API keys, which means they were produced using advanced artificial intelligence techniques. These techniques allow for the generation of high-quality images that closely resemble real retinal scans, providing a valuable resource for researchers and developers in the field of ophthalmology. This dataset can be used for research and training machine learning models to classify retinal images as glaucomatous or normal, with the goal of improving the accuracy and efficiency of glaucoma diagnosis and treatment The dataset consists of 307 retinal images, which can be divided into two categories: 152 images of individuals diagnosed with Glaucoma, and 155 images of individuals with a healthy eye condition
AI generated images for Scene Classification                                                                                  
AI generated text detection 5th place                                                                                  
AI generated text detection data preprocessed                                                                                  This dataset is pre-processes version of original dataset (https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset/data) Various pre-processing techniques like removing punctuation, removing stopwords, removing link urls, removing most frequent words.
AI hobby group image classification                                                                                  
AI images recognition                                                                                  
AI model image                                                                                  
AI vs Human Generated Images                                                                                  
AI vs Non AI Generated Images                                                                                  Dataset focused on identifying the difference between AI generated Images and Non-AI generated Images. 6500 images from various AI generating Engines (Midjourney, Stable Diffusion (2022), DALLE, NightCafeAI, Disco Diffusion). The dataset contains a equal amount of different variation of artworks like Anime / Digital Art etc.
AI worker text                                                                                  
AI-Generated Osteoarthritis X-ray Image Dataset                                                                                  This dataset is an AI-generated collection of X-ray images depicting individuals with arthritis. This compilation presents a diverse set of X-ray images portraying individuals affected by arthritis, focusing particularly on knee joints impacted by Osteoarthritis. The dataset encompasses a spectrum of arthritis manifestations, including varying levels of joint inflammation, erosion, and deformity, meticulously annotated to emphasize specific features visible in X-ray imagery. Included in this dataset are **126 AI-generated X-ray images** featuring individuals grappling with Osteoarthritis. This meticulously curated collection serves as a significant asset for the healthcare sector, addressing a crucial gap by providing medical datasets essential for image classification tasks pertaining to arthritis. Explore and utilize this invaluable resource to advance research and enhance healthcare practices. ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F11608317%2Fa0296ae1f951c4819aed69e16886a153%2Fosteoarthritis_knee_arthritis210.png?generation=1695073812166827&alt=media). ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F11608317%2Fb7a1f9e948b814aa0093106191b9d171%2Fosteoarthritis_knee_arthritis195.png?generation=1695073876032102&alt=media). ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F11608317%2F22edb77296d8784ec378f9059bb034ad%2Fosteoarthritis_knee_arthritis188.png?generation=1695073940206163&alt=media)
AI-and-human text                                                                                  
AIC Arabic abstractive text summarization dataset                                                                                  
AIC24VidTestExternal                                                                                  
AICV Unlabelled WarmUp Text And Extra                                                                                  
AIC_AudioB1v1                                                                                  
AIC_AudioB1v2                                                                                  
AIC_AudioB2v1                                                                                  
AIC_AudioB2v2                                                                                  
AIC_AudioB2v3                                                                                  Video-L19
AIC_AudioB2v4                                                                                  Video_L20
AIC_AudioB3v0                                                                                  
AIC_AudioB3v1                                                                                  
AIC_AudioB3v2                                                                                  
AIC_AudioDetectionB3_2                                                                                  
AIDA_Image_Captioning                                                                                  
AIDA_Image_Captioning_InceptionResNetV2                                                                                  
AIDA_image_captioning_preprocessing_glove                                                                                  
AIGeneratedText                                                                                  
AITEX Fabric Image Database                                                                                  The textile fabric database consists of 245 images of 7 different fabrics. There are 140 defect-free images, 20 for each type of fabric. With different types of defects, there are 105 images. Images have a size of 4096×256 pixels. Defective images have been denominated as follows: nnnn_ddd_ff.png, where nnnn is the image number, ddd is the defect code, and ff is the fabric code. There is a mask of defect, denominated as: nnnn_ddd_ff_mask.png, where white pixels represent the defect area of the defective image. Defect free images have been denominated as follows: nnnn_000_ff.png, where defect code has been replaced by 0000 code. | Defect Code | Defect Description | | --- | --- | | 2 | Broken end | | 6 | Broken yarn | | 10 | Broken pick | | 16 | Weft curling | | 19 | Fuzzyball | | 22 | Cut selvage | | 23 | Crease | | 25 | Warp ball | | 27 | Knots | | 29 | Contamination | | 30 | Nep | | 36 | Weft crack | &gt;Please, if you use this images and the rest of information in your research, we kindly ask that you reference the original authors and their paper: ``` AFID: a public fabric image database for defect detection. Javier Silvestre-Blanes, Teresa Albero-Albero, Ignacio Miralles, Rubén Pérez-Llorens, Jorge Moreno AUTEX Research Journal, No. 4, 2019 https://content.sciendo.com/view/journals/aut/ahead-of-print/article-10.2478-aut-2019-0035.xml ```
AI_Generated_Images                                                                                  This dataset contains 19 images of boys generated by Copilot, an AI companion that can create imaginative and innovative content. The images are suitable for face and pose detection tasks, as they vary in facial expressions, poses, backgrounds, lighting, and occlusion. The images are in PNG format and have a resolution of 1024x1024 pixels. The dataset is licensed under CC0-1.0, which means you can use it for any purpose without attribution.
AI_Generated_Text_Competetion_Synthetic_Dataset                                                                                  
AI_TEXT_DETECTION_DATASET                                                                                  
AI_cartoon_images                                                                                  
AImageNet 256x256 TFRecord Dataset                                                                                  
AImageNet 416x416 TFRecord Dataset                                                                                  
ALBERT Model for Scicite Text Classification                                                                                  
ALL-IDB-Subtypes-Images                                                                                  ALL_IDB_1-2 The ALL_IDB1 version 1.0 can be used both for testing segmentation capability of algorithms, as well as the classification systems and image preprocessing methods. This dataset is composed of 108 images collected during September, 2005. It contains about 150 blood images each class 50 images, where the lymphocytes has been labeled by expert oncologists. The images are taken with different magnifications of the microscope ranging from 300 to 500.
ALLIMAGES                                                                                  
ALPR Image Dataset                                                                                  
AMD_CD_Images_Color                                                                                  
ANT Corpus: Arabic News Texts Corpus                                                                                  ### About the project ANT Corpus stands for 'Arabic News Texts Corpus'. It is a research project that aims to collect texts from different sources of the web by incrementing the amount of data progressively. The acronym ANT can remind the ants' work: '*Every ant should contribute to build the nest progressively*'. You can find other details on the main website page of [ANT (Arabic News Texts) Corpus](http://antcorpus.github.io). Version details --------------- &gt; **Current version:** v1.1 | Version | Articles number | Words number | Number of categories | Source | v1.1 **(current)** | 10 161 | &gt; 1 474 000 | 9 | [JawharaFM](http://www.jawharafm.net/ar/) | | v1.0 | 6 005 | &gt; 865 500 | 9 | [JawharaFM](http://www.jawharafm.net/ar/) | Categories ---------------- | Category (en) | Category (ar) | JawharaFM | # of articles (v1.0) | # of articles (v1.1) | | culture | ثقافة | [x] | 70 | 124 | | diverse | متفرقات | [x] | 194 | 475 | | economy | اقتصاد | [x] | 174 | 326 | | internationalNews | دولية | [x] | 561 | 1 260 | | localNews | وطنية | [x] | 3 090 | 4 832 | | politic | سياسة | [x] | 281 | 514 | | society | مجتمع | [x] | 673 | 1 087 | | sport | رياضة | [x] | 906 | 1 460 | | technology |تكنولوجيا | [x] | 56 | 83 | Files format ---------------- ANT Corpus files are formatted in XML using similar tags to the [TREC](http://trec.nist.gov/) and [CLEF](http://www.clef-initiative.eu/) standard test collections. This a sample of an article from JawharaFM as a news web source in the 'economy' (اقتصاد) category: #### About tags - `` - `
ANT Detection Image Dataset                                                                                  ### Context This dataset contains sequence images of ant colony in indoor and outdoor environments. Used to train multi-target tracking models and test its performance. ### Acknowledgements Cao, xiaoyan (2019), “ANTS--ant detection and tracking”, Mendeley Data, V1, doi: 10.17632/9ws98g4npw.1
APPLE VARIETIES IMAGE DATASET                                                                                  
APTOS color images (resized and improved)                                                                                  
APTOS train-image crop                                                                                  
APTOS2019 - 10K Augmented Images                                                                                  This dataset contains preprocessed 10K diabetic retinopathy images, 300 x 300 resized, to train a model to detect blindness before it happened. [APTOS 2019 Blindness Detection competition](https://www.kaggle.com/c/aptos2019-blindness-detection/). Provided image database was unbalanced so I created 2000 Images for each severity of diabetic retinopathy on a scale of 0 to 4. 0 - No DR 1 - Mild 2 - Moderate 3 - Severe 4 - Proliferative DR Like any real-world data set, you will encounter noise in both the images and labels. Images may contain artifacts, be out of focus, underexposed, or overexposed. The images were gathered from multiple clinics using a variety of cameras over an extended period of time, which will introduce further variation. Please comment if any question.
APTOS2019-Processed-Images                                                                                  
ARABICTEXT                                                                                  
ARCANE Episode 1X1 text                                                                                  This is the text taken from the first episode of ARCANE ( the animated series of the popular game Leauge Of Legends ). The data contains all the subtitles of the first episode in a .txt format
ASAYAR: A Dataset for Arabic-Latin Text Detection                                                                                  ## ASAYAR This is a description for the paper: <br> [ASAYAR: A Dataset for Arabic-Latin Scene Text Localization in Highway Traffic Panels](https://ieeexplore.ieee.org/document/9233923)<br> **Mohammed Akallouch; Kaoutar Sefrioui Boujemaa; Afaf Bouhoute; Khalid Fardousse; Ismail Berrada** ## Overview ASAYAR is the first public dataset dedicated to Latin (French) and Arabic Scene Text Detection in Highway panels. It comprises more than 1800 well-annotated images. The dataset was collected from Moroccan Highway,## ASAYAR ## Annotation format In the dataset, each instance's location is annotated by a rectangle bounding box. The bounding box can be denoted as : <br> `{XMIN, YMIN, XMAX, YMAX}`. An object has a class name denoted as CLASS. The global image information is defined as follows: FOLDER, **PATH**, **NAME**, and **SIZE**. ## Dataset structure ``` Train or Test/ ├── ASAYAR_SIGN/ │ ├── Annotations/ │ │ ├── image_1.xml │ │ └── ... │ └── Images │ ├── image_1.png │ └── ... │ ├── ASAYAR_TXT/ │ ├── Annotations/ │ │ ├── Line-Level/ │ │ │ ├── image_1.xml │ │ │ └── ... │ │ └── Word-Level/ │ │ ├── image_1.xml │ │ └── ... │ └── Images/ │ ├── image_1.png │ └── ... └── ASAYAR_SYM/ ├── Annotations/ │ ├── image_1.xml │ └── ... └── Images/ ├── image_1.png └── ... ``` ## Import data We provide a [Jupyter Notebook](https://github.com/makallouch/ASAYAR/blob/main/Tools/Visualize_data.ipynb) with an example to import images and their annotations. ## Convert to text format To convert annotations from Voc pascal to txt format (`xmin,ymin,xmax,ymax,class`) use [convert2txt.py](https://github.com/makallouch/ASAYAR/blob/main/Tools/convert2txt.py). ## Examples of Annotated Images <img src='https://vcar.github.io/ASAYAR/images/image_895.png'> ## Website The data website: [ASAYAR](https://vcar.github.io/ASAYAR/) ## Citation Our paper introducing the dataset and the evaluations methods is published at the IEEE Transactions on Intelligent Transportation Systems 2020 and is available [here](https://ieeexplore.ieee.org/document/9233923). If you make use of the ASAYAR dataset, please cite our following paper: ``` @ARTICLE{9233923, author={M. {Akallouch} and K. S. {Boujemaa} and A. {Bouhoute} and K. {Fardousse} and I. {Berrada}}, journal={IEEE Transactions on Intelligent Transportation Systems}, title={ASAYAR: A Dataset for Arabic-Latin Scene Text Localization in Highway Traffic Panels}, year={2020}, pages={1-11}, doi={10.1109/TITS.2020.3029451}} ```
ASL Alphabet Training Set 937 images                                                                                  Since the 'J' and 'Z' sign are a dynamic gesture unlike the other static signs I went with a slightly unconventional approach of training the model on a so called 'J-start' and 'J-end' sign. When seen in quick succession of one-another my code has hard-coded logic of recognizing the sign as a 'J'. I used a similar approach with 'Z', training the model on a 'Z-start' and 'Z-end' and when the the software detects: 'Z-start' -&gt; 'Z-end' -&gt; 'Z-start' -&gt; 'Z-end' or 'Z-end' -&gt; 'Z-start' -&gt; 'Z-end' -&gt; 'Z-start' it returns the letter 'Z'. For ease of use I added a 'Spacebar' and 'Backspace' sign to help potential users with interacting with a text form in a GUI.
ASL Fingerspelling Images (RGB & Depth)                                                                                  ### Context I've personally used part of this dataset to train a CNN to recognize ASL (American Sign Language) handshapes ([github.com/MrGeislinger/ASLTransalation][1]). Though the data have 'incorrect' handshapes like the 'G' handshape and produced by only 5 different individuals, the dataset is large enough to do a proof-of-concept of classifying ASL handshape images. ### Content The dataset contains cropped RGB images and depth data (collected from a Microsoft Kinect) of ASL (American Sign Language) handshapes corresponding to 24 letters of the English alphabet (note that 'X' and 'Z' are excluded since they rely on movement). Note that this dataset was produced from 5 different non-native signers. Some of the handshapes are different from what traditional/native signers sign (e.g. 'G' is frequently positioned differently from traditional signers). Data were retrieved from Nicolas Pugeault's [website][2], referenced below. ### Acknowledgements Original paper: [Pugeault, N., and Bowden, R. (2011). Spelling It Out: Real-Time ASL Fingerspelling Recognition In Proceedings of the 1st IEEE Workshop on Consumer Depth Cameras for Computer Vision, jointly with ICCV'2011.][3] ['ASL Finger Spelling Dataset'][4] ### Inspiration My hope is to see more work done in ASL and other sign languages recognition. It'd be great to one day see a tool that can be versatile for sign languages as voice-to-text technology is today. [1]: https://github.com/MrGeislinger/ASLTransalation [2]: https://empslocal.ex.ac.uk/people/staff/np331/index.php?section=SignLanguage [3]: https://empslocal.ex.ac.uk/people/staff/np331/publications/PugeaultBowden2011b.pdf [4]: https://empslocal.ex.ac.uk/people/staff/np331/index.php?section=FingerSpellingDataset
ASL Image Dataset (A-Y, excldg J)                                                                                  
ASL MNIST WITH TWO AUXILIARY SIGNS, 312000 IMAGES                                                                                  This database was created using the 'Sign Language MNIST' database as a reference. It contains 312,000 images of American Sign Language with two additional symbols (space and stop), making it ideal for working on writing or predicting messages. The database consists of 12 mini-databases, each one of them with 1000 samples for each of the 26 signs (24 ASL and 2 auxiliary). Each mini-database has been collected in a different location, using different lightning conditions. The mini-databases are not shuffled, there are firstly 1000 A samples, then 1000 B samples and so on. Each image has a size of 56x56 pixels squared, and they all are in grayscale, so each pixel contains a value between 0 and 255. The labels have no been set, so each user can set them as he likes. In each mini-database the first 1000 images are A, the next 1000 are B and so on. The order is the following: A, B, C, D, E, F, G, H, I, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, SPACE, STOP. This database was created in a Bachelor's Thesis. Was used to train a model used for ASL message writing and prediction. Some video-demostrations (different uses of the model, inside the different folders that can be found on the link), the trained model and the database can be found in the following link: https://drive.google.com/drive/folders/1NdFK7Cu9c_eIjNO1OzNUM72DBorH0WJa?usp=sharing
ASL alphabet pixel value of Images.                                                                                  
ASL-test-images                                                                                  
ASL_image                                                                                  
ATM Image (ATM-I)                                                                                  The image dataset has been created where each image is bounding box annotated for the ATM and person class. To ensure good quality annotation following measures are taken: tight bounding boxes, completeness, and consistency across images. The dataset comprises 1491 images that cover most of the angles in which an ATM box can be viewed in an ATM vestibule. Images in the dataset are augmented with blur (up to 2.25px) and noise (up to 6% of pixels) effects. Augmentation is done to expand the dataset and increase model performance.
AUDIO DATASET: Masry TTS                                                                                  
AUDIO SER                                                                                  
AUDIOO                                                                                  
AUDIOOO                                                                                  
AUDIO_DATA                                                                                  
AUDIO_DIALOGUE                                                                                  
AUNET-On-Livecells-Weights-and-WorstImages                                                                                  
AVWWS.Audio                                                                                  
AWD LSTM WikiText 103                                                                                  
A_Scans_Normal_OCT_Images_Normalised_float32                                                                                  
Aadhar Images                                                                                  
Abbreviations and Slangs for text preprocessing                                                                                  To save time or to be cool , nonetheless abbreviations have become widespread . It's frequently seen in SMS / Twitter posts / reddit / and even informal emails . The is a basic dataset which includes all the most commonly used abbreviations with their meaning . It can be utilised during pre-processsing when dealing with Twitter / Social media data . One can even add colloquial abbreviations over the dataset to make it more accomodating .
Abdo audio                                                                                  
Abstract Art Images                                                                                  [https://gsurma.github.io](https://gsurma.github.io)
Abstract Images (32x32)                                                                                  
Abstract Text Challenge                                                                                  
Abstractive Text Summarization from Fanpage                                                                                  Fanpage dataset, containing news articles taken from Fanpage. There are two features: * source: Input news article. * target: Summary of the article.
Abstractive Text Summarization from Il Post                                                                                  IlPost dataset, containing news articles taken from IlPost. There are two features: * source: Input news article. * target: Summary of the article.
Abstractive Text Summarizer[LSTM Model]                                                                                  
Abtractive-text-summarization-vietnamese-1                                                                                  
Abtractive-text-summarization/law-news-domain                                                                                  
AccentText                                                                                  
Accident Images                                                                                  
Accident Images From CCTV Footage                                                                                  The Dataset can be used for Real Time Accident Detection Model.
Accident detection model image dataset                                                                                  
Acute Lymphoblastic Leukemia (ALL) image dataset                                                                                  As a highly prevalent cancer, the definitive diagnosis of acute lymphoblastic leukemia (ALL) requires invasive, expensive, and time-consuming diagnostic tests. ALL diagnosis using peripheral blood smear (PBS) images plays a vital role in the initial cancer screening from non-cancer cases. The examination of these PBS images by laboratory users is riddled with problems such as diagnostic error because the non-specific nature of ALL signs and symptoms often leads to misdiagnosis. The images of this dataset were prepared in the bone marrow laboratory of Taleqani Hospital (Tehran, Iran). This dataset consisted of 3256 PBS images from 89 suspected of ALL patients whose blood samples were prepared and stained by skillful laboratory staff. This dataset is divided into two classes benign and malignant. The former comprises hematogones; the latter is the ALL group with three subtypes of malignant lymphoblasts: Early Pre-B, Pre-B, and Pro-B ALL. All the images were taken using a Zeiss camera in a microscope with 100x magnification and saved as JPG files. A specialist using the flow cytometry tool made the definitive determination of the types and subtypes of these cells. After color thresholding-based segmentation in the HSV color space, we also provide segmented images. Paper: [A Fast and Efficient CNN Model for B-ALL Diagnosis and its Subtypes Classification using Peripheral Blood Smear Images](https://doi.org/10.1002/int.22753) Source code: [https://github.com/MehradAria/ALL-Subtype-Classification](https://github.com/MehradAria/ALL-Subtype-Classification) If you use this dataset in your research, please credit the authors. Data Citation: `Mehrad Aria, Mustafa Ghaderzadeh, Davood Bashash, Hassan Abolghasemi, Farkhondeh Asadi, and Azamossadat Hosseini, “Acute Lymphoblastic Leukemia (ALL) image dataset.” Kaggle, (2021). DOI: 10.34740/KAGGLE/DSV/2175623.` Publication Citation: `Ghaderzadeh, M, Aria, M, Hosseini, A, Asadi, F, Bashash, D, Abolghasemi, H. A fast and efficient CNN model for B-ALL diagnosis and its subtypes classification using peripheral blood smear images. Int J Intell Syst. 2022; 37: 5113- 5133. doi:10.1002/int.22753`
Acute Lymphoblastic Leukemia image dataset                                                                                  
Ad demand - dtimage                                                                                  
Adarsh_Chaturvedee_LJMU_MSc_Code_and_images_files                                                                                  All files relevant to small object detection for autonomous vehicles is provided here. It is a required component for my MSc in Data Science, specialising in deep learning from Liverpool John Moores University (LJMU).
Adblue_Oficial_Images                                                                                  
Adenocarcinoma Lungs Cancer Image & Mask                                                                                  
Adience Benchmark Text files                                                                                  
Aditya_Testing_Images                                                                                  
Ads from context advertising                                                                                  # Context In order to create assistant for the content-advertising system, there was an automated generator advert content. So a lot of ads were collected from a popular search engine (only for Russian ads campaign). (if someone interesting in, i can upload full 40+GB data) # Content The database was collected from open public sources and contains ads from regions of Russia, Ukraine, Belarus, Kazakhstan and the major cities of these countries. Unique items: 800 000 (part1) Total size about 15MM # Acknowledgements The database was collected in October 2016 - January 2017. No one was harmed when collecting the database (the program does not click on the ads). # Inspiration Try to search patterns in the ads, and develop an automatic text generator for ad systems.
Adversarial Attack on Imagenet Dataset                                                                                  ### Context This is an adversarial Dataset made using an FGSM attack. You can find more about it here https://github.com/anirudh9784/Adversarial-Attacks-and-Defences ### Content There are 1000 Images that are a subset of the Imagenet Dataset.
Adversarial_mnist_dataset_image_format                                                                                  
Advertisement Board Image | Scene Text Recognition                                                                                  ### **This dataset is collected by DataCluster Labs. To download full dataset or to submit a request for your new data collection needs, please drop a mail to:&nbsp;[sales@datacluster.ai](mailto:sales@datacluster.ai)** This dataset is an extremely challenging set of over 3,000+ images of advertisement boards from multiple locations. These images captured and crowdsourced from over 2000+ different locations, where each image is **manually reviewed and verified** by computer vision professionals at Datacluster Labs. It contains a wide variety of ad board images. This dataset can be used scene classification, scene text detection and domestic object detection. ### **Dataset Features** - Dataset size : 3000+ images - Captured by : Over 2000+ crowdsource contributors - Resolution : HD and above (1920x1080 and above) - Location : Captured with 2000+ locations - Diversity : Various lighting conditions like day, night, varied distances, view points etc. - Device used : Captured using mobile phones in 2020-2022 - Usage : Image classification, scene text detection and recognition etc, ### Available Annotation formats COCO, YOLO, PASCAL-VOC, Tf-Record **The images in this dataset are exclusively owned by Data Cluster Labs and were not downloaded from the internet. To access a larger portion of the training dataset for research and commercial purposes, a license can be purchased. Contact us at sales@datacluster.ai Visit www.datacluster.ai to know more.**
Aerial Crop Data for Image SR                                                                                  We acquired original datasets comprising raw LR and HR images captured by two distinct sensors co-mounted on a UAV. Given the varying configurations of the sensors, we employed a three-step preprocessing algorithm to generate datasets suitable for supervised training. **Usage** If you use this dataset for your research, please include the reference below: ```bibtex @article{aslahishahri2021spatial, title={Spatial super resolution of real-world aerial images for image-based plant phenotyping}, author={Aslahishahri, Masoomeh and Stanley, Kevin G and Duddu, Hema and Shirtliffe, Steve and Vail, Sally and Stavness, Ian}, journal={Remote Sensing}, volume={13}, number={12}, pages={2308}, year={2021}, publisher={MDPI} } ```
Aerial Image Segmentation                                                                                  
Aerial Imagery Dataset - FloodNet Challenge                                                                                  ### Overview Frequent, and increasingly severe, natural disasters threaten human health, infrastructure, and natural systems. The provision of accurate, timely, and understandable information has the potential to revolutionize disaster management. For quick response and recovery on a large scale, after a natural disaster such as a hurricane, access to aerial images is critically important for the response team. The emergence of small unmanned aerial systems (UAS) along with inexpensive sensors presents the opportunity to collect thousands of images after each natural disaster with high flexibility and easy maneuverability for rapid response and recovery. Moreover, UAS can access hard-to-reach areas and perform data collection tasks that can be unsafe for humans if not impossible. Despite all these advancements and efforts to collect such large datasets, analyzing them and extracting meaningful information remains a significant challenge in scientific communities. [FloodNet](https://arxiv.org/abs/2012.02951) provides high-resolution UAS imageries with detailed semantic annotation regarding the damages. To advance the damage assessment process for post-disaster scenarios, we present a unique challenge considering classification, semantic segmentation, visual question answering highlighting the UAS imagery-based FloodNet dataset. **Track 1: Image Classification and Semantic Segmentation Track 2: Visual Question Answering** ### Dataset Details The data is collected with a small UAS platform, DJI Mavic Pro quadcopters, after Hurricane Harvey. The whole dataset has 2343 images, divided into training (~60%), validation (~20%), and test (~20%) sets. For Track 1 ( Semi-supervised Classification and Semantic Segmentation), in the training set, we have around 400 labeled images (~25% of the training set) and around 1050 unlabeled images (~75% of the training set ). For Track 2 ( Supervised VQA), in the training set we have around 1450 images and there are a total 4511 image-question pairs. ----- **Track 1** In this track, participants are required to complete two semi-supervised tasks. The first task is image classification, and the second task is semantic segmentation. Semi-Supervised Classification: Classification for FloodNet dataset requires classifying the images into ‘Flooded’ and ‘Non-Flooded’ classes. Only a few of the training images have their labels available, while most of the training images are unlabeled. Semi-Supervised Semantic Segmentation: The semantic segmentation labels include: &gt;0 - Background &gt;1 - Building Flooded &gt;2 - Building Non-Flooded &gt;3 - Road Flooded &gt;4 - Road Non-Flooded &gt;5 - Water &gt;6 - Tree &gt;7 - Vehicle &gt;8 - Pool &gt;9 - Grass. Only a small portion of the training images have their corresponding masks available. ----- **Track 2** For the Visual Question Answering (VQA) task, we provide images associated with multiple questions. These questions will be divided into the following categories: + Simple Counting: Questions will be designed to count the number of objects regardless of their attribute. For example: “how many buildings are there in the image?”. + Complex Counting: Questions will be asked to count the number of objects belonging to a specific attribute. For example: “how many flooded buildings are there in the image?”. + Condition Recognition: In this category, all the questions are mainly designed to ask questions regarding the condition of the object and the neighborhood. For example: “What is the condition of the road in the given image?”. + Yes/No type of question: For this type of question, the answer will be either a ‘Yes’ or a ‘No’. For example: “Is there any flooded road?”. ### Citation ```python @article{rahnemoonfar2020floodnet, title={FloodNet: A High Resolution Aerial Imagery Dataset for Post Flood Scene Understanding}, author={Rahnemoonfar, Maryam and Chowdhury, Tashnim and Sarkar, Argho and Varshney, Debvrat and Yari, Masoud and Murphy, Robin}, journal={arXiv preprint arXiv:2012.02951}, year={2020} } ```
Aerial Images                                                                                  The dataset covers spatial domains. With 13 semantic classes, this dataset provides a robust foundation for advancing land cover mapping techniques. The FLAIR datasets are under the [Open Licence 2.0](https://www.etalab.gouv.fr/wp-content/uploads/2018/11/open-licence.pdf) of Etalab. Remember to cite the associated datapaper to each dataset.
Aerial Images of Cities                                                                                  # **Dataset Overview:** **Name:** SkyCity Aerial City Landscape Dataset **Categories:** - Bridge - Commercial - Industrial - Intersection - Landmark - Park - Parking - Playground - Residential - Stadium **Image Resolution:** 256x256 pixels **Number of Images per Category:** 800 **Total Images:** 8,000 **Description:** SkyCity is a meticulously curated dataset, bringing together urban landscapes from the publicly available AID and NWPU-Resisc45 datasets. Featuring 10 distinct city categories with 800 images each, all at a resolution of 256x256 pixels, this dataset serves as a valuable resource for researchers and developers delving into the realm of urban landscape analysis. Proper credits to the original contributors of the AID and NWPU-Resisc45 datasets are duly acknowledged.
Aerial Landscape Images                                                                                  # **Dataset Overview:** **Name:** Skyview Multi-Landscape Aerial Imagery Dataset **Categories:** - Agriculture - Airport - Beach - City - Desert - Forest - Grassland - Highway - Lake - Mountain - Parking - Port - Railway - Residential - River **Image Resolution:** 256x256 pixels **Number of Images per Category:** 800 **Total Images:** 12,000 **Sources:** AID Dataset: https://captain-whu.github.io/AID/ NWPU-Resisc45 Dataset: https://paperswithcode.com/dataset/resisc45 **Description:** Skyview is a curated dataset for aerial landscape classification featuring 15 diverse categories, each comprising 800 high-quality images at a resolution of 256x256 pixels. This dataset is a fusion of images sourced from the publicly available AID and NWPU-Resisc45 datasets. The compilation is designed to facilitate research and development in the field of computer vision, particularly in the context of aerial landscape analysis. Researchers and developers are encouraged to explore and utilize Skyview, giving due credit to the original contributors of the AID and NWPU-Resisc45 datasets.
Aerial Satellite Images                                                                                  Three main classes: swim pools, parkings and roundabouts
Aerial imagery semantic segmentation dataset                                                                                  ## Context Includes the 256x256 aerial images and their masks. The original dataset can be accessed via [this](https://www.kaggle.com/datasets/humansintheloop/semantic-segmentation-of-aerial-imagery?rvi=1) link. ## Content The dataset consists of aerial imagery of Dubai obtained by MBRSC satellites and annotated with pixel-wise semantic segmentation in 6 classes. The total volume of the dataset is 72 images grouped into 6 larger tiles. The classes are: Building: #3C1098 Land (unpaved area): #8429F6 Road: #6EC1E4 Vegetation: #FEDD3A Water: #E2A929 Unlabeled: #9B9B9B ## How to use Files in the directory are not meaningfully ordered, you may use the below python function to map images and their masks. ```python import glob patch_images = glob.glob('images/*.jpg') patch_masks = glob.glob('masks/*.jpg') sort_key = lambda x: (int(x.split('_')[1]), int(x.split('_')[4]), int(x.split('_')[6].split('.')[0])) sorted_patch_images = sorted(patch_masks, key=sort_key) ```
Aerial images                                                                                  
Aerial images all categories                                                                                  
Aerial images of cars                                                                                  This dataset provides a collection of labeled aerial images of vehicles taken from drones in the campus of Prince Sultan University, Saudi Arabia. The PSU dataset was collected from two sources: an open dataset of aerial images available on Github repo Aerial-car-dataset, available online on: https://github.com/jekhor/aerial-cars-dataset and our own images acquired after flying a 3DR SOLO drone equipped with a GoPro Hero 4 camera, in an outdoor environment at PSU parking lot. The drone recorded videos from which frames were extracted and manually labeled. Since we are only interested in a single class, images with no cars have been removed from the dataset. The training/testing split was made randomly. ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12447376%2F7f567f46d0bd35c3a3d5939356d28bee%2FScreenshot%20from%202023-08-29%2013-23-49.png?generation=1693304666549998&alt=media) **Total number of images: 270** If you use this dataset in a research paper, please cite the following **reference**: Ammar, A., Koubaa, A., Ahmed, M., Saad, A. and Benjdira, B., 2021. *Vehicle detection from aerial images using deep learning: A comparative study*. Electronics, 10(7), p.820.
Aerial images of palm trees                                                                                  We collected a dataset of **349 images**: 258 aerial images in a palm tree farm in Kharj region, in Saudi Arabia, and 91 images taken on Prince Sultan University campus. Both the aspect and density of palm trees in the two sets of images are dissimilar. The images were captured using two different drones: a DJI Phantom 4 Pro drone equipped with a DJI FC6310 camera (resolutions 4864 × 3648 and 4096 × 2160) and a DJI Mavic Pro equipped with a DJI FC220 camera (resolution 4000 × 3000). The dataset contains a total of 13,071 instances (11,150 palms and 1921 other trees) that we **manually labeled** using Labelbox. Then we randomly split it into training (80%) and testing (20%) datasets. If you use this dataset in a research paper, please cite the following **reference**: Ammar, A., Koubaa, A. and Benjdira, B., 2021. Deep-learning-based automated palm tree counting and geolocation in large farms from aerial geotagged images. Agronomy, 11(8), p.1458.
Aerial images of swimming pools                                                                                  # Context I've created this dataset as part of my side project which consists in applying a deep learning solution to automatically detect and count swimming pools from aerial RGB images. I was not able to find a suitable dataset so I've created mine and I'm glad to share it to you. ![](https://cdn-images-1.medium.com/max/1200/0*kbWFs2dOILw7abke.png) # Content Original tile size is 25,000 x 25,000 pixels. I cropped the tile into patches of 512x512 pixels without overlaps and got 1,224 patches. The patches have three bands (RGB) and the pixel values lie in the range [0, 255]. The dataset contains 3,197 annotated pools with different shapes and hues. Moreover I have also annotated out-ground and covered pools. I used the software LabelImg to create bounding boxes around each pool. The number of objects per patch is pretty small as there are few houses (and consequently less pools) within a patch. Original full size images : https://geoservices.ign.fr/bdortho Feel free to contact me if you need any further information
Aerial urban traffic visible-segmentated images                                                                                  **If you use this dataset please cite this paper: Rosende, S.B.; Gavilán, D.S.J.; Fernández-Andrés, J.; Sánchez-Soriano, J. An Urban Traffic Dataset Composed of Visible Images and Their Semantic Segmentation Generated by the CARLA Simulator. Data 2024, 9, 4.** [https://doi.org/10.3390/data9010004](https://doi.org/10.3390/data9010004) A dataset of aerial urban traffic images and their semantic segmentation is presented to be used to train computer vision algorithms, among which those based on convolutional neural networks stand out. This article explains the process of creating the complete dataset, which includes the acquisition of the images, the labeling of vehicles, pedestrians, and pedestrian crossings as well as a description of the structure and content of the dataset (which amounts to 8694 images including visible images and those corresponding to the semantic segmentation). The images were generated using the CARLA simulator (but were like those that could be obtained with fixed aerial cameras or by using multi-copter drones) in the field of intelligent transportation management. The presented dataset is available and accessible to improve the performance of vision and road traffic management systems, especially for the detection of incorrect or dangerous maneuvers. Data collection in real-world settings, such as urban roadways, is a laborious and highly costly task, primarily due to the substantial investment of money and time required to initiate data acquisition and compile a comprehensive dataset. As an alternative to this, there are various simulators that significantly reduce the investment required, thereby expediting the initial stages of a training process. In addition to this, after obtaining the dataset, with all of the inherent complexity, it becomes necessary to process the data to make them suitable for training purposes. The objective of the dataset described in this manuscript is to be instrumental in the analysis of traffic violations. Below, the authors list the systems aimed at data collection for training purposes, whether through real-world data acquisition or simulated environments. It also provides insights into the CARLA simulator and its integration with a robot operating system (ROS), designed to streamline the interaction. **The full description of the characteristics of the dataset, as well as its components and format, can be found here:**[An Urban Traffic Dataset Composed of Visible Images and Their Semantic Segmentation Generated by the CARLA Simulator](https://www.mdpi.com/2306-5729/9/1/4) **Funding:** This publication is part of the I+D+i projects with references PID2019-104793RB-C32, PDC2021-121517-C33, and PDC2022-133684-C33, and funded by MCIN/AEI/10.13039/501100011033.
Aerial(Satellite) Images of Rooftops with Labels                                                                                  This dataset contains two folders. One folder contains the aerial images of rooftops. The other folder contains its segmented labels or masks. The mask of an image has the same corresponding names as the image. This data is used for semantic segmentation.
Aerial-Image-Dataset                                                                                  
AerialImage                                                                                  
AerialImageDataset                                                                                  ### Source That's AID (Aerial Image Dataset) taken from https://captain-whu.github.io/AID/ and uploaded to Kaggle
AerialImageTestDatabase                                                                                  
Aerospace Images                                                                                  Contains images of airliners, hot air balloons, blimps, helicopters, rockets, space shuttles, and warplanes. Images have been cleaned to remove duplicates and non-representative images in each class. ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F8058007%2Ffc975e5073e2d59f5184612f0d6b4731%2Fimage_characteristics2.jpg?generation=1681587070176033&alt=media)
Aerospace segmentation images and masks 11 classes                                                                                  # [Source of the data](https://github.com/ishann/aeroscapes) ### Introduction The AeroScapes aerial semantic segmentation benchmark comprises of images captured using a commercial drone from an altitude range of 5 to 50 metres. The dataset provides 3269 720p images and ground-truth masks for 11 classes. ![aeroscapes_dataset_sample_images](https://github.com/ishann/aeroscapes/blob/master/assets/data_montage.png) ### Instructions The data is available for download on [Google Drive](https://drive.google.com/file/d/1W7yQtrGUnPQ1fB2dPb5wPjrLrlQi395g/view?usp=sharing). On extraction, the downloaded file results in the following directory aeroscapes/ JPEGImages/ 3269 RGB images. SegmentationClass/ 3269 ground-truth segmentation masks. Visualizations/ 3269 RGB ground-truth segmentation visualizations. ImageSets/ Training and validation splits for data. ### Reference If you use AeroScapes in your research, please cite the following: Ensemble Knowledge Transfer for Semantic Segmentation Ishan Nigam, Chen Huang, Deva Ramanan Proceedings of the 2018 IEEE Winter Conference on Applications of Computer Vision ### Acknowledgements We acknowledge the efforts of [Autel Robotics](https://www.autelrobotics.com) in the collection and manual annotation of the dataset. ### Questions and Comments For comments and feedback, contact Ishan Nigam at ishannigam@gmail.com.
Aesthetic Visual Analysis for Satellite Images                                                                                  The dataset consists of 4000 satellite images from across India. These images are classified based on their visual aesthetics into two categories, one containing aesthetically good images and other containing aesthetically bad images.
Aesthetics Text Corpus                                                                                  An exhaustive list of stop lemmas created from 12 corpora across multiple domains, consisting of over 13 million words, from which more than 200,000 lemmas were generated, and 11 publicly available stop word lists comprising over 1000 words, from which nearly 400 unique lemmas were generated. Source: [Novel Language Resources for Hindi: An Aesthetics Text Corpus and a Comprehensive Stop Lemma List](/paper/novel-language-resources-for-hindi-an)
Afaan Oromoo Audio Datasets for Sentiment Analysis                                                                                  
Affective Text                                                                                  Affective Text (Test Corpus of SemEval 2007) by [Carlo Strapparava & Rada Mihalcea](https://www.aclweb.org/anthology/S07-1013/). Source: [Affective Text](https://github.com/sarnthil/unify-emotion-datasets/tree/master/datasets)
African Fabric Images                                                                                  ### Context I needed to work on using GANs to generate African fabrics but they weren't any available dataset for it. So I had to source for image dataset of African fabrics and wax patterns. Open data was definitely the next step. ### Content The current dataset is of about 1056 images with size: 64x64. The dataset is not yet partitioned into test and training set. It is a zipped file of about 7mb. ### Acknowledgements A bulk of these images were gotten from google image searches. ### Inspiration Africa has so much untapped data that when fused with machine intelligence could trigger diverse exploration.
Ag-Crystal Image Characterization                                                                                  The dataset comprises of images from an industrial agrochemical crystallization process with needle-like shape morphology. Data is collected from three batches of the active ingredient with different crystal size distributions. The particles were collected in-situ using an EasyViewer 100 probe where pre-weighed solids were slurried with a known amount of mother liquor to create a known solid loading composition. Starting from lowest solid concentration, images were collected for increasing solid loading concentrations by adding more solid material. The Morphologi G3 was used for off-line crystal size measurement (ground-truth). |Name| Characterization of Needle-like crystals| | --- | --- | |Problem type| Object detection, segmentation| |Number of images| 3888 images (26 folders of cropped images) |Microscopy type| In-situ| |Image format| png| |PAT hardware| EasyViewer-100| |Other Data| Solid concentration loadings and offline particle size distributions| # Starter task to get started with the Dataset: Start by downloading a sample of the “Long Needle (x) “ particle images at the lower concentration (x = 0.1wt%) and at higher concentration (x= 1.0wt%). The user can use any type of image analysis software they desire. ImageJ will be used as the model software. First the data must be loaded into the program. Image treatment is then performed such as converting the image to greyscale, blurring the background, background subtraction, fine tuning contrast. Afterwards, an image segmentation method can be applied using various thresholding methods. The objects that are separated from the background can then be processed with various filters and classifiers to retain objects of interest (particles) and eliminate artifacts (overlapping particles, out-of-focus particles, bubbles). Afterwards, the characteristics of the objects of interest can be calculated and extracted.
Age Groups Images                                                                                  ### Context I gathered the dataset for a hackathon where tasks were to classify groups of people into Adults, Teenagers, and Toddlers. The images are separated into their respective classes they belong to, although there might be some overlaps and some random images mixed. ### Content I gathered the images by scraping google images and searching for appropriate keywords, used the Selenium framework for doing so. I hope this dataset may help you with classifications tasks or any other use cases.
Age-Stratified Imagery                                                                                  
Aged Synthetic Images                                                                                  Dataset inspired from the Disney's face re-aging network FRAN. 2000 images generated from StyleGAN3 aged from 18-83 using Style-based Age Manipulation (SAM). Note, images are 512x512 instead of 1024x1024 due to dataset size. Disney Paper: [https://studios.disneyresearch.com/2022/11/30/production-ready-face-re-aging-for-visual-effects/](url)
Agent_Call_Text-DataSet                                                                                  
Agricultural Image Segmentation                                                                                  # Plantations Segmentation The dataset consist of aerial photography of agricultural plantations with crops such as cabbage and zucchini. The dataset addresses agricultural tasks such as plant detection and counting, health assessment, and irrigation planning. ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2F5fa7e8e62e793dac70dc9e1db6f60a18%2F66666.png?generation=1685972525147537&alt=media) ## OTHER DATASETS WITH SEGMENTATION: - **[Food Images Segmentation](https://www.kaggle.com/datasets/trainingdatapro/food-segmentation)** - **[Vehicle Images & Segmentation](https://www.kaggle.com/datasets/trainingdatapro/car-masks)** - **[Bald People Segmentation Dataset](https://www.kaggle.com/datasets/trainingdatapro/bald-people-segmentation-dataset)** - **[Weapons - Gun Detection & Segmentation](https://www.kaggle.com/datasets/trainingdatapro/people-with-guns-segmentation-and-detection)** - **[UFC/MMA Fights Images Segmentation Dataset ](https://www.kaggle.com/datasets/trainingdatapro/fights-segmentation)** # Get the Dataset ## This is just an example of the data Leave a request on **[https://trainingdata.pro/data-market](https://trainingdata.pro/data-market/agriculture-data-labeling?utm_source=kaggle&utm_medium=cpc&utm_campaign=plantations-segmentation)** to discuss your requirements, learn about the price and buy the dataset # Dataset structure - **Plantations_Segmentation** - contains of original plantation images (folder **img**) and file with annotations (.xml) - **Object_Segmentation** - includes object segmentation masks for the original images - **Class_Segmentation** - includes class segmentation masks for the original images # Types of segmentation The dataset includes two types of segmentation: - **Class Segmentation** - objects corresponding to one class are identified - **Object Segmentation** - all objects are identified separately # Data Format Each image from `img` folder is accompanied by an XML-annotation in the `annotations.xml` file indicating the coordinates of the polygons. For each point, the x and y coordinates are provided. # Example of XML file structure ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2F4107d573b14b40ee2c9c67727ab9ec87%2Fcarbon%20(6).png?generation=1686129907313187&alt=media) # Plantation segmentation might be made in accordance with your requirements. ## **[TrainingData](https://trainingdata.pro/data-market/agriculture-data-labeling?utm_source=kaggle&utm_medium=cpc&utm_campaign=plantations-segmentation)** provides high-quality data annotation tailored to your needs *keywords: agricultural tasks dataset, image segmentation dataset, plantations images dataset, plantations segmentation dataset, land cover dataset, agricultural products dataset, semantic segmentation dataset, agriculture dataset, agricultural data, object detection dataset, plants segmentation dataset, plant detection, plant recognition*
Agricultural Pests Image Dataset                                                                                  The Agricultural Pest Image Dataset is a collection of images of 12 different types of agricultural pests, namely Ants, Bees, Beetles, Caterpillars, Earthworms, Earwigs, Grasshoppers, Moths, Slugs, Snails, Wasps, and Weevils. The images were obtained from Flickr using the API and were resized to have a maximum width or height of 300px. This dataset is designed to aid researchers and practitioners in the development and evaluation of machine learning models for pest detection and classification in agricultural settings. With 12 classes of pests, the dataset provides a diverse range of images that cover a variety of shapes, colors, and sizes, making it suitable for training and testing algorithms to detect and classify pests in various scenarios. The images were collected from Flickr, which is a popular photo-sharing platform. This ensures that the images in the dataset are representative of real-world scenarios and not artificially generated. Additionally, the images were resized to have a maximum width or height of 300px, which makes the dataset lightweight and easy to work with.
Agricultural crops image classification                                                                                  **Dataset contains 30 different types of crop images in separate folders** **Task** To classify all types of agriculture crop images ( rice, sugarcane, maize ,lemon, banana,coconut , jute etc..) with better accuracy. **Inspiration** The question to be answered to classify crops in each type.
Agriculture Crop Image Classification                                                                                  
Agriculture audio                                                                                  
Agriculture crop images                                                                                  ### Context Dataset (Crop Images) contain 40+ images of each Agriculture crop(Maize, Wheat, jute, rice and sugarcane) Dataset (kag2) contains 159+ augmented images of Crop Images of each class. Augmentation contain Horizontal flip, roatation, horizontal shift, vertical shift. ## **Upvote if you like or download dataset** ### Test_data found at https://www.kaggle.com/aman2000jaiswal/testssss or it uploaded in new version of this dataset test_crop_images # Submit Prediction on testdata.csv found in the above link ### Content For every class images vary from arial view to ground view and of their different life cycle. Crop_details.csv include all the images and labels. ### Task To classify 5 types of agriculture crop images (wheat, rice, sugarcane, maize and jute) with better accuracy. ### Inspiration The question to be answered to classify crops in each type.
Ai Generated Images                                                                                  U can use this data for image classification between real world images and ai generated images. I collected this data as i was trying to build image classifier which distinguishes between ai generated images and real world images. This data has been collected from google images(web scraping).
Ai Generated Images | Images Created using Ai                                                                                  This is the dataset of images, arts generated using an Artificial Intelligence System(Model), such as Dalle 2 an Open-Ai System that generates images from text, this dataset is very helpful in many ai works, research works, machine learning, deep learning, model training, etc. This dataset will be updated from time to time so keep following. I created this dataset by keeping in mind to keep the community safe. initially, there are two main folders that contain subfolders, these folders are animal characters, characters, artistic, etc.
Ai Generated Text Preprocessed                                                                                  
Ai Image Classifier                                                                                  
Ai vs Real image classification                                                                                  
Aimages                                                                                  
Aiming soldiers image dataset                                                                                  
Air Pollution Image Dataset from India and Nepal                                                                                  # Air-Pollution-Image-Dataset-From-India-and-Nepal # **Please Fork and Star our work by visiting our GitHub Repository before using or downloading our dataset** # 1. Forking our repository allows you to create your own copy of our repository, which you can modify and use as you wish. # 2. Starring our repository is a way for people to show their support and appreciation for our work. # https://github.com/ICCC-Platform/Air-Pollution-Image-Dataset-From-India-and-Nepal # Introduction: **This dataset contains images of Air Pollution for different cities in India and Nepal. The dataset is divided into two folders: Combined_Dataset and Country_wise_Dataset.** **Total number of image dataset: 12,240 Image size: 224*224** **Air Quality Index (AQI) Class and its defination used in the dataset.** There are a total of six classes of Air Pollution, which we represent in our dataset as follows: **1. Good (0-50):** Air quality is considered satisfactory and air pollution poses little or no risk. **2. Moderate (51-100):** Air quality is acceptable; however, for some pollutants, there may be a moderate health concern for a very small number of people who are unusually sensitive to air pollution. **3. Unhealthy for Sensitive Groups (101-150):** Members of sensitive groups may experience health effects, but the general public is unlikely to be affected. **4. Unhealthy (151-200):** Some members of the general public may experience health effects; members of sensitive groups may experience more serious health effects. **5. Very Unhealthy (201-300):** Health alert: The risk of health effects is increased for everyone. **6. Hazardous/Severe (301-500):** Health warning of emergency conditions: Everyone is more likely to be affected. **Reference:** https://airtw.epa.gov.tw/ENG/Information/Standard/AirQualityIndicator.aspx ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F11024368%2F3865850ad0720dc148c71b79946f4196%2FAQI%20reference.JPG?generation=1681898437013999&alt=media) **Cities of India** **1. ITO, Delhi** **2. Dimapur, Nagaland** **3. Spice Garden, Bengaluru** **4. Knowledge Park III, Greater Noida** **5. New Ind Town, Faridabad** **6. Borivali East, Mumbai** **7. Oragadam, Tamil Nadu** **City of Nepal** **1. Biratnagar** **Combined dataset:** The combined dataset folder contains two subfolders. **1. All_img:** This subfolder contains all the collected images from all AQI classes. **2. IND_and_NEP:** This subfolder contains six different subfolders representing six different classes of AQI. The csv file in this folder contains all the data and its parameters. It is labeled as Location, Filename, Year, Month, Day, Hour, AQI, PM2.5, PM10, O3, CO, SO2, NO2, and AQI_Class **Country_wise_Dataset:** This folder contains two subfolders representing the countries from which the dataset was collected. **1. India:&nbsp;** This subfolder contains the subfolder representing the names of all cities from where data were collected. Each subfolder of cities contains folders representing the data collected for each respective AQI class, as well as a csv file. which contains the details of each image, like we mentioned above. Such as, Location, Filename, Year, Month, Day, Hour, AQI, PM2.5, PM10, O3, CO, SO2, NO2, and AQI_Class **2. Nepal:&nbsp;** We managed to collect the image dataset from Nepal. This subfolder contains the subfolder representing the name of the city from where data were collected. This subfolder of the city contains folders representing the data collected for each AQI class and also a csv file. which contains the details of each image, like we mentioned above. Such as, Location, Filename, Year, Month, Day, Hour, AQI, PM2.5, PM10, O3, CO, SO2, NO2, and AQI_Class //////////////////////////////////////////////////////////////////////////////// **Dataset Collection Process:** **1. Visit the site:** The first step in collecting the air pollution data was to personally visit the site. This involved physically going to the location and capturing images and videos of the area. **2. Note current parameters:** While visiting the site, various parameters related to air pollution were noted. These included measurements of PM2.5, PM10, NO2, SO2, CO, etc. These parameters were noted by referring to publicly available data sources such as the Central Pollution Control Board (CPCB) website. For India we used https://app.cpcbccr.com/AQI_India/ and for Nepal we used: https://www.tomorrow.io/weather/NP/4/Biratnagar/079711/hourly/ **3. Preprocess images:** Once the images and videos were captured, they were preprocessed to remove any images that were blurry, overexposed, or had other quality issues. Only the images that met the desired quality criteria were selected for further analysis. **4. Extract frames from videos:** In addition to the images, videos were also captured at the site. These videos were processed to extract frames that were suitable for further analysis. Frames that were too blurry or otherwise of low quality were discarded. **5. Log data:** Finally, all the data collected during the site visit, including the images, videos, and air pollution parameters, were logged in a structured format. //////////////////////////////////////////////////////////////////////////////// ****Instructions on how to use the AQI image dataset:**** 1. Download the dataset from Kaggle and extract the zip file to a folder of your choice. Please Visit this link to download the Dataset: https://doi.org/10.34740/KAGGLE/DS/3152196 https://www.kaggle.com/datasets/adarshrouniyar/air-pollution-image-dataset-from-india-and-nepal 2. The dataset is divided into two folders: the Combined_Dataset and Country_wise_Dataset.&nbsp; Each folder contains subfolders and CSV files. 3. To access the images in the Combined_Dataset folder, go to the folder corresponding to the class of AQI you are interested in. For example, if you are interested in the 'Unhealthy' class, go to the 'Unhealthy' folder. Inside this folder, You will find a number of images representing different cities. 4. To access the data in the Country_wise_Dataset folder, go to the folder of the country you are interested in, either India or Nepal. Inside each country folder, you will find subfolders representing different cities. Each city folder contains a CSV file that lists the AQI values and other parameters for the city. 5. You can use this dataset to train machine learning models to predict AQI for different cities. You can also use it for research on air pollution in different cities. 6. If you use this dataset for any purpose, please cite it as the source of the data in any publications or presentations, resulting from the use of this dataset. /////////////////////////////////////////////////////////////////////////// **Collected Image Data Distribution for Each AQI Class** ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F11024368%2F5e5cd6a423568a9943e29383de80c2d6%2FData%20Distribution.png?generation=1681887921039078&alt=media) /////////////////////////////////////////////////////////////////////////// **IMPORTANT!!! It is Instructed to Read our License file before using our dataset.** /////////////////////////////////////////////////////////////////////////// **Citation Request** APA: Adarsh Rouniyar, Sapdo Utomo, John A, & Pao-Ann Hsiung. (2023). <i>Air Pollution Image Dataset from India and Nepal</i> [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DS/3152196 Bibtex: @misc{adarsh rouniyar_sapdo utomo_john a_pao-ann hsiung_2023, title={Air Pollution Image Dataset from India and Nepal}, url={https://www.kaggle.com/ds/3152196}, DOI={10.34740/KAGGLE/DS/3152196}, publisher={Kaggle}, author={Adarsh Rouniyar and Sapdo Utomo and John A and Pao-Ann Hsiung}, year={2023} } #Please do Fork and Star our GitHub repository to support our hard work, so that we can collect more dataset like this and bring among you. **Contributors** 1. Adarsh Rouniyar 2. Sapdo Utomo 3. Dr. John A. 4. Dr. Pao-Ann Hsiung If you have any queries, please do contact us. 1. Adarsh Rouniyar, Email: adarsh@csie.io 2. Dr. John A., Email: johnmtech@gmail.com 3. Dr. Pao-Ann Hsiung, Email: pahsiung@gmail.com , pahsiung@ccu.edu.tw
Air Pollution Image Dataset from India and Nepal                                                                                  # Air-Pollution-Image-Dataset-From-India-and-Nepal # **Please Fork and Star our work by visiting our GitHub Repository before using or downloading our dataset** # 1. Forking our repository allows you to create your own copy of our repository, which you can modify and use as you wish. # 2. Starring our repository is a way for people to show their support and appreciation for our work. # https://github.com/ICCC-Platform/Air-Pollution-Image-Dataset-From-India-and-Nepal # Introduction: **This dataset contains images of Air Pollution for different cities in India and Nepal. The dataset is divided into two folders: Combined_Dataset and Country_wise_Dataset.** **Total number of image dataset: 12,240 Image size: 224*224** **Air Quality Index (AQI) Class and its defination used in the dataset.** There are a total of six classes of Air Pollution, which we represent in our dataset as follows: **1. Good (0-50):** Air quality is considered satisfactory and air pollution poses little or no risk. **2. Moderate (51-100):** Air quality is acceptable; however, for some pollutants, there may be a moderate health concern for a very small number of people who are unusually sensitive to air pollution. **3. Unhealthy for Sensitive Groups (101-150):** Members of sensitive groups may experience health effects, but the general public is unlikely to be affected. **4. Unhealthy (151-200):** Some members of the general public may experience health effects; members of sensitive groups may experience more serious health effects. **5. Very Unhealthy (201-300):** Health alert: The risk of health effects is increased for everyone. **6. Hazardous/Severe (301-500):** Health warning of emergency conditions: Everyone is more likely to be affected. **Reference:** https://airtw.epa.gov.tw/ENG/Information/Standard/AirQualityIndicator.aspx ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F11024368%2F3865850ad0720dc148c71b79946f4196%2FAQI%20reference.JPG?generation=1681898437013999&alt=media) **Cities of India** **1. ITO, Delhi** **2. Dimapur, Nagaland** **3. Spice Garden, Bengaluru** **4. Knowledge Park III, Greater Noida** **5. New Ind Town, Faridabad** **6. Borivali East, Mumbai** **7. Oragadam, Tamil Nadu** **City of Nepal** **1. Biratnagar** **Combined dataset:** The combined dataset folder contains two subfolders. **1. All_img:** This subfolder contains all the collected images from all AQI classes. **2. IND_and_NEP:** This subfolder contains six different subfolders representing six different classes of AQI. The csv file in this folder contains all the data and its parameters. It is labeled as Location, Filename, Year, Month, Day, Hour, AQI, PM2.5, PM10, O3, CO, SO2, NO2, and AQI_Class **Country_wise_Dataset:** This folder contains two subfolders representing the countries from which the dataset was collected. **1. India:&nbsp;** This subfolder contains the subfolder representing the names of all cities from where data were collected. Each subfolder of cities contains folders representing the data collected for each respective AQI class, as well as a csv file. which contains the details of each image, like we mentioned above. Such as, Location, Filename, Year, Month, Day, Hour, AQI, PM2.5, PM10, O3, CO, SO2, NO2, and AQI_Class **2. Nepal:&nbsp;** We managed to collect the image dataset from Nepal. This subfolder contains the subfolder representing the name of the city from where data were collected. This subfolder of the city contains folders representing the data collected for each AQI class and also a csv file. which contains the details of each image, like we mentioned above. Such as, Location, Filename, Year, Month, Day, Hour, AQI, PM2.5, PM10, O3, CO, SO2, NO2, and AQI_Class //////////////////////////////////////////////////////////////////////////////// **Dataset Collection Process:** **1. Visit the site:** The first step in collecting the air pollution data was to personally visit the site. This involved physically going to the location and capturing images and videos of the area. **2. Note current parameters:** While visiting the site, various parameters related to air pollution were noted. These included measurements of PM2.5, PM10, NO2, SO2, CO, etc. These parameters were noted by referring to publicly available data sources such as the Central Pollution Control Board (CPCB) website. For India we used https://app.cpcbccr.com/AQI_India/ and for Nepal we used: https://www.tomorrow.io/weather/NP/4/Biratnagar/079711/hourly/ **3. Preprocess images:** Once the images and videos were captured, they were preprocessed to remove any images that were blurry, overexposed, or had other quality issues. Only the images that met the desired quality criteria were selected for further analysis. **4. Extract frames from videos:** In addition to the images, videos were also captured at the site. These videos were processed to extract frames that were suitable for further analysis. Frames that were too blurry or otherwise of low quality were discarded. **5. Log data:** Finally, all the data collected during the site visit, including the images, videos, and air pollution parameters, were logged in a structured format. //////////////////////////////////////////////////////////////////////////////// ****Instructions on how to use the AQI image dataset:**** 1. Download the dataset from Kaggle and extract the zip file to a folder of your choice. Please Visit this link to download the Dataset: https://doi.org/10.34740/KAGGLE/DS/3152196 https://www.kaggle.com/datasets/adarshrouniyar/air-pollution-image-dataset-from-india-and-nepal 2. The dataset is divided into two folders: the Combined_Dataset and Country_wise_Dataset.&nbsp; Each folder contains subfolders and CSV files. 3. To access the images in the Combined_Dataset folder, go to the folder corresponding to the class of AQI you are interested in. For example, if you are interested in the 'Unhealthy' class, go to the 'Unhealthy' folder. Inside this folder, You will find a number of images representing different cities. 4. To access the data in the Country_wise_Dataset folder, go to the folder of the country you are interested in, either India or Nepal. Inside each country folder, you will find subfolders representing different cities. Each city folder contains a CSV file that lists the AQI values and other parameters for the city. 5. You can use this dataset to train machine learning models to predict AQI for different cities. You can also use it for research on air pollution in different cities. 6. If you use this dataset for any purpose, please cite it as the source of the data in any publications or presentations, resulting from the use of this dataset. /////////////////////////////////////////////////////////////////////////// **Collected Image Data Distribution for Each AQI Class** ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F11024368%2F5e5cd6a423568a9943e29383de80c2d6%2FData%20Distribution.png?generation=1681887921039078&alt=media) /////////////////////////////////////////////////////////////////////////// **IMPORTANT!!! It is Instructed to Read our License file before using our dataset.** /////////////////////////////////////////////////////////////////////////// **Citation Request** APA: Utomo, S.; Rouniyar, A.; Hsu, H.-C.; Hsiung, P.-A. Federated Adversarial Training Strategies for Achieving Privacy and Security in Sustainable Smart City Applications. Future Internet 2023, 15, 371. https://doi.org/10.3390/fi15110371 Sapdo Utomo, Adarsh Rouniyar, Guo Hao Jiang, Chun Hao Chang, Kai Chun Tang, Hsiu-Chun Hsu, and Pao-Ann Hsiung. 2023. Eff-AQI: An Efficient CNN-Based Model for Air Pollution Estimation: A Study Case in India. In Proceedings of the 2023 ACM Conference on Information Technology for Social Good (GoodIT '23). Association for Computing Machinery, New York, NY, USA, 165–172. https://doi.org/10.1145/3582515.3609531 Adarsh Rouniyar, Sapdo Utomo, John A, & Pao-Ann Hsiung. (2023). <i>Air Pollution Image Dataset from India and Nepal</i> [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DS/3152196 Bibtex: @article{utomo2023federated, title={Federated Adversarial Training Strategies for Achieving Privacy and Security in Sustainable Smart City Applications}, author={Utomo, Sapdo and Rouniyar, Adarsh and Hsu, Hsiu-Chun and Hsiung, Pao-Ann}, journal={Future Internet}, volume={15}, number={11}, pages={371}, year={2023}, publisher={MDPI} } @inproceedings{10.1145/3582515.3609531, author = {Utomo, Sapdo and Rouniyar, Adarsh and Jiang, Guo Hao and Chang, Chun Hao and Tang, Kai Chun and Hsu, Hsiu-Chun and Hsiung, Pao-Ann}, title = {Eff-AQI: An Efficient CNN-Based Model for Air Pollution Estimation: A Study Case in India}, year = {2023}, isbn = {9798400701160}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3582515.3609531}, doi = {10.1145/3582515.3609531}, booktitle = {Proceedings of the 2023 ACM Conference on Information Technology for Social Good}, pages = {165–172}, numpages = {8}, keywords = {efficient model, image-based AQI estimation, novel dataset, air pollution estimation, air pollution in India}, location = {Lisbon, Portugal}, series = {GoodIT '23} } #Please do Fork and Star our GitHub repository to support our hard work, so that we can collect more dataset like this and bring among you. **Contributors** 1. Adarsh Rouniyar 2. Sapdo Utomo 3. Dr. John A. 4. Dr. Pao-Ann Hsiung If you have any queries, please do contact us. 1. Adarsh Rouniyar, Email: adarsh@csie.io 2. Dr. John A., Email: johnmtech@gmail.com 3. Dr. Pao-Ann Hsiung, Email: pahsiung@gmail.com , pahsiung@ccu.edu.tw
AirBnB Duplicate Image Dataset                                                                                  ### Duplicate Image detection One-shot learning and siamese neural networks are becoming popular as NLP and vision models are approaching human performance at tasks. This dataset contains interior and exterior house pictures scraped from AirBnB over three cities (Boston, Berlin, Seattle). Specifically, every example scraped in this dataset has at least another image which is obviously a duplicate of the same room. In a sense, this dataset is a playground to explore coreferent image detection.
AirBnB US Properties Image                                                                                  This dataset is based on [AirBNB_Data ](https://www.kaggle.com/datasets/paramvir705/airbnb-data/data) by PARAMVIR_705. Original credit goes to PARAMVIR_705. This dataset filters the original dataset and adds the thumbnail images. All images have been resized to 224x224 with black or white padding options.
AirSim_Africa_forest_Images                                                                                  
AirbnbImages                                                                                  
Aircraft Context Dataset                                                                                  The Aircraft Context Dataset, a composition of two inter-compatible large-scale and versatile image datasets focusing on manned aircraft and UAVs, is intended for training and evaluating classification, detection and segmentation models in aerial domains. Additionally, a set of relevant meta-parameters can be used to quantify dataset variability as well as the impact of environmental conditions on model performance.
Airflow_images                                                                                  
Airflow_images_2                                                                                  
Airline_image                                                                                  
Airport Runway Image Dataset                                                                                  
Airport Runways Satellite Imagery                                                                                  
Airport google maps satellite imagery                                                                                  *We have collected a dataset of high-resolution satellite images of airport landscapes, ranging in size from 1500 by 1500 px to 6000 by 6000 px. The dataset includes a total of 4204 runway instances, which were generated through the labeling process. The images were collected from the Federal Aviation Administration (FAA) sources and cover a wide range of geographical locations across the United States. The dataset is diverse in terms of airport sizes, runway shapes, and surroundings, providing a comprehensive sample for training and testing object detection models. We believe this dataset will contribute to advancing the field of airport runway detection using satellite imagery and enable the development of more accurate and efficient models for this important task.* ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F9357365%2Fcdc16f9e8ebf6775b25f13ea8f8545e5%2FH_0009.png?generation=1680818126663850&alt=media)
Alaska2_image_quality_factor                                                                                  
Albanian Textual Combined Dataset                                                                                  
Albanian Textual Combined Dataset 2                                                                                  
Albanian Textual Combined Dataset 3                                                                                  
Albanian Textual Combined Dataset 4                                                                                  
Albert_text_features                                                                                  
Album Covers Images                                                                                  [https://gsurma.github.io](https://gsurma.github.io)
Alcohol Bottle Images | Glass Bottles                                                                                  ### **This dataset is collected by DataCluster Labs. To download full dataset or to submit a request for your new data collection needs, please drop a mail to:&nbsp;[sales@datacluster.ai](mailto:sales@datacluster.ai)** This dataset is an extremely challenging set of over 3000+ original Alcohol bottle images captured and crowdsourced from over 1000+ urban and rural locations, where each image is **manually reviewed and verified** by computer vision professionals at Datacluster Labs. ### **Dataset Features** - Dataset size : 3000+ - Captured by : Over 1500+ crowdsource contributors - Resolution : HD and above (1920x1080 and above) - Location : Captured with 1000+ locations across India - Diversity : Various lighting conditions like day, night, varied distances, view points etc. - Device used : Captured using mobile phones in 2020-2022 - Usage : Glass object detection, Bottle detection, Domestic object detection, Indoor object detection, Computer Vision, etc. ### Available Annotation formats COCO, YOLO, PASCAL-VOC, Tf-Record **To download full datasets or to submit a request for your dataset needs, please ping us at [sales@datacluster.ai](sales@datacluster.ai) Visit [www.datacluster.ai](www.datacluster.ai) to know more.** **Note**: All the images are manually captured and verified by a large contributor base on DataCluster platform.
AlexandreMotorImagery MOABB                                                                                  
Algerian Ultrasound Images Thyroid Dataset: AUITD                                                                                  We imported this data from Algeria's hospitals exactly Setif city , and it was labeled by volunteer doctors. the dataset contain 3 classes: Benign with 1,472 ultrasound image and Malignant with 1,895 ultrasound image,Normal thyroid 171 with ultrasound image.
Algumas Imagens                                                                                  
Alien vs. Predator images                                                                                  ### Context Bored with cats vs dogs? Let's try some extraterrestrial beings! Used in: * A deepsense.ai blog post [Keras vs. PyTorch - Alien vs. Predator recognition with transfer learning](https://deepsense.ai/keras-vs-pytorch-avp-transfer-learning) in which we compare and contrast Keras and PyTorch approaches. * Repo with code: [github.com/deepsense-ai/Keras-PyTorch-AvP-transfer-learning](https://github.com/deepsense-ai/Keras-PyTorch-AvP-transfer-learning). * **Free event:** [upcoming webinar (10 Oct 2018)](https://www.crowdcast.io/e/KerasVersusPyTorch/register), in which we walk through the code (and you will be able to ask questions). ### Content ![Alien vs Predator][1] Alien and Predator images (JPG, various ), for transfer learning. Split into Keras folder structure. * Format: JPG images, various thumbnail sizes (around 250 x 250 px). * Volume: - train: 247 aliens and 247 predators - validation: 100 aliens and 100 predators ### Acknowledgements Collected manually by Patryk Miziuła, using Google Image Search and manual filtering. ### Inspiration We wanted to use some new dataset for transfer learning. [1]: https://crowdcast-prod.imgix.net/-Kab5fihhH1Q3R92MsOJ/event-cover-4763?w=800
Aligned ISV texts                                                                                  
All Car Images Dataset (Indian Cars)                                                                                  Are you a car enthusiast or a data scientist working on computer vision projects? This All Car Images Dataset offers a comprehensive collection of high-quality images showcasing a wide range of cars in India. Whether you're interested in exploring the diverse automotive landscape of India or seeking data for your machine learning and computer vision projects, this dataset has you covered. This extensive dataset comprises all the images, featuring various Indian car models captured from different angles and in diverse settings. The images encompass popular car brands, such as Maruti Suzuki, Hyundai, Tata Motors, Mahindra, Honda, and many others. From sleek sedans to rugged SUVs, this dataset offers an array of car types to satisfy your research and analysis needs
All Image Datasets for Object Detection                                                                                  All the images collected from internet mostly Kaggle. These images are collected for build a model that can identify objects including objects name.
All The News text                                                                                  
AllAudio                                                                                  
All_Images                                                                                  
All_Train_Images_600_Original_and_CLAHE                                                                                  
All_images                                                                                  
All_images_capstone                                                                                  
Allimages                                                                                  
Aloe Vera images                                                                                  ### Context our data set is based on aloe vera leaves. ### Content it consists of three different folders i.e. healthy, rust and rot images ### Acknowledgements it will help the recognition of aloe diseases
Alpaca Dataset for Image Classification                                                                                  ### Context Small Dataset of JPEG images useful for Classification of images into 'Alpaca' and 'Not Alpaca' **This is a small dataset and is useful only for Transfer Learning** ### Content Contains two Directories : 1. alpaca : Images containing Alpaca. 2. not alpaca : Images not containing alpaca but has subjects similar to alpaca. ### Acknowledgements This dataset is extracted from 'Convolutional Neural Network' Course on Coursera by Deeplearning.Ai : [here](https://www.coursera.org/learn/convolutional-neural-networks)
Alphabet images written in sans-serif google fonts                                                                                  
Alzhaimer and frontotemporal preprocessed Image                                                                                  
Alzheimer Disease Classification Image Dataset                                                                                  
Alzheimer Disease Classification using MRI Images                                                                                  
Alzheimer's Dataset ( 4 class of Images)                                                                                  ### Context The Data is hand collected from various websites with each and every labels verified. ### Content The data consists of MRI images. The data has four classes of images both in training as well as a testing set: 1. Mild Demented 2. Moderate Demented 3. Non Demented 4. Very Mild Demented ### Inspiration The main inspiration behind sharing this Dataset is to make a very highly accurate model predict the stage of Alzheimers..
Alzheimer's MRI Brain Scan Images (Augmented)                                                                                  
Alzheimer's MRI Images Balanced Classes                                                                                  
Alzheimers disease MRI images dataset                                                                                  The dataset is about Alzheimer's disease (AD). It contains MRI images of 26 subjects, of which 10 subjects have AD, 10 subjects have Mild Cognitive impairment (MCI) and 4 subjects are normal controls. The images were collected from Firoozgar Hospital in Tehran, Iran.
Amazon Audio Books                                                                                  
Amazon Bin Image Dataset                                                                                  The Amazon Bin Image Dataset contains 50,000 images and metadata from bins of a pod in an operating Amazon Fulfillment Center. The bin images in this dataset are captured as robot units carry pods as part of normal Amazon Fulfillment Center operations. This dataset can be used for research in variety of areas like computer vision, counting genetic items and learning from weakly-tagged data. For each image, there is a corresponding entry of its metadata in JSON format stored in `metadata.sqlite` i.e. for image 01290.jpg, there is a corresponding json object in the data field of the metadata file which can be retrieved with query `SELECT data FROM metadata WHERE img_id = 01290`; Refer the [Starter Notebook](https://www.kaggle.com/dhruvildave/starter-amazon-bin-image-dataset) to see how to work with the dataset. Amazon uses a random storage scheme where items are placed into accessible bins with available space, so the contents of each bin are random, rather than organized by specific product types. Thus, each bin image may show only one type of product or a diverse range of products. Occasionally, items are misplaced while being handled, so the contents of some bin images may not match the recorded inventory of that bin. These are some typical images in the dataset. A bin contains multiple object categories and various number of instances. The corresponding metadata exist for each bin image and it includes the object category identification (ASIN - Amazon Standard Identification Number), quantity and dimensions of objects. The size of bins are various depending on the size of objects in it. The tapes in front of the bins are for preventing the items from falling out of the bins and sometimes it might make the objects unclear. Objects are sometimes heavily occluded by other objects or limited viewpoint of the images. Image Credits: [Unsplash - helloimnik](https://unsplash.com/photos/r22qS5ejODs)
Amazon Bin Image Dataset (536,434 images, 224x224)                                                                                  **Note that you can download quickly via CLI. (Kaggle Environment: 1min 36s, Colab: 1min)** ``` ! kaggle datasets download williamhyun/amazon-bin-image-dataset-536434-images-224x224 ``` ## Amazon Bin Image Dataset The Amazon Bin Image Dataset contains 536,434 images and metadata from bins of a pod in an operating Amazon Fulfillment Center. The bin images in this dataset are captured as robot units carry pods as part of normal Amazon Fulfillment Center operations. This dataset has many images and the corresponding medadata. The image files have three groups according to its naming scheme. - A file name with 1~4 digits (1,200): 1.jpg ~ 1200.jpg - A file name with 5 digits (99,999): 00001.jpg ~ 99999.jpg - A file name with 6 digits (435,235): 100000.jpg ~ 535234.jpg **Amazon Bin Image Dataset (536,434 images, 224x224)** dataset aims to provide a **resized** image files and a full metadata SQLite file for Kaggle Kernel environments. You can download a single 4GB archive file via `Download` button on this page. ### Documentation - https://github.com/awslabs/open-data-docs/tree/main/docs/aft-vbi-pds ### Download - https://registry.opendata.aws/amazon-bin-imagery/ - https://github.com/awslabs/open-data-registry/blob/main/datasets/amazon-bin-imagery.yaml
Amazon Bin Image Dataset File List                                                                                  ## Amazon Bin Image Dataset The Amazon Bin Image Dataset contains 536,434 images and metadata from bins of a pod in an operating Amazon Fulfillment Center. The bin images in this dataset are captured as robot units carry pods as part of normal Amazon Fulfillment Center operations. This dataset has many images and the corresponding medadata. The image files have three groups according to its naming scheme. - A file name with 1~4 digits (1,200): 1.jpg ~ 1200.jpg - A file name with 5 digits (99,999): 00001.jpg ~ 99999.jpg - A file name with 6 digits (435,235): 100000.jpg ~ 535234.jpg **Amazon Bin Image Dataset File List** dataset aims to provide a CSV file to contain all file locations and the quantity to help the analysis and distributed learning. ### Documentation - https://github.com/awslabs/open-data-docs/tree/main/docs/aft-vbi-pds ### Download - https://registry.opendata.aws/amazon-bin-imagery/ - https://github.com/awslabs/open-data-registry/blob/main/datasets/amazon-bin-imagery.yaml
Amazon Chatbot - Image Captioning Trained Model                                                                                  
Amazon Fashion Images for unsupervised learning                                                                                  
Amazon Product's Image                                                                                  
Amazon_Image_Product_Data                                                                                  
Amazon_Textual_Reviews_Electronics                                                                                  
Ambulance Audio Dataset                                                                                  ### Context This dataset we used for the audio analysis for visualization provides the visual representation of the sounds. The collection of the data from the sensor, online sources, and also by using the verified Siren of the ambulance. We installation of the sensors has been done by using a laptop. ### Acknowledgements This dataset has been conducted and collected at the National Center in Big Data and Cloud Computing (NCBC) Lab. ### Inspiration Your data will be in front of the world's largest data science community. What questions do you want to see answered? This dataset has been shared with the data science community and I'm happy to shared with the world
Ambulance_64x64 image.cv/dataset                                                                                  
American Sign Language Image Dataset                                                                                  This dataset contains labelled training and testing data for 9 signs. These signs are for bathroom, hello, help, I love you, more, no, repeat, thanks, and yes.
Amharic-News-Text-classification-Dataset                                                                                  https://github.com/IsraelAbebe/An-Amharic-News-Text-classification-Dataset An-Amharic-News-Text-classification-Dataset Amharic news text classification dataset with baseline performance Usage Extract the data before running the code. unzip Amharic News Dataset.zip publication arXiv:2103.05639 cite @misc{azime2021amharic, title={An Amharic News Text classification Dataset}, author={Israel Abebe Azime and Nebil Mohammed}, year={2021}, eprint={2103.05639}, archivePrefix={arXiv}, primaryClass={cs.CL} } paper with code
An Amharic News Text classification Dataset                                                                                  In NLP, text classification is one of the primary problems we try to solve and its uses in language analyses are indisputable. The lack of labeled training data made it harder to do these tasks in low resource languages like Amharic. The task of collecting, labeling, annotating, and making valuable this kind of data will encourage junior researchers, schools, and machine learning practitioners to implement existing classification models in their language. In this short paper, we aim to introduce the Amharic text classification dataset that consists of more than 50k news articles that were categorized into 6 classes. This dataset is made available with easy baseline performances to encourage studies and better performance experiments.
An Image Quality Assessment Dataset for Portraits                                                                                  
Analog gauge images                                                                                  
AnalyzeImages                                                                                  
Ancient Chinese Text (wenyanwen)                                                                                  ### Context Classical Chinese(文言文) and ancient poetry (古诗词) are, probably the most reliable primary history source about the China. They entail stories about ancient kings, legends of gods, of struggling braveries, of uncelebrated love, of how stars look like when dynasties toppled, of how people whistling, farming, entertaining and math puzzling with algebra thousands of years ago. They hold different philosophies, some worship order and courtesy, some excel in deception of war, others believe in balance and nature. They give birth to a language derived into thousands of living dialects and still spoken, written among more than a billion of human being on this planet. ### Content Data is from the 2020, March's data dump from wikisource &gt; The data is in csv format with 4 columns: * id: id from datadump * url: The original wikisource file * title: The title of the article/ poetry * text: The textual data in Chinese ### Acknowledgements This dataset was parsed from [Wikisource](https://zh.wikisource.org/wiki/Main_Page)'s data dump, thanks to all the contributor editing these words, as honest to the original as possible ### Inspiration * What's the relationship between words, names? * Any generative model for such material? * Any way we can search through these text for event/ figure/ story better?
Ancient Language Images                                                                                  
Ancient Temple Vimana Images Dataset                                                                                  The KU-UBDTCE-JNNCE Temple Vimana Database for Ancient Temple Architecture research is constructed by collecting the temple images with three basic types of architecture such as 1) Nagara , 2) Dravida, and 3) Vesara. The database is collected from various places such as 1. Amrutheshwara Temple at Amruthapura Village, 2. Yoga Narasimha Temple at Baggavalli Village , 3. Aghoreshwara Temple at Ikkeri Village, 4. Parvathi Brahmeshwara Temple at Kudli Village, 5. Sri Malikarjuna Temple at HireNalluru Village, 6. Hucharaya Swamy temple at Shikaripura Village in and around the shivamogga and chikamagaluru districts of Karnataka state, India. Along with these some images were also collected from the web with the help of google search and the trip advisor travel portal. KU-UBDTCE-JNNCE comprises three sub-databases namely 1) Nagara , 2) Dravida, and 3) Vesara. These three sub-databases in turn it contains two sub-databases Input_Images and Ground_Truth_Images. A brief description of the KU-UBDTCE-JNNCE Ancient Temple vimana Dataset is as described bellow: Sl.No. Type of Vimana Architecture Total Number of Samples 1. Nagara 253 2. Dravida 313 3. Vesara 300 Total 866 Description: Camera Device used for capturing images is Samsung having the SM-G615F with F-stop: f/1.7. All Images are captured in 3 different time slots 10.00 AM to 10.45 AM, 2.00 PM to 2.40 PM & 5.15 PM to 5.40 PM. In order to get better images for processing Purpose: The KU-UBDTCE-JNNCE Ancient Temple Vimana Database is meant to be used for research purposes and shall not be used nor included in commercial applications in any form (e.g., original files, encrypted files, files containing extracted features, etc).
Ancient text 'English to Sundanese' data                                                                                  
Android dex bytecode images                                                                                  
Animal Classification Dataset - Image Recognition                                                                                  This dataset contains a collection of images of ten different animals for the purpose of classification. The images are labeled as Dog, Cat, Duck, Deer, Horse, Goat, Lion, Tiger, Frog, and Bird. The images were collected from various sources. This dataset can be used for various applications such as object detection, image recognition, and machine learning.
Animal Image Classification Dataset                                                                                  
Animal Image Classification Dataset                                                                                  Dataset Summary: The Animal Image Classification Dataset is a comprehensive collection of images tailored for the development and evaluation of machine learning models in the field of computer vision. It contains 3,000 JPG images, carefully segmented into three classes representing common pets and wildlife: cats, dogs, and snakes. Dataset Contents: cats/: A set of 1,000 JPG images of cats, showcasing a wide array of breeds, environments, and postures. dogs/: A diverse compilation of 1,000 dog images, capturing a multitude of breeds in various activities and settings. snakes/: An assortment of 1,000 images of snakes, depicting numerous species in both natural and controlled habitats. Image Details: Resolution: Each image maintains a uniform resolution of 256x256 pixels, providing clarity and consistency for model training. File Format: JPG Color Space: RGB Intended Applications: This dataset is primed for use in developing and testing AI models specialized in multi-class animal recognition. It offers valuable resources for researchers and hobbyists in fields such as zoology, pet technology, and biodiversity conservation. Acknowledgments and Licensing: This dataset is a collective effort of various photographers and organizations. All images are distributed with permissions for academic and non-commercial usage, provided that proper attribution is given to the original sources.
Animal Image Classification Main (A-Z Dataset)                                                                                  
Animal Image Classification using CNN model                                                                                  
Animal Image Classification(Dogs & Cats)                                                                                  ### Context Images of animals for Binary Classification ![Dog & Cat ](https://miro.medium.com/max/639/1*vg3kxX3H9eCNWKgBBeQHeA.jpeg) ### Content Train Set contains about 4000 images each of Cats and Dogs Test Set contains 1000 images each of Cats and Dogs.
Animal Image Data                                                                                  
Animal Image Dataset (90 Different Animals)                                                                                  ### Context Animals (also called Metazoa) are multicellular, eukaryotic organisms in the biological kingdom Animalia. With few exceptions, animals consume organic material, breathe oxygen, are able to move, can reproduce sexually, and go through an ontogenetic stage in which their body consists of a hollow sphere of cells, the blastula, during embryonic development. Over 1.5 million living animal species have been described—of which around 1 million are insects—but it has been estimated there are over 7 million animal species in total. Animals range in length from 8.5 micrometers (0.00033 in) to 33.6 meters (110 ft). They have complex interactions with each other and their environments, forming intricate food webs. The scientific study of animals is known as zoology. Most living animal species are in Bilateria, a clade whose members have a bilaterally symmetric body plan. The Bilateria include the protostomes—in which many groups of invertebrates are found, such as nematodes, arthropods, and mollusks—and the deuterostomes, containing both the echinoderms as well as the chordates, the latter containing the vertebrates. Life forms interpreted as early animals were present in the Ediacaran biota of the late Precambrian. Many modern animal phyla became clearly established in the fossil record as marine species during the Cambrian explosion, which began around 542 million years ago. 6,331 groups of genes common to all living animals have been identified; these may have arisen from a single common ancestor that lived 650 million years ago. Source: Wikipedia ### Content In this Dataset, we have 5400 Animal Images in 90 different categories or classes.
Animal Image Dataset (90 Different Animals)                                                                                  
Animal Image Dataset resized                                                                                  Class of the animal is the folder name and images are in the folder
Animal Image Dataset(DOG, CAT and PANDA)                                                                                  
Animal Images 5 Classes Bird Cat Dog Fish Rabbit                                                                                  
Animal Images Dataset                                                                                  ### Context With the recent advancements in TPUs and GPUs, there has been a boom in computer vision tasks in the AI domain. State-of-the-art neural network architectures are deep and contain millions of parameters. To train these networks you need thousands or sometimes millions of data point to get a proper model. The process of data collection when done manually is very tedious and is a boring process especially when there are thousands of data points. I wrote an automated script to scrape new data so that I can keep the dataset up to date. ### Content The data is scraped from Sri Lanka's most famous online marketplace: Ikman.lk. The Credit for all the information goes to Ikman. This Dataset is just a tool for all the Data Scientists out there interested in inferring something more out of the data. ### Acknowledgement You could use this dataset for image classification tasks in the AI domain. You could also use this dataset to build an animal image captioner using variants of RNNs. Additionally, you can use this dataset as a supplementary dataset in your other image classification tasks. The opportunities are limitless! ### Future I will be updating this dataset every month. I will try to improve the quality of the data collected as well as increase the number of sources being used. --- Finally, I would be very happy if this dataset could bring a positive impact on your life or your workflow. And cheers!!!
Animal Visual Cortext and CNN                                                                                  
Animal image classification using CNN Part II                                                                                  Animal classification of all kinds.
Animal image for Recognition                                                                                  
Animal image recognition                                                                                  
AnimalImages                                                                                  
Animals Detection Images Dataset                                                                                  Animals (Object) Detection dataset extracted using Google Open Images V6+. Classes of animals added -&gt; 1. Dog 2. Cat 3. Zebra 4. Lion 5. Leopard 6. Cheetah 7. Tiger 8. Bear 9. Brown Bear 10. Butterfly 11. Canary 12. Crocodile 13. Polar Bear 14. Bull 15. Camel 16. Crab 17. Chicken 18. Centipede 19. Cattle 20. Caterpillar 21. Duck
Animaux_images                                                                                  
Anime Characters Personality And Facial Images                                                                                  I'm broadly interested in the MBTI personality framework, and I also love anime. For those who don't know what MBTI is, here is [a great diagram](https://www.researchgate.net/profile/Mohammad-Hossein-Amirhosseini/publication/339935842/figure/fig1/AS:869042915131393@1584207385930/Personality-types-key-10.ppm) that illustrate what it is. Once I was searching for the personality type of my favorite anime character, and I noticed that there might be a relationship between how an anime character looks v.s. what's their personality types. [Personality database](https://www.personality-database.com/top-story/8) is a great way to obtain character arts, anime name, and their personality types. Here we scraped all characters in the top anime in the chart. The information include: anime name, genre, character name, MBTI type and enneagram type. All enneagram types are in the format of [main_type]w[wing_type]. Wing type is the type that is nearby the main type (plus/minus one from the main type) that you identify most with. In the image folder, you have subfolders named with the anime and avatars of characters in that anime. Note that: 1. The folder name could be different from the anime name because folder name doesn't allow certain characters. 2. Same with the image file name. 3. Avatar image could be empty. Using this dataset, one can train a facial attributes detector to extract attributes like hair color, eye color, eye wear, facial hair, and so on. On the other hand, you can also just train it as a classical image classification task. One might also consider just use this to analyze popular types in anime. Happy coding! Github source code: https://github.com/tianyimasf/tidy-tuesday-social-dataset-analysis/blob/main/scrape_anime_character.py
Anime Image Dataset (deep learning)                                                                                  
Anime Images                                                                                  
Anime Images Dataset                                                                                  ## Context This dataset contains anime images for 231 different anime, with approximately 380 image for each of those anime. Please note that you might need to clean the image directories a bit, since the images might contain merchandise and live-action photos in addition to the actual anime itself. ## Scripts If you'd like to take a look at the scripts used to make this dataset, you can find them on this [GitHub repo](https://github.com/Etrama/anime-image-scraper). Feel free to extend it, scrape your own images, etc. etc. ## Inspiration As a big anime fan, I found a lot of anime related datasets on Kaggle. I was however disappointed to find no dataset containing anime specific images for popular anime. Some other great datasets that I've been inspired by include: - [Top 250 Anime 2023](https://www.kaggle.com/datasets/gianinamariapetrascu/top-250-anime-2023) - [Anime Recommendations Database](https://www.kaggle.com/datasets/CooperUnion/anime-recommendations-database) - [Anime Recommendation Database 2020](https://www.kaggle.com/datasets/hernan4444/anime-recommendation-database-2020) - [Anime Face Dataset](https://www.kaggle.com/datasets/splcher/animefacedataset) - [Safebooru - Anime Image Metadata](https://www.kaggle.com/datasets/alamson/safebooru) ## Process 1. You need a list of anime to scrape it. You can either: - Make your own list. This is what I do in the directory called 'scraped_anime_list'. - Use someone else's list. This is what I do in the directory called 'kaggle_anime_list' and 'top_anime_list'. 2. To be honest, I wanted to make my own list. To make a list of anime, I used the python wrapper of the unofficial MAL (MyAnimeList) API called [JikanPy](https://github.com/abhinavk99/jikanpy). JikanPy scraped MAL. 3. Animes on MAL have a unique identifier called anime id, think of this as a unique number for each anime. This is supposed to be sequential but there are a lot of gaps from valid anime id to the next, which I discovered based on this [post](https://myanimelist.net/forum/?topicid=2000753). 4. These IDs can go from 1 - 100,000 and maybe beyond. However, I decided to go through the anime ids one by one from 1-50,000 and retrive the **id**, **rank** and **anime_name**. This is what you will find in the folder called 'scraped_anime_list'. Note that I prefer using the English name of the anime if it exists, and if it doesn't I get the Japanese name. Please use this list to obtain the anime ids if you intend to scrape MAL yourself, it will save you a LOT of time. 5. I thought that someone else might've gone through and same process and voila, I found [MyAnimeList Dataset](https://www.kaggle.com/datasets/azathoth42/myanimelist) on kaggle. I didn't want to wait for my scraper to finish scraping, so I decided to use this 'anime_cleaned.csv' version of this list. The lists from this dataset are what you find in the 'kaggle_anime_list' folder. 6. Cleaning anime names is a task in and of itself. Within the [GitHub repo](https://github.com/Etrama/anime-image-scraper), refer to the file called 'notes_and_todo.md' to look at all the cleaning troubles. I tried my best to remove all: - Anime Movies: Since you have for instance One Piece (the anime) and One Piece Movie 1, One Piece Movie 2, and so on. - Seasons: MAL is an anime ranker. Different anime seasons can show up on the list with different ranks. I retain the original anime name (the most basic ones, for instance, just 'Gintama' instead of 'Gintama Season 4'. 7. Ultimately, I manually curated around 300 anime names, which reduced to 231 after removing duplicates, since after the curation, 'Gintama' and 'Gintama: Enchousen' would both be named 'Gintama'. This list with the duplicates is what you find in the file called 'UsableAnimeList.xlsx' within the 'top_anime_list' folder. 8. This list is then rid of the duplicates and used to scrape the image URLs for each anime found in the folder called 'anime_img_urls'. 9. These URLs are then used to scrape the anime images themselves, found in the folder called 'anime_images'. 10. Also the tags are only a guide, feel free to use this dataset for any Deep Learning task. ## Sources - [JikanPy](https://github.com/abhinavk99/jikanpy) - [Useful MAL forum post](https://myanimelist.net/forum/?topicid=2000753) - Google Image Search - Cover image and thumnail obtained from [Safebooru](https://safebooru.org/index.php?page=post&s=view&id=4293259)
Anime Images Dataset With Their Description                                                                                  This Dataset contains total 3980 (`768x768`) Anime Style Images Here is the dataset deconstruction: - It has 2 folders, `90s Styled Anime Images` & `Randomly Styled Anime Images` - `90s Styled Anime Images` contains 1980 images styled as 90s anime - `Randomly Styled Anime Images` contains 2000 randomly anime styled images - Each folder (`90s Styled Anime Images` & `Randomly Styled Anime Images`) has a file called `metadata.json` that contains relative path of image with it's description - This means whole dataset has 3982 files divided across 2 folders Data is generated using rule based prompt generation technique
Anime Names and Images Dataset                                                                                  ## Welcome to the world of anime data 👇 👇 💙 😍 😉 🎶 ### **Show some love 🙌 💚 💙 if you found this dataset useful** ![Anime World](https://animeallinblog.files.wordpress.com/2016/08/anime-characters-best-multi.jpg?w=705&h=435&crop=1) # Context Following dataset contains the names, images and links of anime characters. # Content Data contains 3 parts. 1: **anime_links.csv** file contains the links to all anime characters pages present in above mentioned site. There are around 73000 links. (That's a huge family xd😅 ). 2: **anime_names.csv** file contains the names of all anime characters excluding the one's with characters other than ascii in them (You can include them by editing anime-links.csv.). There are around 71000 names. (Those 2000 characters have a very complex names lol👀 , sorry if you are a fan of one of them🙁) 3: **final_names.csv** file contains the names of anime characters but not with names having the characters other than ASCII (eg: Raoul_Mathias_Jean_AimÃ©e ) 3: **dataset folder** contains all the images of anime characters(over 58000 images are present) with name of file as it's character name. # Usage Well the data can be used in many ways, Some of the ways are 0: If you are an anime fan, you can use images in dataset folder as desktop wallpaper😄. 1: ***anime_links.csv*** can be used for web scraping to extract other features of anime characters (True anime lover will do this😄) 2: ***anime_names.csv*** can be used to generate new anime character names. 3: ***dataset images*** can be used to generate new anime images. 4: My target is to combine 2,3 and create a new anime character with a new name. (Very exciting right💥) **If you find any other usage, kindly inform me , we can work on that together**🙏 # Acknowledgements All the data is extracted from [Anime and Manga](https://myanimelist.net/character.php)
Anime Scene Image                                                                                  
Anime comped image dataset                                                                                  Anime portriats from https://www.gwern.net/Crops#portraits-dataset.
Anlog-clock sintetic images made with textures                                                                                  **How it was made?** Data set of 50000 synthetic analog-clock images labeled automatically with a program that having textures and the relevant parts of a clock generates clocks with different textures. Created to make the neural network learn the relevant parts of the problem This was used to test a project of reading clocks using neural networks.
Annotated & Segmented Images                                                                                  
Annotated Audioset - Speech, Music & Noise                                                                                  
Annotated Bridge Image Collection                                                                                  
Annotated Facial Images for Stroke Classification                                                                                  This is an annotated dataset crucial for a deep learning project focused on acute stroke detection and classification in medical images. With 3745 JPG images, the dataset is divided into two classes: 1245 images depict individuals diagnosed with acute stroke, while 2500 images show those without. Employing diverse data augmentation methods such as flipping, rotation, and scaling, I ensure dataset diversity and robustness, reflecting real-world scenarios more accurately. This versatile dataset offers a comprehensive resource for researchers and healthcare professionals in stroke medicine, facilitating the training of machine learning models for stroke diagnosis. By making the dataset and annotations publicly available, I encourage academic use, fostering collaboration and advancement in stroke detection and diagnosis research.
Annotated Plankton Images2007                                                                                  
Annotated Potholes Image Dataset                                                                                  # A Fully Annotated Image Dataset for Pothole Detection ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4217328%2F93f30861c0d29b73ad3d7145571b6bcb%2Fchart.svg?generation=1584011275397299&alt=media) Fig: Distribution of Different Categories of Potholes ### Pothole Size-Categories - Small: `BoundingBox Area &lt;= 1024px` - Medium: `1024px &lt; BoundingBox Area &lt;= 9216px` - Large: `BoundingBox Area &gt; 9216px` Note: These size-categories were calculated after resizing the images to 300x300 pixels keeping the aspect ratio. This is similar to Microsoft COCO Size Metrics. ## Splits - The directory `annotated-images` contains the images having pothole and their respective annotations (as XML file). - The file `splits.json` contains the annotation filenames (.xml) of the **training** (80%) and **test** (20%) dataset in following format--- ```javascript { 'train': ['img-110.xml', 'img-578.xml', 'img-455.xml', ...], 'test': ['img-565.xml', 'img-498.xml', 'img-143.xml', ...] } ```
Annotated Rickshaw Images                                                                                  Contributors: - Mansib Mursalin - Ahasanul Karim - Saiem Intesar Tamzid - Kazi Akhir Mahmud Abstract: Vehicle tracking is one of the leading fields of application of computer vision and deep learning. Though there are many datasets for tracking cars, trucks, buses, and other widely available vehicles around the world, there's a lack of datasets on Rickshaws found in Bangladesh and India. So we set out to create a dataset for tracking Rickshaws as part of an ongoing project we were doing at the time. Various tracking algorithms can be used to track Rickshaws using the dataset. The aim of this dataset is to use it to find which technique works best in real-life scenarios. You can use YOLO, TensorFlow Object Detection API, etc.
Annotated food crops and weed images                                                                                  The dataset is composed of 1176 images (folder 'raw images') in which 6 food crops and 8 weed species are identified, altogether 7853 annotations were made in total. Three RGB digital cameras were used for image capturing: Intel RealSense D435, Canon EOS 800D, and Sony W800. The images were taken on food crops and weeds grown in controlled environment and field conditions at different growth stages. Folder 'annotations' consists of annotated XML files related to each raw image.
Anomaly detection in water pump using audio data                                                                                  
Anoop-Harbor-Images                                                                                  
AnotImages-25/3/23                                                                                  
Anti drone hybrid image dataset                                                                                  该数据集是为城市低空场景中，无人机探测识别所用，数据集总计10731张RGB图片，含有7类城市低空场景，两种型号的无人机，图像分辨率包含1920x1080，2560x1440，4096x2160，简单背景、中等背景、复杂背景分别占据了整个数据集的21.25%、45.59%、33.16%，数据集标注格式为YOLO格式，标签为txt文档。数据集已经按照6：2：2的比例划分为训练集，验证机，测试集
Anti-LGBT Cyberbullying Texts                                                                                  This dataset was derived from the 'Measuring Hate Speech' corpus available at https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech and published as: Pratik Sachdeva, Renata Barreto, Geoff Bacon, Alexander Sahn, Claudia von Vacano, and Chris Kennedy. 2022. The Measuring Hate Speech Corpus: Leveraging Rasch Measurement Theory for Data Perspectivism. In Proceedings of the 1st Workshop on Perspectivist Approaches to NLP @LREC2022, pages 83–94, Marseille, France. European Language Resources Association. This dataset contains a cleaned and re-annotated version developed for the purpose of training machine language systems for binary classification of anti-LGBT cyberbullying. Messages were originally sourced from Twitter, Reddit, and YouTube.
Ants Image Dataset                                                                                  
Aozora Bunko (青空文庫) Japanese Text Corpus                                                                                  ## *Purpose* A compilation of the texts available through [青空文庫](https://www.aozora.gr.jp/) (Aozora Bunko Japanese Literature Corpus) into more friendly csv form, to jumpstart projects looking at written Japanese. --- ## *Introduction* Aozora Bunko is a collection of public domain Japanese texts, digitized and available in a range of formats. The corpus is currently available in its entirety [*here*](https://github.com/aozorabunko/aozorabunko), and more information is available [*here*](http://en.wikipedia.org/wiki/Aozora_Bunko). The repository contains (or links to) all texts available through 青空文庫, but these files and links are contained in individual directories with varying file types, so the full text of the corpus is a bit scattered. Compiling as many of the full texts as possible into one file while preserving available metainformation will (hopefully!) provide a more accessible dataset for those looking to work with a large corpus of literary Japanese.
Apparel image dataset 2                                                                                  For the data set provided first, see the next page. + original: [https://www.kaggle.com/trolukovich/apparel-images-dataset](https://www.kaggle.com/trolukovich/apparel-images-dataset) I added a csv file containing colors and labels. See data. ex) black_dress --&gt; [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] Also, the image column of the csv file contains the full path where the image exists. ## Content The dataset consist of 11385 images and includes next categories: + black_dress: 450 + black_pants: 871 + black_shirt: 715 + black_shoes: 766 + black_shorts: 328 + blue_dress: 502 + blue_pants: 798 + blue_shirt: 741 + blue_shoes: 523 + blue_shorts: 299 + brown_pants: 311 + brown_shoes: 464 + brown_shorts: 40 + green_pants: 227 + green_shirt: 230 + green_shoes: 455 + green_shorts: 135 + red_dress: 800 + red_pants: 308 + red_shoes: 610 + white_dress: 818 + white_pants: 274 + white_shoes: 600 + white_shorts: 120
Apparel images dataset                                                                                  ### Context This dataset have been created by me to practice multi-label classification, so I decided to put it here, I hope it will help anyone to practice same tasks. ### Content The dataset consist of 11385 images and includes next categories: - black_dress: 450 - black_pants: 871 - black_shirt: 715 - black_shoes: 766 - black_shorts: 328 - blue_dress: 502 - blue_pants: 798 - blue_shirt: 741 - blue_shoes: 523 - blue_shorts: 299 - brown_pants: 311 - brown_shoes: 464 - brown_shorts: 40 - green_pants: 227 - green_shirt: 230 - green_shoes: 455 - green_shorts: 135 - red_dress: 800 - red_pants: 308 - red_shoes: 610 - white_dress: 818 - white_pants: 274 - white_shoes: 600 - white_shorts: 120
Apple Diseases Image Dataset                                                                                  
Apple Hyperspectral Images Dataset                                                                                  Hyperspectral dataset of pure and fertilizer-coated apples for measuring the level of chemicals used. The dataset consists of hyperspectral images of various apples. They are divided into three categories: 1.'fresh' - images of apple directly bought from the market 2.'low concentration' - images of apples immersed in fungicide/insecticide solution at low concentration level i.e 1g or 1ml of fertilizer in 1 liter water and 3.'high concentration' - images of apples immersed in fungicide/insecticide solution at low concentration level i.e 3g or 3ml of fertilizer in 1 liter water and The hyperspectral images are saved by default in .bil format. This dataset is given in .tif format. The entire dataset is classified in three folders.1.Apple_Samples,2.Fungicide_Apple,3.Insecticide_Apple Apple_Samples folder consist of two folders, monostar and nativo. 'Monostar' is further classified into four folders which sums up to a total of 207 images. 'Nativo' consists of three folders which as a total of 73 images. Fungicide_Apple consist of 162 images and is divided into three category i.e. fresh apples, apples immersed in low concentration solution and apples immersed in high concentration solution. The fungicide used for this experiment is NATIVO. Similarly, Insecticide_Apple consist of 175 images and is also divided into three category i.e. fresh apples, apples immersed in low concentration solution and apples immersed in high concentration solution. The insecticide used for this experiment is MONOSTAR.
Apple Products Image Dataset                                                                                  ### Context I wanted to use a unique and different dataset for an exercise of image classification task. That's when I decided to create my own image dataset. This dataset consists of 403 Macbook images, 555 iPad images, and 556 iPhone images, all scraped from the top results of Google Image Search. These images are pretty robust, as they have product images from multiple different angles and in multiple lighting conditions.
Apple Ripeness Levels Image Dataset                                                                                  
Apple dataset images                                                                                  This a copy from Fruits 360 dataset A dataset with 90380 images of 131 fruits and vegetables https://www.kaggle.com/datasets/moltean/fruits
Apple vs Orange Image Classification Dataset                                                                                  Images of the juicy apples and succulent oranges in their own bowls(folders), and there's your afternoon snack! I'm really craving an Orange right now. Anyway, I was lonely and bored enough to scrape the images from Pinterest with an API, Have fun!
Apples or tomatoes - image classification                                                                                  Images of tomatoes and apples collected via web scraping. These two fruits have been chosen because they look similar and so should be challenging classify. Images of red, yellow and green apples/tomatoes have been split into a train and test set. Image sizes usually range between 100x100 pixels and 300x300 pixels. Since the train set is small, you might want to consider using transfer learning to build the best model.
Applicability of OCR Engines for text recognition                                                                                  The input images or data required for conducting the experiment and performance evaluation were taken from internet only the relevant images which can measure the actual performance of OCR engines are considered.
Aptos and Messidor eye images                                                                                  Early detection of Diabetic Retinopathy is a key challenge to prevent a patient from potential vision loss. The task of DR detection often requires special expertise from ophthalmologists. In remote places of the world such facilities may not be available, so In an attempt to automate the detection of DR, machine learning and deep learning techniques can be adopted. Some of the recent papers have proven such success on various publicly available dataset. Another challenge of deep learning techniques is the availability of rightly processed standardized data. Cleaning and preprocessing the data often takes much longer time than the model training. As a part of my research work, I had to preprocess the images taken from APTOS and Messidor before training the model. I applied circle-crop and Graham Ben's preprocessing technique and scaled all the images to 512X512 format. Also, I applied the data augmentation technique and increased the number of samples from 3662 data of APTOS to 18310, and 400 messidor samples to 3600 samples. I divided the images into two classes class 0 (NO DR) and class 1 (DR). The large number of data is essential for transfer learning. This process is very cumbersome and time-consuming. So I thought to upload the newly generated dataset in Kaggle so that some people might find it useful for their work. I hope this will help many people. Feel free to use the data.
Arabic (Iraqi dialect) text classification dataset                                                                                  Citation: This dataset was generated for the study presented in the paper: 'Hussein A. Nasrullah, Mohammed A. Nasrullah, Wameedh N. Flayyih; Sentiment analysis in arabic language using machine learning: Iraqi dialect case study. AIP Conf. Proc. 27 March 2023; 2651 (1): 060015. https://doi.org/10.1063/5.0111448' Sentiment Analysis in Arabic Language : Iraqi Dialect , online movie reviews.
Arabic Audio                                                                                  
Arabic Data for Scene Text Recognition                                                                                  The dataset consists of 50,000 cropped images with embedded Arabic text. The labels were generated from an Arabic words corpus, which consists of 15 thousand words. The second dataset was collected from Twitter Arabic hashtags and contins 100 cropped images. The datasets were used in our publuished paper 'Detecting Spam Images with Embedded Arabic Text in Twitter'. source : https://data.mendeley.com/datasets/gfc32vndz8/2
Arabic Letters Images Created by Master's Students                                                                                  The dataset was created by the students of the Master's program in Artificial Intelligence and Information Processing, class of 2022/2024, at Badji Mokhtar University - Annaba. It consists of 28 Arabic letters, totaling 8,418 images in JPG and PNG formats. Each letter is represented by a varying number of images as follows: 'أ': The letter 'أ' has 306 images. 'ب': The letter 'ب' has 301 images. 'ت': The letter 'ت' has 312 images. 'ث': The letter 'ث' has 258 images. 'ج': The letter 'ج' has 310 images. 'ح': The letter 'ح' has 307 images. 'خ': The letter 'خ' has 321 images. 'د': The letter 'د' has 305 images. 'ذ': The letter 'ذ' has 302 images. 'ر': The letter 'ر' has 317 images. 'ز': The letter 'ز' has 263 images. 'س': The letter 'س' has 308 images. 'ش': The letter 'ش' has 306 images. 'ص': The letter 'ص' has 394 images. 'ض': The letter 'ض' has 389 images. 'ط': The letter 'ط' has 308 images. 'ظ': The letter 'ظ' has 312 images. 'ع': The letter 'ع' has 309 images. 'غ': The letter 'غ' has 306 images. 'ف': The letter 'ف' has 334 images. 'ق': The letter 'ق' has 333 images. 'ك': The letter 'ك' has 298 images. 'ل': The letter 'ل' has 309 images. 'م': The letter 'م' has 312 images. 'ن': The letter 'ن' has 307 images. 'ه': The letter 'ه' has 282 images. 'و': The letter 'و' has 304 images. 'ي': The letter 'ي' has 305 images. The letter 'ص' has the highest count with 394 images, while 'ز' has the lowest with 263. Most letters have a relatively balanced distribution, ranging from 301 to 334 images.
Arabic Natural Audio Dataset                                                                                  Emotion expression is an essential part of human interaction. The same text can hold different meanings when expressed with different emotions. Thus understanding the text alone is not enough for getting the meaning of an utterance. Acted and natural corpora have been used to detect emotions from speech. Many speech databases for different languages including English, German, Chinese, Japanese, Russian, Italian, Swedish and Spanish exist for modeling emotion recognition. Since there is no reported reference of an available Arabic corpus, we decided to collect the first Arabic Natural Audio Dataset (ANAD) to recognize discrete emotions. Embedding an effective emotion detection feature in speech recognition system seems a promising solution for decreasing the obstacles faced by the deaf when communicating with the outside world. There exist several applications that allow the deaf to make and receive phone calls normally, as the hearing-impaired individual can type a message and the person on the other side hears the words spoken, and as they speak, the words are received as text by the deaf individual. However, missing the emotion part still makes these systems not hundred percent reliable. Having an effective speech to text and text to speech system installed in their everyday life starting from a very young age will hopefully replace the human ear. Such systems will aid deaf people to enroll in normal schools at very young age and will help them to adapt better in classrooms and with their classmates. It will help them experience a normal childhood and hence grow up to be able to integrate within the society without external help. Eight videos of live calls between an anchor and a human outside the studio were downloaded from online Arabic talk shows. Each video was then divided into turns: callers and receivers. To label each video, 18 listeners were asked to listen to each video and select whether they perceive a happy, angry or surprised emotion. Silence, laughs and noisy chunks were removed. Every chunk was then automatically divided into 1 sec speech units forming our final corpus composed of 1384 records. Twenty five acoustic features, also known as low-level descriptors, were extracted. These features are: intensity, zero crossing rates, MFCC 1-12 (Mel-frequency cepstral coefficients), F0 (Fundamental frequency) and F0 envelope, probability of voicing and, LSP frequency 0-7. On every feature nineteen statistical functions were applied. The functions are: maximum, minimum, range, absolute position of maximum, absolute position of minimum, arithmetic of mean, Linear Regression1, Linear Regression2, Linear RegressionA, Linear RegressionQ, standard Deviation, kurtosis, skewness, quartiles 1, 2, 3 and, inter-quartile ranges 1-2, 2-3, 1-3. The delta coefficient for every LLD is also computed as an estimate of the first derivative hence leading to a total of 950 features. I would have never reached that far without the help of my supervisors. I warmly thank and appreciate Dr. Rached Zantout, Dr. Lama Hamandi, and Dr. Ziad Osman for their guidance, support and constant supervision.
Arabic Natural Audio Dataset                                                                                  ### Context This is the first Arabic Natural Audio Dataset (ANAD) developed to recognize 3 discrete emotions: Happy, angry, and surprised. ### Content Eight videos of live calls between an anchor and a human outside the studio were downloaded from online Arabic talk shows. Each video was then divided into turns: callers and receivers. To label each video, 18 listeners were asked to listen to each video and select whether they perceive a happy, angry or surprised emotion. Silence, laughs and noisy chunks were removed. Every chunk was then automatically divided into 1 sec speech units forming our final corpus composed of 1384 records. Twenty five acoustic features, also known as low-level descriptors, were extracted. These features are: intensity, zero crossing rates, MFCC 1-12 (Mel-frequency cepstral coefficients), F0 (Fundamental frequency) and F0 envelope, probability of voicing and, LSP frequency 0-7. On every feature nineteen statistical functions were applied. The functions are: maximum, minimum, range, absolute position of maximum, absolute position of minimum, arithmetic of mean, Linear Regression1, Linear Regression2, Linear RegressionA, Linear RegressionQ, standard Deviation, kurtosis, skewness, quartiles 1, 2, 3 and, inter-quartile ranges 1-2, 2-3, 1-3. The delta coefficient for every LLD is also computed as an estimate of the first derivative hence leading to a total of 950 features. ### Acknowledgements klaylat, Samira; Osman, ziad; Zantout, Rached; Hamandi, Lama (2018), “Arabic Natural Audio Dataset”, Mendeley Data, V1, doi: 10.17632/xm232yxf7t.1
Arabic News Texts Corpus                                                                                  ### Context This is Arabic news data with 9 categories in csv format original data link: https://www.kaggle.com/antcorpus/antcorpus
Arabic Short Vowels Audio Dataset                                                                                  
Arabic Text Dataset                                                                                  
Arabic Text Diacritization                                                                                  Extracted from the Tashkeela Corpus, the dataset consists of 55K lines containing about 2.3M words. Source: [https://github.com/AliOsm/arabic-text-diacritization](https://github.com/AliOsm/arabic-text-diacritization)
Arabic Text Summarization Dataset                                                                                  This dataset hosted on Kaggle is a curated collection specifically tailored for Arabic text summarization tasks. It comprises two fundamental columns: 'Paragraph' and 'Summary,' both presented in the Arabic language. The 'Paragraphs' column encompasses a diverse range of textual content, spanning various subjects, genres, and writing styles in Arabic.
Arabic sign language binary image dataset                                                                                  
Arabic text paired with sentiment labels                                                                                  This dataset consists of Arabic text samples paired with sentiment labels. Each row contains a piece of text in Arabic, presumably representing user feedback, reviews, or opinions, along with a sentiment label indicating whether the sentiment expressed in the text is positive or negative. Here's a breakdown of the columns: Text: This column contains the Arabic text samples expressing opinions or sentiments. Sentiment: This column contains the sentiment labels associated with each text sample. It indicates whether the sentiment expressed in the corresponding text is positive or negative. This dataset can be used for sentiment analysis tasks, where the objective is to automatically classify the sentiment of text data. It could be employed to train machine learning models to categorize Arabic text into positive or negative sentiment categories, enabling various applications such as customer feedback analysis, product reviews analysis, and opinion mining in Arabic-speaking contexts.
Arabic text summarization 30_000                                                                                  
Arabic tweets for tickets texts                                                                                  
Arabic-letters-1000-images-per-letter                                                                                  
Arabic-letters-2500-images-per-letter                                                                                  just deleted the 'main.py' from the 'ل' folder in [this dataset](https://www.kaggle.com/datasets/mahmoudreda55/arabic-letters-numbers-ocr)
Arabic-letters-900-images-per-letter                                                                                  taken from [here](https://github.com/HossamBalaha/HMBD-v1/tree/master/Dataset) , then merged some folders together
Arabic_Text                                                                                  
Architectural Heritage Elements Image64 Dataset                                                                                  Architectural Heritage Elements Dataset (AHE) is an image dataset for developing deep learning algorithms and specific techniques in the classification of architectural heritage images. This dataset consists of 10235 images classified in 10 categories: Altar, Apse, Bell tower, Column, Dome (inner), Dome (outer), Flying buttress, Gargoyle, Stained glass, Vault. It is inspired by the CIFAR-10 dataset but with the objective in mind of developing tools that facilitate the tasks of classifying images in the field of cultural heritage documentation. Most of the images have been obtained from Flickr and Wikimedia Commons (all of them under creative commons license).
Architectural images for GANs                                                                                  
Arctic Sea Ice Image Masking                                                                                  ### Context I was inspired to compile this dataset after finding the excellent sea ice charts published weekly by the [Canadian Ice Service](https://www.canada.ca/en/environment-climate-change/services/ice-forecasts-observations/latest-conditions.html). I thought it would be interesting to see if such charts could be generated automatically from satellite imagery. Github repo for project: [https://github.com/asylve/Sea-Ice](https://github.com/asylve/Sea-Ice) ### Content This dataset contains 3392 satellite images of Hudson Bay sea ice in the Canadian Arctic from 2016-1-1 to 2018-7-31. Images are from Sentinel-2 satellites and are made up of bands 3, 4, and 8 ([false color](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/composites/)). Each image has an associated mask showing the concentration of sea ice throughout the image. The naming conventions for files is: P[patch id]-yyyymmddhh For example, P0-2016042417 is an image on patch 0 taken from April 24, 2016 and 17:00h Patch-id refers to which section of the Husdon Bay area the image was taken. There are 581 patches and there locations are shown below. Note that some patches had no available satellite data so are not represented in the dataset. ![Grid](https://github.com/asylve/Sea-Ice/blob/main/Images/Region-Grid.png?raw=true) Masks are encoded in SIGRID-3 format, which I have included below for reference. | Definition | Code Figure | | --- | --- | | Ice Free | 55 | | Less than 1/10 (open water) | 01 | | Bergy Water | 02 | |1/10 | 10| |2/10 | 20| |3/10 | 30| |4/10 | 40| |5/10 | 50| |6/10 | 60| |7/10 | 70| |8/10 | 80| |9/10 | 90| |10/10 | 92| |Concentration intervals (lowest concentration in interval followed by highest concentration in interval)| | |9/10 –10/10 or 9+/10 | 91 | |8/10 – 9/10 | 89| |8/10 – 10/10 | 81| |7/10 – 9/10 | 79| |7/10 – 8 /10 | 78| |6/10 – 8/10 | 68| |6/10 – 7/10 | 67| |5/10 – 7/10 | 57| |5/10 – 6/10 | 56| |4/10 – 6/10 | 46| |4/10 – 5/10 | 45| |3/10 – 5/10 | 35| |3/10 – 4/10 | 34| |2/10 – 4/10 | 24| |2/10 – 3/10 | 23| |1/10 – 3/10 | 13| |1/10 – 2/10 | 12| |Undetermined / Unknown | 99|
Arctic_ice_images_data                                                                                  
Arrays of Artificial Images                                                                                  $$ color{#9911ff}{ mathcal{CONTEXT}}$$ This data collection was created for exercises in Machine Learning. Images are generated completely artificially using the math parametric functions with three random coefficients. One of them (an integer number) became the 'label' for classification, the other two (real numbers) - the 'targets' for regression analysis. Different random colors are planned to be a 'noise' for predictions. Of course, the data is free for noncommercial and nongovernmental goals. $$ color{#9911ff}{ mathcal{CONTENT}}$$ The process of data building - [Synthetic Data 3](https://www.kaggle.com/olgabelitskaya/synthetic-data-3). All images, labels, and targets are numeric arrays with the same data types and shapes. They are collected here in .h5 files. In every file: - images (`float32` =&gt; 288x288 pixels, 3 color channels); - labels (`int32` =&gt; 7 classes); - targets (`float32` =&gt; 2 coefficients). $$ color{#9911ff}{ mathcal{ACKNOWLEDGMENTS}}$$ Thanks for your attention. $$ color{#9911ff}{ mathcal{INSPIRATION}}$$ Discovering the capabilities of algorithms in the recognition of absolutely synthetic data.
Art (Commonsense Contexts + Explanations)                                                                                  _____ # Art (Commonsense Contexts + Explanations) ### 20k Commonsense Narrative Contexts And 200k Explanations By Huggingface Hub [[source]](https://huggingface.co/datasets/art) _____ ### About this dataset &gt; AI2's Logical Art dataset is an incredible resource for artificial intelligence researchers who are looking to expand their understanding of natural language and common sense reasoning. The dataset includes over 20,000 common-sense narrative contexts and more than 200,000 possible explanations for each. All of these data points are labeled according to the context's logical relationships, allowing AI researchers to build better models for understanding human speech and developing semantic algorithms. With this powerful dataset, AI developers can create systems that can reason with natural language more effectively than ever before! ### More Datasets &gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets). ### Featured Notebooks &gt; - 🚨 **Your notebook can be here!** 🚨! ### How to use the dataset &gt; # How to Use AI2’s Logical Art Dataset &gt; AI2’s Logical Art dataset is a useful resource for machine learning researchers looking to develop artificial intelligence technologies that can reason with common sense and natural language. This guide will provide an overview of the dataset, as well as instructions on how to use it in your research. &gt; &gt; ### What is the Logical Art Dataset? &gt; The Logical Art dataset contains over 20,000 common-sense narrative context scenarios and over 200,000 possible explanations for each context scenario. Each context scenario and explanation written about it is labeled according to its logical relationships, which makes the dataset suited for training AI models in understanding common sense. &gt; &gt; ### Structure of the Dataset: &gt; The Logical Arts Dataset consists of two CSV files – train.csv and validation.csv– that contain the labeled context scenarios and explanations used by AI models when learning common sense reasoning tasks. The structure of both files are similar; they consist of two sets of observations (observation_1 & observation_2) and two sets of hypotheses (hypothesis_1 & hypothesis_2), along with their associated labels (label). &gt; &gt; ### How To Use The Logical Arts Dataset: &gt; To use this dataset in your research project or ML model development, you should first download both train and validation datafiles, then load them into your choice software application or programming language that supports processing CSV files – such as R/Python/JavaScript etc Depending on what kind of purpose you are developing a model for – such as classification/regression tasks -&nbsp;you should manually select data from these datasets by filtering desired labels from them so that you have only relevant samples in hand when going into further stages like feature extraction or prepping data for ML algorithms . Once you have prepared a clean version having just related records can be used either directly feed into AI models or start standardization methodologies like normalization etc before handing it off directly to ML algorithms as input signals ! ### Research Ideas &gt; - Developing AI models to better understand natural language context and reasoning with common sense. &gt; - Creating applications that can explain complex reasoning tasks in human-readable language. &gt; - Training AI systems to generate creative narratives and logical explanations for them using labeled data from the dataset ### Acknowledgements &gt; If you use this dataset in your research, please credit the original authors. &gt; [Data Source](https://huggingface.co/datasets/art) ### License &gt; &gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)** &gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/). ### Columns **File: validation.csv** | Column name | Description | |:------------------|:------------------------------------------------------------------------------------------------| | **observation_1** | This column contains the first observation related to the context scenario. (String) | | **observation_2** | This column contains the second observation related to the context scenario. (String) | | **hypothesis_1** | This column contains the first hypothesis related to the context scenario. (String) | | **hypothesis_2** | This column contains the second hypothesis related to the context scenario. (String) | | **label** | This column contains the logical relationship between the observations and hypotheses. (String) | _____ **File: train.csv** | Column name | Description | |:------------------|:------------------------------------------------------------------------------------------------| | **observation_1** | This column contains the first observation related to the context scenario. (String) | | **observation_2** | This column contains the second observation related to the context scenario. (String) | | **hypothesis_1** | This column contains the first hypothesis related to the context scenario. (String) | | **hypothesis_2** | This column contains the second hypothesis related to the context scenario. (String) | | **label** | This column contains the logical relationship between the observations and hypotheses. (String) | ### Acknowledgements &gt; If you use this dataset in your research, please credit the original authors. &gt; If you use this dataset in your research, please credit [Huggingface Hub](https://huggingface.co/datasets/art).
Art Generation images                                                                                  
Art Images                                                                                  
Art Images Clear and Distorted                                                                                  *Dataset Name: ArtImage512x512 Distortion Dataset* *Description:* The ArtImage512x512 Distortion Dataset is a comprehensive collection of high-resolution art images, each measuring 512x512 pixels in dimension. This dataset is a valuable resource for researchers, artists, and machine learning practitioners interested in image processing, computer vision, and the study of visual perception. It encompasses a total of 85,1000 images, consisting of 17,020 clear images and 50 distorted versions for each of these clear images. *Dataset Contents:* 1. Clear Images (17,020 images): This subset of the dataset comprises pristine, high-quality art images. Each image is precisely 512x512 pixels in size and exhibits the original artwork without any alterations, making it ideal for various computer vision and image analysis tasks. Distorted Images (85,1000 images): To enrich the dataset and facilitate the exploration of image distortion, each clear image has been subjected to 50 different types and degrees of distortion. These distortions may include but are not limited to, transformations like blurring, noise addition, compression artefacts, brightness and contrast adjustments, rotations, and scale variations. Distorted images allow researchers to test the robustness of their algorithms and models against real-world challenges in image processing and recognition. *Potential Use Cases:* The ArtImage512x512 Distortion Dataset can be applied to a wide range of research and application areas, including: 1. Image Processing Algorithms: Researchers can use this dataset to develop and evaluate image enhancement, denoising, and restoration algorithms by comparing their results against the original clear images. Computer Vision: The dataset provides a valuable resource for training and testing computer vision models, particularly those designed to recognize and classify art and cultural objects. Machine Learning: Machine learning practitioners can use this dataset for training and fine-tuning deep learning models for tasks like image classification, object detection, and image generation under various distortion scenarios. Visual Perception Studies: Psychologists and cognitive scientists can use this dataset to conduct experiments and studies on human visual perception and how it interacts with image distortion. Art Restoration and Preservation: Art conservators and historians can benefit from this dataset by studying the effects of different distortions on digital representations of art, aiding in developing restoration techniques. *Dataset Characteristics:* Resolution: All images in the dataset are precisely 512x512 pixels. Image Diversity: The dataset includes various art styles, subjects, and content, making it suitable for diverse research applications. Distortion Types: The 50 types of distortions applied to clear images offer a comprehensive range of challenges, allowing for robustness testing and model evaluation. Citation: If you use the ArtImage512x512 Distortion Dataset in your research or projects, please provide appropriate attribution and citation to acknowledge the dataset's source and its creators. *Note:*Please note that this dataset description provides a general overview, and specific details about the types and degrees of distortions applied to the clear images and any licensing or usage restrictions should be provided in the dataset documentation.
Art Images: Drawing/Painting/Sculptures/Engravings                                                                                  ### Context Dataset for classifying different styles of art. Main categories have been taken [here][1] ### Content 5 kinds of data downloaded from google images, yandex images and [this site][1]: 1. Drawings and watercolours 2. Works of painting 3. Sculpture 4. Graphic Art 5. Iconography (old Russian art) Data is separated on training and validation sets [1]: http://rusmuseumvrm.ru/collections/index.php?lang=en
Art Images: Drawing/Painting/Sculptures/Engravings                                                                                  
Art for generative modelling (images)                                                                                  ### The data This dataset contains 11k+ unique,512x512 px RGB images scraped from art sub-reddits like r/Art, r/ArtPorn, r/MosaicPorn, r/museum. Suitable for training generative models such as generative adversarial networks or auto-encoders.<br> - For progressive growing GANs, multiple log2 resolutions(4,8,16,32,64,128,256,512) were added in the recent update. - Scraping script: [scrape](https://github.com/Srujan35007/Art-GAN/blob/master/p1_get_image_urls.py)
ArtImage                                                                                  **ArtImage** is a synthetic dataset of articulated object models of 5 categories from PartNet-Mobility for articulated object tasks in category level.
ArtImages                                                                                  
Artbench Customized Images Extraction                                                                                  Description: The 'Artbench Customized Images Extraction' dataset is a curated collection of diverse artistic images extracted from the original Artbench dataset. This customized dataset aims to provide researchers, artists, and enthusiasts with an enriched and tailored resource for in-depth analysis and exploration of various artistic expressions. Key Features: Wide Range of Artistic Styles: The dataset encompasses a broad spectrum of artistic styles, ranging from classical to contemporary, allowing users to explore and study the evolution of art across different periods. Customized Selection Criteria: The images in this dataset have been carefully selected based on specific criteria, such as subject matter, technique, or cultural significance, resulting in a collection that highlights diverse aspects of artistic creativity. Exploration and Analysis: With this dataset, researchers and art enthusiasts can conduct extensive analysis, such as style recognition, genre classification, or deep learning-based artistic generation, fostering new insights and advancements in the field of digital art analysis. Versatile Applications: The dataset is designed to cater to a wide range of applications, including art history research, educational purposes, creative inspiration, and the development of AI-driven tools for art-related tasks
ArtiFact: Real and Fake Image Dataset                                                                                  ## ArtiFact: A Large-Scale Dataset with Artificial and Factual Images for Generalizable and Robust Synthetic Image Detection <img src='https://github.com/awsaf49/artifact/raw/main/images/header.png'> ## Source * GitHub: https://github.com/awsaf49/artifact/ * Paper: https://arxiv.org/abs/2302.11970 ## Description The ArtiFact dataset is a large-scale image dataset that aims to include a diverse collection of real and synthetic images from multiple categories, including Human/Human Faces, Animal/Animal Faces, Places, Vehicles, Art, and many other real-life objects. The dataset comprises 8 sources that were carefully chosen to ensure diversity and includes images synthesized from 25 distinct methods, including 13 GANs, 7 Diffusion, and 5 other miscellaneous generators. The dataset contains 2,496,738 images, comprising 964,989 real images and 1,531,749 fake images. To ensure diversity across different sources, the real images of the dataset are randomly sampled from source datasets containing numerous categories, whereas synthetic images are generated within the same categories as the real images. Captions and image masks from the COCO dataset are utilized to generate images for text2image and inpainting generators, while normally distributed noise with different random seeds is used for noise2image generators. The dataset is further processed to reflect real-world scenarios by applying random cropping, downscaling, and JPEG compression, in accordance with the [IEEE VIP Cup 2022 standards](https://grip-unina.github.io/vipcup2022/). The ArtiFact dataset is intended to serve as a benchmark for evaluating the performance of synthetic image detectors under real-world conditions. It includes a broad spectrum of diversity in terms of generators used and syntheticity, providing a challenging dataset for image detection tasks. ## Statistics: * Total number of images: 2,496,738 * Number of real images: 964,989 * Number of fake images: 1,531,749 * Number of generators used for fake images: 25 (including 13 GANs, 7 Diffusion, and 5 miscellaneous generators) * Number of sources used for real images: 8 * Categories included in the dataset: Human/Human Faces, Animal/Animal Faces, Places, Vehicles, Art, and other real-life objects * Image Resolution: 200 x 200 ## Citation ```bibtex @misc{rahman2023artifact, title={ArtiFact: A Large-Scale Dataset with Artificial and Factual Images for Generalizable and Robust Synthetic Image Detection}, author={Md Awsafur Rahman and Bishmoy Paul and Najibul Haque Sarker and Zaber Ibn Abdul Hakim and Shaikh Anowarul Fattah}, year={2023}, eprint={2302.11970}, archivePrefix={arXiv}, primaryClass={cs.CV} } ``` ## License ArtiFact dataset takes leverage of data from multiple methods thus different parts of the dataset come with different licenses. All the methods and their associated licenses are mentioned in the table, | Method | License | |:------------------------|:---------------------------------------------------------------------------------------| | ImageNet | Non Commercial | | COCO | Creative Commons Attribution 4.0 License | | LSUN | Unknown | | AFHQ | Creative Commons Attribution-NonCommercial 4.0 International Public | | FFHQ | Creative Commons BY-NC-SA 4.0 license | | Metfaces | Creative Commons BY-NC 2.0 | | CelebAHQ | Creative Commons Attribution-NonCommercial 4.0 International Public | | Landscape | MIT license | | Glide | MIT license | | StyleGAN2 | Nvidia Source Code License | | StyleGAN3 | Nvidia Source Code License | | Generative Inpainting | Creative Commons Public Licenses | | Taming Transformer | MIT License | | MAT | Creative Commons Public Licenses | | LaMa | Apache-2.0 License | | Stable Diffusion | Apache-2.0 License | | VQ Diffusion | MIT License | | Palette | MIT License | | StyleGAN1 | Creative Commons Public Licenses | | Latent Diffusion | MIT License | | CIPS | MIT License | | StarGAN | MIT License | | BigGAN | MIT License | | GANformer | MIT License | | ProjectedGAN | MIT License | | SFHQ | MIT License | | FaceSynthetics | Research Use of Data Agreement v1.0 | | Denoising Diffusion GAN | NVIDIA License | | DDPM | Unknown | | DiffusionGAN | MIT License | | GauGAN | Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License | | ProGAN | Attribution-NonCommercial 4.0 International | | CycleGAN | BSD
ArticleImages                                                                                  
Artificial water simulation images                                                                                  Simulated liquid on Blender (3D software), and rendered several images from it
Aruzz22.5K: An Image Dataset of Rice Varieties                                                                                  Please CITE in your ARTICLE or RESEARCH: **Dataset Citation:** *Islam, Md Masudul; Himel, Galib Muhammad Shahriar; Moazzam, Md. Golam; Uddin, Mohammad Shorif (2023), “An Image Dataset of Rice Varieties”, Mendeley Data, V3, doi: 10.17632/3mn9843tz2.3* This extensive dataset presents a meticulously curated collection of low-resolution images showcasing 20 well-established rice varieties native to diverse regions of Bangladesh. The rice samples were carefully gathered from both rural areas and local marketplaces, ensuring a comprehensive and varied representation. Serving as a visual compendium, the dataset provides a thorough exploration of the distinct characteristics of these rice varieties, facilitating precise classification. **#Dataset Composition#** The dataset encompasses 20 distinct classes, encompassing '1_Subol_Lota','2_Bashmoti','3_Ganjiya','4_Shampakatari','5_Katarivog','6_BR28','7_BR29','8_Paijam','9_Bashful','10_Lal_Aush','11_Jirashail','12_Gutisharna','13_Red_Cargo','14_Najirshail','15_Katari_Polao','16_Lal_Bi'roi,'17_Chinigura_Polao','18_Amon','19_Shorna5','20_Lal_Binni'. In total, the dataset comprises 4,500 original JPG images and 22,500 augmented images. **#Image Capture and Dataset Organization#** These images were captured using an iPhone 11 camera with a 5x zoom feature. Each image capturing these rice varieties was diligently taken between October 18 and November 29, 2023. To facilitate efficient data management and organization, the dataset is structured into two variants: Original images and Augmented images. Each variant is systematically categorized into 20 distinct sub-directories, each corresponding to a specific rice variety. **#Original and Augmented Image Dataset#** The primary and the augmented image set comprises 4,500 JPG images, uniformly sized at 512 × 512 pixels. To address the substantial image volume requirements of deep learning models for machine vision, data augmentation techniques were implemented. Transformations such as rotation (90° left, 90° right, 180° left) and flip were applied, generating an additional set of augmented images in every class, totaling 22,500 augmented images. ** #Dataset Storage and Access#** The datasets are stored in two distinct folders namely '1_TRAIN' and '2_VALID'. Both zip files contain 20 sub-folders representing a unique rice variety, namely 1_Subol_Lota, 2_Bashmoti, 3_Ganjiya, 4_Shampakatari, 5_Katarivog, 6_BR28, 7_BR29, 8_Paijam, 9_Bashful, 10_Lal_Aush, 11_Jirashail, 12_Gutisharna, 13_Red_Cargo,14_Najirshail, 15_Katari_Polao, 16_Lal_Biroi, 17_Chinigura_Polao, 18_Amon, 19_Shorna5, 20_Lal_Binni.
Asia Furniture image and video dataset                                                                                  
Asoul Image                                                                                  ### Context 嘉然！我真的好——喜欢你啊！Mua！为了你！我要做Asoul的数据集！ Diana! I'm sooooo attached to you! Hmmm~~ For my love, I am going to make a dataset about you! ### Content Five members and the symbol of staff's image. 1. Diana: 嘉然 2. Ava: 向晚 3. Carol: 珈乐 4. Kira: 贝拉 5. Eileen: 乃琳 -Acao: 阿草 ### Acknowledgements Thanks to [asoulcnki](https://asoulcnki.asia/rank) and all the Asoul's fans around world!. ### Inspiration Make some classification like CNN about Asoul image.
Asoul Image Classification : MobileNet V2                                                                                  
Asoul Image Classification : ResNet-50 V2                                                                                  
Assamese Text-to-Speech Dataset                                                                                  The **Assamese Text-to-Speech (TTS)** dataset is a valuable resource for researchers and developers interested in the field of speech synthesis for the Assamese language. Assamese is an Indo-Aryan language spoken primarily in the northeastern state of Assam in India. With a rich cultural heritage and a significant number of speakers, Assamese plays a vital role in regional communication and literature. <br> This dataset is specifically curated to support the development and training of text-to-speech systems for the Assamese language. It comprises a total of 1877 text samples in Assamese along with their corresponding audio recordings. The audio files are short and on average are about 3-4 seconds long. # Applications 1. **Accessibility**: The Assamese TTS dataset opens up opportunities for the development of assistive technologies, enabling visually impaired individuals to access written content in Assamese through synthesized speech. 2. **Language Learning**: The dataset can be utilized to create interactive language learning applications or tools, aiding learners in improving their pronunciation and fluency in Assamese. 3. **Content Generation**: TTS systems trained on the dataset can be employed in content creation, such as audiobook production, podcasting, or voice-over services, to generate high-quality spoken content in Assamese. &gt;As the dataset is small, it is recommended to utilize pretrained models as a starting point and fine-tune them using the provided data to achieve better performance and accuracy in Assamese TTS applications.
Assignment text files                                                                                  
Astronomy Image Classification Dataset                                                                                  Introducing the Astronomy Image Classification Dataset, hosted on Kaggle. It contains six folders, each with around 170 carefully selected images of celestial objects. Categories include constellations, stars, nebulae, the cosmos, planets, and galaxies. This dataset is perfect for exploring and classifying mesmerizing astronomical images. Join the journey through the cosmos and unlock its wonders!
Atcapmihc-peed 1FPS 1024x576 Image Dataset                                                                                  
Atcapmihc-peed 1FPS 1280x720 Image Dataset                                                                                  
Atcapmihc-peed 1FPS 384x216 Image Dataset                                                                                  
Atcapmihc-peed 1FPS 640x360 Ext Image Dataset                                                                                  
Atcapmihc-peed 1FPS 640x360 Image Dataset                                                                                  
Atcapmihc-peed 1FPS 800x450 Image Dataset                                                                                  
Atcapmihc-peed 1FPS Image Dataset                                                                                  
Athlete_images                                                                                  
Audi cars part1 over 5k labeled car images                                                                                  Occulta Insights is contributing this labeled image datasets to the kaggle community. Data is acquired through an expensive process of web crawling and scraping. We have done our best to accurately categorize the data. However due to the large volume out current accuracy is around 95%. Please use this dataset only for learning and research purposes. The dataset is provided as a set of image links categorized into folders. The images in the dataset need to be downloaded using the included python script. Please read the Disclaimer before downloading the dataset or running the python script. Dataset cover photo by Thomas Hetzler on Unsplash How can we improve this dataset? For any questions regarding this dataset please checkout https://occultainsights.io/
Audi cars part2 over 5k labeled car images                                                                                  Occulta Insights is contributing this labeled image datasets to the kaggle community. Data is acquired through an expensive process of web crawling and scraping. We have done our best to accurately categorize the data. However due to the large volume out current accuracy is around 95%. Please use this dataset only for learning and research purposes. The dataset is provided as a set of image links categorized into folders. The images in the dataset need to be downloaded using the included python script. Please read the Disclaimer before downloading the dataset or running the python script. Dataset cover photo by Thomas Hetzler on Unsplash How can we improve this dataset? For any questions regarding this dataset please checkout https://occultainsights.io/
Audio ABAW5                                                                                  
Audio Anomoly                                                                                  Audio Anomaly Dataset
Audio Benchmark                                                                                  
Audio Binary Classification: Barbie VS Puppy                                                                                  A set of audio files in .wav format for binary audio classification task. Includes two classes. The first class is the word 'barbie' (50 files). The second class is the word 'puppy' (47 files). The dataset collected from different recorders and different people. The length of the audio is not fixed.
Audio Book Data                                                                                  
Audio Cats and Dogs                                                                                  ### Context With this dataset we hope to do a nice cheeky wink to the 'cats and dogs' image dataset. In fact, this dataset is aimed to be the audio counterpart of the famous 'cats and dogs' image classification task, here available on Kaggle. ### Content The dataset consists in many 'wav' files for both the cat and dog classes : - cat has 164 WAV files to which corresponds 1323 sec of audio - dog has 113 WAV files to which corresponds 598 sec of audio You can have an visual description of the Wav here : [Visualizing woofs & meows 🐱][1]. In [Accessing the Dataset 2][2] we propose a train / test split which can be used. All the WAV files contains 16KHz audio and have variable length. ### Acknowledgements We have not much credit in proposing the dataset here. Much of the work have been done by the [AE-Dataset][3] creator (From which we extracted the two classes) and by the humans behind [FreeSound][4] From which was extracted the AE-Dataset. PS: the AE-Dataset has a policy saying you can mention them: Naoya Takahashi, Michael Gygli, Beat Pfister and Luc Van Gool, 'Deep Convolutional Neural Networks and Data Augmentation for Acoustic Event Recognition', Proc. Interspeech 2016, San Fransisco. ### Inspiration You might use this dataset to test raw audio classification challenge ;) A more challenging dataset is available [here][5] [1]: https://www.kaggle.com/rtatman/visualizing-woofs-meows [2]: https://www.kaggle.com/mmoreaux/accessing-the-data-2/ [3]: https://data.vision.ee.ethz.ch/cvl/ae_dataset/ [4]: https://freesound.org/ [5]: https://www.kaggle.com/mmoreaux/environmental-sound-classification-50
Audio Cats and Dogs : VGGish                                                                                  
Audio Cats and Dogs : YamNet                                                                                  
Audio Classification                                                                                  ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F15777390%2F47ba34f2bdd6bd8d4e028a78ee1e646f%2FtMwXMSzSLDHgAUhPvOeGpmVEwzwpvOF5fsrVqk6disE.webp?generation=1702978991015914&alt=media)
Audio Classification - Predict the Emotions                                                                                  Human beings have various emotions that can now be recognized by machines and computers. Emotion recognition is the process of identifying human emotion. Nowadays, more and more intelligent systems are using emotion recognition models to improve their interaction with humans. This is important because: • Human recognition of the emotions of others varies widely in their accuracy • Systems can adapt their responses and behavioral patterns according to the emotions of humans and make interactions more natural Task You are given a dataset that contains audio files that represent various emotions. Your tasks are as follows: 1. Detect the emotions from the dataset provided 2. Build an application using the Machine Learning model that you created in task 1. Dataset description The dataset folder contains the following files: train.csv. 5816 x 2 test.csv. 2492 x 1 sample submission.csv. 5 x 2 TrainAudioFiles 5816 audio files corresponding to train.csv TestAudioFiles: 2492 audio files corresponding to test.csv The dataset contains the following columns: Column name Column description Filename Represent the name of the audio file emotion Represent the emotion in the audio file Acknowledgements All the credit for the dataset goes to Cogito(Intelligence Augmentation for AI) and HackerEarth (the platform on which the Hackathon was hosted)
Audio Classification Data                                                                                  
Audio Classification Dataset                                                                                  ALL_Audio dataset is for ADAS classification. The Urban sound 8k is the road sound data set.
Audio Classification Dataset                                                                                  
Audio Classifier Dataset                                                                                  **Audio Classifier** An audio classifier is a machine learning model that can categorize or classify audio files into different categories based on specific characteristics. In this project, data was collected from YouTube on top 11 famous motivational speakers' podcasts or talks. 1. Python's Selenium library was used to download the main audio as a WAV file. 2. The next step involved using MoviePy to trim the main audio into 3-second audio clips. More over 8000 data are containing in this dataset. The purpose of this was likely to extract features from the audio files and use them to train an audio classifier model. The resulting model could then be used to automatically classify new audio files based on the learned patterns in the training data. Feel free to use this dataset.
Audio Data                                                                                  
Audio Data                                                                                  
Audio Data Dataset Wise                                                                                  
Audio Dataset                                                                                  
Audio Dataset                                                                                  
Audio Dataset                                                                                  
Audio Dataset Containing Number                                                                                  
Audio Dataset Noised                                                                                  
Audio Dataset of Indoor and Outdoor Scenes                                                                                  ### Description The raw audio data was collected from **[DCASE2020 competition](http://dcase.community/challenge2020)**. There were 3 categories ( **Indoor, Outdoor, Transport** ) of audio scenes whose sounds were recorded in 10 second clip from 10 different cities. Of the three categories of audio type provided we streamlined it into two, omitting 'Transport'. Next we split the data into 5 sec clip and implemented simple Audio Augmentation by adding noise data and doubled the samples. Finally we converted them into spectrograms to represent the data in pictoral form. This manupulation allows us to use this dataset as input to many neural networks and solve various Machine Learning tasks involving sound data. There are two folders (Train and Test) each with two more sub folders named Indoor and Outdoor. The dataset contains a total of 34560 spectrograms of Indoor and outdoor acoustic scenes split into Train(80%) and Test(20%). ### Acknowledgements A massive thank you to[ DCASE](http://dcase.community/) for promoting research and growth in this domain and providing the community with rich sound datasets. The dataset was used in the research paper titled [**A Comparative Study on Approaches to Acoustic Scene Classification Using CNNs** ](https://link.springer.com/chapter/10.1007%2F978-3-030-89817-5_6). The relevant code and research materials are given in [github](https://github.com/coreprinciple97/Acoustic-Scene-Classification).
Audio Dataset of Low-Flying Aircraft: AeroSonicDB                                                                                  AeroSonicDB may be used for a number of audio signal processing tasks; from simple binary classification (aircraft detection), to more complex multi-class classification tasks like differentiating the type of engine or aircraft manufacturer. The strong labels provided in this dataset could be used to pose questions about which type of aircraft is the 'noisiest'? What aircraft features contribute the most noise; is it the type of frame, aircraft size (MTOW), number of engines, type of engine, propeller model etc? Additionally, can we train a model to learn these fine-grained labels and apply them to large, broadly labelled datasets (like AudioSet) to improve their specificity? Can we make these models small enough to deploy to microcontrollers? Can we perform inference in real-time? If you're just starting to learn audio signal processing, check out the beginner friendly Aircraft Detection notebook which will get you familiar with the dataset and walk you through the entire process of feature extraction, training and evaluating a deep binary classification model (CNN). Dataset Summary: - 12.4 hours of audio for training and testing - 625 samples of 301 unique aircraft - 14 non-acoustic labels to describe each aircraft (manufacturer, model, engine type, propeller model etc) - 6 additional hours of continuous urban soundscapes (with aircraft annotations) for validation and testing real-time applications The vast majority of aircraft samples (467 of 625 samples) are twin turbofan aeroplanes, of which Boeing 737-800’s feature predominantly (231). Turboprop aircraft feature in this dataset 114 times, with the most common make and model being the Pilatus PC-12 (27). The remainder of the dataset is made up of piston-powered aeroplanes (36), helicopters (5) and quad-engine turbofan aeroplanes (3).
Audio Dataset of Scream and Non Scream                                                                                  
Audio Dataset with 10 Indian Languages                                                                                  
Audio Detection                                                                                  
Audio Dictionary                                                                                  
Audio Emotion Dataset                                                                                  
Audio Emotions                                                                                  
Audio Features for Playlist Creation                                                                                  # Context This data was compiled as part of our undergrad project that used machine learning to classify songs based on themes or activities songs are associated with. For the project we, four activities were choose. 1. Dinner: Songs that sound good when played in a dinner setting or at a restaurant. 2. Sleep: Songs that promote sleep when they are played. 3. Party: Songs that sound good when played at a party. 4. Workout: Songs that sound good when one is exercising/ working out. The collection of data started with collecting playlist details form Spotify. Spotify web API was used for the collection of the playlist of each category. Track title, album name and artist names were used to extract low level and high level Audio features like MFCC, Spectral centroid, Spectral Roll-off, Spectral Bandwidth, Tempo, Spectral Contrast and Root Mean Square Energy of the songs. For ease of computation, the mean of the values were calculated and added to the tables. Data was also curated using Spotify's audio analysis API. A larger set of songs is part of this data set. # Content The data set has eight tables. 1. Four tables with names *playlist*_audio_features have the signal processing features like MFCC, spectral centroid etc. 2. Four more tables with names *playlist*_spotify_features have the data extracted from Spotify's audio feature API. These tables have larger number of features. The data set size is quite large. ##Description of the *'playlist'*_audio_features columns: 1. The first column has the simple integer id if the track. (This id is local to that file). 2. The second column has the name of the track. 3. The third column name *mfcc* has the mean of the calculated MFCC for that track. 20 MFC coefficients were extracted from one frame of the track. 4. The forth column is named *scem*: This is the mean of Spectral centroid. Spectral centroid was calculated for each frame. 5. The fifth column is named *scom*: This is the mean of Spectral contrast. Spectral contrast was calculated for each frame. 6. The sixth column is named *srom*: This is the mean of Spectral Roll-off. Spectral roll-off was calculated for each frame. 7. The seventh column is named *sbwm*: This is the mean of Spectral Bandwidth. Spectral Bandwidth was calculated for each frame. 8. The eight column is name *tempo*: This is the estimated tempo of the track. 9. The ninth column is name *rmse*: This is the mean of the RSME was calculated for each frame. ##Description of the <playlist>_spotify_features columns: 1. id: This is the Spotify id of the track. 2. name: This is the name of the track. 3. url: This is a Spotify uri of the track. 4. artist: This is a one or more artists who worked on the track. 5-13: Description of each of the column can be found at https://developer.spotify.com/web-api/get-audio-features/ # Acknowledgements We would like to thank Librosa an opensource audio feature extraction library in python for developing a great tool. We would also thank the large research done on music genre classification using audio feature which helped us in developing this data set as well as the classification. A special thanks to Spotify
Audio Files                                                                                  
Audio Files for MIRACL-VC-1                                                                                  This dataset contains spectrograms for MIRACL-VC-1 data. This data contains 10 phrases and 10 words.
Audio Forensics                                                                                  
Audio Genre Classification                                                                                  
Audio MNIST                                                                                  ### Context A Large dataset of Audio MNIST, 30000 audio samples of spoken digits (0-9) of 60 different speakers. ### Content **data (audioMNIST)** - The dataset consists of 30000 audio samples of spoken digits (0-9) of 60 folders and 500 files each. - There is one directory per speaker holding the audio recordings. - Additionally 'audioMNIST_meta.txt' provides meta information such as gender or age of each speaker. ### Acknowledgements [Here's](https://github.com/soerenab/AudioMNIST) the original git repo for the project. ### Inspiration A high quality Audio MNIST was missing.
Audio MNIST                                                                                  Audio Clips of recordings, where speakers say digits out loud. A very good beginner's dataset to audio classification (audio version of MNIST) Source: https://github.com/Jakobovski/free-spoken-digit-dataset/blob/master Credit goes to Jakobovski
Audio MNIST                                                                                  
Audio Noise Dataset                                                                                  # Audio Noise Dataset Noise is an unwanted behavior in audio datasets. Noise plays an important part in the machine learning field of audio data type. The dataset can be used for noise filtering, noise generation & noise recognition in audio classification, audio recognition, audio generation, and audio-related machine learning. I, Min Si Thu, used this dataset on open-source projects. I collected ten types of noise in this dataset. ## Ten types of noise - noise of crowded place - the noise of urban area with people talking - the noise of restaurant - the noise of a working place, people discussion - the noise of mosquitos - the noise of car traffic - the noise of painful sound - the noise of raining day - the noise of motorbike and people talking - the noise of a festival
Audio Samples of 100 Numbers in Marathi                                                                                  These are audio recordings of numbers 1 through 100 in marathi, an Indian language. There are roughly 50 samples of each of the numbers. The recordings are .wav files. Silence is removed from these samples as it was discovered to be crucial in building a classification like model. Please remove silence from your recording / sample if you want to use the built model for inferencing.
Audio Signal Processing - Artificial Intelligence                                                                                  Audio Signal Processing Artificial Intelligence Project I developed the Audio Signal Processing Project for an interview. The company gave this assignment as an assigment. I coded this project in about 38-40 minutes and created the dataset. I created the algorithm in about 7 minutes. The algorithm is very performant and very strong against overfitting. Therefore, the software correctly predicted all of the test data without overfitting. The algorithm that I developed has a total of 8114 neurons (total of input and output layers) and an average of 2 million parameters. In my Audio Signal Processing Project, I gave importance to data cleaning by removing noise while processing data. In this way, the data focused only on the specified sound. In this software, I distinguish between walking and speaking sounds. The software that I made detected how a person makes a sound while walking. Kind regards, Emirhan Bulut You can download this model as using this link. (Emirhan_Model.zip) : https://github.com/emirhanai/Audio-Signal-Processing-Artificial-Intelligence-Project/releases/tag/model The coding language used: Python 3.9.8 Libraries Used: Tensorflow NumPy Shutil Keras os pathlib IPython Audio Signal Processing Artificial Intelligence Project Developer Information: Name-Surname: Emirhan BULUT Contact (Email) : emirhan@isap.solutions LinkedIn : https://www.linkedin.com/in/artificialintelligencebulut/ Kaggle: https://www.kaggle.com/emirhanai Official Website: https://www.emirhanbulut.com.tr [Kaggle Link]: https://www.kaggle.com/emirhanai
Audio Speech Bootstrap Data                                                                                  
Audio Speech Processing Bootstrap BIIG                                                                                  
Audio Speech Sentiment                                                                                  The dataset consists of audio files with different emotions that can be categorized into 3 classess:- - Positive - Negative - Neutral The dataset consists of 4 directories and 1 .csv files: - 'TRAIN' directory (contains .wav files for training) - 'TEST' directory (contains .wav files for testing) - 'train_images' directory (melspectrogram features as images for train samples) - 'test_images' directory (melspectrogram features as images for test samples) - TRAIN.csv contains filenames and classes
Audio Summarization                                                                                  The dataset was collected from various sources for the task of audio summarization and includes 500 audio, texts, and summaries. Using this dataset you can implement both step-by-step audio summarization (speech recognition + text summarization) and end-to-end audio summarization. You can also use parts of the dataset: - Texts and Sammaries for the text summarization problem - Audio and texts for speech recognition tasks
Audio Tags for TAU Urban Scenes                                                                                  _____ # Audio Tags for TAU Urban Scenes ### Web-Based Annotations for Airport, Public Square, and Park Scenes By [[source]](https://zenodo.org/record/4774960#.Y9Y6VtJBwUE) _____ ### About this dataset > This Multi-Annotator Tagged Soundscapes (MATS) dataset provides thoughtful audio tags describing a unique collection of airport, public square, and park scenes from TAU Urban Acoustic Scenes 2019. Annotations are provided in both raw and processed formats, with 133 annotators providing their opinions on each audio file. From providing an understanding of current sound levels to assisting in the development of noise-reduction algorithms, this dataset has something for everyone who wants to explore soundscapes from around the world. So whether you're looking for insight into urban sounds or are just interested in what you'll hear when visiting different locations around the world, this is your perfect resource! ### More Datasets > For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets). ### Featured Notebooks > - 🚨 **Your notebook can be here!** 🚨! ### How to use the dataset > This dataset provides metadata for the TAU Urban Scenes 2019 development dataset. It can be used to explore and analyze soundscapes from urban environments, in order to better understand the acoustic environment of an urban setting. > > To use this dataset, start by exploring the audio tags associated with each audio file. This will give you an overview of the type of sounds present in each scene. Then, use the provided annotations files (MATS_labels_mace100_competence06 and MATS_labels_majority_vote) to study how annotations differ between annotators and how different methods handle multi-annotator data. You can also take a closer look at individual audio files by downloading them directly from zenodo or using audio players such as Audacity or SoX to open them up. > > You can then use this information to develop analyses that deep dive into various aspects of soundscapes in cities such as sound sources, noise levels, and temporal trends across different sites within these cities. This dataset provides a platform for researchers who wish to identify features that distinguish one scene from another or identify changes between time periods for specific locations! ### Research Ideas > - Using a majority vote to determine the 'consensus' tags of an audio file, or to measure the agreement between multiple annotators on specific labels. > - Training a machine learning model on the MACE100 processed annotations and using it to accurately detect audio tags for new sets of audio files. > - Combining different annotation methods (MACE100/Competence06) for more robust analysis and comparison of results from multiple annotators ### Acknowledgements > If you use this dataset in your research, please credit the original authors. > [Data Source](https://zenodo.org/record/4774960#.Y9Y6VtJBwUE) > > ### License > > > **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)** > No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/). ### Columns **File: MATS_labels_mace100_competence06.csv** | Column name | Description | |:--------------|:--------------------------------------------------------| | **filename** | The name of the audio file. (String) | | **tags** | The audio tags associated with the audio file. (String) | _____ **File: MATS_labels_majority_vote.csv** | Column name | Description | |:--------------|:--------------------------------------------------------| | **filename** | The name of the audio file. (String) | | **tags** | The audio tags associated with the audio file. (String) | ### Acknowledgements > If you use this dataset in your research, please credit the original authors. > If you use this dataset in your research, please credit [](https://zenodo.org/record/4774960#.Y9Y6VtJBwUE).
Audio Tensor                                                                                  
Audio Test Data                                                                                  
Audio Train                                                                                  
Audio Train Data                                                                                  
Audio and Image Files                                                                                  
Audio and Image dataset for pump anomaly detection                                                                                  
Audio assamese Female                                                                                  
Audio data                                                                                  
Audio dataset Alphabet Only                                                                                  
Audio de mosquitos Aedes Aegypti                                                                                  **Dataset Description:** The dataset comprises audio recordings of the wing beats of Aedes aegypti mosquitoes and others, conducted in a semi-controlled environment. It encompasses approximately 18,706 seconds of recording, with properly labeled samples. **Dataset Characteristics:** - **Data Type:** Audio recordings. - **Total Duration:** Approximately 18,706 seconds. - **Labeling:** Labeled samples. - **Environment:** Semi-controlled. - **Data Type:** Audio recordings. - **Total Duration:** Approximately 18,706 seconds. - **Species:** Aedes aegypti and others.
Audio demo files                                                                                  Audio files that supplement 'Treatise on Hearing: The Temporal Auditory Imaging Theory Inspired by Optics and Communication'.
Audio emotions                                                                                  ### Content Data set contains files from RAVDESS [1], CREMA-D [2], SAVEE [3], TESS [4]. Recordings are in .wav format and are sorted in folders: Angry - 2167 records. (16.7%) Happy - 2167 records. (16.46%) Sad - 2167 records. (16.35%) Neutral - 1795 records. (14.26%) Fearful - 2047 records. (16.46%) Disgusted - 1863 records. (15.03%) Surprised - 592 records. (4.74%) Out of all files data sets make up: CREMA-D - 7,442 (58.15%) TESS - 2,800 (21.88%) RAVDESS 2,076 (16.22%) SAVEE 480 (3.75%) ### Acknowledgements [1]Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): [2] Houwei Cao, D., Cooper, Keutmann, Gur, Nenkova, and Verma. 'CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset.' IEEE Transactions on Affective Computing 5.4 (2014): 377-90. Web. [3] Jackson, Philip & ul haq, Sana. (2011). Surrey Audio-Visual Expressed Emotion (SAVEE) database. [4] Dupuis, K., & Pichora-Fuller, M. K. (2010). Toronto emotional speech set (TESS). Toronto: University of Toronto, Psychology Department. I don't own anything i just put them together.
Audio features and lyrics of Spotify songs                                                                                  ### Context This dataset contains various types of information over more that 18000 Spotify songs including, artist, album, audio features (e.g. loudness), lyrics, the language of lyrics, genres and sub-genres. The original dataset was used in the [third week of the TidyTeusday project ](https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-01-21)and only contained audio features and genres. I added lyrics to the dataset using [genius library in R](https://cran.r-project.org/web/packages/genius/index.html) and included the language of the lyrics using [langdetect ](https://pypi.org/project/langdetect/)library in python. However, almost only half of the original songs are available in this dataset as for many songs the lyrics could not be retreived. ### Content |variable |class |description | |:---|:---|:-----------| |track_id |character | Song unique ID| |track_name |character | Song Name| |track_artist |character | Song Artist| |lyrics |character | lyrics for the song | |track_popularity |double | Song Popularity (0-100) where higher is better | |track_album_id |character | Album unique ID| |track_album_name |character | Song album name | |track_album_release_date |character | Date when album released | |playlist_name |character | Name of playlist | |playlist_id |character | Playlist ID| |playlist_genre |character | Playlist genre | |playlist_subgenre |character | Playlist subgenre| |danceability |double | Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable. | |energy |double | Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. | |key |double | The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1. | |loudness |double | The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.| |mode |double | Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.| |speechiness |double | Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. | |acousticness |double | A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.| |instrumentalness |double | Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. | |liveness |double | Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live. | |valence |double | A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry). | |tempo |double | The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration. | |duration_ms |double | Duration of song in milliseconds | |language |character | Language of the lyrics | ### Acknowledgements A big thanks to the R and TidyTuesday community who provided the original dataset including [Charlie Thompson](https://twitter.com/_RCharlie), [Josiah Parry](https://twitter.com/JosiahParry), Donal Phipps, and Tom Wolff. ### Inspiration Some projects that can be done using this dataset: https://www.kaylinpavlik.com/classifying-songs-genres/
Audio features for ITMO ED Competition                                                                                  
Audio features of songs ranging from 1922 to 2011                                                                                  ### Context The Million Song Dataset (MSD) is a freely-available collection of audio features and metadata for a million contemporary popular music tracks. This is a subset of the MSD and contains audio features of songs with the year of the song. The purpose being to predict the release year of a song from audio features. ### Content The owners recommend that you split the data like this to avoid the 'producer effect' by making sure no song from a given artist ends up in both the train and test set. - train: first 463,715 examples - test: last 51,630 examples Field descriptions: - The first value is the year (target), ranging from 1922 to 2011. - Then there are 90 attributes - TimbreAverage[1-12] - TimbreCovariance[1-78] These features were extracted from the 'timbre' features from The Echo Nest API. The authors took the average and covariance over all 'segments' and each segment was described by a 12-dimensional timbre vector. ### Acknowledgements Original dataset: Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere. The Million Song Dataset. In Proceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR 2011), 2 Subset downloaded from: https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd ### Inspiration Use this dataset to predict the years that each song was released based on it's audio features
Audio file                                                                                  
Audio files generated after training TTS                                                                                  These are 3 audio samples of the phrase 'Hello, World' generated after training a simple transformer text-to-speech (TTS) model for 1000, 34000, and 68000 steps, respectively. The model was trained on the [LJ Speech Dataset](https://www.kaggle.com/datasets/mathurinache/the-lj-speech-dataset). During training, the model generated audio files every 1000 steps as a test. These 3 samples were utilized in this [notebook](https://www.kaggle.com/code/raul23/simple-transformer-text-to-speech) to provide a glimpse into the training progression of the model, showcasing its evolving performance at different stages of training.
Audio npy                                                                                  
Audio pack                                                                                  
Audio pre-processing data                                                                                  
Audio recordings of Atelpus varius                                                                                  ### Context Anurans (frogs and toads) are among the most globally threatened taxonomic groups. Successful conservation of anurans will rely on improved data on the status and changes in local populations, particularly for rare and threatened species. Automated sensors, such as acoustic recorders, have the potential to provide such data by massively increasing the spatial and temporal scale of population sampling efforts. ### Content This dataset contains 70 audio files in .wav format, each 60 seconds long. The audio was recorded using AudioMoth acoustic recorders. ### Acknowledgements Kitzes, Justin et al. (2021), Audio recordings of Atelpus varius calls from Panama, Dryad, Dataset, https://doi.org/10.5061/dryad.ncjsxkstd ### License - CC0 1.0 Universal (CC0 1.0) Public Domain Dedication
Audio sample language classification                                                                                  The dataset is composed of a 1000 samples, each sample is a 5 second audio recording sampled to 16KHz. The samples are named with a prefix indicating their language: - JP for Japaneese - EN for English - AR for Arabic - FR for French
Audio samples HON NST                                                                                  
Audio speech recognition processed data                                                                                  
Audio to numpy 1                                                                                  
Audio to numpy 2                                                                                  
Audio to numpy 3                                                                                  
Audio to numpy 4                                                                                  
Audio-Anomaly-Dataset                                                                                  
Audio-alarm-dataset                                                                                  
Audio-based Violence Detection Dataset                                                                                  The 'Audio-based Violence Detection Dataset' is a curated collection of audio files specifically designed to aid in detecting and analyzing violent events based purely on sound. Originating from many YouTube videos, these files represent a wide range of violent incidents, most of which were captured using low-fidelity devices such as mobile phones. The predominant sounds within these recordings capture the essence of human vocal expressions during heightened aggression. These may encompass shouts, screams, aggressive verbal confrontations, and the discernible sounds of physical confrontations. Labeling the dataset was meticulously performed through a two-stage process involving dual researchers to ensure maximum objectivity.
Audio1                                                                                  
AudioBook App Data                                                                                  
AudioBooks Data                                                                                  
AudioCaps                                                                                  **AudioCaps** is a dataset of sounds with event descriptions that was introduced for the task of audio captioning, with sounds sourced from the [AudioSet](https://paperswithcode.com/dataset/audioset) dataset. Annotators were provided the audio tracks together with category hints (and with additional video hints if needed). Source: [Audio Retrieval with Natural Language Queries](/paper/audio-retrieval-with-natural-language-queries) Image source: [https://audiocaps.github.io/](https://audiocaps.github.io/)
AudioClassify                                                                                  
AudioData                                                                                  
AudioData                                                                                  
AudioDataSet                                                                                  
AudioDataTest                                                                                  
AudioDataset                                                                                  
AudioFile Wheel                                                                                  
AudioFiles                                                                                  
AudioFiles                                                                                  
AudioFiles                                                                                  
AudioFilesN                                                                                  
AudioLM                                                                                  
AudioLSTM                                                                                  
AudioMelmagesDogAndCat                                                                                  
AudioMnist2                                                                                  
AudioMnist3                                                                                  
AudioNoise                                                                                  Audio data containing noise, collected from [AudioSet](https://research.google.com/audioset/) The data is useful if one wants a sound recognition system to segregate noisy audio from some specific audio.
AudioSet                                                                                  Audioset is an audio event dataset, which consists of over 2M human-annotated 10-second video clips. These clips are collected from YouTube, therefore many of which are in poor-quality and contain multiple sound-sources. A hierarchical ontology of 632 event classes is employed to annotate these data, which means that the same sound could be annotated as different labels. For example, the sound of barking is annotated as Animal, Pets, and Dog. All the videos are split into Evaluation/Balanced-Train/Unbalanced-Train set. Source: [Curriculum Audiovisual Learning](https://arxiv.org/abs/2001.09414)
AudioSet ( EnCodec at 3K Bitrates )                                                                                  A compressed version of the full AudioSet dataset was created using EnCodec at a 3K bitrate. The original dataset can be downloaded following instructions in [PANNs](https://github.com/qiuqiangkong/audioset_tagging_cnn) github repository. The compressed version is approximately 9 GB in size, significantly smaller than the 1TB original version. In recent research called [AudioFormer](https://arxiv.org/pdf/2308.07221v5.pdf), compressed discrete tokens were used as input for the network, yielding impressive results. I have shared this data to make it more accessible to the community, as sharing the original dataset is challenging due to license issues. Additionally, downloading the PANNs version from Baidu storage is not straightforward for non-Chinese users.
AudioSet CC                                                                                  The subset of audio samples from the AudioSet ontology which are licensed with Creative Commons. This set contains approximately 10,000 samples of 10s long clips, and is freely modifiable and distributable. Each clip has with it, its full label set and unique ID.
AudioSet Dataset                                                                                  
AudioSet Train Dataset                                                                                  
AudioSet Valid Dataset                                                                                  
AudioSet [Train]                                                                                  ### Context The AudioSet dataset is a large-scale collection of human-labeled 10-second sound clips drawn from YouTube videos. AudioSet is brought to you by the Sound and Video Understanding teams pursing Machine Perception research at Google. The official AudioSet site is located [here](https://research.google.com/audioset/index.html). The main problem is that AudioSet wasn't releases as audio-files rather as just Youttube links which were hard to use. In this dataset you can find extracted raw WAV-files for balanced train, evaluation and manually created test data. ### Content Dataset consists of following folders and files: * **train _wav** - folder with audio files in WAV format * **class _label _indices.csv** - file with class_id mapping * **train.csv** - meta-data including target classes for train audio files * **train _missed.csv** - files which are not available (comparing with original dataset) ### Additional data * [Validation data](https://www.kaggle.com/zfturbo/audioset-valid) * [Test data (including Leaderboard)](https://www.kaggle.com/c/audioset-leaderboard/data) ### Current Problems Around 10% of data already anavialable due to removal of some videos from YouTube.
AudioSet [Valid]                                                                                  ### Context The AudioSet dataset is a large-scale collection of human-labeled 10-second sound clips drawn from YouTube videos. AudioSet is brought to you by the Sound and Video Understanding teams pursing Machine Perception research at Google. The official AudioSet site is located [here](https://research.google.com/audioset/index.html). The main problem is that AudioSet wasn't releases as audio-files rather as just Youttube links which were hard to use. In this dataset you can find extracted raw WAV-files for balanced train, evaluation and manually created test data. ### Content Dataset consists of following folders and files: * **valid _wav** - folder with validation audio files in WAV format * **valid.csv** - meta-data including target classes for validation audio files * **valid _missed.csv** - files which are not available (comparing with original dataset) ### Additional data * [Train data](https://www.kaggle.com/zfturbo/audioset) * [Test data (including Leaderboard)](https://www.kaggle.com/c/audioset-leaderboard/data) ### Current Problems Around 10% of data already anavialable due to removal of some videos from YouTube.
AudioSet-20K                                                                                  
AudioSet-2M-part00                                                                                  
AudioSet-2M-part01                                                                                  
AudioSet-2M-part02                                                                                  
AudioSet-2M-part03                                                                                  
AudioSet-2M-part04                                                                                  
AudioSet-2M-part05                                                                                  
AudioSet-2M-part06                                                                                  
AudioSet-2M-part07                                                                                  
AudioSet-2M-part08                                                                                  
AudioSet-2M-part09                                                                                  
AudioSet-2M-part10                                                                                  
AudioSet-eval                                                                                  
AudioSet-metadata                                                                                  
AudioSet_subset                                                                                  
AudioTest                                                                                  
AudioTrain                                                                                  
Audio_Analyse                                                                                  
Audio_Features_Data_Set                                                                                  
Audio_File_to_Encrypt                                                                                  
Audio_MINIST_Dataset                                                                                  
Audio_Sentiment_Analysis                                                                                  ## Context I'm on a journey to create an emotion classifier from audio and this collection of dataset is one of the 4 key datasets that I was lucky to stumble upon. What's interesting is that this dataset is both female and male and is of very high quality audio.this dataset would serve a very good training dataset for the emotion classifier in terms of generalisation (not overfitting). ## Content #### This dataset is a Mix of other datasets such as : - CREMA-D : set of [7,442] files - RAVDESS Emotional speech audio : [1440] files - Surrey Audio-Visual Expressed Emotion (SAVEE) [480] files - Toronto emotional speech set (TESS) [2800] emotions (Anger, Disgust, Fear, Happy, Neutral and Sad) and four different emotion levels (Low, Medium, High, and Unspecified). #### Overall_data = 11318 Audio for all Labels Mentioned.
Audio_anomaly_fan_data                                                                                  
Audio_clap_feat                                                                                  
Audio_data                                                                                  
Audio_data_augment3                                                                                  
Audio_dataset                                                                                  
Audio_dataset                                                                                  
Audio_fai                                                                                  
Audio_file                                                                                  
Audio_records                                                                                  
Audio_segments CSV                                                                                  
Audio_test                                                                                  
Audio_visual_Deepfake_detaction                                                                                  
Audio_words                                                                                  ### Context I was working with lots of cnn project.And my friends and professor. ### Content Data set contain audio of few words. each words have 5 samples ### Acknowledgements I have got this dataset from github.io ### Inspiration I would like to thank my friend who appreciated me to work on this project. My prof. Dr. Yaduvir Singh My friend/colleague : Santosh Srivastav
Audiobook                                                                                  
Audiobook App Data                                                                                  #### Abstract - The data set contains data sourced from an audiobook application. Each customer in the database has made a purchase atleast once.The data represents two years worth of engagement. - One can **predict if the customer will make a purchase again** from the audiobook company. - The main idea is that the company shouldn't spend its advertising budget targeting individuals who are unlikely to come back. If we can focus our efforts on customers likely to convert again, we can obtain improved sales and profitability figures. #### Account - **Booklength_overall (mins)** - **Booklength_avg (mins)** : The average book length is basically the sum divided by the number of purchases - **Price_overall** - **Price_avg** - **Review** : Boolean value for user engagement in the comments section (1: left a feedback, 0: did not leave a feedback). This can be either positive or negative feedback - **Review 10/10** : This column has already been worked on with average of reviews (8.91) in place of empty fields (where user did not rate the audiobook). If the Review 10/10 &gt; 8.91, the user has above average 'feelings' and if less than 8.91, user has below average 'feelings' - **Minutes listened** - **Completion** : The total minutes listened divided by the total length of books a person has purchased, assuming people don't re-listen to books - **Support requests** : Total number of support requests a person has opened. Support is anything from forgotten password to assistance on using the platform. This is a measure of engagement. It may turn out that the more support a person needed, the more they got fed up with the platform and abandoned it. Or he or she likes it so much that by using it, stumbles upon different issues unlike someone who never opens the app - **Last visited minus first purchase** : The bigger the difference, the better. If a person engages regularly with a platform, this difference will be bigger. Thus, the customer is likely to convert again. If the value of this variable is zero, we are sure the customer has never accessed what he has bought or perhaps he did it on the first day only, so it is unlikely he or she will convert again - **Targets** : Boolean value, one if a person converted and zero if they didn't. An extra six months of data is taken after the two-year period to check if a user converted. The first two years are contained in the dataset. The next six months will show us if a person converted. In other words, if he or she bought another book. And if that happened, we can count them as a conversion and the target will be one. Otherwise, it is zero.
Audiobook_sales_prediction                                                                                  
Audiobooks Data                                                                                  
Audiobuks                                                                                  
Audiodataset                                                                                  
Audiofiles                                                                                  
Audiofiles                                                                                  
Audiogram_images                                                                                  
Audiomentations                                                                                  
Audiomentations                                                                                  A Python library for audio data augmentation. Inspired by albumentations. Useful for deep learning. Runs on CPU. Supports mono audio and multichannel audio. Can be integrated in training pipelines in e.g. Tensorflow/Keras or Pytorch. Has helped people get world-class results in Kaggle competitions. Is used by companies making next-generation audio products. Need a Pytorch-specific alternative with GPU support? Check out torch-audiomentations!
Audios                                                                                  
Audios                                                                                  
Audios                                                                                  
Audios Entornos Sonoros                                                                                  
Audios Videos CTCON                                                                                  
Audios en Español Sobre Verdad y Mentira                                                                                  El siguiente dataset consiste en una colección de audios de formato WAV. El dataset está compuesto por 800 audios compuesto por 20 personas (10 hombres y 10 mujeres), de cada persona se grabó 40 audios de los cuales 2 responden a una pregunta, cada audio tiene una etiqueta si la persona intentó respondió las respuesta con la verdad o con la mentira, todos los audios tienen una duración de 10 segundos. La etiqueta de cada audio esta en su nombre, cada etiqueta tiene 3 caracteristicas del audio separada por guiones bajos indicando el sexo, numero de pregunta y tipo de respuesta de la persona que dio el audio. (m/f): masculino (male), femenino (female) + número de persona. (q): pregunta (question) + número de pregunta. (t/f): true (verdadero), false (mentira) + número de respuesta. La población está compuesta por personas entre 18 - 25 años de edad de habla español, los audios fueron grabados en un estudio de voz con ayuda de micrófonos, las respuestas que dieron los voluntarios varían en sus palabras y duración, para que cada audio llegue a los 10 segundos se cortaron partes de las respuestas y en caso que durarán menos de los 10 segundos para evitar rellenarlo con largos silencio se repite el audio en algunos ejemplos.
Audioset - Derived Features                                                                                  
Audioset-balanced                                                                                  
Audioset_speech_music_noise_4k                                                                                  
Audiotest                                                                                  
Audiototext                                                                                  
Audiovisual dataset for Syrian dialect(AVDSYD)                                                                                  In recent years, significant progress has been made in automatic lip reading. However, these methods require large-scale datasets that do not exist for many low-resource dialect. In this dataset, I have presented a new multipurpose audio-visual dataset for the Syrian Dialect. This dataset consists of IDs of YouTube videos (all the videos are under Creative Commons licenses). In addition to lip reading, the dataset is suitable for automatic speech recognition, audio-visual speech recognition, and speaker recognition. The reason for creating this dataset is twofold: Firstly, due to the popularity of Syrian dialect in the Arabic world, it is widely understandable in almost all neighboring Arab countries like Jordan and Lebanon. Secondly, the absence such as dataset for Syrian Dialect; most research in Arabic speech recognition targets, particularly the Egyptian dialect.
Augmentation of Curated Chest X-Ray Images                                                                                  This dataset is the augmented version of Curated Chest X-Ray Image dataset. I have performed rotation, saturation, increasing brightness and contrast. This dataset has 4 class. Each class have 4000 different images in total. I have split these 4000 images into 80% for train which is 3200 images, rest 90% is for validation which is 720 images and at the last rest 10% for test which is 80 images. This dataset has 4 class which is - 1. COVID-19 2. Normal 3. Pneumonia-Bacterial 4. Pneumonia-Viral There are a total of 16000 images in this dataset. Original dataset you may find in the following link - https://www.kaggle.com/datasets/unaissait/curated-chest-xray-image-dataset-for-covid19
Augmented Disgust Images for Facial Emotions                                                                                  
Augmented data for LLM - Detect AI Generated Text                                                                                  This dataset takes the original data from the following contributions: https://www.kaggle.com/datasets/radek1/llm-generated-essays https://www.kaggle.com/datasets/alejopaullier/argugpt https://www.kaggle.com/datasets/nbroad/daigt-data-llama-70b-and-falcon180b https://www.kaggle.com/datasets/thedrcat/daigt-proper-train-dataset https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset https://www.kaggle.com/datasets/darraghdog/hello-claude-1000-essays-from-anthropic https://www.kaggle.com/datasets/carlmcbrideellis/llm-7-prompt-training-dataset https://www.kaggle.com/datasets/nbroad/persaude-corpus-2 Some of those already compile the others inside them, so I first removed the duplicates comparing by full text. After that the data augmentation took place, with a process composed of 2 steps that were iterated over and over, first I tried correcting typos on the texts by using language_tool_python, then I introduced noise the way the organizators seem to have done it (see https://www.kaggle.com/competitions/llm-detect-ai-generated-text/discussion/452279), then I corrected typos again, and repeat. After repeating these steps a couple of times I removed duplicates again comparing by full text. The result is this dataset, it's split in train and test because I wanted to prevent information leaking between the train and test sections, so I did the steps independently on each of them (I split them before doing the data augmentation). If you don't care about train and test you can just concatenate both into a single dataset for training purposes. If you find this dataset helpful, please upvote.
Augmented images                                                                                  
Augmented images dataset for cancer detection                                                                                  
AugmentedImagesRicky-20240216T134945Z-001                                                                                  
Augmented_Images(CherBhav)                                                                                  
Augmented_Stanford_Car_Images                                                                                  
Augumented_Aerial_Imagery                                                                                  
Austin Zoning Satellite Images                                                                                  ### Context I am replicating the project shown in the City Class project by Roman Kuchokov. The objective is to classify a satellite picture of a small part of a city into one of its zones (such as residential, industrial, etc). I obtained the information about the zoning in Austin to work on the same problem but with a different dataset. ### Content There are 3666 small satellite images of the whole city of Austin and a csv file which relates the name of each of these images to the corresponding zoning tag. ### Acknowledgements This project was inspired by Roman Kuchukov's City: https://towardsdatascience.com/cityclass-project-eng-15bc5fcd8e1 The zoning information was obtained in the official city of Austin data portal: https://data.austintexas.gov/Locations-and-Maps/Zoning/5rzy-nm5e ### Inspiration I want to be able to train a classification model which can take a satellite picture of any city and create a grid where each cell is classified as a type of zone. This may start with zoning classification but can also be tweaked for other urban geo-tagging projects.
Austin_Satellite_Images                                                                                  
Australia Animal Species Image Dataset (47)                                                                                  ## Citation: Q. Zhang, K. Ahmed, N. Sharda and H. Wang, 'Australian Animal Species Selection and Image Data Collection,' 2023 27th International Conference Information Visualisation (IV), Tampere, Finland, 2023, pp. 55-63, doi: 10.1109/IV60283.2023.00020. ## Abstract This dataset contains 47 different Australian animal species. Every category is names as the serial number plus animal’s scientific name which based on taxonomic ranks. All these species are observed in Australia, some of them are Australian endemic species. Every category folder includes three subfolders: annotations, images with backgrounds, and images without backgrounds. Each of these subdirectories comprises 600 files. The ’images with background’ subdirectory contains 600 original images of the animals, complete with their respective backgrounds. The original meta-images are taken from the Global Biodiversity Information Facility (GBIF) and have been manually screened to remove images that do not meet the requirements, such as highly occluded images, animal corpses, scratches, specimens, excrement, etc. The ’images without background’ subdirectory contains images from which the backgrounds have been removed, leaving only the original animal subjects. Lastly, the ’annotation’ subdirectory consists of 600 files in JSON format, each encompassing detailed information about the original image, such as the species’ scientific name, label identification, and metadata concerning the animal’s spatial orientation within the image. It is worth noting that the meta images of some species are less than 600 images due to various factors, such as individual size, population size, habitat and region, etc. Therefor, we applied image augmentation (horizontal flip and vertical flip) for those species. We use Efficient Interactive Segmentation (EISeg) to assist in the manual labelling. Each original image will be saved as four files after manual labelling, including the image with the background removed, a polygon of the label, a JavaScript Object Notation (JSON) format file and a pseudo-colour image.
AuthorsTexts                                                                                  
Autism Facial Image Dataset                                                                                  This dataset consists of images categorized into two classes: 'autistic' and 'non_autistic', aiming to facilitate research and analysis in understanding autism spectrum traits. The dataset is divided into three main folders: 'train', 'valid', and 'test', each containing two subfolders representing the two classes. Folder Structure: train: Contains 1263 images in each of the 'autistic' and 'non_autistic' subfolders, totaling 2526 images. valid: Contains 100 images in each of the 'autistic' and 'non_autistic' subfolders, totaling 200 images. test: Contains 100 images in each of the 'autistic' and 'non_autistic' subfolders, totaling 200 images. Purpose: This dataset serves as a resource for training, validating, and testing Deep Learning models and algorithms in the domain of autism spectrum analysis. Researchers and practitioners can utilize this dataset to develop and evaluate classification and detection systems, as well as to explore visual characteristics associated with autism spectrum traits. Citation: If you use this dataset in your research or work, please cite it.
Autism_Image_Data                                                                                  I changed the datas for myself from this link the https://www.kaggle.com/gpiosenka/autistic-children-data-set-traintestvalidate
Auto Image                                                                                  
Auto Image Colorization Project Checkpoints                                                                                  This contains the model parameters saved for the [Image Colorization using Conditional GAN](https://www.kaggle.com/code/arpitpandey992/image-colorization-gan) project made by [Arpit Kumar Pandey](https://github.com/Arpitpandey992). Please look at the project source code to understand how to use these parameters
Auto tag images of Gala                                                                                  
Auto-tag images of galas                                                                                  
Autorickshaw Image Dataset | Niche Vehicle Dataset                                                                                  This dataset is an extremely challenging set of over 8000+ original Fire and Smoke images captured and crowdsourced from over 1200+ urban and rural areas, where each image is **manually reviewed and verified** by computer vision professionals at Datacluster Labs. ### **Dataset Features** - Dataset size : 8000+ - Captured by : Over 1200+ crowdsource contributors - Resolution : 99% images HD and above (1920x1080 and above) - Location : Captured with 800+ cities accross India - Diversity : Various lighting conditions like day, night, varied distances, view points etc. - Device used : Captured using mobile phones in 2021-2022 - Usage : Vehicle detection, Autorickshaw detection, Self driving, Indian vehicles, Number Plate detection, etc. ### Available Annotation formats COCO, YOLO, PASCAL-VOC, Tf-Record **To download full datasets or to submit a request for your dataset needs, please ping us at [sales@datacluster.ai](sales@datacluster.ai) Visit [www.datacluster.ai](www.datacluster.ai) to know more.** **Note**: All the images are manually captured and verified by a large contributor base on DataCluster platform
Autorickshaw Image Dataset | Niche Vehicle Dataset                                                                                  ### **This dataset is collected by DataCluster Labs, India. To download full dataset or to submit a request for your new data collection needs, please drop a mail to:&nbsp;[sales@datacluster.ai](mailto:sales@datacluster.ai)** This dataset is an extremely challenging set of over 8000+ original Fire and Smoke images captured and crowdsourced from over 1200+ urban and rural areas, where each image is **manually reviewed and verified** by computer vision professionals at Datacluster Labs. ### **Dataset Features** - Dataset size : 8000+ - Captured by : Over 1200+ crowdsource contributors - Resolution : 99% images HD and above (1920x1080 and above) - Location : Captured with 800+ cities accross India - Diversity : Various lighting conditions like day, night, varied distances, view points etc. - Device used : Captured using mobile phones in 2021-2022 - Usage : Vehicle detection, Autorickshaw detection, Self driving, Indian vehicles, Number Plate detection, etc. ### Available Annotation formats COCO, YOLO, PASCAL-VOC, Tf-Record **The images in this dataset are exclusively owned by Data Cluster Labs and were not downloaded from the internet. To access a larger portion of the training dataset for research and commercial purposes, a license can be purchased. Contact us at sales@datacluster.ai Visit www.datacluster.ai to know more.**
Avengers Actors Images Dataset                                                                                  This dataset contain the images of Avenger actors [Robert Downey Jr., Chris Evans, Jeremy Renner, Scarlett Johansson, Chris Hemsworth, Mark Ruffalo]. The data is downloaded from google images. I have cleaned the raw dataset using python script and 'Cleaned Dataset' contains the cleaned data. I used opencv library to clean data, the images which has clear image and two eyes showing are selected and rest are rejected by the script.
Aviation Safety Reports Text Classification                                                                                  _____ # Aviation Safety Reports Text Classification ### Using Reports to Discover Incidents and Problem Types By US Open Data Portal, data.gov [[source]](https://data.world/datagov-us) _____ ### About this dataset &gt; This U.S. Government Works Aviation Safety Reports Dataset for Text Mining is part of the SIAM 2007 Text Mining Competition dataset which has been used to create algorithms to classify documents according to the types of problems described. The documents in this dataset consist of reports on incidents that occurred during certain flights and are collected from human-generated reports as part of the Aviation Safety Reporting System (ASRS). The files for this competition come in raw text format, with each row representing a single document and its associated problem type label. &gt; &gt; This dataset provides invaluable insights into aviation safety incidents and is an excellent resource for researchers interested in developing text mining techniques for categorizing documents by their contents. Analyzing these documents can help identify potential safety issues, both within individual aircrafts’ operations and more broadly online, driving domestic flying safety forward in an era when ever increasing numbers of people are travelling by air ### More Datasets &gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets). ### Featured Notebooks &gt; - 🚨 **Your notebook can be here!** 🚨! ### How to use the dataset &gt; This dataset contains aviation safety reports which have been labelled according to the type of problem that occurred during a certain flight. It is a great resource for developing text mining algorithms for document classification. &gt; ### Research Ideas &gt; - Build an AI-powered Machine Learning classifier to identify problematic aviation incidents more quickly and accurately. &gt; - Predict the risk of a particular flight, taking into consideration the type of incident that has occurred before on a similar flight. &gt; - Construct an interactive searchable interface to allow users to better analyze and visualize aviation safety reports in order to uncover trends and suggest ways for improvement across all levels of relevant stakeholders within the sector, such as regulators, airlines, aircraft operators or pilots ### Acknowledgements &gt; If you use this dataset in your research, please credit the original authors. &gt; [Data Source](https://data.world/datagov-us) &gt; &gt; ### License &gt; &gt; &gt; **Unknown License - Please check the dataset description for more information.** ### Columns **File: testtruth-csv-gz-3.csv** | Column name | Description | |:--------------|:------------------------------------| | **-1** | Document Number (String) | | **-1.1** | Aircraft Autopilot Problem (String) | | **-1.2** | Auxiliary Power Problem (String) | | **-1.3** | Avionics Problem (String) | | **-1.4** | Cabin Pressure Problem (String) | | **-1.5** | Communications Problem (String) | | **-1.6** | Electrical System Problem (String) | | **-1.7** | Engine Problem (String) | | **-1.8** | Fire/Smoke Problem (String) | | **-1.9** | Fuel System Problem (String) | | **-1.10** | Ground Service Problem (String) | | **-1.11** | Hydraulic System Problem (String) | | **-1.12** | Ice/Frost Problem (String) | | **-1.13** | Landing Gear Problem (String) | | **-1.14** | Maintenance Problem (String) | | **-1.15** | Navigation Problem (String) | | **-1.16** | Oxygen System Problem (String) | | **-1.17** | Structural Problem (String) | | **-1.18** | Other Problem (String) | _____ **File: traincategorymatrix-csv-gz-5.csv** | Column name | Description | |:--------------|:------------------------------------| | **-1** | Document Number (String) | | **-1.1** | Aircraft Autopilot Problem (String) | | **-1.2** | Auxiliary Power Problem (String) | | **-1.3** | Avionics Problem (String) | | **-1.4** | Cabin Pressure Problem (String) | | **-1.5** | Communications Problem (String) | | **-1.6** | Electrical System Problem (String) | | **-1.7** | Engine Problem (String) | | **-1.8** | Fire/Smoke Problem (String) | | **-1.9** | Fuel System Problem (String) | | **-1.10** | Ground Service Problem (String) | | **-1.11** | Hydraulic System Problem (String) | | **-1.12** | Ice/Frost Problem (String) | | **-1.13** | Landing Gear Problem (String) | | **-1.14** | Maintenance Problem (String) | | **-1.15** | Navigation Problem (String) | | **-1.16** | Oxygen System Problem (String) | | **-1.17** | Structural Problem (String) | | **-1.18** | Other Problem (String) | ### Acknowledgements &gt; If you use this dataset in your research, please credit the original authors. &gt; If you use this dataset in your research, please credit [US Open Data Portal, data.gov](https://data.world/datagov-us).
Avo_images                                                                                  
Awesome Lungs - Synthetic CT and X Ray Images                                                                                  This dataset contains synthetic lungs X Ray and CT images. All the images in this dataset are generated using Neural Diffusion Model. More details will follow soon. # How to cite Cite this paper: H. Ali, S. Murad, Z. Shah, Spot the fake lungs: Generating Synthetic Medical Images using Neural Diffusion Models, 30th Irish Conference on Artificial Intelligence and Cognitive Science, December 2022, Ireland. **Disclaimer**: Use at your own risk.
Ayurvedic Medicinal Plants & Leaf Segments Images                                                                                  This dataset consist of 170 different India medicinal plant species dataset. This dataset can be used for research purpose and software development purposes. The below provided GitHub link page gives access to Identification of Medicinal Plants using Image processing algorithms for public contribution. GitHub project - https://github.com/PROFESSOR-DJ/Identification-of-medicinal-plants-using-image-processing-algorithms GitHub Profile - https://github.com/PROFESSOR-DJ LinkedIn - https://www.linkedin.com/in/dhiraajkv/
B +G +R image 600 600                                                                                  
BACH: Breast Cancer Histology images                                                                                  A large annotated dataset, composed of both microscopy (classification task) and whole-slide images (segmentation task), was specifically compiled and made publicly available for the BACH challenge. Following a positive response from the scientific community, a total of 64 submissions, out of 677 registrations, effectively entered the competition. From the submitted algorithms it was possible to push forward the state-of-the-art in terms of accuracy (87%) in automatic classification of breast cancer with histopathological images. There are two main folders for classification task: train and test. In Photos folder, there are totally four classes: benign, in situ, invasive, and normal. There is also a ground truth csv file for labels. Images are tif format. Paper: https://arxiv.org/abs/1808.04277 Citation: Aresta, G., Araújo, T., Kwok, S., Chennamsetty, S. S., Safwan, M., Alex, V., ... & Aguiar, P. (2019). Bach: Grand challenge on breast cancer histology images. Medical image analysis, 56, 122-139. Dataset: https://zenodo.org/record/3632035
BACKTEXTINGIT                                                                                  
BAN-Cap: English-Bangla Image Descriptions Dataset                                                                                  As computers have become very efficient at understanding visual information and transforming it into a written representation, research interest in tasks like automatic image captioning has seen a significant leap over the last few years. While most of the research attention is given to the English language in a monolingual setting, resource-constrained languages like Bangla remain out of focus, predominantly due to a lack of standard datasets. Addressing this issue, we present a new dataset *BAN-Cap* following the widely used Flickr8k dataset, where we collect Bangla captions of the images provided by qualified annotators. Our dataset represents a wider variety of image caption styles annotated by trained people from different backgrounds. We present a quantitative and qualitative analysis of the dataset as well as the baseline evaluation of the recent models in Bangla image captioning. We investigated the effect of text augmentation and demonstrated that combining adaptive attention-based model and text augmentation using Contextualized Word Replacement (CWR) outperforms all state-of-the-art models for Bangla image captioning. We also present this dataset's multipurpose nature, especially on machine translation for Bangla-English and English-Bangla. This dataset and all the models will be useful for further research.
BBC Full Text Document Classification                                                                                  
BBC Full Text Document Classification                                                                                  this is the csv and clean version of this dataset [link_to_the_original_Data](https://www.kaggle.com/datasets/shivamkushwaha/bbc-full-text-document-classification). You can use this data to train your NLP skills.
BBC Full Text Preprocessed                                                                                  ### Original Dataset [Original dataset][1] consists of 2225 documents (as text files) from the BBC news website corresponding to stories in five topical areas from 2004-2005. Files are segregated into 5 folders: 1. business 2. entertainment 3. politics 4. sport 5. tech ### This Dataset As part of Data Wrangling, original dataset is pre-processed in three stages: 1. **Stage 1:** Extract Metadata from files that are segregated in 5 folders into a single csv. 2. **Stage 2:** Clean and compress text content (remove extra spaces and newlines) in files into a single csv. 3. **Stage 3:** Process English language (stop-word removal, lemmatization and NER) using [spaCy][2]. &gt; **Note:** Every next stage persists and improves data from previous stage into a new csv file. [1]: https://www.kaggle.com/datasets/shivamkushwaha/bbc-full-text-document-classification [2]: https://spacy.io/api/
BBC Text                                                                                  
BBC WaZoBia Text corpus                                                                                  The dataset was gotten from bbc website, and it is uploaded in it raw form (without any preprocessing) and it should be noted that diacritics maybe incorrect or missing in some sentences the dataset was inspired by the scarcity of African language textual data the thumbnail of dataset was gotten from https://www.instagram.com/maiskine_arts/
BBC articles fulltext and category                                                                                  
BCCD White Blood Cell Images                                                                                  A version of the Kaggle BCCD White Blood Cell (WBC) dataset modified for out-of-domain few-shot classification. We recommend using this dataset as an out-of-domain testing target for few-shot classification. As such, no training/testing splits are published. The original version can be found [here](https://www.kaggle.com/datasets/paultimothymooney/blood-cells). All credit goes to the original authors of this dataset (Shenggan and Paul Mooney). ## Dataset Structure and Composition This dataset is structured as an image folder dataset, meaning each folder represents a class and contains images of the respective class. Total Number of Images: 3500 Total Number of Classes: 5 Images per Class: 700 Image Size: 84x84px ## Changes - All novel cell images have been pooled into a singular image dataset. - Basophil cell images have been extracted from the master dataset and added as a new class. Images in the basophil class have been resampled following the same procedure applied in the original dataset. ## Testing Setup In-domain Training Set: mini-imagenet training set Testing Settings: 5-way 1-shot and 5-way 5-shot ## License MIT https://github.com/Shenggan/BCCD_Dataset
BC_Split_Images                                                                                  
BDB_Images                                                                                  
BDC Image                                                                                  
BDD100K Images                                                                                  BDD100K, A Large-scale Diverse Driving Video Database. in May 2018 Berkeley AI Lab (BAIR) released BDD100K, the largest publicly available driving dataset with the most diverse content, and also designed an image annotation system. the BDD100K dataset contains 100,000 high-definition videos, each video is about 40 seconds 720p 30 fps. The key frames are sampled at the 10th second of each video to obtain 100,000 images (image size: 1280*720), which are annotated with 10 categories of object bounding boxes, drivable areas, lane markers and full-frame instance segmentation. The dataset is geographically, environmentally and weather diverse. The dataset is divided into 70,000/10,000/20,000 for training/validation/testing respectively. For more information see https://doc.bdd100k.com/index.html. The images in this package are the frames at the 10th second in the videos. The split of train, validation, and test sets are the same with the whole video set. They are used for object detection, drivable area, lane marking.
BDD100K_Drivable_area_and_lane_marking_GT_images                                                                                  
BDMI audioPlot                                                                                  
BERT_Model_Classifies_Text_Output                                                                                  
BF_BM_OF_OM_WF_WM_face_images                                                                                  
BHP_Images                                                                                  
BIQ2021: A Dataset for Image Quality Assessment                                                                                  The BIQ2021 dataset is a large-scale blind image quality assessment database, consisting of 12,000 authentically distorted images. Each image in the dataset has been quality rated by 30 observers, resulting in a total of 360,000 quality ratings. This dataset was created in a controlled laboratory environment, ensuring consistent and reliable subjective scoring. Moreover, the dataset provide a train/test split by which the researchers can report their results for benchmarking. The dataset is openly available and serves as a valuable resource for evaluating and benchmarking image quality assessment algorithms. The paper providing a detailed description of the dataset and its creation process is openly accessible at the following link: BIQ2021: A large-scale blind image quality assessment database. The paper can be sited as: Ahmed, N., & Asif, S. (2022). BIQ2021: a large-scale blind image quality assessment database. Journal of Electronic Imaging, 31(5), 053010. Dataset Description: Images: The dataset contain a folder named images containing 12,000 images to be used for training and testing. Train (Images and MOS): It is a CSV file containing randomly partitioned train set of the dataset containing 10,000 images with their corresponding MOS. Test (Images and MOS): It is a CSV file containing randomly partitioned test set of the dataset containing 2,000 images with their corresponding MOS. Benchmarking: In order to compare the performance of a predictive model trained on the dataset, Pearson and Spearman's correlation can be computed and compared with the existing approaches and the CNN models listed at the following gitHub repository: https://github.com/nisarahmedrana/BIQ2021
BIRD (BIg Bench for LaRge-scale Database Grounded Text-to-SQL Evaluation)                                                                                  BIRD (BIg Bench for LaRge-scale Database Grounded Text-to-SQL Evaluation) represents a pioneering, cross-domain dataset that examines the impact of extensive database contents on text-to-SQL parsing. BIRD contains over 12,751 unique question-SQL pairs and 95 big databases with a total size of 33.4 GB. It also covers more than 37 professional domains, such as blockchain, hockey, healthcare and education, etc.
BIRDS 20 SPECIES- IMAGE CLASSIFICATION                                                                                  BIRDS 20 SPECIES- IMAGE CLASSIFICATION Data set of 20 bird species. 3208 training images, 100 test images(5 images per species) and 100 validation images(5 images per species. This is a very high quality dataset where there is only one bird in each image and the bird typically takes up at least 50% of the pixels in the image. As a result even a moderately complex model will achieve training and test accuracies in the mid 90% range. Note: all images are original and not created by augmentation All images are 224 X 224 X 3 color images in jpg format. Data set includes a train set, test set and validation set. Each set contains 475 sub directories, one for each bird species. The data structure is convenient if you use the Keras ImageDataGenerator.flow_from_directory to create the train, test and valid data generators. The data set also include a file birds.csv. This cvs file contains 5 columns. The filepaths column contains the relative file path to an image file. The labels column contains the bird species class name associated with the image file. The scientific label column contains the latin scientific name for the image. The data set column denotes which dataset (train, test or valid) the filepath resides in. The class_id column contains the class index value associated with the image file's class. NOTE: The test and validation images in the data set were hand selected to be the 'best' images so your model will probably get the highest accuracy score using those data sets versus creating your own test and validation sets. However the latter case is more accurate in terms of model performance on unseen images. Images were gather from internet searches by species name. Once the image files for a species was downloaded they were checked for duplicate images using a python duplicate image detector program I developed. All duplicate images detected were deleted in order to prevent their being images common between the training, test and validation sets. After that the images were cropped so that the bird in most cases occupies at least 50% of the pixel in the image. Then the images were resized to 224 X 224 X3 in jpg format. The cropping ensures that when processed by a CNN their is adequate information in the images to create a highly accurate classifier. Even a moderately robust model should achieve training, validation and test accuracies in the high 90% range. Because of the large size of the dataset I recommend if you try to train a model use and image size of 150 X 150 X 3 in order to reduce training time. All files were also numbered sequential starting from one for each species. So test images are named 1.jpg to 5.jpg. Similarly for validation images. Training images are also numbered sequentially with 'zeros' padding. For example 001.jpg, 002.jpg ….010.jpg, 011.jpg …..099.jpg, 100jpg, 102.jpg etc. The zero's padding preserves the file order when used with python file functions and Keras flow from directory. The training set is not balanced, having a varying number of files per species. However each species has at least 130 training image files. One significant shortcoming in the data set is the ratio of male species images to female species images. About 80% of the images are of the male and 20% of the female. Males typical are far more diversely colored while the females of a species are typically bland. Consequently male and female images may look entirely different .Almost all test and validation images are taken from the male of the species. Consequently the classifier may not perform as well on female specie images.
BIRDS 525 SPECIES- IMAGE CLASSIFICATION                                                                                  Data set of 250 bird species. 35215 training images, 1250 test images(5 per species) and 1250 validation images(5 per species. All images are 224 X 224 X 3 color images in jpg format. Also includes a 'consolidated' image set that combines the training, test and validation images into a single data set. This is useful for users that want to create their own training, test and validation sets. Each set contains 250 sub directories, one for each bird species. Images for each species are contained in a separate sub directory. Convenient if you use Keras flow from directory as a means to input the data. Images were gather from internet searches by species name. Once the image files for a species was downloaded they were checked for duplicate images using a python duplicate image detector program I developed. All duplicates detected were deleted in order to prevent their being images common between the training, test and validation sets. After that the images were cropped so that the bird occupies at least 50% of the pixel in the image. Then the images were resized to 224 X 224 X3 in jpg format. The cropping ensures that when processed by a CNN their is adequate information in the images to create a highly accurate classifier. All files were also numbered sequential starting from one for each species. So test images are named 1.jpg to 5.jpg. Similarly for validation images. Training images are also numbered sequentially with 'zeros' padding. For example 001.jpg, 002.jpg ....010.jpg, 011.jpg .....099.jpg, 100jpg, 102.jpg etc. The zero's padding preserves the file order when used with python file functions and Keras flow from directory. The training set is not balanced, having a varying number of files per species. However each species has at least 100 training image files. This imbalanced did not effect my kernel classifier as it achieved over 98% accuracy on the test set. One significant imbalance in the data set is the ratio of male species images to female species images. About 80% of the images are of the male and 20% of the female. Males typical are far more diversely colored while the females of a species are typically bland. Consequently male and female images may look entirely different .Almost all test and validation images are taken from the male of the species. Consequently the classifier may not perform as well on female specie images.
BISh 100: Nepali Text Driven AI Anchor                                                                                  **Overview** In recent years, the advancement of AI technology has led to significant breakthroughs in various domains, including natural language understanding and computer vision. However, many of these advancements have been primarily focused on English language data. This dataset aims to address this gap by providing a comprehensive dataset specifically tailored for Nepali language processing tasks. **Key Features** **Nepali Text Data** The dataset includes a Nepali text samples from Newspaper, covering various topics and genres to ensure robustness and versatility. ग्रामीण क्षेत्रमा पहिलोपटक भटमासबाट दूध उत्पादन सुरु भएको छ जिल्लाको उत्तरी भेगमा पर्ने शिखरपुर कठमडामा यस्तो प्रविधि भित्र्याइएको हो स्थानीय स्तरमा प्रशस्त उत्पादन हुने भटमास सस्तो मूल्यमा निकासी हुन थालेपछि गाउँमै दुग्धजन्य पदार्थको उत्पादन सुरु गरेको केदार दुग्ध विकास समूहका अध्यक्ष रूपसिंह साउदले बताए **Video Data** Alongside the text, the dataset also provides corresponding video files of varying length, enabling researchers to explore multimodal approaches for AI-driven tasks. **Usage** Researchers and developers interested in Nepali language processing, multimodal AI, and related fields can leverage this dataset for a wide range of applications, including: 1. Natural Language Understanding 2. Text-to-Video Generation 3. Gesture Recognition 4. Pose Estimation **Contribution** We believe in the power of open collaboration to drive innovation and progress in AI research. Therefore, we encourage contributions from the community to enhance the quality and diversity of the dataset. Whether it's adding new text samples, annotating additional videos, or improving existing annotations, your contributions are invaluable in making this resource more comprehensive and impactful. **Open Source Philosophy** By open-sourcing this dataset, we aim to foster a collaborative environment where researchers and developers can freely access and build upon each other's work. We believe that sharing knowledge and resources is essential for advancing the field of AI and creating technologies that benefit society as a whole. **Support** If you encounter any issues or have suggestions for improving the dataset, please feel free to give your feedback. Your feedback is valuable in helping us enhance the quality and usability of the dataset.
BLDG-SIM Building Simulation Email List Text                                                                                  ## This dataset and the explanation below are from the 2019 IBPSA Paper: [Twenty years of building simulation trends: Text mining and topic modeling of the Bldg-sim email list archive](http://www.ibpsa.org/proceedings/BS2019/BS2019_211087.pdf) ### Context The recent growth of the building performance re- search community has been in parallel with the [onebuilding.org Bldg-sim email list](http://lists.onebuilding.org/listinfo.cgi/bldg-sim-onebuilding.org). This list was formed in 1999 and steadily grew into a major venue for building simulation community announcements, dis- cussions, and questions and answers (Q&A). This pa- per presents an analysis of the Bldg-sim email archive to determine how traffic has grown over the years and what general trends have come and gone in this par- ticular user community. ### Content We use a text mining ap- proach to find the most prominent topics over the years and create trend metrics from the frequency of the most common words from those topics. The results illustrate the relative rise and fall of various soft- ware tools, organizations, and simulation topics from the portion of the simulation community who used the email list. Visualizations showing the trends are presented and discussed. The paper also discusses the generalizability of results as it should be noted that the users of this email list are primarily English- speaking simulation practitioners from North Amer- ica. All data and code used in this analysis are avail- able in a reproducible GitHub repository. ### Acknowledgements This paper and analysis is only possible with the help of Jason Glazer, the founder of the list serv ### Inspiration Slice and dice the email list content to find trends and interesting insights about the building simulation community [Banner image source](https://en.wikipedia.org/wiki/Email)
BMS 4M images                                                                                  
BMS EXTRA IMAGES A                                                                                  
BMS EXTRA IMAGES B                                                                                  
BMS EXTRA IMAGES C                                                                                  
BMS EXTRA IMAGES D                                                                                  
BMS EXTRA IMAGES E                                                                                  
BMS EXTRA IMAGES F                                                                                  
BMS EXTRA IMAGES G                                                                                  
BMS EXTRA IMAGES H                                                                                  
BMS-300x480 Image data                                                                                  
BMS-MT: CSV files for Image Size Information                                                                                  
BMS_Image380X380                                                                                  
BMW cars over 11k labeled car images                                                                                  Occulta Insights is contributing this labeled image datasets to the kaggle community. Data is acquired through an expensive process of web crawling and scraping. We have done our best to accurately categorize the data. However due to the large volume out current accuracy is around 95%. Please use this dataset only for learning and research purposes. The dataset is provided as a folder containing the categorized images as well as a set of image links categorized into folders. The images in the dataset need to be downloaded using the included python script. If you choose to run the Python Script please read the Disclaimer before downloading the dataset or running the python script. How can we improve this dataset? For any questions regarding this dataset please checkout https://occultainsights.io/ ******Also checkout our other datasets :****** https://www.kaggle.com/occultainsights/lexus-cars-over-7k-labeled-images https://www.kaggle.com/occultainsights/toyota-cars-over-20k-labeled-images https://www.kaggle.com/occultainsights/honda-cars-over-11k-labeled-images https://www.kaggle.com/occultainsights/audi-cars-part2-over-5k-over-5k-labeled-images https://www.kaggle.com/occultainsights/audi-cars-over-5k-labeled-images Cover photo by Stock Photography on Unsplash How can we improve this dataset? For any questions regarding this dataset please checkout https://occultainsights.io/
BNATURE-Bengali Image Captioning Dataset                                                                                  
BNCI 2014-001 Motor Imagery dataset.                                                                                  ## BNCI 2014-001 Motor Imagery dataset Dataset IIa from BCI Competition 4 [1]. ### Dataset Description This data set consists of EEG data from 9 subjects. The cue-based BCI paradigm consisted of four different motor imagery tasks, namely the imagination of movement of the left hand (class 1), right hand (class 2), both feet (class 3), and tongue (class 4). Two sessions on different days were recorded for each subject. Each session is comprised of 6 runs separated by short breaks. One run consists of 48 trials (12 for each of the four possible classes), yielding a total of 288 trials per session. The subjects were sitting in a comfortable armchair in front of a computer screen. At the beginning of a trial ( t = 0 s), a fixation cross appeared on the black screen. In addition, a short acoustic warning tone was presented. After two seconds ( t = 2 s), a cue in the form of an arrow pointing either to the left, right, down or up (corresponding to one of the four classes left hand, right hand, foot or tongue) appeared and stayed on the screen for 1.25 s. This prompted the subjects to perform the desired motor imagery task. No feedback was provided. The subjects were ask to carry out the motor imagery task until the fixation cross disappeared from the screen at t = 6 s. Twenty-two Ag/AgCl electrodes (with inter-electrode distances of 3.5 cm) were used to record the EEG; the montage is shown in Figure 3 left. All signals were recorded monopolarly with the left mastoid serving as reference and the right mastoid as ground. The signals were sampled with. 250 Hz and bandpass-filtered between 0.5 Hz and 100 Hz. The sensitivity of the amplifier was set to 100 μV . An additional 50 Hz notch filter was enabled to suppress line noise. ### References [1] Tangermann, M., Müller, K.R., Aertsen, A., Birbaumer, N., Braun, C., Brunner, C., Leeb, R., Mehring, C., Miller, K.J., Mueller-Putz, G. and Nolte, G., 2012. Review of the BCI competition IV. Frontiers in neuroscience, 6, p.55.
BNCI 2014-002 Motor Imagery dataset                                                                                  **Dataset Description** This data set consists of EEG data from 9 subjects. The cue-based BCI paradigm consisted of four different motor imagery tasks, namely the imag- ination of movement of the left hand (class 1), right hand (class 2), both feet (class 3), and tongue (class 4). Two sessions on different days were recorded for each subject. Each session is comprised of 6 runs separated by short breaks. One run consists of 48 trials (12 for each of the four possible classes), yielding a total of 288 trials per session. The subjects were sitting in a comfortable armchair in front of a computer screen. At the beginning of a trial ( t = 0 s), a fixation cross appeared on the black screen. In addition, a short acoustic warning tone was presented. After two seconds ( t = 2 s), a cue in the form of an arrow pointing either to the left, right, down or up (corresponding to one of the four classes left hand, right hand, foot or tongue) appeared and stayed on the screen for 1.25 s. This prompted the subjects to perform the desired motor imagery task. No feedback was provided. The subjects were ask to carry out the motor imagery task until the fixation cross disappeared from the screen at t = 6 s. Twenty-two Ag/AgCl electrodes (with inter-electrode distances of 3.5 cm) were used to record the EEG; the montage is shown in Figure 3 left. All signals were recorded monopolarly with the left mastoid serving as reference and the right mastoid as ground. The signals were sampled with. 250 Hz and bandpass-filtered between 0.5 Hz and 100 Hz. The sensitivity of the amplifier was set to 100 μV . An additional 50 Hz notch filter was enabled to suppress line noise References ---------- [1] Tangermann, M., Müller, K.R., Aertsen, A., Birbaumer, N., Braun, C., Brunner, C., Leeb, R., Mehring, C., Miller, K.J., Mueller-Putz, G. and Nolte, G., 2012. Review of the BCI competition IV. Frontiers in neuroscience, 6, p.55.
BNCI 2014-004 Motor Imagery dataset.                                                                                  **Dataset description** This data set consists of EEG data from 9 subjects of a study published in [1]_. The subjects were right-handed, had normal or corrected-to-normal vision and were paid for participating in the experiments. All volunteers were sitting in an armchair, watching a flat screen monitor placed approximately 1 m away at eye level. For each subject 5 sessions are provided, whereby the first two sessions contain training data without feedback (screening), and the last three sessions were recorded with feedback. Three bipolar recordings (C3, Cz, and C4) were recorded with a sampling frequency of 250 Hz.They were bandpass- filtered between 0.5 Hz and 100 Hz, and a notch filter at 50 Hz was enabled. The placement of the three bipolar recordings (large or small distances, more anterior or posterior) were slightly different for each subject (for more details see [1]). The electrode position Fz served as EEG ground. In addition to the EEG channels, the electrooculogram (EOG) was recorded with three monopolar electrodes. The cue-based screening paradigm consisted of two classes, namely the motor imagery (MI) of left hand (class 1) and right hand (class 2). Each subject participated in two screening sessions without feedback recorded on two different days within two weeks. Each session consisted of six runs with ten trials each and two classes of imagery. This resulted in 20 trials per run and 120 trials per session. Data of 120 repetitions of each MI class were available for each person in total. Prior to the first motor im- agery training the subject executed and imagined different movements for each body part and selected the one which they could imagine best (e. g., squeezing a ball or pulling a brake). Each trial started with a fixation cross and an additional short acoustic warning tone (1 kHz, 70 ms). Some seconds later a visual cue was presented for 1.25 seconds. Afterwards the subjects had to imagine the corresponding hand movement over a period of 4 seconds. Each trial was followed by a short break of at least 1.5 seconds. A randomized time of up to 1 second was added to the break to avoid adaptation For the three online feedback sessions four runs with smiley feedback were recorded, whereby each run consisted of twenty trials for each type of motor imagery. At the beginning of each trial (second 0) the feedback (a gray smiley) was centered on the screen. At second 2, a short warning beep (1 kHz, 70 ms) was given. The cue was presented from second 3 to 7.5. At second 7.5 the screen went blank and a random interval between 1.0 and 2.0 seconds was added to the trial.
BNCI 2015-001 Motor Imagery dataset                                                                                  **Dataset description** We acquired the EEG from three Laplacian derivations, 3.5 cm (center-to- center) around the electrode positions (according to International 10-20 System of Electrode Placement) C3 (FC3, C5, CP3 and C1), Cz (FCz, C1, CPz and C2) and C4 (FC4, C2, CP4 and C6). The acquisition hardware was a g.GAMMAsys active electrode system along with a g.USBamp amplifier (g.tec, Guger Tech- nologies OEG, Graz, Austria). The system sampled at 512 Hz, with a bandpass filter between 0.5 and 100 Hz and a notch filter at 50 Hz. The order of the channels in the data is FC3, FCz, FC4, C5, C3, C1, Cz, C2, C4, C6, CP3, CPz, CP4. The task for the user was to perform sustained right hand versus both feet movement imagery starting from the cue (second 3) to the end of the cross period (sec- ond 8). A trial started with 3 s of reference period, followed by a brisk audible cue and a visual cue (arrow right for right hand, arrow down for both feet) from second 3 to 4.25. The activity period, where the users received feedback, lasted from second 4 to 8. There was a random 2 to 3 s pause between the trials. References ---------- [1] J. Faller, C. Vidaurre, T. Solis-Escalante, C. Neuper and R. Scherer (2012). Autocalibration and recurrent adaptation: Towards a plug and play online ERD- BCI. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 20(3), 313-319.
BNCI 2015-004 Motor Imagery dataset                                                                                  **Dataset description** We provide EEG data recorded from nine users with disability (spinal cord injury and stroke) on two different days (sessions). Users performed, follow- ing a cue-guided experimental paradigm, five distinct mental tasks (MT). MTs include mental word association (condition WORD), mental subtraction (SUB), spatial navigation (NAV), right hand motor imagery (HAND) and feet motor imagery (FEET). Details on the experimental paradigm are summarized in Figure 1. The session for a single subject consisted of 8 runs resulting in 40 trials of each class for each day. One single experimental run consisted of 25 cues, with 5 of each mental task. Cues were presented in random order. EEG was recorded from 30 electrode channels placed on the scalp according to the international 10-20 system. Electrode positions included channels AFz, F7, F3, Fz, F4, F8, FC3, FCz, FC4, T3, C3, Cz, C4, T4, CP3, CPz,CP4, P7, P5, P3, P1, Pz, P2, P4, P6, P8, PO3, PO4, O1, and O2. Reference and ground were placed at the left and right mastoid, respectively. The g.tec GAMMAsys system with g.LADYbird active electrodes and two g.USBamp biosignal amplifiers (Guger Technolgies, Graz, Austria) was used for recording. EEG was band pass filtered 0.5-100 Hz (notch filter at 50 Hz) and sampled at a rate of 256 Hz. The duration of a single imagery trials is 10 s. At t = 0 s, a cross was presented in the middle of the screen. Participants were asked to relax and fixate the cross to avoid eye movements. At t = 3 s, a beep was sounded to get the participant’s attention. The cue indicating the requested imagery task, one out of five graphical symbols, was presented from t = 3 s to t = 4.25 s. At t = 10 s, a second beep was sounded and the fixation-cross disappeared, which indicated the end of the trial. A variable break (inter-trial-interval, ITI) lasting between 2.5 s and 3.5 s occurred before the start of the next trial. Participants were asked to avoid movements during the imagery period, and to move and blink during the ITI. Experimental runs began and ended with a blank screen (duration 4 s) References ---------- [1] Scherer R, Faller J, Friedrich EVC, Opisso E, Costa U, Kübler A, et al. (2015) Individually Adapted Imagery Improves Brain-Computer Interface Performance in End-Users with Disability. PLoS ONE 10(5). https://doi.org/10.1371/journal.pone.0123727
BNG(audiology,1000,1)                                                                                  
BNG(audiology,1000,10)                                                                                  
BNG(audiology,1000,5)                                                                                  
BNG(audiology,5000,1)                                                                                  
BNG(audiology,5000,10)                                                                                  
BNG(audiology,5000,5)                                                                                  
BNG(satellite_image)                                                                                  
BNG(satimage)                                                                                  
BOOKS ru texts                                                                                  
BOVText                                                                                  BOVText is a new large-scale benchmark dataset named Bilingual, Open World Video Text(BOVText), the first large-scale and multilingual benchmark for video text spotting in a variety of scenarios. All data are collected from KuaiShou and YouTube
BP Monitor Reading | Medical Device Images                                                                                  ### **This dataset is collected by DataCluster Labs. To download full dataset or to submit a request for your new data collection needs, please drop a mail to:&nbsp;[sales@datacluster.ai](mailto:sales@datacluster.ai)** This dataset is an extremely challenging set of over 3000+ images of different types of digital BP Monitor which are commonly found in home. These images captured and crowdsourced from over 2000+ different locations, where each image is **manually reviewed and verified** by computer vision professionals at Datacluster Labs. It contains a wide variety digital thermometer along with text readings. This dataset can be used for digital screen OCR, medical device classification and object detection. ### **Dataset Features** - Dataset size : 3000+ images - Captured by : Over 2000+ crowdsource contributors - Resolution : HD and above (1920x1080 and above) - Location : Captured with 2000+ locations - Diversity : Various lighting conditions like day, night, varied distances, view points etc. - Device used : Captured using mobile phones in 2020-2022 - Usage : English Text Detection, Hindi Text Detection, Product Content, Indoor Object Detecction, English OCR, Hindi OCR, Object Detection, Computer Vision, etc. ### Available Annotation formats COCO, YOLO, PASCAL-VOC, Tf-Record **To download full datasets or to submit a request for your dataset needs, please ping us at [sales@datacluster.ai](sales@datacluster.ai) Visit [www.datacluster.ai](www.datacluster.ai) to know more.** **Note**: All the images are manually captured and verified by a large contributor base on DataCluster platform.
BRAIN TUMOR DETECTION USING MRI IMAGES                                                                                  
BSR images                                                                                  
BSR: TFRecords all audio 1 sample ds                                                                                  
BSR: TFRecords all audio ds 1                                                                                  
BSR: TFRecords all audio ds 10                                                                                  
BSR: TFRecords all audio ds 2                                                                                  
BSR: TFRecords all audio ds 3                                                                                  
BSR: TFRecords all audio ds 4                                                                                  
BSR: TFRecords all audio ds 5                                                                                  
BSR: TFRecords all audio ds 5a                                                                                  
BSR: TFRecords all audio ds 6                                                                                  
BSR: TFRecords all audio ds 7                                                                                  
BSR: TFRecords all audio ds 8                                                                                  
BSR: TFRecords all audio ds 9                                                                                  
BSR: TFRecords no audio ds                                                                                  
BSR: musan to TFRecords music audio sample ds                                                                                  
BSR: musan to TFRecords noise audio sample ds                                                                                  
BT images                                                                                  
BTP - ImageDB2000 - features                                                                                  
BTS 147 Songs Audio Features (Spotify)                                                                                  ### Global Top artist - BTS 147 songs 'Audio Features' information on Spotify. This dataset includes are below items. Title Artist Release danceability energy key loudness mode speechiness acousticness instrumentalness liveness valence tempo id duration_ms *This file was created on November 2021.
BUET SONOROUS Audio Data                                                                                  
BUSI (Breast Ultrasound Images Dataset)                                                                                  The data collected at baseline include breast ultrasound images among women in ages between 25 and 75 years old. This data was collected in 2018. The number of patients is 600 female patients. The dataset consists of 780 images with an average image size of 500*500 pixels. The images are in PNG format. The ground truth images are presented with original images. The images are categorized into three classes, which are normal, benign, and malignant. **Research Curtesy**: ***Al-Dhabyani W, Gomaa M, Khaled H, Fahmy A. Dataset of breast ultrasound images. Data in Brief. 2020 Feb;28:104863. DOI: 10.1016/j.dib.2019.104863***.
BUSI-image                                                                                  
BWImages                                                                                  
Baby Catfish Image Dataset                                                                                  **Context** this dataset was made by myself when i couldn't find one that suit my needs, i decided to put it here so anyone could use it if they wanted. This dataset is used for computer vision in my case, but what about yours? **Content** This dataset consist of images of baby catfish taken with a smartphone camera, here is the name of the model and camera specifications: Model name : Huawei Nova 5T Camera Specs: 48 MP, f/1.8, 28mm (wide), 1/2.0', 0.8µm, PDAF 16 MP, f/2.2, 13mm (ultrawide), 1/3.1' 2 MP, f/2.4, (macro) 2 MP, f/2.4, (depth) Features: LED flash, HDR, panorama Video: 4K@30fps, 1080p@30/60fps; gyro-EIS This dataset was taken with a ring light with a diameter of 26cm and the fish were put in a semi-transparent bucket with a diameter of 26cm and height of 35cm, the ring light is placed right on the mouth of the bucket while the camera is on the middle of the ring light with max brightness. HDR feature of the camera were used to capture these images. **Inspiration** &gt; What will these images be used for? exciting! &gt; Kaggle lacks this kinds of dataset... so i tried making one. &gt; what type of computer vision models are best suited for this dataset?
Baby-Product-Image-Dataset                                                                                  
Background Image 2.0                                                                                  
Bacteria images TPS Feb 22                                                                                  
Balanced Skin Disease Image Classification Dataset                                                                                  
Balanced_Race_Gender_Face_Images                                                                                  
Bald Classification OR Detection 200K Images                                                                                  ### Context This bald detection dataset consists of 200K images which are already split into three categories (Train, Test, and Validation). The side of the dataset is 1.3GB ### Acknowledgements This dataset is preprocessed from the CelebFace dataset created by Jessica Li (https://www.kaggle.com/jessicali9530). Thank you so much Jessica for providing a wonderful dataset to the community. ### Inspiration The inspiration to create this dataset is the CelebFace dataset created by Jessica Li (https://www.kaggle.com/jessicali9530). I have extracted this dataset from the CelebFace dataset so that you can directly use the dataset for bald classification without preprocessing it.
Bald People Dataset, 5000 images                                                                                  # Dataset of bald people Dataset consists of 5000 photos of people with 7 stages of hairloss according to the Norwood scale. Dataset is useful for training neural networks for the recommendation systems, optimizing the work processes of trichologists and applications in the Med / Beauty spheres. # Get the Dataset This is just an example of the data. Leave a request on **[https://trainingdata.pro/data-market](https://trainingdata.pro/data-market/bald-spots-of-men-and-women?utm_source=kaggle&utm_medium=cpc&utm_campaign=dataset-of-bald-people)** to discuss your requirements, learn about the price and buy the dataset # Image Similar images are presented in the dataset: ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F618942%2F5b27d70193f24c3dd1bc107d6d7282c3%2FGRxNq7_nc_o.jpg?generation=1695642899815415&alt=media) # Hamilton–Norwood scale - **type_1**: There is a lack of bilateral recessions along the anterior border of the hairline in the frontoparietal regions. No notable hair loss or recession of the hairline. - **type_2**: There is a small recession of the hairline around the temples. Hair is also lost, or sparse, along the midfrontal border of the scalp, but the depth of the affected area is much less than in the frontoparietal regions. This is commonly referred to as an adult or mature hairline. - **type_3**: The first signs of significant balding appear. There is a deep, symmetrical recession at the temples that are only sparsely covered by hair. - **type_4**: The hairline recession is harsher than in stage 2, and there is scattered hair or no hair on the vertex. There are deep frontotemporal recessions, usually symmetrical, and are either bare or very sparsely covered by hair. - **type_5**: The areas of hair loss are more significant than in stage 4. They are still divided, but the band of hair between them is thinner and sparser. - **type_6**: The connection of hair that crosses the crown is gone with only sparse hair remaining. The frontotemporal and vertex regions are joined together, and the extent of hair loss is more significant. - **type_7**: The most drastic stage of hair loss, only a band of hair, going around the sides of the head persists. This hair usually is not thick and might be dainty. ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F618942%2F9ecea1768d64ac8444d74ad3c068160b%2F.png?generation=1686041157081408&alt=media) # Data Format Each image from `img` folder is accompanied by an XML-annotation in the `annotations.xml` file indicating the Hamilton–Norwood type of hairloss for each person in the dataset. # Example of XML file structure ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F618942%2F65fe0cc71ef8761886faf32a392f6a58%2F.png?generation=1686042340119038&alt=media) # **[TrainingData](https://trainingdata.pro/data-market/bald-spots-of-men-and-women?utm_source=kaggle&utm_medium=cpc&utm_campaign=dataset-of-bald-people)** provides high-quality data annotation tailored to your needs. *keywords: bald people, early balding, male pattern baldness, bald head, hair loss, hairline, bald spot, image dataset, bald dataset, hair segmentation, facial images, bald computer vision, bald classification, bald detection, balding men, baldness, bald scalp, bald head, biometric dataset, biometric data dataset, deep learning dataset, facial analysis, human images dataset, deep learning, machine learning*
Ballon d'Or 2021 Images                                                                                  ### Context These photos are collected using Scrapebox using name of 30 top soccer players (Ballon d'Or 2021) ![Ballon d'Or Image](http://infos-sport.com/wp-content/uploads/2021/08/ballon-dor.jpg) ### Content From Bing Images ### Inspiration You can use this data to train images classification.
Banana Ripeness Images Datasets                                                                                  Banana Ripeness Images Datasets Real Dataset Banana Images The real dataset developed consists of 3,495 images of Cavendish bananas. Synthetic Dataset Banana Images The synthetic dataset developed consists of 161,280 images of Cavendish bananas.
Band_pass_TrainTest_G2NET_audio_wave_data                                                                                  
Bangla FastText                                                                                  
Bangla Handwritten Digits Dataset(raw image)                                                                                  This dataset contains 200 Bangla handwritten digit images. All the digits are handwritten on white paper by the author then the images are taken using a smartphone camera. After taking the images extra white areas are cropped. If you use this dataset in your research, please credit the authors. Citation Imran Ahmed. (2022). <i>Bangla Handwritten Digits Dataset(raw image)</i> [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/4177273
Bangla Image Captioning Dataset(Modified)                                                                                  
Bangla Informal FastText Embedding                                                                                  The dataset contains bangla informal texts from social media comments. Cite: (Currently being evaluated by the Reviewers)
Bangla Natural Language Image to Text                                                                                  ### Context We represented a new Bangla dataset with a Hybrid Recurrent Neural Network model which generated Bangla natural language description of images. This dataset achieved by a large number of images with classification and containing natural language process of images. We conducted experiments on our self-made Bangla Natural Language Image to Text (BNLIT) dataset. Our dataset contained 8,743 images. We made this dataset using Bangladesh perspective images. We used one annotation for each image. In our repository, we added two types of pre-processed data which is 224 × 224 and 500 × 375 respectively alongside annotations of full dataset. We also added CNN features file of whole dataset in our repository which is features.pkl. ### Acknowledgements Jishan, Md. Asifuzzaman; Mahmud, Khan Raqib; Azad, Abul Kalam Al (2020), “Bangla Natural Language Image to Text (BNLIT)”, Mendeley Data, V4, doi: 10.17632/ws3r82gnm8.4

1000 Rows. -- 3273 msec.
