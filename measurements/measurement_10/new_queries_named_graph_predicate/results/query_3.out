Result of /data/leuven/370/vsc37064/new_queries_named_graph_predicate/query_3.txt:
OpenLink Virtuoso Interactive SQL (Virtuoso)
Version 07.20.3240 as of Mar 11 2025
Type HELP; for help and EXIT; to exit.
Connected to OpenLink Virtuoso
Driver: 07.20.3240 OpenLink Virtuoso ODBC Driver
work                                                                              title                                                                             abstract
LONG VARCHAR                                                                      LONG VARCHAR                                                                      LONG VARCHAR
_______________________________________________________________________________

http://w3id.org/mlsea/pwc/scientificWork/%20%23Turki%20%24hTweets%3A%20A%20Benchmark%20Dataset%20for%20Turkish%20Text%20Correction                                                                                   #Turki $hTweets: A Benchmark Dataset for Turkish Text Correction                                                                                  { #}Turki{ $}hTweets is a benchmark dataset for the task of correcting the user misspellings, with the purpose of introducing the first public Turkish dataset in this area. { #}Turki{ $}hTweets provides correct/incorrect word annotations with a detailed misspelling category formulation based on the real user data. We evaluated four state-of-the-art approaches on our dataset to present a preliminary analysis for the sake of reproducibility.
http://w3id.org/mlsea/pwc/scientificWork/%21Qu%C3%A9%20maravilla%21%20Multimodal%20Sarcasm%20Detection%20in%20Spanish%3A%20a%20Dataset%20and%20a%20Baseline                                                                                  !Qué maravilla! Multimodal Sarcasm Detection in Spanish: a Dataset and a Baseline                                                                                  We construct the first ever multimodal sarcasm dataset for Spanish. The audiovisual dataset consists of sarcasm annotated text that is aligned with video and audio. The dataset represents two varieties of Spanish, a Latin American variety and a Peninsular Spanish variety, which ensures a wider dialectal coverage for this global language. We present several models for sarcasm detection that will serve as baselines in the future research. Our results show that results with text only (89%) are worse than when combining text with audio (91.9%). Finally, the best results are obtained when combining all the modalities: text, audio and video (93.1%).
http://w3id.org/mlsea/pwc/scientificWork/%24%20ell_2%24-norm%20Flow%20Diffusion%20in%20Near-Linear%20Time                                                                                  $ ell_2$-norm Flow Diffusion in Near-Linear Time                                                                                  Diffusion is a fundamental graph procedure and has been a basic building block in a wide range of theoretical and empirical applications such as graph partitioning and semi-supervised learning on graphs. In this paper, we study computationally efficient diffusion primitives beyond random walk. We design an $ widetilde{O}(m)$-time randomized algorithm for the $ ell_2$-norm flow diffusion problem, a recently proposed diffusion model based on network flow with demonstrated graph clustering related applications both in theory and in practice. Examples include finding locally-biased low conductance cuts. Using a known connection between the optimal dual solution of the flow diffusion problem and the local cut structure, our algorithm gives an alternative approach for finding such cuts in nearly linear time. From a technical point of view, our algorithm contributes a novel way of dealing with inequality constraints in graph optimization problems. It adapts the high-level algorithmic framework of nearly linear time Laplacian system solvers, but requires several new tools: vertex elimination under constraints, a new family of graph ultra-sparsifiers, and accelerated proximal gradient methods with inexact proximal mapping computation.
http://w3id.org/mlsea/pwc/scientificWork/%24%20mathcal%7BH%7D_2%2F%20mathcal%7BH%7D_%7B-%7D%24%20Distributed%20Fault%20Detection%20and%20Isolation%20for%20Heterogeneous%20Multi-Agent%20Systems                                                                                  $ mathcal{H}_2/ mathcal{H}_{-}$ Distributed Fault Detection and Isolation for Heterogeneous Multi-Agent Systems                                                                                  The paper deals with the problem of distributed fault detection and isolation (FDI) for a group of heterogeneous multi-agent systems. The developed formation for the FDI is taken into account as a distributed observer design methodology, where the interaction between the agent and its neighbors is described as a vector of distributed relative output measurements. Based on two performance indexes $ mathcal{H}_2$ and $ mathcal{H}_{-}$, sufficient conditions are given to ensure the residual signals robust to the disturbances and sensitive with respect to the fault signals. In addition, we show that by using our proposed approach, each agent is able to estimate both its own states and states of its nearest neighbors in the presence of disturbances and faults. Finally, numerical simulations are provided to demonstrate the effectiveness of the theoretically analyzed results.
http://w3id.org/mlsea/pwc/scientificWork/%24%CE%B1%24-Geodesical%20Skew%20Divergence                                                                                  $α$-Geodesical Skew Divergence                                                                                  The asymmetric skew divergence smooths one of the distributions by mixing it, to a degree determined by the parameter $ lambda$, with the other distribution. Such divergence is an approximation of the KL divergence that does not require the target distribution to be absolutely continuous with respect to the source distribution. In this paper, an information geometric generalization of the skew divergence called the $ alpha$-geodesical skew divergence is proposed, and its properties are studied.
http://w3id.org/mlsea/pwc/scientificWork/%24C%5E3%24%3A%20Compositional%20Counterfactual%20Contrastive%20Learning%20for%20Video-grounded%20Dialogues                                                                                  $C^3$: Compositional Counterfactual Contrastive Learning for Video-grounded Dialogues                                                                                  Video-grounded dialogue systems aim to integrate video understanding and dialogue understanding to generate responses that are relevant to both the dialogue and video context. Most existing approaches employ deep learning models and have achieved remarkable performance, given the relatively small datasets available. However, the results are partly accomplished by exploiting biases in the datasets rather than developing multimodal reasoning, resulting in limited generalization. In this paper, we propose a novel approach of Compositional Counterfactual Contrastive Learning ($C^3$) to develop contrastive training between factual and counterfactual samples in video-grounded dialogues. Specifically, we design factual/counterfactual sampling based on the temporal steps in videos and tokens in dialogues and propose contrastive loss functions that exploit object-level or action-level variance. Different from prior approaches, we focus on contrastive hidden state representations among compositional output tokens to optimize the representation space in a generation setting. We achieved promising performance gains on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark and showed the benefits of our approach in grounding video and dialogue context.
http://w3id.org/mlsea/pwc/scientificWork/%24N%24-player%20and%20Mean-field%20Games%20in%20It%C3%B4-diffusion%20Markets%20with%20Competitive%20or%20Homophilous%20Interaction                                                                                  $N$-player and Mean-field Games in Itô-diffusion Markets with Competitive or Homophilous Interaction                                                                                  In It ^{o}-diffusion environments, we introduce and analyze $N$-player and common-noise mean-field games in the context of optimal portfolio choice in a common market. The players invest in a finite horizon and also interact, driven either by competition or homophily. We study an incomplete market model in which the players have constant individual risk tolerance coefficients (CARA utilities). We also consider the general case of random individual risk tolerances and analyze the related games in a complete market setting. This randomness makes the problem substantially more complex as it leads to ($N$ or a continuum of) auxiliary ''individual'' It ^{o}-diffusion markets. For all cases, we derive explicit or closed-form solutions for the equilibrium stochastic processes, the optimal state processes, and the values of the games.
http://w3id.org/mlsea/pwc/scientificWork/%24Q%5E%7B2%7D%24%3A%20Evaluating%20Factual%20Consistency%20in%20Knowledge-Grounded%20Dialogues%20via%20Question%20Generation%20and%20Question%20Answering                                                                                  $Q^{2}$: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering                                                                                  Neural knowledge-grounded generative models for dialogue often produce content that is factually inconsistent with the knowledge they rely on, making them unreliable and limiting their applicability. Inspired by recent work on evaluating factual consistency in abstractive summarization, we propose an automatic evaluation metric for factual consistency in knowledge-grounded dialogue using automatic question generation and question answering. Our metric, denoted $Q^2$, compares answer spans using natural language inference (NLI), instead of token-based matching as done in previous work. To foster proper evaluation, we curate a novel dataset of dialogue system outputs for the Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We perform a thorough meta-evaluation of $Q^2$ against other metrics using this dataset and two others, where it consistently shows higher correlation with human judgements.
http://w3id.org/mlsea/pwc/scientificWork/%27Alexa%2C%20what%20do%20you%20do%20for%20fun%3F%27%20Characterizing%20playful%20requests%20with%20virtual%20assistants                                                                                  'Alexa, what do you do for fun?' Characterizing playful requests with virtual assistants                                                                                  Virtual assistants such as Amazon's Alexa, Apple's Siri, Google Home, and Microsoft's Cortana, are becoming ubiquitous in our daily lives and successfully help users in various daily tasks, such as making phone calls or playing music. Yet, they still struggle with playful utterances, which are not meant to be interpreted literally. Examples include jokes or absurd requests or questions such as, 'Are you afraid of the dark?', 'Who let the dogs out?', or 'Order a zillion gummy bears'. Today, virtual assistants often return irrelevant answers to such utterances, except for hard-coded ones addressed by canned replies. To address the challenge of automatically detecting playful utterances, we first characterize the different types of playful human-virtual assistant interaction. We introduce a taxonomy of playful requests rooted in theories of humor and refined by analyzing real-world traffic from Alexa. We then focus on one node, personification, where users refer to the virtual assistant as a person ('What do you do for fun?'). Our conjecture is that understanding such utterances will improve user experience with virtual assistants. We conducted a Wizard-of-Oz user study and showed that endowing virtual assistant s with the ability to identify humorous opportunities indeed has the potential to increase user satisfaction. We hope this work will contribute to the understanding of the landscape of the problem and inspire novel ideas and techniques towards the vision of giving virtual assistants a sense of humor.
http://w3id.org/mlsea/pwc/scientificWork/%27Average%27%20Approximates%20%27First%20Principal%20Component%27%3F%20An%20Empirical%20Analysis%20on%20Representations%20from%20Neural%20Language%20Models                                                                                  'Average' Approximates 'First Principal Component'? An Empirical Analysis on Representations from Neural Language Models                                                                                  Contextualized representations based on neural language models have furthered the state of the art in various NLP tasks. Despite its great success, the nature of such representations remains a mystery. In this paper, we present an empirical property of these representations -- 'average' approximates 'first principal component'. Specifically, experiments show that the average of these representations shares almost the same direction as the first principal component of the matrix whose columns are these representations. We believe this explains why the average representation is always a simple yet strong baseline. Our further examinations show that this property also holds in more challenging scenarios, for example, when the representations are from a model right after its random initialization. Therefore, we conjecture that this property is intrinsic to the distribution of representations and not necessarily related to the input structure. We realize that these representations empirically follow a normal distribution for each dimension, and by assuming this is true, we demonstrate that the empirical property can be in fact derived mathematically.
http://w3id.org/mlsea/pwc/scientificWork/%27BNN%20-%20BN%20%3D%20%3F%27%3A%20Training%20Binary%20Neural%20Networks%20without%20Batch%20Normalization                                                                                  'BNN - BN = ?': Training Binary Neural Networks without Batch Normalization                                                                                  Batch normalization (BN) is a key facilitator and considered essential for state-of-the-art binary neural networks (BNN). However, the BN layer is costly to calculate and is typically implemented with non-binary parameters, leaving a hurdle for the efficient implementation of BNN training. It also introduces undesirable dependence between samples within each batch. Inspired by the latest advance on Batch Normalization Free (BN-Free) training, we extend their framework to training BNNs, and for the first time demonstrate that BNs can be completed removed from BNN training and inference regimes. By plugging in and customizing techniques including adaptive gradient clipping, scale weight standardization, and specialized bottleneck block, a BN-free BNN is capable of maintaining competitive accuracy compared to its BN-based counterpart. Extensive experiments validate the effectiveness of our proposal across diverse BNN backbones and datasets. For example, after removing BNs from the state-of-the-art ReActNets, it can still be trained with our proposed methodology to achieve 92.08%, 68.34%, and 68.0% accuracy on CIFAR-10, CIFAR-100, and ImageNet respectively, with marginal performance drop (0.23%~0.44% on CIFAR and 1.40% on ImageNet). Codes and pre-trained models are available at: https://github.com/VITA-Group/BNN_NoBN.
http://w3id.org/mlsea/pwc/scientificWork/%27Cellular%20Network%20Densification%20Increases%20Radio-Frequency%20Pollution%27%3A%20True%20or%20False%3F                                                                                  'Cellular Network Densification Increases Radio-Frequency Pollution': True or False?                                                                                  A very popular theory circulating among non-scientific communities claims that the massive deployment of Base Stations (BSs) over the territory, a.k.a. cellular network densification, always triggers an uncontrolled and exponential increase of human exposure to Radio Frequency 'Pollution' (RFP). To face such concern in a way that can be understood by the layman, in this work we develop a very simple model to compute the RFP, based on a set of worst-case and conservative assumptions. We then provide closed-form expressions to evaluate the RFP variation in a pair of candidate 5G deployments, subject to different densification levels. Results, obtained over a wide set of representative 5G scenarios, dispel the myth: cellular network densification triggers an RFP decrease (up to three orders of magnitude) when the radiated power from the BS is adjusted to ensure a minimum sensitivity at the cell edge. Eventually, we analyze the conditions under which the RFP may increase when the network is densified (e.g., when the radiated power does not scale with the cell size), proving that the amount of RFP is always controlled. Finally, the results obtained by simulation confirm the outcomes of the RFP model.
http://w3id.org/mlsea/pwc/scientificWork/%27Don%27t%20quote%20me%20on%20that%27%3A%20Finding%20Mixtures%20of%20Sources%20in%20News%20Articles                                                                                  'Don't quote me on that': Finding Mixtures of Sources in News Articles                                                                                  Journalists publish statements provided by people, or textit{sources} to contextualize current events, help voters make informed decisions, and hold powerful individuals accountable. In this work, we construct an ontological labeling system for sources based on each source's textit{affiliation} and textit{role}. We build a probabilistic model to infer these attributes for named sources and to describe news articles as mixtures of these sources. Our model outperforms existing mixture modeling and co-clustering approaches and correctly infers source-type in 80 % of expert-evaluated trials. Such work can facilitate research in downstream tasks like opinion and argumentation mining, representing a first step towards machine-in-the-loop textit{computational journalism} systems.
http://w3id.org/mlsea/pwc/scientificWork/%27Forget%27%20the%20Forget%20Gate%3A%20Estimating%20Anomalies%20in%20Videos%20using%20Self-contained%20Long%20Short-Term%20Memory%20Networks                                                                                  'Forget' the Forget Gate: Estimating Anomalies in Videos using Self-contained Long Short-Term Memory Networks                                                                                  Abnormal event detection is a challenging task that requires effectively handling intricate features of appearance and motion. In this paper, we present an approach of detecting anomalies in videos by learning a novel LSTM based self-contained network on normal dense optical flow. Due to their sigmoid implementations, standard LSTM's forget gate is susceptible to overlooking and dismissing relevant content in long sequence tasks like abnormality detection. The forget gate mitigates participation of previous hidden state for computation of cell state prioritizing current input. In addition, the hyperbolic tangent activation of standard LSTMs sacrifices performance when a network gets deeper. To tackle these two limitations, we introduce a bi-gated, light LSTM cell by discarding the forget gate and introducing sigmoid activation. Specifically, the LSTM architecture we come up with fully sustains content from previous hidden state thereby enabling the trained model to be robust and make context-independent decision during evaluation. Removing the forget gate results in a simplified and undemanding LSTM cell with improved performance effectiveness and computational efficiency. Empirical evaluations show that the proposed bi-gated LSTM based network outperforms various LSTM based models verifying its effectiveness for abnormality detection and generalization tasks on CUHK Avenue and UCSD datasets.
http://w3id.org/mlsea/pwc/scientificWork/%27I%27m%20Not%20Mad%27%3A%20Commonsense%20Implications%20of%20Negation%20and%20Contradiction                                                                                  'I'm Not Mad': Commonsense Implications of Negation and Contradiction                                                                                  Natural language inference requires reasoning about contradictions, negations, and their commonsense implications. Given a simple premise (e.g., 'I'm mad at you'), humans can reason about the varying shades of contradictory statements ranging from straightforward negations ('I'm not mad at you') to commonsense contradictions ('I'm happy'). Moreover, these negated or contradictory statements shift the commonsense implications of the original premise in nontrivial ways. For example, while 'I'm mad' implies 'I'm unhappy about something,' negating the premise (i.e., 'I'm not mad') does not necessarily negate the corresponding commonsense implications. In this paper, we present the first comprehensive study focusing on commonsense implications of negated statements and contradictions. We introduce ANION1, a new commonsense knowledge graph with 624K if-then rules focusing on negated and contradictory events. We then present joint generative and discriminative inference models for this new resource, providing novel empirical insights on how logical negations and commonsense contradictions reshape the commonsense implications of their original premises.
http://w3id.org/mlsea/pwc/scientificWork/%27Subverting%20the%20Jewtocracy%27%3A%20Online%20Antisemitism%20Detection%20Using%20Multimodal%20Deep%20Learning                                                                                  'Subverting the Jewtocracy': Online Antisemitism Detection Using Multimodal Deep Learning                                                                                  The exponential rise of online social media has enabled the creation, distribution, and consumption of information at an unprecedented rate. However, it has also led to the burgeoning of various forms of online abuse. Increasing cases of online antisemitism have become one of the major concerns because of its socio-political consequences. Unlike other major forms of online abuse like racism, sexism, etc., online antisemitism has not been studied much from a machine learning perspective. To the best of our knowledge, we present the first work in the direction of automated multimodal detection of online antisemitism. The task poses multiple challenges that include extracting signals across multiple modalities, contextual references, and handling multiple aspects of antisemitism. Unfortunately, there does not exist any publicly available benchmark corpus for this critical task. Hence, we collect and label two datasets with 3,102 and 3,509 social media posts from Twitter and Gab respectively. Further, we present a multimodal deep learning system that detects the presence of antisemitic content and its specific antisemitism category using text and images from posts. We perform an extensive set of experiments on the two datasets to evaluate the efficacy of the proposed system. Finally, we also present a qualitative analysis of our study.
http://w3id.org/mlsea/pwc/scientificWork/%27TL%3BDR%3A%27%20Out-of-Context%20Adversarial%20Text%20Summarization%20and%20Hashtag%20Recommendation                                                                                  'TL;DR:' Out-of-Context Adversarial Text Summarization and Hashtag Recommendation                                                                                  This paper presents Out-of-Context Summarizer, a tool that takes arbitrary public news articles out of context by summarizing them to coherently fit either a liberal- or conservative-leaning agenda. The Out-of-Context Summarizer also suggests hashtag keywords to bolster the polarization of the summary, in case one is inclined to take it to Twitter, Parler or other platforms for trolling. Out-of-Context Summarizer achieved 79% precision and 99% recall when summarizing COVID-19 articles, 93% precision and 93% recall when summarizing politically-centered articles, and 87% precision and 88% recall when taking liberally-biased articles out of context. Summarizing valid sources instead of synthesizing fake text, the Out-of-Context Summarizer could fairly pass the 'adversarial disclosure' test, but we didn't take this easy route in our paper. Instead, we used the Out-of-Context Summarizer to push the debate of potential misuse of automated text generation beyond the boilerplate text of responsible disclosure of adversarial language models.
http://w3id.org/mlsea/pwc/scientificWork/%27Weak%20AI%27%20is%20Likely%20to%20Never%20Become%20%27Strong%20AI%27%2C%20So%20What%20is%20its%20Greatest%20Value%20for%20us%3F                                                                                  'Weak AI' is Likely to Never Become 'Strong AI', So What is its Greatest Value for us?                                                                                  AI has surpassed humans across a variety of tasks such as image classification, playing games (e.g., go, 'Starcraft' and poker), and protein structure prediction. However, at the same time, AI is also bearing serious controversies. Many researchers argue that little substantial progress has been made for AI in recent decades. In this paper, the author (1) explains why controversies about AI exist; (2) discriminates two paradigms of AI research, termed 'weak AI' and 'strong AI' (a.k.a. artificial general intelligence); (3) clarifies how to judge which paradigm a research work should be classified into; (4) discusses what is the greatest value of 'weak AI' if it has no chance to develop into 'strong AI'.
http://w3id.org/mlsea/pwc/scientificWork/%27Who%20can%20help%20me%3F%27%3A%20Knowledge%20Infused%20Matching%20of%20Support%20Seekers%20and%20Support%20Providers%20during%20COVID-19%20on%20Reddit                                                                                  'Who can help me?': Knowledge Infused Matching of Support Seekers and Support Providers during COVID-19 on Reddit                                                                                  During the ongoing COVID-19 crisis, subreddits on Reddit, such as r/Coronavirus saw a rapid growth in user's requests for help (support seekers - SSs) including individuals with varying professions and experiences with diverse perspectives on care (support providers - SPs). Currently, knowledgeable human moderators match an SS with a user with relevant experience, i.e, an SP on these subreddits. This unscalable process defers timely care. We present a medical knowledge-infused approach to efficient matching of SS and SPs validated by experts for the users affected by anxiety and depression, in the context of with COVID-19. After matching, each SP to an SS labeled as either supportive, informative, or similar (sharing experiences) using the principles of natural language inference. Evaluation by 21 domain experts indicates the efficacy of incorporated knowledge and shows the efficacy the matching system.
http://w3id.org/mlsea/pwc/scientificWork/%27Why%20Would%20I%20Trust%20Your%20Numbers%3F%27%20On%20the%20Explainability%20of%20Expected%20Values%20in%20Soccer                                                                                  'Why Would I Trust Your Numbers?' On the Explainability of Expected Values in Soccer                                                                                  In recent years, many different approaches have been proposed to quantify the performances of soccer players. Since player performances are challenging to quantify directly due to the low-scoring nature of soccer, most approaches estimate the expected impact of the players' on-the-ball actions on the scoreline. While effective, these approaches are yet to be widely embraced by soccer practitioners. The soccer analytics community has primarily focused on improving the accuracy of the models, while the explainability of the produced metrics is often much more important to practitioners. To help bridge the gap between scientists and practitioners, we introduce an explainable Generalized Additive Model that estimates the expected value for shots. Unlike existing models, our model leverages features corresponding to widespread soccer concepts. To this end, we represent the locations of shots by fuzzily assigning the shots to designated zones on the pitch that practitioners are familiar with. Our experimental evaluation shows that our model is as accurate as existing models, while being easier to explain to soccer practitioners.
http://w3id.org/mlsea/pwc/scientificWork/%27Wikily%27%20Supervised%20Neural%20Translation%20Tailored%20to%20Cross-Lingual%20Tasks                                                                                  'Wikily' Supervised Neural Translation Tailored to Cross-Lingual Tasks                                                                                  We present a simple but effective approach for leveraging Wikipedia for neural machine translation as well as cross-lingual tasks of image captioning and dependency parsing without using any direct supervision from external parallel data or supervised models in the target language. We show that first sentences and titles of linked Wikipedia pages, as well as cross-lingual image captions, are strong signals for a seed parallel data to extract bilingual dictionaries and cross-lingual word embeddings for mining parallel text from Wikipedia. Our final model achieves high BLEU scores that are close to or sometimes higher than strong supervised baselines in low-resource languages; e.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh. Moreover, we tailor our wikily supervised translation models to unsupervised image captioning, and cross-lingual dependency parser transfer. In image captioning, we train a multi-tasking machine translation and image captioning pipeline for Arabic and English from which the Arabic training data is a translated version of the English captioning data, using our wikily-supervised translation models. Our captioning results on Arabic are slightly better than that of its supervised model. In dependency parsing, we translate a large amount of monolingual text, and use it as artificial training data in an annotation projection framework. We show that our model outperforms recent work on cross-lingual transfer of dependency parsers.
http://w3id.org/mlsea/pwc/scientificWork/%27You%20made%20me%20feel%20this%20way%27%3A%20Investigating%20Partners%27%20Influence%20in%20Predicting%20Emotions%20in%20Couples%27%20Conflict%20Interactions%20using%20Speech%20Data                                                                                  'You made me feel this way': Investigating Partners' Influence in Predicting Emotions in Couples' Conflict Interactions using Speech Data                                                                                  How romantic partners interact with each other during a conflict influences how they feel at the end of the interaction and is predictive of whether the partners stay together in the long term. Hence understanding the emotions of each partner is important. Yet current approaches that are used include self-reports which are burdensome and hence limit the frequency of this data collection. Automatic emotion prediction could address this challenge. Insights from psychology research indicate that partners' behaviors influence each other's emotions in conflict interaction and hence, the behavior of both partners could be considered to better predict each partner's emotion. However, it is yet to be investigated how doing so compares to only using each partner's own behavior in terms of emotion prediction performance. In this work, we used BERT to extract linguistic features (i.e., what partners said) and openSMILE to extract paralinguistic features (i.e., how they said it) from a data set of 368 German-speaking Swiss couples (N = 736 individuals) who were videotaped during an 8-minutes conflict interaction in the laboratory. Based on those features, we trained machine learning models to predict if partners feel positive or negative after the conflict interaction. Our results show that including the behavior of the other partner improves the prediction performance. Furthermore, for men, considering how their female partners spoke is most important and for women considering what their male partner said is most important in getting better prediction performance. This work is a step towards automatically recognizing each partners' emotion based on the behavior of both, which would enable a better understanding of couples in research, therapy, and the real world.
http://w3id.org/mlsea/pwc/scientificWork/%27Zero-Shot%27%20Point%20Cloud%20Upsampling                                                                                  'Zero-Shot' Point Cloud Upsampling                                                                                  Recent supervised point cloud upsampling methods are restricted by the size of training data and are limited in terms of covering all object shapes. Besides the challenges faced due to data acquisition, the networks also struggle to generalize on unseen records. In this paper, we present an internal point cloud upsampling approach at a holistic level referred to as 'Zero-Shot' Point Cloud Upsampling (ZSPU). Our approach is data agnostic and relies solely on the internal information provided by a particular point cloud without patching in both self-training and testing phases. This single-stream design significantly reduces the training time by learning the relation between low resolution (LR) point clouds and their high (original) resolution (HR) counterparts. This association will then provide super resolution (SR) outputs when original point clouds are loaded as input. ZSPU achieves competitive/superior quantitative and qualitative performances on benchmark datasets when compared with other upsampling methods.
http://w3id.org/mlsea/pwc/scientificWork/%28ASNA%29%20An%20Attention-based%20Siamese-Difference%20Neural%20Network%20with%20Surrogate%20Ranking%20Loss%20function%20for%20Perceptual%20Image%20Quality%20Assessment                                                                                  (ASNA) An Attention-based Siamese-Difference Neural Network with Surrogate Ranking Loss function for Perceptual Image Quality Assessment                                                                                  Recently, deep convolutional neural networks (DCNN) that leverage the adversarial training framework for image restoration and enhancement have significantly improved the processed images' sharpness. Surprisingly, although these DCNNs produced crispier images than other methods visually, they may get a lower quality score when popular measures are employed for evaluating them. Therefore it is necessary to develop a quantitative metric to reflect their performances, which is well-aligned with the perceived quality of an image. Famous quantitative metrics such as Peak signal-to-noise ratio (PSNR), The structural similarity index measure (SSIM), and Perceptual Index (PI) are not well-correlated with the mean opinion score (MOS) for an image, especially for the neural networks trained with adversarial loss functions. This paper has proposed a convolutional neural network using an extension architecture of the traditional Siamese network so-called Siamese-Difference neural network. We have equipped this architecture with the spatial and channel-wise attention mechanism to increase our method's performance. Finally, we employed an auxiliary loss function to train our model. The suggested additional cost function surrogates ranking loss to increase Spearman's rank correlation coefficient while it is differentiable concerning the neural network parameters. Our method achieved superior performance in textbf{ textit{NTIRE 2021 Perceptual Image Quality Assessment}} Challenge. The implementations of our proposed method are publicly available.
http://w3id.org/mlsea/pwc/scientificWork/%28Integral-%29ISS%20of%20switched%20and%20time-varying%20impulsive%20systems%20based%20on%20global%20state%20weak%20linearization                                                                                  (Integral-)ISS of switched and time-varying impulsive systems based on global state weak linearization                                                                                  It is shown that impulsive systems of nonlinear, time-varying and/or switched form that allow a stable global state weak linearization are jointly input-to-state stable (ISS) under small inputs and integral ISS (iISS). The system is said to allow a global state weak linearization if its flow and jump equations can be written as a (time-varying, switched) linear part plus a (nonlinear) pertubation satisfying a bound of affine form on the state. This bound reduces to a linear form under zero input but does not force the system to be linear under zero input. The given results generalize and extend previously existing ones in many directions: (a) no (dwell-time or other) constraints are placed on the impulse-time sequence, (b) the system need not be linear under zero input, (c) existence of a (common) Lyapunov function is not required, (d) the perturbation bound need not be linear on the input.
http://w3id.org/mlsea/pwc/scientificWork/%28When%29%20should%20you%20adjust%20inferences%20for%20multiple%20hypothesis%20testing%3F                                                                                  (When) should you adjust inferences for multiple hypothesis testing?                                                                                  Multiple hypothesis testing practices vary widely, without consensus on which are appropriate when. We provide an economic foundation for these practices. In studies of multiple interventions or sub-populations, adjustments may be appropriate depending on scale economies in the research production function, with control of classical notions of compound errors emerging in some but not all cases. Studies with multiple outcomes motivate testing using a single index, or adjusted tests of several indices when the intended audience is heterogeneous. Data on actual research costs in two applications suggest both that some adjustment is warranted and that standard procedures are overly conservative.
http://w3id.org/mlsea/pwc/scientificWork/%5BRE%5D%20CNN-generated%20images%20are%20surprisingly%20easy%20to%20spot...for%20now                                                                                  [RE] CNN-generated images are surprisingly easy to spot...for now                                                                                  This work evaluates the reproducibility of the paper 'CNN-generated images are surprisingly easy to spot... for now' by Wang et al. published at CVPR 2020. The paper addresses the challenge of detecting CNN-generated imagery, which has reached the potential to even fool humans. The authors propose two methods which help an image classifier to generalize from being trained on one specific CNN to detecting imagery produced by unseen architectures, training methods, or data sets. The paper proposes two methods to help a classifier generalize: (i) utilizing different kinds of data augmentations and (ii) using a diverse data set. This report focuses on assessing if these techniques indeed help the generalization process. Furthermore, we perform additional experiments to study the limitations of the proposed techniques.
http://w3id.org/mlsea/pwc/scientificWork/%5BRE%5D%20Double-Hard%20Debias%3A%20Tailoring%20Word%20Embeddings%20for%20Gender%20Bias%20Mitigation                                                                                  [RE] Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation                                                                                  Despite widespread use in natural language processing (NLP) tasks, word embeddings have been criticized for inheriting unintended gender bias from training corpora. programmer is more closely associated with man and homemaker is more closely associated with woman. Such gender bias has also been shown to propagate in downstream tasks.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20A%20Reproduction%20of%20Ensemble%20Distribution%20Distillation                                                                                  [Re] A Reproduction of Ensemble Distribution Distillation                                                                                  Scope of Reproducibility The authors claim that their proposed method is able to, given an ensemble of deep neural networks, capture the uncertainty estimation and decomposition capabilities of the ensemble into a single model. The authors also claim that this only results in a small reduction in classification performance compared to the ensemble. We examine these claims by reproducing most of the authorsʼ experiments on the CIFAR-10 dataset. Methodology The proposed method was re-implemented in tf.keras. The surrounding data pipelines, pre-processing, and experimentation code were also re-implemented. As in the original paper, the models were based on VGG-16 networks trained from scratch with random initialization. Training and evaluation was done on two consumer-grade GPUs, for a total of 273 hours. Results Our findings support the authorsʼ central claims. In terms of uncertainty estimation our EnD2 achieved (99 ± 1) % of the AUC-ROC of our ensemble on the OOD-detection task. The corresponding value in the original paper was (100±1) %. In terms of classification our EnD2 had (16 ± 1)% higher error than our ensemble. The corresponding values in the original paper was (11 ± 6)%. Other metrics showed similar agreement, but, significantly, in the OOD-detection task our EnD performed at least as well as our EnD2 . This is in stark contrast with the original paper. We also took a novel approach to visualizing the uncertainty decomposition by plotting the resulting distributions on a simplex, offering a visual explanation to some surprising results in the original paper, while mostly supporting the authorsʼ intuitive justifications for the model. What was easy The original paper features a thorough mathematical formulation of the method, aiding conceptual understanding. The datasets used by the authors are publicly available. The use of the simpler datasets also meant that it was computationally feasible for us to reproduce these results. The base model used is well known with several implementation available, allowing us to focus on the novel aspects of the method. What was difficult While the theoretical explanations of the method are excellent, we initially found it hard to translate this into an implementation. Our difficulty was likely caused by our inexperience with the subject matter. Nonetheless, a pseudocode, such as the one we have provided, would havee simplified the re-implementation. We were not able to reproduce the results on some of the datasets due to limited computational resources. Communication with original authors We did not contact the original authors directly, but we did refer to a public GitHub and blog post created by one of the authors. At the same time as submitting this report to the ML Reproducibility Challenge 2020 we also sent a copy to the authors and asked for their feedback.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Can%20gradient%20clipping%20mitigate%20label%20noise%3F                                                                                  [Re] Can gradient clipping mitigate label noise?                                                                                  Scope of Reproducibility The original paper proposes partially Huberised losses, which possess label noise robustness. The authors claim that there exist label noise scenarios that defeat Huberised but not partially Huberised losses, and that partially Huberised versions of existing losses perform well on real-world datasets subject to symmetric label noise. Methodology All the experiments described in the paper were fully re-implemented using NumPy, SciPy and PyTorch. The experiments on synthetic data were run on a CPU, while the deep learning experiments were run using a Nvidia RTX 2080 Ti GPU. Running the experimentation necessary to gain some insight on some of the network architectures used and reproducing the real-world experiments required over 550 GPU hours. Results Overall, our results mostly support the claims of the original paper. For the synthetic experiments, our results differ when using the exact values described in the paper, although they still support the main claim. After slightly modifying some of the experiment settings, our reproduced figures are nearly identical to the figures from the original paper. For the deep learning experiments, our results differ, with some of the baselines reaching a much higher accuracy on MNIST, CIFAR-10 and CIFAR-100. Nonetheless, with the help of an additional experiment, our results support the authorsʼ claim that partially Huberised losses perform well on real-world datasets subject to label noise. What was easy The original paper is well written and insightful, which made it fairly easy to implement the partially Huberised version of standard losses based on the information given. In addition, recreating the synthetic datasets used in two of the original paperʼs experiments was relatively straightforward. What was difficult Even though the authors were very detailed in their feedback, finding the exact hyperparameters used in the real-world experiments required many iterations of inquiry and experimentation. In addition, the CIFAR-10 and CIFAR-100 experiments can be difficult to reproduce due to the high number of experiment configurations, resulting in many training runs and a relatively high computational cost of over 550 GPU hours. Communication with original authors We contacted the authors on multiple occasions regarding some of the hyperparameters used in their experiments, to which they promptly replied with very detailed explanations.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Deep%20Fair%20Clustering%20for%20Visual%20Learning                                                                                  [Re] Deep Fair Clustering for Visual Learning                                                                                  Scope of Reproducibility Deep Fair Clustering (DFC) aims to provide a clustering algorithm that is fair, clusteringfavourable, and which can be used on high-dimensional and large-scale data. In existing frameworks there is a trade-off between clustering quality and fairness. In this report we aim to reproduce a selection of the results of DFC; using two of four datasets and all four metrics that were used in the original paper, namely accuracy, Normalized Mutual Information (NMI), balance and entropy. We use the authorsʼ implementation and check whether it is consistent with the description in the paper. As extensions to the original paper we look into the effects of 1) using no pretrained cluster centers, 2) using different divergence functions as clustering regularizers and 3) using non-binary/corrupted sensitive attributes. Methodology The open source code of the authors has been used. The datasets and data-preprocessing has been done with our code, since the authors did not provide the datasets in their code. Also the pretrained Variational Autoencoder (VAE) dataset had to be re-implemented for the Color Reverse MNIST . For the extensions we wrote extra functions. For measuring the influence of discarding the pretrained cluster centers, the code was already provided by the authors. Results For the MNIST-USPS dataset, we report similar accuracy and NMI values that are within 1.2% and 0.5% of the values reported in the original paper. However, the balance and entropy differed significantly, where our results were within 73.1% and 30.3% of the original values respectively. For the Color Reverse MNIST dataset, we report similar values on accuracy, balance and entropy, which are within 5.3%, 2.6% and 0.2% respectively. Only the value of the NMI differed significantly, name within 12.9% of the original value In general, our results still support the main claim of the original paper, even though on some metrics the results differ significantly. What was easy The open source code of the authors was beneficial; it was well structured and ordered into multiple files. Furthermore, the code to use randomly initialized instead of pretrained cluster centers was already provided. What was difficult First of all, the main difficulty in reproducing the paper was caused by the coding style; due to the lack of comments it was difficult to get a good understanding of the code. Secondly, we were required to download the data ourselves. However, these filenames and labels did not correspond to the included txt-files by the authors. Therefore, the model did not learn and we regenerated train_mnist.txt and train_usps.txt. Finally, the authors only included pretrained models for the MNIST-USPS dataset. As a consequence, we had to pre-train some parts of the DFC algorithm for the Color Reverse MNIST dataset.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Don%27t%20Judge%20an%20Object%20by%20Its%20Context%3A%20Learning%20to%20Overcome%20Contextual%20Bias                                                                                  [Re] Don't Judge an Object by Its Context: Learning to Overcome Contextual Bias                                                                                  Scope of Reproducibility Singh et al. [1] point out the dangers of contextual bias in visual recognition datasets. They propose two methods, CAM-based and feature-split, that better recognize an object or attribute in the absence of its typical context while maintaining competitive withincontext accuracy. To verify their performance, we attempted to reproduce all 12 tables in the original paper, including those in the appendix. We also conducted additional experiments to better understand the proposed methods, including increasing the regularization in CAM-based and removing the weighted loss in feature-split. Methodology As the original code was not made available, we implemented the entire pipeline from scratch in PyTorch 1.7.0. Our implementation is based on the paper and email exchanges with the authors. We spent approximately four months reproducing the paper, with the first two months focused on implementing all 10 methods studied in the paper and the next two months focused on reproducing the experiments in the paper and refining our implementation. Total training times for each method ranged from 35–43 hours on COCO-Stuff [2], 22–29 hours on DeepFashion [3], and 7–8 hours on Animals with Attributes [4] on a single RTX 3090 GPU. Results We found that both proposed methods in the original paper help mitigate contextual bias, although for some methods, we could not completely replicate the quantitative results in the paper even after completing an extensive hyperparameter search. For example, on COCO-Stuff, DeepFashion, and UnRel, our feature-split model achieved an increase in accuracy on out-of-context images over the standard baseline, whereas on AwA, we saw a drop in performance. For the proposed CAM-based method, we were able to reproduce the original paperʼs results to within 0.5% mAP. What was easy Overall, it was easy to follow the explanation and reasoning of the experiments. The implementation of most (7 of 10) methods was straightforward, especially after we received additional details from the original authors. What was difficult Since there was no existing code, we spent considerable time and effort re-implementing the entire pipeline from scratch and making sure that most, if not all, training/evaluation details are true to the experiments in the paper. For several methods, we went through many iterations of experiments until we were confident that our implementation was accurate. Communication with original authors We reached out to the authors several times via email to ask for clarifications and additional implementation details. The authors were very responsive to our questions, and we are extremely grateful for their detailed and timely responses.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Explaining%20Groups%20of%20Points%20in%20Low-Dimensional%20Representations                                                                                  [Re] Explaining Groups of Points in Low-Dimensional Representations                                                                                  Scope of Reproducibility In this paper we present an analysis and elaboration of [1], in which an algorithm is posed by Plumb et al. for the purpose of finding human-understandable explanations in terms of given explainable features of input data for differences between groups of points occurring in a lower-dimensional representation of that input data. Methodology We have upgraded the original code provided by the authors such that it is compatible with recent versions of popular deep learning frameworks, namely the TensorFlow 2.x- and PyTorch 1.7.x-libraries. Furthermore, we have created our own implementation of the algorithm in which we have incorporated additional experiments in order to evaluate the algorithmʼs relevance in the scope of different dimensionality reduction techniques and differently structured data. We have performed the same experiments as described in the original paper using both the upgraded version of the code and our own implementation taking the authorsʼ code and paper as references. Results The results presented in [1] were reproducible, both by using the provided code and our own implementation. Our additional experiments have highlighted several limitations of the explanatory algorithm in question: the algorithm severely relies on the shape and variance of the clusters present in the data (and, if applicable, the method used to label these clusters), and highly non-linear dimensionality reduction algorithms perform worse in terms of explainability. What was easy The authors have provided an implementation that cleanly separates different experiments on different datasets and the core functional methodology. Given a working environment, it is easy to reproduce the experiments performed in [1]. What was difficult Minor difficulties were experienced in setting up the required environment for running the code provided by Plumb et al. locally (i.e. trivial changes in the code such as the usage of absolute paths and obtaining external dependencies). Evidently, it was timeconsuming to rewrite all corresponding code, including the architecture for the variational auto-encoder provided by an external package, scvis 0.1.02. Communication with original authors No communication with the original authors was required to reproduce their work.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Explaining%20Groups%20of%20Points%20in%20Low-Dimensional%20Representations                                                                                  [Re] Explaining Groups of Points in Low-Dimensional Representations                                                                                  Scope of Reproducibility This report covers our reproduction of the paper ʼExplaining Low dimensional Representationʼ [1] by Plumb et al. In this paper, a method (Transitive Global Translations, TGT) is proposed for explaining different clusters in low dimensional representations of high dimensional data. They show their method outperforms the Difference Between the Means (DBM) method, is consistent in explaining differences with few features and matches real patterns in data. We verify these claims by reproducing their experiments and testing their method on new data. We also investigate the use of more complex transformations to explain differences between clusters. Methodology We reproduce the original experiments using their source code. We also replicate their findings by re-implementing the authorsʼ method in PyTorch [2] and evaluating on two of the dataset used in the paper and two new ones. Furthermore, we compare TGT with our own extension of TGT, which uses a larger class of transformations. Results We were able to reproduce their results using their code, yielding mostly similar results. TGT generally outperforms DBM, especially when explanations use few features. TGT is consistent in terms of the features to which it attributes cluster differences, across different sparsity levels. TGT matches real patterns in data. When extending the types of functions used for explanations, performance did not improve significantly, suggesting translations make for adequate explanations. However, the scaling extension shows promising performance on the modified synthetic data to recover the original signal. What was easy The easiest part was running the existing code with the pre-trained model files. The original authors had set up their code base in an organized manner with clear instructions. What was difficult The first difficulty that we encounter was finding the right environment. The source code depends on deprecated functionality. The clustering method they used, had to be re implemented for us to use it in our replication. Another difficulty was the selection of clusters. The authors did not prove a consistent method for selecting clusters in a latent space representation. When retraining the provided models, we get a latent space representation different to the original experiments. The clusters have to be manually selected. The metrics that they used to evaluate their explanations are also depend on the clustering. This means that there is some variability in the exact verification of reproducibility. Communication with original authors We asked the original authors for clarification on how to choose the ϵ hyper-parameter. However, it became apparent that we had misread, and the procedure is indeed adequately reported in the paper.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Hamiltonian%20Generative%20Networks                                                                                  [Re] Hamiltonian Generative Networks                                                                                  Scope of Reproducibility The main objective of the paper is to ”learn the Hamiltonian dynamics of simple physical systems from high-dimensional observations without restrictive domain assumptions”. To do so, the authors train a generative model that reconstructs an inputted sequence of images of the evolution of some physical system. For instance, they learn the dynamics of a pendulum, a body-spring system, and 2,3-bodies. In addition to these environments, we further expand the testing on two new environments and we explore architecture tweaks looking for performance gains. Methodology We implement the project with Python using Pytorch [1] as a deep learning library. Previous to ours, there was no public implementation of this work. Thus, we had to write the code of the simulated environments, the deep models, and the training process. The code can be found in this repository: https://github.com/CampusAI/HamiltonianGenerative-Networks A single training takes around 4 hours and 1910MB of GPU memory (NVIDIA GeForce RTX2080Ti). Results We found the modelʼs input-output data slightly unclear in the original paper. First, it seems that the model reconstructs the same sequence that has been inputted. Nevertheless, further discussion with the authors seems to indicate that they input the first few frames to the network and reconstructed the rest of the rollout. We test both approaches and analyze the results. We generally obtain comparable results to those of the original authors when just reconstructing the input sequence (30% average absolute relative error w.r.t. to their reported values) and worse results when trying to reconstruct unseen frames (107% error). In this report, we include our intuition on possible reasons that would explain these observations. What was easy The architecture of the model and training procedure was easy to understand from the paper. Besides, creating simulation environments similar to those of the original authors was also straightforward. What was difficult While the overall model architecture and data generation were easy to understand, we encountered the optimization to be especially tricky to perform. In particular, finding a good balance between the reconstruction loss and KL divergence loss was challenging. We implemented GECO [2] to dynamically adapt the Lagrange multiplier but it proved to be surprisingly brittle to its hyper-parameters, resulting in very unstable behavior. We were unable to identify the cause of the problem and ended up training with simpler techniques such as using a fixed Lagrange multiplier as presented in [3]. Communication with original authors We exchanged around 6 emails with doubts and answers with the original authors.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Improving%20Multi-hop%20Question%20Answering%20over%20Knowledge%20Graphs%20using%20Knowledge%20Base%20Embeddings                                                                                  [Re] Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings                                                                                  Scope of Reproducibility Our work consists of four parts: 1. Reproducing the results from [1]. 2. Exploring the effect of various knowledge graph embedding models in the Knowledge Graph Embedding module. 3. Exploring the effect of various transformer models in the Question Embedding module. 4. Verifying the importance of the Relation Matching (RM) module. Based on the code shared by the authors, we have reproduced the results for EmbedKGQA[1]. We have not performed relation matching deliberately to validate point-4. Methodology We have used the code provided by [1] with some customization for reproducibility. In addition to making the codebase more modular and easy to navigate, we have made changes to incorporate different transformers in the question embedding module. QuestionAnswering models were trained from scratch as no pre-trained models were available for our particular dataset. The code for this work is available on GitHub (See page footer for the link). Results We were able to reproduce the Hits@1 to be within ±2.4% of the reported value (in most cases). Anomalies were observed in 2 cases. 1. In MetaQA-KG-Full (3-hop) dataset. 2. WebQSP-KG-Full dataset. From our experiments on the QA model, we have found that a recent transformer architecture, SBERT[2] produced better accuracy than the original paper. Replacing RoBERTa[3] with SBERT[2] increased the absolute accuracy by ≈3.4% and ≈0.6% in the half KG and the full KG case respectively. (KG: Knowledge Graph, ”≈”: Approximately) What was easy As the code was open-sourced, we didnʼt have to implement the paper giving us the liberty to customize the codebase to focus on the authorʼs claim validation, perform extended experiments and explore shared as well as new models. In addition to this, pretrained KG embedding models were shared which helped in the reproduction experiment. What was difficult The lack of comprehensive documentation along with missing comments defining functions/classes/attributes etc. made it laborious to review the code and modify it. In addition to large training times for question answering models, the knowledge graph embeddings also required a significant amount of computing resources. Communication with original authors We had a couple of virtual meetings with Apoorv Saxena, the primary author of EmbedKGQA[1].
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Neural%20Networks%20Fail%20to%20Learn%20Periodic%20Functions%20and%20How%20to%20Fix%20It                                                                                  [Re] Neural Networks Fail to Learn Periodic Functions and How to Fix It                                                                                  Scope of Reproducibility Neural Networks Fail to Learn Periodic Functions and How to Fix It [1] demonstrates experimentally that standard activations such as ReLU, tanh, sigmoid and their variants all fail to learn how to extrapolate simple periodic functions. The original paper goes on to propose a new activation, which the authors name the snake function. The central claims of the paper are two-fold: (1) The properties of the activation functions are carried over to the neural networks. A tanh network will be smooth and extrapolates to a constant function, while ReLU extrapolates in a linear way. Standard neural networks with conventional activation functions are insufficient for extrapolating periodic functions. (2) The proposed activation function manages to learn periodic functions while being able to optimize as well as conventional activation functions. While both experimental proof and theoretical justifications are provided for the claims, we shall only be concerned with testing the claims via experimental means. Methodology While one of the authors was contacted to clarify certain difficulties, the reproduction of all experiments was completed using only the information provided in the paper. With one exception, the links to all datasets used were also provided in the original paper. This allowed us to implement most experiments from scratch. Results We were able to successfully replicate experiments supporting the central claim of the paper, that the proposed snake non-linearity can learn periodic functions. We also analyze the suitability of the snake activation for other tasks like generative modeling and sentiment analysis. What was easy Many experiments included descriptions of the neural network architectures and graphs showcasing performance, giving us a clear benchmark to compare our results against. Links to datasets for all experiments, barring one, were also included in the paper itself. What was difficult Data for the human body temperature experiment was not available. Proper implementation details were not given for initializing the weights in neural networks with snake and using snake with RNNs. Communication with original authors Liu Ziyin, one of the authors, was contacted to provide the dataset used for the human body temperature experiment, elaborate upon the implementation of variance correction and provide the implementation of RNNs using snake. Liu provided the GitHub link to the authorsʼ original code for the human body temperature, market index, and extrapolation experiments. Liu also provided an explanation on how to implement variance correction. While the code for the RNN implementation using the snake activation was not made public, a screenshot of the same was provided. We thank the authors for their assistance.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20On%20end-to-end%206%7BDoF%7D%20object%20pose%20estimation%20and%20robustness%20to%20object%20scale                                                                                  [Re] On end-to-end 6{DoF} object pose estimation and robustness to object scale                                                                                  Scope of Reproducibility This report contains a set of experiments that seek to reproduce the claims of two recent works related to keypoint estimation, one specific to 6DoF object pose estimation, and the other presenting a generic architectural improvement for keypoint estimation but demonstrated in human pose estimation. More specifically, in the backpropagatable PnP [1], the authors claim that incorporating geometric optimization in a deep-learning pipeline and predicting an objectʼs pose in an end-to-end manner yields improved performance. On the other hand, HigherHRNet [2] introduces a novel heatmap aggregation method that allows for scale-aware pose estimations, offering higher keypoint localization accuracy for small scale objects. Methodology We used the publicly provided code where available, adapting it to fit into a model development kit to facilitate our experiments. We used a dataset fit for validating both claims simultaneously, and designed a set of experiments based on the published methodologies, but also went beyond seeking to validate the higher level concepts. Our experiments were conducted on a Nvidia 2080 12 GB GPU with an average training time of 14 hours. Results We reproduce the claims of both papers by conducting several experiments in the UAVA dataset [3]. The integration of a differentiable geometric module within an keypointbased object pose estimation model improved its performance in metrics. We additionally verify that this is the case for other differentiable PnP implementations (i.e. EPnP). Further, our results indicate that indeed HigherHRNet improves keypoint localisation performance on small scale objects. What was easy Both papers provided publicly available implementations. In addition, many different variations were also found online. Finally, the papers themselves were very clearly written, offering insights on various important details. What was difficult The main issue that required more effort was identifying the appropriate weights for BPnP [1] in order to balance the different optimization objectives. As expected, this varies for the context that it is applied (task, dataset) and the values presented in the paper did not work in our case. Sub-optimal selection of weights leads to convergence issues. Communication with original authors We communicated with the authors of [1] through GitHub, and we would like to thank them as they provided a fast and detailed response. Furthermore, their responsiveness to past issues had already provided a nice knowledge base regarding reproduction.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Parameterized%20Explainer%20for%20Graph%20Neural%20Network                                                                                  [Re] Parameterized Explainer for Graph Neural Network                                                                                  Scope of Reproducibility In this work we perform a replication study of the paper Parameterized Explainer for Graph Neural Network. The replication experiment focuses on three main claims: (1) Is it possible to reimplement the proposed method in a different framework? (2) Do the main claims with respect to the GNNExplainer hold? (3) Is the used evaluation method a valid method for explaining the classification decision by Graph Neural Networks? Methodology The authorsʼ TensorFlow code was largely used as starting point for our reimplementation in PyTorch. However, large parts of the evaluation setup were missing and differences were found between the listed configurations in the paper and the code. As a result, our codebase contains a large portion of novel code and introduces a different method for tracking experimental configurations. Using the new codebase all experiments are replicated. In addition to this, a short ablation study is performed. Results Due to numerous inconsistencies between code and paper, it is not possible to replicate the original results using the paper alone. With help of the original codebase, a number of the original results can be retrieved. The main comparison claim of the paper, to improve over the preceding GNNExplainer, does hold. However, after performing the replication experiments, some questions regarding the validity of the used evaluation setup in the original paper remain. What was easy The method proposed by the authors for explaining the Graph Neural Networks is easy to comprehend and intuitive. Re-implementation of the method is straightforward using a modern deep learning framework. The datasets used for the experimental setup were all provided together with their codebase. What was difficult The main difficulty arose from the difference between the experimental configurations discussed in the paper and implemented in the code. There were a number of small inconsistencies (eg. incorrect hyperparameter settings), but also some major ones (eg. using batch-normalization in training mode during evaluation). This issue was worsened by the fractured reporting of configurations in the code. Communication with original authors Contact was made with the authors on two occasions. During the first exchange the authors confirmed a number of clarifying questions and confirmed that the configurations as presented in the codebase were to be used instead of those provided in the paper. In the second exchange our reservations concerning the used experimental evaluation were conveyed to the authors. The authors did not share our concerns.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Reimplementation%20of%20FixMatch%20and%20Investigation%20on%20Noisy%20%28Pseudo%29%20Labels%20and%20Confirmation%20Errors%20of%20FixMatch                                                                                  [Re] Reimplementation of FixMatch and Investigation on Noisy (Pseudo) Labels and Confirmation Errors of FixMatch                                                                                  FixMatch is a semi-supervised learning method, which achieves comparable results with fully supervised learning by leveraging a limited number of labeled data (pseudo labelling technique) and taking a good use of the unlabeled data (consistency regularization ). In this work, we reimplement FixMatch and achieve reasonably comparable performance with the official implementation, which supports that FixMatch outperforms semi-superivesed learning benchmarks and demonstrates that the authorʼs choices with respect to those ablations were experimentally sound. Next, we investigate the existence of a major problem of FixMatch, confirmation errors, by reconstructing the batch structure during the training process. It reveals existing confirmation errors, especially the ones caused by asymmetric noise in pseudo labels. To deal with the problem, we apply equal-frequency and confidence entropy regularization to the labeled data and add them in the loss function. Our experimental results on CIFAR-10 show that using either of the entropy regularization in the loss function can reduce the asymmetric noise in pseudo labels and improve the performance of FixMatch in the presence of (pseudo) labels containing (asymmetric) noise. Our code is available at the url: https://github.com/Celiali/FixMatch.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Replication%20Study%20of%20%27Generative%20causal%20explanations%20of%20black-box%20classifiers%27                                                                                  [Re] Replication Study of 'Generative causal explanations of black-box classifiers'                                                                                  Scope of Reproducibility We verify the outcome of the methodology proposed in the article, which attempts to provide post-hoc causal explanations for black-box classifiers through causal reference. This is achieved by replicating the code step by step, according to the descriptions in the paper. All the claims in the paper have been examined, and we provide additional metric to evaluate the portability, expressive power, algorithmic complexity and the data fidelity of their framework. We have further extended their analyses to consider all benchmark datasets used, confirming results. Methodology We use the same architecture and (hyper)parameters for replication. However, the code has a different structure and we provide a more efficient implementation for the measure of information flow. In addition, Algorithm 1 in the original paper is not implemented in their repository, so we have also implemented Algorithm 1 ourselves and further extend their framework to another domain (text data), although unsuccessfully. Furthermore, we make a detailed table in the paper to show the time used to produce the results for different experiments reproduced. All models were trained on Nvidia GeForce GTX 1080 GPUs provided by Surfsaraʼs LISA cluster computing service at university of Amsterdam1. Results We reproduced the framework in the original paper and verified the main claims made by the authors in the original paper. However, the GCE model in extension study did not manage to separate causal factors and non-causal factors for a text classifier due to the complexity of fine-tuning the model. What was easy The original paper comes with extensive appendices, many of which contain crucial details for implementation and understanding of the intended function. The authors provide code for most of the experiments presented in the paper. Although at the beginning their code repository was not functional, we use it as a reference to re-implement our code. The author also updated their code two weeks after we start our own implementation, which made it easy for us to verify the correctness of our re-implementation. What was difficult The codebase the authors provided was initially unusable, with missing or renamed imports, hardcoded filepaths and an all-around convoluted structure. Additionally, the description of Algorithm 1 is quite vague and no implementation of it was given. Beyond this, computational expense was a serious issue, given the need for inefficient training steps, and re-iterating training several times for hyperparameter search. Communication with original authors This reproducibility study is part of a course on fairness, accountability, confidentiality and transparency in AI. Since it is a course project where we interacted with other group in the forum, and another group also working with this paper has reached out to the authors about problems with the initial repository, we did not find necessary to do it again.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Reproducibility%20report%20of%20%27Interpretable%20Complex-Valued%20Neural%20Networks%20for%20Privacy%20Protection%27                                                                                  [Re] Reproducibility report of 'Interpretable Complex-Valued Neural Networks for Privacy Protection'                                                                                  Scope of Reproducibility The original work by Xiang et al.1 claimed (1) that complex-valued DNNs effectively increase the difficulty of inferring inputs for the adversary attacks compared to the baseline. In addition, Xiang et al.1 stated that the (2) proposed privacy-protecting complexvalued DNN effectively preserves the accuracy when compared to the baseline. Methodology Since the original paperʼs code was not published, all of the codebase was written independently from scratch, based solemnly on how it was described in the paper. We mostly used a Nvidiaʼs RTX 2060 Super as the GPU and a AMD Ryzen 3600x as the CPU. The runtime of each model was highly dependant on the architecture used. The runtimes for each model can be found in Table 2. Results In contrast to the first claim, we have discovered that for most of the architectures, reconstruction errors for the attacks are quite low, which means that in our models the first claim is not supported. We also found that for most of the models, the classification error is somewhat higher than those provided in the paper. However, these indeed relate to the original work and partially support the second claim of the authors. What was easy Authors of the original paper utilized famous architectures for some of architecturesʼ parts, such as ResNet and LeNet, that were well explained and defined in the literature. In addition, authors, provided formulas on the modified rotation-invariant Complex DNN modules (ReLU, max pooling etc.), implementation of which was relatively straightforward. The paper was based on the openly available datasets. What was difficult The paper did not provide any information on the architecture of the critic for the WGAN, along with the architecture of the angle discriminator utilized in inversion attack 1. It also does not provide any information about crucial hyperparameters, such as the k value used for k-anonimity. Communication with original authors We did not contact the original authors of the publication.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Reproducibility%20study%20-%20Does%20enforcing%20diversity%20in%20hidden%20states%20of%20LSTM-Attention%20models%20improve%20transparency%3F                                                                                  [Re] Reproducibility study - Does enforcing diversity in hidden states of LSTM-Attention models improve transparency?                                                                                  Scope of Reproducibility For this reproducibility study, we focus on the main claims made in this paper: • The attention weights in standard LSTM attention models do not provide faithful and plausible explanations for its predictions. This is potentially because the conicity of the LSTM hidden vectors is high. • Two methods can be applied to reduce conicity: Orthogonalization and Diversity Driven Training. When applying these methods, the resulting attention weights offer more faithful and plausible explanations of the modelʼs predictions, without sacrificing model performance. Methodology The paper includes a link to a repository with the code used to generate its results. We follow four investigative routes: (i) Replication: we rerun experiments on datasets from the paper in order to replicate the results, and add the results that are missing in the paper; (ii) Code review: we scrutinize the code to validate its correctness; (iii) Evaluation methodology: we extend the set of evaluation metrics used in the paper with the LIME method, in an attempt to resolve inconclusive results; (iv) Generalization to other architectures: we test whether the authorsʼ claims apply to variations of the base model (more complex forms of attention and a BiLSTM encoder). Results We confirm that the Orthogonal and Diversity LSTM achieve similar accuracies as the Vanilla LSTM, while lowering conicity. However, we cannot reproduce the results of several of the experiments in the paper that underlie their claim of better transparency. In addition, a close inspection of the code base reveals some potentially problematic inconsistencies. Despite this, under certain conditions, we do confirm that the Orthogonal and Diversity LSTM can be useful methods to increase transparency. How to formulate these conditions more generally remains unclear and deserves further research. The single input sequence tasks appear to benefit most from the methods. For these tasks, the attention mechanism does not play a critical role for achieving performance. What was easy/difficult The codebase of the authors is accessible and can be run easily, with good facilities to prepare datasets and define configurations. The Orthogonalization and Diversity Training methods are well explained in the paper and mostly cleanly implemented. The larger datasets (Amazon and CNN) are difficult to run due to memory requirements and compute times. The codebase can be hard to navigate, a consequence of the choice to accommodate a large variation of models and datasets in one framework. Communication with original authors We reached out to the authors on a fundamental but unexplained choice in the model architecture but unfortunately did not hear back before the deadline of our assignment. 
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Reproducing%20%27Identifying%20through%20flows%20for%20recovering%20latent%20representations%27                                                                                  [Re] Reproducing 'Identifying through flows for recovering latent representations'                                                                                  Scope of Reproducibility The authors claim to introduce a model for recovering the latent representation of observed data, that outperforms the state-of-the-art method for this task; namely the iVAE model. They claim that iFlow outperforms iVAE both in preservation of the original geometry of source manifold and correlation per dimension of the latent space. Methodology To reproduce the results of the paper, the main experiments are reproduced and the figures are recreated. To do so, we largely worked with code from the repository belonging to the original paper. We added plotting functionality as well as various bug fixes and optimisations. Additionally, attempts were made to improve the iVAE by making it more complex and fixing a mistake in its implementation.We also tried to investigate possible correlation between the structure of the dataset and the performance. All code used is publicly available at https://github.com/HiddeLekanne/Reproducibility-Challenge-iFlow. Results The obtained mean and standard deviation of the MCC over 100 seeds are within 1 percent of the results reported in the paper. The iFlow model obtained a mean MCC score of 0.718 (0.067). Efforts to improve and correct the baseline increased the mean MCC score from 0.483 (0.059) to 0.556 (0.061). The performance, however, remains worse than the performance of iFlow, further supporting the authorsʼ claim that the iFlow implementation is correct and more effective than iVAE. What was easy The GitHub repository associated with the paper provided most necessary code and ran with only minor changes. The code included all model implementations and data generation. The script that was used to obtain results was provided, which allowed us to determine which exact hyperparameters were used with experiments on the iFlow models. Overall, the code was well organised and the structure was easy to follow. What was difficult The specific versions of the Python libraries used were unknown, which made it infeasible to achieve the exact results from the paper when running on the same seeds. The code used to create figures 1-3 in the original paper was missing and had to be recreated. Furthermore, the long time the models needed to train made experimentation with e.g., different hyperparameters challenging. Finally, the code was largely undocumented. Communication with original authors Communication with the authors was attempted but could not be established.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Reproducing%20Learning%20to%20Deceive%20With%20Attention-Based%20Explanations                                                                                  [Re] Reproducing Learning to Deceive With Attention-Based Explanations                                                                                  Scope of Reproducibility Based on the intuition that attention in neural networks is what the model focuses on, attention is now being used as an explanation for a modelsʼ prediction (see Galassi, Lippi, and Torroni1 for a survey). Pruthi et al.2 challenge the usage of attention-based explanation through a series of experiments using classification and sequence-to-sequence (seq2seq) models. They examine the modelʼs use of impermissible tokens, which are user-defined tokens that can introduce bias e.g. gendered pronouns. Across multiple datasets, the authors show that with the impermissible tokens removed the model accuracy drops, implying their usage in prediction. And then by penalising attention paid to the impermissible tokens but keeping them in, they train models that retain full accuracy hence must be using the impermissible tokens, but that does not show attention being paid to the impermissible tokens. As the paperʼs claims have such significant implications for the use of attention-based explanations, we seek to reproduce their results. Methodology Using the authorsʼ code, for classifiers we attempt to reproduce their embedding, BiLSTM, and BERT results across the occupation prediction, gender identify, and SST + wiki datasets. Further, we reimplemented BERT using HuggingFaceʼs transformer library [3] with restricted self-attention (information cannot flow between permissible and impermissible tokens). For seq2seq we used the authorsʼ code to reproduce results across Bigram Flip, Sequence Copy, Sequence Reverse, and English-German (En-De) machine translation datasets. We performed refactoring on the authorsʼ code aiming toward a more uniformly usable code style as well as porting across to PyTorch Lightning. All experiments were run in approximately 130 GPU hours on a computing cluster with nodes containing Titan RTX GPUs. Results We reproduced the authorsʼ results across all models and all available datasets, confirming their findings that attention-based explanations can be manipulated and that mod els can learn to deceive. We also replicated their BERT results using our reimplemented model. There was only one result not as strongly (> 1 S.D.) in their experimental direction. What Was Easy The authorsʼ methods were largely well described and easy to follow, and we could quickly produce the first results as their code worked straightaway with minor adjustments. They were also extremely responsive and helpful via email. What Was Difficult Re-implementing the BERT-based classification model to perform replicability, with further specification details on model architecture, penalty mechanism, and training procedure needed. Also, porting code across to PyTorch Lightning. Communication With Original Authors There was a continuous email chain with the authors for several weeks during the reproducibility work. They made additional code and datasets available per our requests, along with providing detailed responses and clarifications to our emailed questions. They encouraged the work and we wish to thank them for their time and support.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Rigging%20the%20Lottery%3A%20Making%20All%20Tickets%20Winners                                                                                  [Re] Rigging the Lottery: Making All Tickets Winners                                                                                  Scope of Reproducibility For a fixed parameter count and compute budget, the proposed algorithm (RigL) claims to directly train sparse networks that match or exceed the performance of existing denseto-sparse training techniques (such as pruning). RigL does so while requiring constant Floating Point Operations (FLOPs) throughout training. The technique obtains state-ofthe-art performance on a variety of tasks, including image classification and characterlevel language-modelling. Methodology We implement RigL from scratch in Pytorch using boolean masks to simulate unstructured sparsity. We rely on the description provided in the original paper, and referred to the authorsʼ code for only specific implementation detail such as handling overflow in ERK initialization. We evaluate sparse training using RigL for WideResNet-22-2 on CIFAR-10 and ResNet-50 on CIFAR-100, requiring 2 hours and 6 hours respectively per training run on a GTX 1080 GPU. Results We reproduce RigLʼs performance on CIFAR-10 within 0.1% of the reported value. On both CIFAR-10/100, the central claim holds—given a fixed training budget, RigL surpasses existing dynamic-sparse training methods over a range of target sparsities. By training longer, the performance can match or exceed iterative pruning, while consuming constant FLOPs throughout training. We also show that there is little benefit in tuning RigLʼs hyper-parameters for every sparsity, initialization pair—the reference choice of hyperparameters is often close to optimal performance. Going beyond the original paper, we find that the optimal initialization scheme depends on the training constraint. While the Erdos-Renyi-Kernel distribution outperforms Random distribution for a fixed parameter count, for a fixed FLOP count, the latter performs better. Finally, redistributing layer-wise sparsity while training can bridge the performance gap between the two initialization schemes, but increases computational cost. What was easy The authors provide code for most of the experiments presented in the paper. The code was easy to run and allowed us to verify the correctness of our re-implementation. The paper also provided a thorough and clear description of the proposed algorithm without any obvious errors or confusing exposition. What was difficult Tuning hyperparameters involved multiple random seeds and took longer than anticipated. Verifying the correctness of a few baselines was tricky and required ensuring that the optimizerʼs gradient (or momentum) buffers were sparse (or dense) as specified by the algorithm. Compute limits restricted us from evaluating on larger datasets such as Imagenet. Communication with original authors We had responsive communication with the original authors, which helped clarify a few implementation and evaluation details, particularly regarding the FLOP counting procedure.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Satellite%20Image%20Time%20Series%20Classification%20with%20Pixel-Set%20Encoders%20and%20Temporal%20Self-Attention                                                                                  [Re] Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention                                                                                  Scope of Reproducibility The evaluated paper presents a method to classify crop types from multispectral satellite image time series with a newly developed pixel-set encoder and an adaption of the Transformer [2], called temporal attention encoder. Methodology In order to assess both the architecture and the performance of the approach, we first attempted to implement the method from scratch, followed by a study of the authorsʼ openly provided code. Additionally, we also compiled an alternative dataset similar to the one presented in the paper and evaluated the methodology on it. Results During the study, we were not able to reproduce the method due to a conceptual misinterpretation of ours regarding the authorsʼ adaption of the Transformer [2]. However, the publicly available implementation helped us answering our questions and proved its validity during our experiments on different datasets. Additionally, we compared the papersʼ temporal attention encoder to our adaption of it, which we came across while we were trying to reimplement and grasp the authorsʼ ideas. What was easy Running the provided code and obtaining the presented dataset turned out to be easily possible. Even adapting the method to our own ideas did not cause issues, due to a well documented and clear implementation. What was difficult Reimplementing the approach from scratch turned out to be harder than expected, especially because we had a certain type of architecture in mind that did not fit the dimensions of the layers mentioned in the paper. Furthermore, knowing how the dataset was exactly assembled would have been beneficial for us, as we tried to retrace these steps, and therefore would have made the results on our dataset easier to compare to the ones from the paper. Communication with original authors While working on the challenge, we stood in E-mail contact with the first and second author, had two online meetings and got feedback to our implementation on GITHUB. Additionally, one of the authors of the Transformer paper [2] provided us with further answers regarding their modelsʼ architecture. 
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Spatial-Adaptive%20Network%20for%20Single%20Image%20Denoising                                                                                  [Re] Spatial-Adaptive Network for Single Image Denoising                                                                                  Scope of Reproducibility The original paper proposes an encoder-decoder structure exploiting a residual spatialadaptive block and a context block to capture multi-scale information for achieving the state-of-the-art on real and synthetic noise removal. Methodology We have implemented the model, namely SADNet, from scratch in PyTorch as described in the paper, and also adopted the training loop and proposed blocks from the authorʼs code. Since the weight initialization of proposed blocks was not implicitly defined in the paper, we have decided to use the default initialization method for convolutional layers in PyTorch (i.e. Kaiming). Experiments have been completed on a single RTX 2080 Ti in 3 days for each, and it requires ∼3GB GPU memory for training, and ∼8GB CPU memory for loading the data, due to the file structure of datasets. Results We have achieved to reproduce the results qualitatively and quantitatively on synthetic and noise removal tasks. SADNet has the capacity to learn to remove the synthetic and real noise in images, and it produces visually-plausible outputs even after a few epochs. Moreover, we have employed SSIM and PSNR metrics to measure the quantitative performance for all settings. The quantitative results on both tasks are on-par when compared to the reported results in the paper. What was easy The code was open-source, and implemented in PyTorch, hence adopting the training loop and proposed blocks to our implementation facilitated our reproduction study. The loss function is straightforward and the architecture has a U-Net-like structure, so that we could achieve to implement the architecture in a fair time. What was difficult Due to the lack of compatibility with the current versions of PyTorch and TorchVision and the dependency on an external CUDA implementation of deformable convolutions, we have encountered several issues during our implementation. Then, we have considered to re-implement residual spatial-adaptive block and context block from scratch for deferring these dependencies, however, we could not achieve it just by referring to the paper in limited time. Therefore, we have decided to directly use the provided blocks as in the authorʼs code. Communication with original authors We did not make any contact with the authors since we achieved to solve the issues encountered during the implementation of SADNet by examining the authorʼs code.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Training%20Binary%20Neural%20Networks%20using%20the%20Bayesian%20Learning%20Rule                                                                                  [Re] Training Binary Neural Networks using the Bayesian Learning Rule                                                                                  Scope of Reproducibility We try to verify the performance of our re-implementation of the BayesBiNN optimizer on various classification and regression benchmarks. We also implemented the STE optimizer which was the central baseline model used in the paper. Finally, we tried to evaluate the results of BayesBiNN on the continual learning benchmark to get a better insight. Methodology We developed our separate code-base, consisting of an end-to-end trainer with a Keraslike interface, for the reproduction which includes the implementation of the BayesBiNN and STE optimizer. We did refer to the authorʼs code open-sourced on GitHub to get some insights about the hyperparameters and other doubts that emerged during code development. Results We reproduced the accuracy of the BayesBiNN optimizer within less than 0.5% of the originally reported value, which upholds the conclusion that it performs nearly as well as its full precision counterpart in classification tasks. When we tried this in a semantic segmentation context, we found that the results were very underwhelming and in contrast with the seemingly good results by the STE optimizer even with much hyperparameter tuning. We can conclude that, like other Bayesian methods, it is difficult to train BayesBiNN on more complex tasks. What was easy After we worked out the mathematics behind the BayesBiNN approach, we developed a pseudo-code for the optimization process which along with references from the authorʼs code, helped us a lot in our reproduction study. What was difficult Some of the hyperparameters were not mentioned by the authors in their paper so it was difficult to approximate the values of those parameters. The lack of resources was the next big difficulty that we faced. Communication with original authors We had a very fruitful conversation with the authors, which helped us in better understanding the BayesBiNN approach and its extension to the segmentation domain. The detailed pointers are given at the end of this report.
http://w3id.org/mlsea/pwc/scientificWork/%5BRe%5D%20Warm-Starting%20Neural%20Network%20Training                                                                                  [Re] Warm-Starting Neural Network Training                                                                                  Scope of Reproducibility We reproduce the results of the paper ”On Warm-Starting Neural Network Training.” In many real-world applications, the training data is not readily available and is accumulated over time. As training models from scratch is a time-consuming task, it is preferred to use warm-starting, i.e., using the already existing models as the starting point to obtain faster convergence. This paper investigates the effect of warm-starting on the final modelʼs performance. It identifies a noticeable gap between warm-started and randomly-initialized models, hereafter referenced as the warm-starting gap. Furthermore, they provide a solution to mitigate this side-effect. In addition to reproducing the original paperʼs results, we propose an alternative solution and assess its effectiveness. Methodology We reproduced almost every figure and table in the main text and some of those in the appendix. We used our implementation to produce these results. In case of a mismatch of the results, we also investigated the cause and proposed possible explanations. We mainly used GPUs to train our models using infrastructure offered by public clouds and those that were available to us privately. Results Most of our results closely match the reported results in the original paper. Therefore, we confirm that the warm-starting gap exists in certain settings and that the ShrinkPerturb method successfully reduces or eliminates this gap. However, in some cases, we were not able to completely reproduce their results. By investigating the root of such mismatches, we provide another solution to avoid this gap. In particular, we show that data augmentation also helps to reduce the warm-starting gap. What was easy The experiments described in the paper were based on regular training of neural networks on a portion of widely-used datasets, possibly from a pre-trained model. Therefore implementing each experiment was relatively easy to do. Furthermore, since many of the parameters were reported in the original paper, we did not need much tuning in most experiments. Finally, it is straightforward to implement and use the proposed solution. What was difficult Though implementing each experiment is relatively simple, the numerosity of experiments proved to be slightly challenging. In particular, each of the online experiments in the original setting requires training a deep network to convergence more than 30 times. In these cases, we sometimes changed the settings, sacrificing granularity to reduce computation time. However, these changes did not affect the interpretability of the final results. Communication with original authors We briefly communicated with the authors to clarify the experimentsʼ details, such as the convergence conditions.
http://w3id.org/mlsea/pwc/scientificWork/%60%60Are%20you%20kidding%20me%3F%27%27%3A%20Detecting%20Unpalatable%20Questions%20on%20Reddit                                                                                  ``Are you kidding me?'': Detecting Unpalatable Questions on Reddit                                                                                  Abusive language in online discourse negatively affects a large number of social media users. Many computational methods have been proposed to address this issue of online abuse. The existing work, however, tends to focus on detecting the more explicit forms of abuse leaving the subtler forms of abuse largely untouched. Our work addresses this gap by making three core contributions. First, inspired by the theory of impoliteness, we propose a novel task of detecting a subtler form of abuse, namely unpalatable questions. Second, we publish a context-aware dataset for the task using data from a diverse set of Reddit communities. Third, we implement a wide array of learning models and also investigate the benefits of incorporating conversational context into computational models. Our results show that modeling subtle abuse is feasible but difficult due to the language involved being highly nuanced and context-sensitive. We hope that future research in the field will address such subtle forms of abuse since their harm currently passes unnoticed through existing detection systems.
http://w3id.org/mlsea/pwc/scientificWork/%60%60I%27m%20Not%20Mad%27%27%3A%20Commonsense%20Implications%20of%20Negation%20and%20Contradiction                                                                                  ``I'm Not Mad'': Commonsense Implications of Negation and Contradiction                                                                                  Natural language inference requires reasoning about contradictions, negations, and their commonsense implications. Given a simple premise (e.g., {``}I{'}m mad at you{''}), humans can reason about the varying shades of contradictory statements ranging from straightforward negations ({``}I{'}m not mad at you{''}) to commonsense contradictions ({``}I{'}m happy{''}). Moreover, these negated or contradictory statements shift the commonsense implications of the original premise in interesting and nontrivial ways. For example, while {``}I{'}m mad{''} implies {``}I{'}m unhappy about something,{''} negating the premise does not necessarily negate the corresponding commonsense implications. In this paper, we present the first comprehensive study focusing on commonsense implications of negated statements and contradictions. We introduce ANION, a new commonsense knowledge graph with 624K if-then rules focusing on negated and contradictory events. We then present joint generative and discriminative inference models for this new resource, providing novel empirical insights on how logical negations and commonsense contradictions reshape the commonsense implications of their original premises.
http://w3id.org/mlsea/pwc/scientificWork/%60%60Killing%20Me%27%27%20Is%20Not%20a%20Spoiler%3A%20Spoiler%20Detection%20Model%20using%20Graph%20Neural%20Networks%20with%20Dependency%20Relation-Aware%20Attention%20Mechanism                                                                                  ``Killing Me'' Is Not a Spoiler: Spoiler Detection Model using Graph Neural Networks with Dependency Relation-Aware Attention Mechanism                                                                                  Several machine learning-based spoiler detection models have been proposed recently to protect users from spoilers on review websites. Although dependency relations between context words are important for detecting spoilers, current attention-based spoiler detection models are insufficient for utilizing dependency relations. To address this problem, we propose a new spoiler detection model called SDGNN that is based on syntax-aware graph neural networks. In the experiments on two real-world benchmark datasets, we show that our SDGNN outperforms the existing spoiler detection models.
http://w3id.org/mlsea/pwc/scientificWork/%60%60Laughing%20at%20you%20or%20with%20you%27%27%3A%20The%20Role%20of%20Sarcasm%20in%20Shaping%20the%20Disagreement%20Space                                                                                  ``Laughing at you or with you'': The Role of Sarcasm in Shaping the Disagreement Space                                                                                  Detecting arguments in online interactions is useful to understand how conflicts arise and get resolved. Users often use figurative language, such as sarcasm, either as persuasive devices or to attack the opponent by an ad hominem argument. To further our understanding of the role of sarcasm in shaping the disagreement space, we present a thorough experimental setup using a corpus annotated with both argumentative moves (agree/disagree) and sarcasm. We exploit joint modeling in terms of (a) applying discrete features that are useful in detecting sarcasm to the task of argumentative relation classification (agree/disagree/none), and (b) multitask learning for argumentative relation classification and sarcasm detection using deep learning architectures (e.g., dual Long Short-Term Memory (LSTM) with hierarchical attention and Transformer-based architectures). We demonstrate that modeling sarcasm improves the argumentative relation classification task (agree/disagree/none) in all setups.
http://w3id.org/mlsea/pwc/scientificWork/%60%60LazImpa%27%27%3A%20Lazy%20and%20Impatient%20neural%20agents%20learn%20to%20communicate%20efficiently                                                                                  ``LazImpa'': Lazy and Impatient neural agents learn to communicate efficiently                                                                                  Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes. This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length. Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation (ZLA) observed in all natural languages. Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified. We hence introduce a new communication system, {``}LazImpa{''}, where the speaker is made increasingly lazy, i.e., avoids long messages, and the listener impatient, i.e., seeks to guess the intended content as soon as possible.
http://w3id.org/mlsea/pwc/scientificWork/%60%60Nice%20Try%2C%20Kiddo%27%27%3A%20Investigating%20Ad%20Hominems%20in%20Dialogue%20Responses                                                                                  ``Nice Try, Kiddo'': Investigating Ad Hominems in Dialogue Responses                                                                                  Ad hominem attacks are those that target some feature of a person{'}s character instead of the position the person is maintaining. These attacks are harmful because they propagate implicit biases and diminish a person{'}s credibility. Since dialogue systems respond directly to user input, it is important to study ad hominems in dialogue responses. To this end, we propose categories of ad hominems, compose an annotated dataset, and build a classifier to analyze human and dialogue system responses to English Twitter posts. We specifically compare responses to Twitter topics about marginalized communities ({ #}BlackLivesMatter, { #}MeToo) versus other topics ({ #}Vegan, { #}WFH), because the abusive language of ad hominems could further amplify the skew of power away from marginalized populations. Furthermore, we propose a constrained decoding technique that uses salient n-gram similarity as a soft constraint for top-k sampling to reduce the amount of ad hominems generated. Our results indicate that 1) responses from both humans and DialoGPT contain more ad hominems for discussions around marginalized communities, 2) different quantities of ad hominems in the training data can influence the likelihood of generating ad hominems, and 3) we can use constrained decoding techniques to reduce ad hominems in generated dialogue responses.
http://w3id.org/mlsea/pwc/scientificWork/%60%60Talk%20to%20me%20with%20left%2C%20right%2C%20and%20angles%27%27%3A%20Lexical%20entrainment%20in%20spoken%20Hebrew%20dialogue                                                                                  ``Talk to me with left, right, and angles'': Lexical entrainment in spoken Hebrew dialogue                                                                                  It has been well-documented for several languages that human interlocutors tend to adapt their linguistic productions to become more similar to each other. This behavior, known as entrainment, affects lexical choice as well, both with regard to specific words, such as referring expressions, and overall style. We offer what we believe to be the first investigation of such lexical entrainment in Hebrew. Using two existing measures, we analyze Hebrew speakers interacting in a Map Task, a popular experimental setup, and find rich evidence of lexical entrainment. Analyzing speaker pairs by the combination of their genders as well as speakers by their individual gender, we find no clear pattern of differences. We do, however, find that speakers in a position of less power entrain more than those with greater power, which matches theoretical accounts. Overall, our results mostly accord with those for American English, with a lack of entrainment on hedge words being the main difference.
http://w3id.org/mlsea/pwc/scientificWork/%60%60where%20is%20this%20relationship%20going%3F%27%27%3A%20Understanding%20Relationship%20Trajectories%20in%20Narrative%20Text                                                                                  ``where is this relationship going?'': Understanding Relationship Trajectories in Narrative Text                                                                                  We examine a new commonsense reasoning task: given a narrative describing a social interaction that centers on two protagonists, systems make inferences about the underlying relationship trajectory. Specifically, we propose two evaluation tasks: Relationship Outlook Prediction MCQ and Resolution Prediction MCQ. In Relationship Outlook Prediction, a system maps an interaction to a relationship outlook that captures how the interaction is expected to change the relationship. In Resolution Prediction, a system attributes a given relationship outlook to a particular resolution that explains the outcome. These two tasks parallel two real-life questions that people frequently ponder upon as they navigate different social situations: {``}where is this relationship going?{''} and {``}how did we end up here?{''}. To facilitate the investigation of human social relationships through these two tasks, we construct a new dataset, Social Narrative Tree, which consists of 1250 stories documenting a variety of daily social interactions. The narratives encode a multitude of social elements that interweave to give rise to rich commonsense knowledge of how relationships evolve with respect to social interactions. We establish baseline performances using language models and the accuracies are significantly lower than human performance. The results demonstrate that models need to look beyond syntactic and semantic signals to comprehend complex human relationships.
http://w3id.org/mlsea/pwc/scientificWork/%60Just%20because%20you%20are%20right%2C%20doesn%27t%20mean%20I%20am%20wrong%27%3A%20Overcoming%20a%20bottleneck%20in%20development%20and%20evaluation%20of%20Open-Ended%20VQA%20tasks                                                                                  `Just because you are right, doesn't mean I am wrong': Overcoming a bottleneck in development and evaluation of Open-Ended VQA tasks                                                                                  GQA (CITATION) is a dataset for real-world visual reasoning and compositional question answering. We found that many answers predicted by the best vision-language models on the GQA dataset do not match the ground-truth answer but still are semantically meaningful and correct in the given context. In fact, this is the case with most existing visual question answering (VQA) datasets where they assume only one ground-truth answer for each question. We propose Alternative Answer Sets (AAS) of ground-truth answers to address this limitation, which is created automatically using off-the-shelf NLP tools. We introduce a semantic metric based on AAS and modify top VQA solvers to support multiple plausible answers for a question. We implement this approach on the GQA dataset and show the performance improvements.
http://w3id.org/mlsea/pwc/scientificWork/%CE%B4-CLUE%3A%20Diverse%20Sets%20of%20Explanations%20for%20Uncertainty%20Estimates                                                                                  δ-CLUE: Diverse Sets of Explanations for Uncertainty Estimates                                                                                  To interpret uncertainty estimates from differentiable probabilistic models, recent work has proposed generating Counterfactual Latent Uncertainty Explanations (CLUEs). However, for a single input, such approaches could output a variety of explanations due to the lack of constraints placed on the explanation. Here we augment the original CLUE approach, to provide what we call $ delta$-CLUE. CLUE indicates $ it{one}$ way to change an input, while remaining on the data manifold, such that the model becomes more confident about its prediction. We instead return a $ it{set}$ of plausible CLUEs: multiple, diverse inputs that are within a $ delta$ ball of the original input in latent space, all yielding confident predictions.
http://w3id.org/mlsea/pwc/scientificWork/%E2%80%9C%E9%95%BF%E8%B5%90%E5%8F%B7%E2%80%9D%E8%BD%AE%E8%88%B9%E5%8D%A1%E4%BD%8F%E8%8B%8F%E4%BC%8A%E5%A3%AB%E8%BF%90%E6%B2%B3%2C%2014%E8%89%98%E6%8B%96%E8%88%B9%E9%83%BD%E6%97%A0%E6%B3%95%E6%8B%96%E5%87%BA%EF%BC%8C%E2%80%9C%E7%BD%AA%E9%AD%81%E7%A5%B8%E9%A6%96%E2%80%9D%E5%B1%85%E7%84%B6%E6%98%AF%E5%AE%83%EF%BC%9F%E7%A7%91%E5%AD%A6%E5%8D%9A%E5%A3%AB                                                                                  “长赐号”轮船卡住苏伊士运河, 14艘拖船都无法拖出，“罪魁祸首”居然是它？科学博士                                                                                  “长赐号”轮船卡住苏伊士运河, 14艘拖船都无法拖出，“罪魁祸首”居然是它？ #科学博士​ #苏伊士运河​ #长赐号轮船​ #罪魁祸首​ #原因​ ★推荐影片★ 直博与硕博连读的区别，很重要！可能影响你的一生 https://youtu.be/QGZojkqnNKM​ 新冠病毒预防，为何要用肥皂洗手？肥皂不止洗涤还能杀灭病毒的原理！COVID-19 https://youtu.be/PRYXpZrQjec​ 世上居然有三黄蛋！三黄蛋！99.9%的人一生都没有见过！ https://youtu.be/YvkXjYK0fd4​ 稀奇的双黄蛋：万分之一的鸡蛋！双黄蛋密码| 科学博士 https://youtu.be/1rDJq_vUl6Q​ 哇！透明的双黄蛋，怎么做的？你在家也可以的！科学博士的家庭实验室 https://youtu.be/rlRWZBZ9sf0
http://w3id.org/mlsea/pwc/scientificWork/%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9A%84%E8%B6%85%E5%A4%A7%E8%8C%83%E5%9B%B4%E6%B7%B7%E6%B2%8C%E7%B3%BB%E7%BB%9F%E5%8F%8A%E5%85%B6%E8%87%AA%E9%80%82%E5%BA%94%E6%BB%91%E6%A8%A1%E6%8E%A7%E5%88%B6                                                                                  一个新的超大范围混沌系统及其自适应滑模控制                                                                                  提出了一个超大范围的混沌系统，其中参数 b 的取值为［0，107］。理论分析了系统的动力学特性，考察了 系统的 Lyapunov 指数谱、分岔图以及 Poincare 映射。设计了系统的硬件电路，并用 Multisim 软件进行了电路仿真，构建了一个系统在未知参数条件下全局稳定的自适应控制器和一个对给定信号追踪与未知参数辨识的自适应滑模控制器。仿真结果表明所设计控制器是有效的
http://w3id.org/mlsea/pwc/scientificWork/%E4%B8%89%E6%98%9F%E5%A0%86%E6%96%87%E6%98%8E%E7%9A%84%E7%B2%BE%E9%AB%93%EF%BC%8C%E9%9D%92%E9%93%9C%E7%BA%B5%E7%9B%AE%E9%9D%A2%E5%85%B7%E7%9A%84%E5%90%AB%E4%B9%89%EF%BC%9A%E5%B4%87%E6%8B%9C%E7%9F%A5%E8%AF%86%E4%B8%8E%E5%A5%BD%E5%A5%87%E5%BF%83                                                                                  三星堆文明的精髓，青铜纵目面具的含义：崇拜知识与好奇心                                                                                  三星堆文明的精髓，青铜纵目面具的含义：崇拜知识与好奇心 #科学博士​ #三星堆文明​ #青铜纵目面具​ #知识​ #好奇心​ ★推荐影片★ “长赐号”轮船卡住苏伊士运河, 14艘拖船都无法拖出，“罪魁祸首”居然是它？ https://youtu.be/AsP8capcf-k​ 直博与硕博连读的区别，很重要！可能影响你的一生 https://youtu.be/QGZojkqnNKM​ 新冠病毒预防，为何要用肥皂洗手？肥皂不止洗涤还能杀灭病毒的原理！COVID-19 https://youtu.be/PRYXpZrQjec​ 世上居然有三黄蛋！三黄蛋！99.9%的人一生都没有见过！ https://youtu.be/YvkXjYK0fd4​ 稀奇的双黄蛋：万分之一的鸡蛋！双黄蛋密码| 科学博士 https://youtu.be/1rDJq_vUl6Q​ 哇！透明的双黄蛋，怎么做的？你在家也可以的！科学博士的家庭实验室 https://youtu.be/rlRWZBZ9sf0
http://w3id.org/mlsea/pwc/scientificWork/%E4%B8%A4%E9%99%A2%E9%99%A2%E5%A3%ABNature%E8%A2%AB%E8%B4%A8%E7%96%91%EF%BC%8C%E4%BB%96%E6%84%9F%E8%B0%A2%E6%8F%90%E5%87%BA%E8%B4%A8%E7%96%91%E7%9A%84%E5%B9%B4%E8%BD%BB%E7%A7%91%E5%AD%A6%E5%AE%B6%EF%BC%81                                                                                  两院院士Nature被质疑，他感谢提出质疑的年轻科学家！                                                                                  We once again thank Deng for having motivated a fruitful dialogue and updates to the TEM studies of the original paper.【来源：Reply to: Perovskite decomposition and missing crystal planes in HRTEM】 加拿大科学院和工程院两院院士Edward H. Sargent教授是国际顶级科学家，Yu-Hao Deng老师敢于从科学的角度质疑他的这篇Nature论文，作者们敢于从科学的角度进行正面回应，对一些问题进行勘误，并对一些问题进行更进一步的探讨和阐述，这都是科学家该有的精神。 近日，根特大学邓玉豪（Yu-Hao Deng）提出，在2015年发表的这篇工作中，相关研究无法充分证明PbS修饰在钙钛矿，尤其是文章中展示的PbI2 TEM照片无法与钙钛矿、PbS之间显著区别。邓玉豪发现，通过对原文中的数据进行分析，对应的层间距应该为57°，而不是作者认为的60°，此外邓提出文章缺乏证明钙钛矿（112）晶面的衍射斑点，因此作者观测的PbS可能是钙钛矿降解的产物。
http://w3id.org/mlsea/pwc/scientificWork/%E5%8C%97%E5%A4%A7%E5%8D%9A%E5%A3%AB%E5%AF%B9%E8%AF%9D%E9%AB%98%E8%80%83%E7%8A%B6%E5%85%83%EF%BC%9A%E5%A6%82%E4%BD%95%E5%A1%91%E9%80%A0%E8%89%AF%E5%A5%BD%E5%AD%A6%E4%B9%A0%E4%B9%A0%E6%83%AF%E4%B8%8E%E7%94%9F%E6%B4%BB%E4%B9%A0%E6%83%AF%EF%BC%9F%E5%8A%A9%E5%8A%9B%E7%90%86%E6%83%B3%E5%8F%96%E5%BE%97%E6%88%90%E5%8A%9F                                                                                  北大博士对话高考状元：如何塑造良好学习习惯与生活习惯？助力理想取得成功                                                                                  北大博士对话高考状元：如何塑造良好学习习惯与生活习惯？助力理想取得成功 #学习习惯​ #生活习惯​ #北大博士对话高考状元​ #科学博士​ #理想与成功​ ★推荐影片★ 毕业季！如何一个月内写出高质量【博士论文】？方法在这里 https://youtu.be/KpvLQgbWEOI​ 直博与硕博连读的区别，很重要！可能影响你的一生 https://youtu.be/QGZojkqnNKM​ 太神奇了！水中剪玻璃？视频是真的吗？答案揭晓 https://youtu.be/zWhefpDgy1s​ 新冠疫苗，为什么要打两针？打一针可以吗？性命攸关的重要问题！COVID-19 https://youtu.be/JQpdRHJnVt0​ 三星堆文明的精髓，青铜纵目面具的含义：崇拜知识与好奇心 https://youtu.be/vVVEyrbgGgY​ “长赐号”轮船卡住苏伊士运河, 14艘拖船都无法拖出，“罪魁祸首”居然是它？ https://youtu.be/AsP8capcf-k​ 成都肖美丽女士劝止“渣男”室内吸烟，居然被骂神经病并且被泼油？北大博士有话说：为什么不能在室内公共场所吸烟！ https://youtu.be/lENcBBGxq_E​ 新冠病毒预防，为何要用肥皂洗手？肥皂不止洗涤还能杀灭病毒的原理！COVID-19 https://youtu.be/PRYXpZrQjec
http://w3id.org/mlsea/pwc/scientificWork/%E5%8C%97%E5%A4%A7%E6%9C%AA%E5%90%8D%E6%B9%96%E7%95%94%E7%9A%84%E5%B0%8F%E7%8C%AB%E5%92%AA%E5%B1%85%E7%84%B6%E4%B8%8D%E6%80%95%E4%BA%BA%E5%A5%BD%E8%90%8C%E5%91%80%EF%BC%8C%E8%B5%8F%E7%BE%8E%E6%99%AF%E6%92%B8%E7%8C%AB%E5%92%AA%EF%BC%8C%E4%B9%88%E4%B9%88%E5%93%92%EF%BC%81%E8%B6%85%E6%B8%85%E8%A7%86%E9%A2%91%E3%80%90%E5%AD%97%E5%B9%95%E3%80%91                                                                                  北大未名湖畔的小猫咪居然不怕人好萌呀，赏美景撸猫咪，么么哒！超清视频【字幕】                                                                                  北大未名湖畔的小猫咪居然不怕人好萌呀，赏美景撸猫咪，么么哒！超清视频 #科学博士​ #北京大学​ #未名湖小猫咪​ #北大撸猫​ 北京大学是中国最高学府，人文气息浓厚，景色优美，作为北京大学最美的地方未名湖，坐在湖畔撸小猫咪会是怎样的一种享受呢？快点击进入观看同时订阅关注科学博士频道吧！ 本视频记得打开CC字幕。 ★推荐影片★ 好奇，国家总理吃过的北京大学食堂到底长啥样？都有哪些特色菜？超清视频 https://youtu.be/Vf2JfClMges​ 走进北京大学图书馆，亚洲最大，知识的圣殿，多少人梦寐以求想在这里看书 https://youtu.be/jErniaRkv9k​ 真的吗？北大未名湖结冰后人可以在湖面行走？北京大学 https://youtu.be/fT9JIlQjJBQ
http://w3id.org/mlsea/pwc/scientificWork/%E5%8C%97%E5%A4%A7%E6%9C%AA%E5%90%8D%E6%B9%96%E9%95%87%E6%B9%96%E4%B9%8B%E5%AE%9D%7C%20%E5%9C%86%E6%98%8E%E5%9B%AD%E5%8E%86%E5%8F%B2%E6%96%87%E7%89%A9%E7%BF%BB%E5%B0%BE%E7%9F%B3%E9%B1%BC%EF%BC%8C%E8%BF%91%E8%B7%9D%E7%A6%BB%E8%A7%82%E8%B5%8F%E3%80%90%E5%AD%97%E5%B9%95%E3%80%91                                                                                  北大未名湖镇湖之宝| 圆明园历史文物翻尾石鱼，近距离观赏【字幕】                                                                                  北大未名湖镇湖之宝| 圆明园历史文物翻尾石鱼，近距离观赏 #科学博士​ #北大未名湖​ #翻尾石鱼​ #圆明园文物​ #镇湖之宝​ #北京大学​ 本视频记得打开CC字幕。 ★推荐影片★ 好奇，国家总理吃过的北京大学食堂到底长啥样？都有哪些特色菜？超清视频 https://youtu.be/Vf2JfClMges​ 北大未名湖畔的小猫咪居然不怕人好萌呀，赏美景撸猫咪，么么哒！超清视频 https://youtu.be/NkdHdWJ9-SI​ 走进北京大学图书馆，亚洲最大，知识的圣殿，多少人梦寐以求想在这里看书 https://youtu.be/jErniaRkv9k​ 真的吗？北大未名湖结冰后人可以在湖面行走？北京大学 https://youtu.be/fT9JIlQjJBQ​ 北大校园内的宫廷小红楼，这些古建筑文物里面到底是啥样？北京大学 https://youtu.be/WDcbmpfcQGc
http://w3id.org/mlsea/pwc/scientificWork/%E5%8E%89%E5%AE%B3%E4%BA%86%EF%BC%8C%E5%90%83%E5%A1%91%E6%96%99%E7%9A%84%E7%BB%86%E8%8F%8C%EF%BC%81%E8%A7%A3%E5%86%B3%E7%99%BD%E8%89%B2%E5%9E%83%E5%9C%BE%E6%B1%A1%E6%9F%93%E6%88%96%E6%AF%81%E7%81%AD%E5%9C%B0%E7%90%83%EF%BC%9F                                                                                  厉害了，吃塑料的细菌！解决白色垃圾污染或毁灭地球？                                                                                  厉害了，吃塑料的细菌！解决白色垃圾污染或毁灭地球？ #塑料​ #细菌​ #垃圾污染​ #塑料污染​ #吃塑料的细菌​ #科学博士​ ★推荐影片★ 异形血液成分之谜揭晓！酸性比硫酸强10亿倍，世上只有一种材料可以防御它！ https://youtu.be/i3yEyfxp6oo​ 太神奇了！水中剪玻璃？视频是真的吗？答案揭晓 https://youtu.be/zWhefpDgy1s​ 新冠疫苗，为什么要打两针？打一针可以吗？性命攸关的重要问题！COVID-19 https://youtu.be/JQpdRHJnVt0​ 三星堆文明的精髓，青铜纵目面具的含义：崇拜知识与好奇心 https://youtu.be/vVVEyrbgGgY​ “长赐号”轮船卡住苏伊士运河, 14艘拖船都无法拖出，“罪魁祸首”居然是它？ https://youtu.be/AsP8capcf-k​ 成都肖美丽女士劝止“渣男”室内吸烟，居然被骂神经病并且被泼油？北大博士有话说：为什么不能在室内公共场所吸烟！ https://youtu.be/lENcBBGxq_E​ 科学博士：注射新冠疫苗的4个理由，最后一个让人简直无法拒绝！ https://youtu.be/oms3gP_Yzr0​ 新冠病毒预防，为何要用肥皂洗手？肥皂不止洗涤还能杀灭病毒的原理！COVID-19 https://youtu.be/PRYXpZrQjec​ 毕业季！如何一个月内写出高质量【博士论文】？方法在这里 https://youtu.be/KpvLQgbWEOI​ 直博与硕博连读的区别，很重要！可能影响你的一生 https://youtu.be/QGZojkqnNKM
http://w3id.org/mlsea/pwc/scientificWork/%E5%A4%AA%E7%A5%9E%E5%A5%87%E4%BA%86%EF%BC%81%E6%B0%B4%E4%B8%AD%E5%89%AA%E7%8E%BB%E7%92%83%EF%BC%9F%E8%A7%86%E9%A2%91%E6%98%AF%E7%9C%9F%E7%9A%84%E5%90%97%EF%BC%9F%E7%AD%94%E6%A1%88%E6%8F%AD%E6%99%93%20%E7%A7%91%E5%AD%A6%E5%8D%9A%E5%A3%AB                                                                                  太神奇了！水中剪玻璃？视频是真的吗？答案揭晓 科学博士                                                                                  太神奇了！水中剪玻璃？视频是真的吗？答案揭晓 #科学博士​ #水中剪玻璃​ #真的吗​ #科学实验​ #剪玻璃​ ★推荐影片★ 新冠疫苗，为什么要打两针？打一针可以吗？性命攸关的重要问题！COVID-19 https://youtu.be/JQpdRHJnVt0​ 三星堆文明的精髓，青铜纵目面具的含义：崇拜知识与好奇心 https://youtu.be/vVVEyrbgGgY​ “长赐号”轮船卡住苏伊士运河, 14艘拖船都无法拖出，“罪魁祸首”居然是它？ https://youtu.be/AsP8capcf-k​ 直博与硕博连读的区别，很重要！可能影响你的一生 https://youtu.be/QGZojkqnNKM​ 新冠病毒预防，为何要用肥皂洗手？肥皂不止洗涤还能杀灭病毒的原理！COVID-19 https://youtu.be/PRYXpZrQjec​ 世上居然有三黄蛋！三黄蛋！99.9%的人一生都没有见过！ https://youtu.be/YvkXjYK0fd4​ 稀奇的双黄蛋：万分之一的鸡蛋！双黄蛋密码| 科学博士 https://youtu.be/1rDJq_vUl6Q​ 哇！透明的双黄蛋，怎么做的？你在家也可以的！科学博士的家庭实验室 https://youtu.be/rlRWZBZ9sf0
http://w3id.org/mlsea/pwc/scientificWork/%E5%A5%BD%E5%A5%87%EF%BC%8C%E5%9B%BD%E5%AE%B6%E6%80%BB%E7%90%86%E5%90%83%E8%BF%87%E7%9A%84%E5%8C%97%E4%BA%AC%E5%A4%A7%E5%AD%A6%E9%A3%9F%E5%A0%82%E5%88%B0%E5%BA%95%E9%95%BF%E5%95%A5%E6%A0%B7%EF%BC%9F%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B%E7%89%B9%E8%89%B2%E8%8F%9C%EF%BC%9F%E8%B6%85%E6%B8%85%E8%A7%86%E9%A2%91%E3%80%90%E5%AD%97%E5%B9%95%E3%80%91                                                                                  好奇，国家总理吃过的北京大学食堂到底长啥样？都有哪些特色菜？超清视频【字幕】                                                                                  好奇，国家总理吃过的北京大学食堂到底长啥样？都有哪些特色菜？超清视频 #科学博士​ #北京大学​ #食堂​ #美食​ #特色菜​ #国家总理​ 本视频记得打开CC字幕。 ★推荐影片★ 北大未名湖畔的小猫咪居然不怕人好萌呀，赏美景撸猫咪，么么哒！超清视频 https://youtu.be/NkdHdWJ9-SI​ 走进北京大学图书馆，亚洲最大，知识的圣殿，多少人梦寐以求想在这里看书 https://youtu.be/jErniaRkv9k​ 真的吗？北大未名湖结冰后人可以在湖面行走？北京大学 https://youtu.be/fT9JIlQjJBQ
http://w3id.org/mlsea/pwc/scientificWork/%E5%B0%96%E6%A4%92%E9%83%A8%E8%90%BD-%E7%94%B5%E6%8A%A5%E9%A2%91%E9%81%93%EF%BC%88%E5%A5%B3%E6%80%A7%EF%BC%8C%E5%A5%B3%E6%9D%83%EF%BC%8C%E5%A5%B3%E5%B7%A5%EF%BC%8C%E5%A5%B3%E7%AB%A5%EF%BC%8C%E6%80%A7%E5%B0%91%E6%95%B0LGBT%EF%BC%89%20%40jianjiaobuluo                                                                                  尖椒部落-电报频道（女性，女权，女工，女童，性少数LGBT） @jianjiaobuluo                                                                                  尖椒部落，放大妳的声音！中国女工专属资讯平台，欢迎您的关注！@jianjiaobuluo 关键词：女性权益，性别平等，性教育，反防性骚扰性侵家庭暴力，女性主义，女权，女生，女工，女童，同性恋，性少数LGBT，赋权，平等，自由，人权，联合国公约。 尖椒部落，放大妳的声音！中国女工专属资讯平台，欢迎您的关注！@jianjiaobuluo 关键词：女性权益，性别平等，性教育，反防性骚扰性侵家庭暴力，女性主义，女权，女生，女工，女童，同性恋，性少数LGBT，赋权，平等，自由，人权，联合国公约。
http://w3id.org/mlsea/pwc/scientificWork/%E5%BC%82%E5%BD%A2%E8%A1%80%E6%B6%B2%E6%88%90%E5%88%86%E4%B9%8B%E8%B0%9C%E6%8F%AD%E6%99%93%EF%BC%81%E9%85%B8%E6%80%A7%E6%AF%94%E7%A1%AB%E9%85%B8%E5%BC%BA10%E4%BA%BF%E5%80%8D%EF%BC%8C%E4%B8%96%E4%B8%8A%E5%8F%AA%E6%9C%89%E4%B8%80%E7%A7%8D%E6%9D%90%E6%96%99%E5%8F%AF%E4%BB%A5%E9%98%B2%E5%BE%A1%E5%AE%83%EF%BC%81                                                                                  异形血液成分之谜揭晓！酸性比硫酸强10亿倍，世上只有一种材料可以防御它！                                                                                  异形血液成分之谜揭晓！酸性比硫酸强10亿倍，世上只有一种材料可以防御它！ #异形​ #血液​ #强酸​ #异形血液成分​ #世界最强酸​ #科学博士​ ★推荐影片★ 太神奇了！水中剪玻璃？视频是真的吗？答案揭晓 https://youtu.be/zWhefpDgy1s​ 新冠疫苗，为什么要打两针？打一针可以吗？性命攸关的重要问题！COVID-19 https://youtu.be/JQpdRHJnVt0​ 三星堆文明的精髓，青铜纵目面具的含义：崇拜知识与好奇心 https://youtu.be/vVVEyrbgGgY​ “长赐号”轮船卡住苏伊士运河, 14艘拖船都无法拖出，“罪魁祸首”居然是它？ https://youtu.be/AsP8capcf-k​ 成都肖美丽女士劝止“渣男”室内吸烟，居然被骂神经病并且被泼油？北大博士有话说：为什么不能在室内公共场所吸烟！ https://youtu.be/lENcBBGxq_E​ 科学博士：注射新冠疫苗的4个理由，最后一个让人简直无法拒绝！ https://youtu.be/oms3gP_Yzr0​ 新冠病毒预防，为何要用肥皂洗手？肥皂不止洗涤还能杀灭病毒的原理！COVID-19 https://youtu.be/PRYXpZrQjec​ 毕业季！如何一个月内写出高质量【博士论文】？方法在这里 https://youtu.be/KpvLQgbWEOI​ 直博与硕博连读的区别，很重要！可能影响你的一生 https://youtu.be/QGZojkqnNKM
http://w3id.org/mlsea/pwc/scientificWork/%E5%BF%83%E6%80%81%E5%86%B3%E5%AE%9A%E6%88%90%E8%B4%A5%EF%BC%81%E5%A6%82%E4%BD%95%E5%A1%91%E9%80%A0%E5%BC%BA%E5%A4%A7%E7%9A%84%E9%AB%98%E8%80%83%E5%BF%83%E6%80%81%7C%20%E5%8C%97%E5%A4%A7%E5%8D%9A%E5%A3%AB%E5%AF%B9%E8%AF%9D%E9%AB%98%E8%80%83%E7%8A%B6%E5%85%83                                                                                  心态决定成败！如何塑造强大的高考心态| 北大博士对话高考状元                                                                                  心态决定成败！如何塑造强大的高考心态| 北大博士对话高考状元 #高考心态​ #考试心态​ #北大博士对话高考状元​ #科学博士​ #心态​ ★推荐影片★ 北大博士对话高考状元：如何塑造良好学习习惯与生活习惯？助力理想取得成功 https://youtu.be/s-cuquFVCbQ​ 毕业季！如何一个月内写出高质量【博士论文】？方法在这里 https://youtu.be/KpvLQgbWEOI​ 直博与硕博连读的区别，很重要！可能影响你的一生 https://youtu.be/QGZojkqnNKM​ 太神奇了！水中剪玻璃？视频是真的吗？答案揭晓 https://youtu.be/zWhefpDgy1s​ 新冠疫苗，为什么要打两针？打一针可以吗？性命攸关的重要问题！COVID-19 https://youtu.be/JQpdRHJnVt0​ 三星堆文明的精髓，青铜纵目面具的含义：崇拜知识与好奇心 https://youtu.be/vVVEyrbgGgY​ “长赐号”轮船卡住苏伊士运河, 14艘拖船都无法拖出，“罪魁祸首”居然是它？ https://youtu.be/AsP8capcf-k​ 成都肖美丽女士劝止“渣男”室内吸烟，居然被骂神经病并且被泼油？北大博士有话说：为什么不能在室内公共场所吸烟！ https://youtu.be/lENcBBGxq_E​ 新冠病毒预防，为何要用肥皂洗手？肥皂不止洗涤还能杀灭病毒的原理！COVID-19 https://youtu.be/PRYXpZrQjec
http://w3id.org/mlsea/pwc/scientificWork/%E6%96%B0%E5%86%A0%E7%97%85%E6%AF%92%E9%A2%84%E9%98%B2%EF%BC%8C%E4%B8%BA%E4%BD%95%E8%A6%81%E7%94%A8%E8%82%A5%E7%9A%82%E6%B4%97%E6%89%8B%EF%BC%9F%E8%82%A5%E7%9A%82%E4%B8%8D%E6%AD%A2%E6%B4%97%E6%B6%A4%E8%BF%98%E8%83%BD%E6%9D%80%E7%81%AD%E7%97%85%E6%AF%92%E7%9A%84%E5%8E%9F%E7%90%86%EF%BC%81COVID-19%20%E7%A7%91%E5%AD%A6%E5%8D%9A%E5%A3%AB                                                                                  新冠病毒预防，为何要用肥皂洗手？肥皂不止洗涤还能杀灭病毒的原理！COVID-19 科学博士                                                                                  新冠病毒预防，为何要用肥皂洗手？肥皂不止洗涤还能杀灭病毒的原理！COVID-19 #科学博士​ #肥皂​ #新冠肺炎​ ＃COVID19​ #病毒​ #健康​ #科普 ★推荐影片★ 世上居然有三黄蛋！三黄蛋！99.9%的人一生都没有见过！ https://youtu.be/YvkXjYK0fd4​ 稀奇的双黄蛋：万分之一的鸡蛋！双黄蛋密码| 科学博士 https://youtu.be/1rDJq_vUl6Q​ 哇！透明的双黄蛋，怎么做的？你在家也可以的！科学博士的家庭实验室 https://youtu.be/rlRWZBZ9sf0
http://w3id.org/mlsea/pwc/scientificWork/%E6%AF%94%E5%88%A9%E6%97%B6%E6%A0%B9%E7%89%B9%E7%95%99%E5%AD%A6%E7%94%9F%E7%BE%A4-%E6%A0%B9%E7%89%B9%E5%A4%A7%E5%AD%A6%E4%BA%A4%E5%8F%8B%E7%BE%A4-%E6%A0%B9%E7%89%B9%E5%A4%A7%E5%AD%A6%E5%AD%A6%E7%94%9F%E4%BA%92%E5%8A%A9%E7%BE%A4%20%40belgiumlife                                                                                  比利时根特留学生群-根特大学交友群-根特大学学生互助群 @belgiumlife                                                                                  比利时根特大学留学生电报群-根特大学交友电报群-根特大学学生互助群-根特大学群-根特留学生群 @belgiumlife 比利时生活电报群，讨论生活消息，包括本土新闻、留学、签证、租房、衣食住行、疫情等消息，除辱骂攻击群成员以及违反Telegram平台规则外其他话题不限。@belgiumlife 比利时生活汇频道 @belgiumlifenews 打开电报，搜索群号@belgiumlife即可加入，此群人数上限20万，群成员匿名保护隐私，聊天记录自己可以主动删除，大家可自由加入，自由发言，自由互相分享消息，互帮互助！比利时根特大学留学生电报群-根特大学交友电报群-根特大学学生互助群 @belgiumlife
http://w3id.org/mlsea/pwc/scientificWork/%E6%AF%94%E5%88%A9%E6%97%B6%E7%95%99%E5%AD%A6%E7%94%9F%E7%BE%A4-%E6%AF%94%E5%88%A9%E6%97%B6%E7%95%99%E5%AD%A6%E7%94%9F%E7%94%B5%E6%8A%A5%E7%BE%A4-%E6%AF%94%E5%88%A9%E6%97%B6%E5%8D%8E%E4%BA%BA%E7%BE%A4%20%40belgiumlife                                                                                  比利时留学生群-比利时留学生电报群-比利时华人群 @belgiumlife                                                                                  比利时留学生群-比利时留学生电报群-比利时华人群 @belgiumlife 比利时生活电报群，讨论生活消息，包括本土新闻、留学、签证、租房、衣食住行、疫情等消息，除辱骂攻击群成员以及违反Telegram平台规则外其他话题不限。@belgiumlife 比利时生活汇频道 @belgiumlifenews 打开电报，搜索群号@belgiumlife即可加入，此群人数上限20万，大家可自由加入，自由发言，自由互相分享消息，互帮互助！ 比利时留学生群-比利时留学生电报群-比利时华人群 @belgiumlife
http://w3id.org/mlsea/pwc/scientificWork/%E6%AF%95%E4%B8%9A%E5%AD%A3%EF%BC%81%E5%A6%82%E4%BD%95%E4%B8%80%E4%B8%AA%E6%9C%88%E5%86%85%E5%86%99%E5%87%BA%E9%AB%98%E8%B4%A8%E9%87%8F%E3%80%90%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87%E3%80%91%EF%BC%9F%E6%96%B9%E6%B3%95%E5%9C%A8%E8%BF%99%E9%87%8C%20%E7%A7%91%E5%AD%A6%E5%8D%9A%E5%A3%AB                                                                                  毕业季！如何一个月内写出高质量【博士论文】？方法在这里 科学博士                                                                                  毕业季！如何一个月内写出高质量【博士论文】？方法在这里 #博士论文​ #一个月​ #毕业论文​ #学位论文​ #论文书写​ #北大博士​ ★推荐影片★ 直博与硕博连读的区别，很重要！可能影响你的一生 https://youtu.be/QGZojkqnNKM​ 太神奇了！水中剪玻璃？视频是真的吗？答案揭晓 https://youtu.be/zWhefpDgy1s​ 新冠疫苗，为什么要打两针？打一针可以吗？性命攸关的重要问题！COVID-19 https://youtu.be/JQpdRHJnVt0​ 三星堆文明的精髓，青铜纵目面具的含义：崇拜知识与好奇心 https://youtu.be/vVVEyrbgGgY​ “长赐号”轮船卡住苏伊士运河, 14艘拖船都无法拖出，“罪魁祸首”居然是它？ https://youtu.be/AsP8capcf-k​ 肖美丽女士劝止“渣男”室内吸烟，居然被骂神经病并且被泼油？北大博士有话说：为什么不能在室内公共场所吸烟！ https://youtu.be/lENcBBGxq_E​ 新冠病毒预防，为何要用肥皂洗手？肥皂不止洗涤还能杀灭病毒的原理！COVID-19 https://youtu.be/PRYXpZrQjec
http://w3id.org/mlsea/pwc/scientificWork/%E7%9B%B4%E5%8D%9A%E4%B8%8E%E7%A1%95%E5%8D%9A%E8%BF%9E%E8%AF%BB%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%8C%E5%BE%88%E9%87%8D%E8%A6%81%EF%BC%81%E5%8F%AF%E8%83%BD%E5%BD%B1%E5%93%8D%E4%BD%A0%E7%9A%84%E4%B8%80%E7%94%9F%7C%20%E7%A7%91%E5%AD%A6%E5%8D%9A%E5%A3%AB                                                                                  直博与硕博连读的区别，很重要！可能影响你的一生| 科学博士                                                                                  直博与硕博连读的区别，很重要！可能影响你的一生！ #科学博士​ #直博​ #硕博连读​ #区别​ #研究生​ #博士​ #科普​ ★推荐影片★ 新冠病毒预防，为何要用肥皂洗手？肥皂不止洗涤还能杀灭病毒的原理！COVID-19 https://youtu.be/PRYXpZrQjec​ 世上居然有三黄蛋！三黄蛋！99.9%的人一生都没有见过！ https://youtu.be/YvkXjYK0fd4​ 稀奇的双黄蛋：万分之一的鸡蛋！双黄蛋密码| 科学博士 https://youtu.be/1rDJq_vUl6Q​ 哇！透明的双黄蛋，怎么做的？你在家也可以的！科学博士的家庭实验室 https://youtu.be/rlRWZBZ9sf0
http://w3id.org/mlsea/pwc/scientificWork/%E7%9C%9F%E7%9A%84%E5%90%97%EF%BC%9F%E5%8C%97%E5%A4%A7%E6%9C%AA%E5%90%8D%E6%B9%96%E7%BB%93%E5%86%B0%E5%90%8E%E4%BA%BA%E5%8F%AF%E4%BB%A5%E5%9C%A8%E6%B9%96%E9%9D%A2%E8%A1%8C%E8%B5%B0%EF%BC%9F%E5%8C%97%E4%BA%AC%E5%A4%A7%E5%AD%A6%E3%80%90%E5%AD%97%E5%B9%95%E3%80%91                                                                                  真的吗？北大未名湖结冰后人可以在湖面行走？北京大学【字幕】                                                                                  真的吗？北大未名湖结冰后人可以在湖面行走？ #科学博士​ #北京大学​ #未名湖湖面行走​ #未名湖结冰​ 北京大学是中国最高学府，人文气息浓厚，景色优美，作为北京大学最美的地方未名湖，冬天景色也是别具一格，未名湖湖面结冰到底哟多厚，人真的可以在湖面行走吗？不会掉进湖里然后XX？快点击进入揭晓答案同时订阅关注科学博士频道吧！ 本视频记得打开CC字幕。 ★推荐影片★ 好奇，国家总理吃过的北京大学食堂到底长啥样？都有哪些特色菜？超清视频 https://youtu.be/Vf2JfClMges​ 北大未名湖畔的小猫咪居然不怕人好萌呀，赏美景撸猫咪，么么哒！超清视频 https://youtu.be/NkdHdWJ9-SI​ 走进北京大学图书馆，亚洲最大，知识的圣殿，多少人梦寐以求想在这里看书 https://youtu.be/jErniaRkv9k
http://w3id.org/mlsea/pwc/scientificWork/%E7%A5%9E%E4%BB%99%E6%89%93%E6%9E%B6%EF%BC%81%E9%82%93%E7%8E%89%E8%B1%AA%E5%8D%9A%E5%A3%ABNature%E5%8F%91%E6%96%87%EF%BC%8C%E8%B4%A8%E7%96%91%E8%91%97%E5%90%8D%E5%AD%A6%E8%80%85Sargent%E9%99%A2%E5%A3%AB6%E5%B9%B4%E5%89%8D%E7%A0%94%E7%A9%B6%E6%88%90%E6%9E%9C%EF%BC%81                                                                                  神仙打架！邓玉豪博士Nature发文，质疑著名学者Sargent院士6年前研究成果！                                                                                  2021年6月10日凌晨，根特大学邓玉豪博士（毕业于北京大学）在《Nature》发文质疑Sargent院士团队在2015年文章中的HRTEM结果。邓玉豪博士发现，文中MAPbI3钙钛矿材料的HRTEM数据中存在晶面缺失的现象！这意味着，Sargent院士团队文章图中的材料不是MAPbI3，而可能是其被电子束辐射分解的产物----碘化铅 (PbI2)。 最后，邓博士强调，这一质疑旨在提高研究人员的认识，为了避免科研人员未来在电子束敏感材料的 HRTEM 表征中可能出现的错误。
http://w3id.org/mlsea/pwc/scientificWork/%E7%A7%91%E5%AD%A6%E5%8D%9A%E5%A3%AB%EF%BC%9A%E6%96%B0%E5%86%A0%E7%96%AB%E8%8B%97%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%89%93%E4%B8%A4%E9%92%88%EF%BC%9F%E6%89%93%E4%B8%80%E9%92%88%E5%8F%AF%E4%BB%A5%E5%90%97%EF%BC%9F%E6%80%A7%E5%91%BD%E6%94%B8%E5%85%B3%E7%9A%84%E9%87%8D%E8%A6%81%E9%97%AE%E9%A2%98%EF%BC%81COVID-19                                                                                  科学博士：新冠疫苗，为什么要打两针？打一针可以吗？性命攸关的重要问题！COVID-19                                                                                  科学博士：新冠疫苗，为什么要打两针？打一针可以吗？性命攸关的重要问题！COVID-19 #科学博士​ #新冠疫苗​ #打两针​ #接种两剂​ #COVID19​ ★推荐影片★ 三星堆文明的精髓，青铜纵目面具的含义：崇拜知识与好奇心 https://youtu.be/vVVEyrbgGgY​ “长赐号”轮船卡住苏伊士运河, 14艘拖船都无法拖出，“罪魁祸首”居然是它？ https://youtu.be/AsP8capcf-k​ 直博与硕博连读的区别，很重要！可能影响你的一生 https://youtu.be/QGZojkqnNKM​ 新冠病毒预防，为何要用肥皂洗手？肥皂不止洗涤还能杀灭病毒的原理！COVID-19 https://youtu.be/PRYXpZrQjec​ 世上居然有三黄蛋！三黄蛋！99.9%的人一生都没有见过！ https://youtu.be/YvkXjYK0fd4​ 稀奇的双黄蛋：万分之一的鸡蛋！双黄蛋密码| 科学博士 https://youtu.be/1rDJq_vUl6Q​ 哇！透明的双黄蛋，怎么做的？你在家也可以的！科学博士的家庭实验室 https://youtu.be/rlRWZBZ9sf0
http://w3id.org/mlsea/pwc/scientificWork/%E7%A7%91%E5%AD%A6%E5%8D%9A%E5%A3%AB%EF%BC%9A%E6%B3%A8%E5%B0%84%E6%96%B0%E5%86%A0%E7%96%AB%E8%8B%97%E7%9A%844%E4%B8%AA%E7%90%86%E7%94%B1%EF%BC%8C%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%E8%AE%A9%E4%BA%BA%E7%AE%80%E7%9B%B4%E6%97%A0%E6%B3%95%E6%8B%92%E7%BB%9D%EF%BC%81COVID-19                                                                                  科学博士：注射新冠疫苗的4个理由，最后一个让人简直无法拒绝！COVID-19                                                                                  科学博士：注射新冠疫苗的4个理由，最后一个让人简直无法拒绝！ #新冠疫苗​ #新冠肺炎​ #注射理由​ #健康​ #COVID19​ #科学博士​ ★推荐影片★ 新冠病毒预防，为何要用肥皂洗手？肥皂不止洗涤还能杀灭病毒的原理！COVID-19 https://youtu.be/PRYXpZrQjec​ 毕业季！如何一个月内写出高质量【博士论文】？方法在这里 https://youtu.be/KpvLQgbWEOI​ 直博与硕博连读的区别，很重要！可能影响你的一生 https://youtu.be/QGZojkqnNKM​ 太神奇了！水中剪玻璃？视频是真的吗？答案揭晓 https://youtu.be/zWhefpDgy1s​ 新冠疫苗，为什么要打两针？打一针可以吗？性命攸关的重要问题！COVID-19 https://youtu.be/JQpdRHJnVt0​ 三星堆文明的精髓，青铜纵目面具的含义：崇拜知识与好奇心 https://youtu.be/vVVEyrbgGgY​ “长赐号”轮船卡住苏伊士运河, 14艘拖船都无法拖出，“罪魁祸首”居然是它？ https://youtu.be/AsP8capcf-k​ 成都肖美丽女士劝止“渣男”室内吸烟，居然被骂神经病并且被泼油？北大博士有话说：为什么不能在室内公共场所吸烟！ https://youtu.be/lENcBBGxq_E
http://w3id.org/mlsea/pwc/scientificWork/%E8%82%96%E7%BE%8E%E4%B8%BD%E5%A5%B3%E5%A3%AB%E5%8A%9D%E6%AD%A2%E2%80%9C%E6%B8%A3%E7%94%B7%E2%80%9D%E5%AE%A4%E5%86%85%E5%90%B8%E7%83%9F%EF%BC%8C%E5%B1%85%E7%84%B6%E8%A2%AB%E9%AA%82%E7%A5%9E%E7%BB%8F%E7%97%85%E5%B9%B6%E4%B8%94%E8%A2%AB%E6%B3%BC%E6%B2%B9%EF%BC%9F%E5%8C%97%E5%A4%A7%E5%8D%9A%E5%A3%AB%E6%9C%89%E8%AF%9D%E8%AF%B4%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E8%83%BD%E5%9C%A8%E5%AE%A4%E5%86%85%E5%85%AC%E5%85%B1%E5%9C%BA%E6%89%80%E5%90%B8%E7%83%9F%EF%BC%81                                                                                  肖美丽女士劝止“渣男”室内吸烟，居然被骂神经病并且被泼油？北大博士有话说：为什么不能在室内公共场所吸烟！                                                                                  肖美丽女士劝止“渣男”室内吸烟，居然被骂神经病并且被泼油？北大博士有话说：为什么不能在室内公共场所吸烟！ #肖美丽​ #室内吸烟​ #泼油​ #被骂神经病​ #四川成都​ #北大博士​ ★推荐影片★ 太神奇了！水中剪玻璃？视频是真的吗？答案揭晓 https://youtu.be/zWhefpDgy1s​ 新冠疫苗，为什么要打两针？打一针可以吗？性命攸关的重要问题！COVID-19 https://youtu.be/JQpdRHJnVt0​ 三星堆文明的精髓，青铜纵目面具的含义：崇拜知识与好奇心 https://youtu.be/vVVEyrbgGgY​ “长赐号”轮船卡住苏伊士运河, 14艘拖船都无法拖出，“罪魁祸首”居然是它？ https://youtu.be/AsP8capcf-k​ 直博与硕博连读的区别，很重要！可能影响你的一生 https://youtu.be/QGZojkqnNKM​ 新冠病毒预防，为何要用肥皂洗手？肥皂不止洗涤还能杀灭病毒的原理！COVID-19 https://youtu.be/PRYXpZrQjec
http://w3id.org/mlsea/pwc/scientificWork/%E9%82%93%E7%8E%89%E8%B1%AA2015%E5%B9%B48%E6%9C%881%E6%97%A5%E5%A3%B0%E6%98%8E%EF%BC%9A%E9%80%80%E5%87%BA%E4%B8%AD%E5%9B%BD%E5%85%B1%E4%BA%A7%E5%85%9A%E4%BB%A5%E5%8F%8A%E4%B8%AD%E5%9B%BD%E5%85%B1%E4%BA%A7%E5%85%9A%E6%89%80%E6%8E%A7%E5%88%B6%E7%9A%84%E6%89%80%E6%9C%89%E7%BB%84%E7%BB%87%E4%B8%8E%E6%9C%BA%E6%9E%84%E3%80%82                                                                                  邓玉豪2015年8月1日声明：退出中国共产党以及中国共产党所控制的所有组织与机构。                                                                                  邓玉豪2015年8月1日声明：退出中国共产党以及中国共产党所控制的所有组织与机构。 声明日期：2015年8月1日 声明人: 邓玉豪 2015-08-01 中国大陆
http://w3id.org/mlsea/pwc/scientificWork/1-Bit%20Compressive%20Sensing%20for%20Efficient%20Federated%20Learning%20Over%20the%20Air                                                                                  1-Bit Compressive Sensing for Efficient Federated Learning Over the Air                                                                                  For distributed learning among collaborative users, this paper develops and analyzes a communication-efficient scheme for federated learning (FL) over the air, which incorporates 1-bit compressive sensing (CS) into analog aggregation transmissions. To facilitate design parameter optimization, we theoretically analyze the efficacy of the proposed scheme by deriving a closed-form expression for the expected convergence rate of the FL over the air. Our theoretical results reveal the tradeoff between convergence performance and communication efficiency as a result of the aggregation errors caused by sparsification, dimension reduction, quantization, signal reconstruction and noise. Then, we formulate 1-bit CS based FL over the air as a joint optimization problem to mitigate the impact of these aggregation errors through joint optimal design of worker scheduling and power scaling policy. An enumeration-based method is proposed to solve this non-convex problem, which is optimal but becomes computationally infeasible as the number of devices increases. For scalable computing, we resort to the alternating direction method of multipliers (ADMM) technique to develop an efficient implementation that is suitable for large-scale networks. Simulation results show that our proposed 1-bit CS based FL over the air achieves comparable performance to the ideal case where conventional FL without compression and quantification is applied over error-free aggregation, at much reduced communication overhead and transmission latency.
http://w3id.org/mlsea/pwc/scientificWork/1-bit%20LAMB%3A%20Communication%20Efficient%20Large-Scale%20Large-Batch%20Training%20with%20LAMB%27s%20Convergence%20Speed                                                                                  1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed                                                                                  To train large models (like BERT and GPT-3) on hundreds of GPUs, communication has become a major bottleneck, especially on commodity systems with limited-bandwidth TCP network. On one side large batch-size optimization such as LAMB algorithm was proposed to reduce the frequency of communication. On the other side, communication compression algorithms such as 1-bit Adam help to reduce the volume of each communication. However, we find that simply using one of the techniques is not sufficient to solve the communication challenge, especially under low network bandwidth. Motivated by this we aim to combine the power of large-batch optimization and communication compression, but we find that existing compression strategies cannot be directly applied to LAMB due to its unique adaptive layerwise learning rates. To this end, we design a new communication-efficient algorithm, 1-bit LAMB, which introduces a novel way to support adaptive layerwise learning rates under compression. In addition, we introduce a new system implementation for compressed communication using the NCCL backend of PyTorch distributed, which improves both usability and performance. For BERT-Large pre-training task with batch sizes from 8K to 64K, our evaluations on up to 256 GPUs demonstrate that 1-bit LAMB with NCCL-based backend is able to achieve up to 4.6x communication volume reduction, up to 2.8x end-to-end time-wise speedup, and the same sample-wise convergence speed (and same fine-tuning task accuracy) compared to uncompressed LAMB.
http://w3id.org/mlsea/pwc/scientificWork/10%20Years%20of%20the%20PCG%20workshop%3A%20Past%20and%20Future%20Trends                                                                                  10 Years of the PCG workshop: Past and Future Trends                                                                                  As of 2020, the international workshop on Procedural Content Generation enters its second decade. The annual workshop, hosted by the international conference on the Foundations of Digital Games, has collected a corpus of 95 papers published in its first 10 years. This paper provides an overview of the workshop's activities and surveys the prevalent research topics emerging over the years.
http://w3id.org/mlsea/pwc/scientificWork/10%2C000%20km%20Straight-line%20Transmission%20using%20a%20Real-time%20Software-defined%20GPU-Based%20Receiver                                                                                  10,000 km Straight-line Transmission using a Real-time Software-defined GPU-Based Receiver                                                                                  Real-time operation of a software-defined, GPU-based optical receiver is demonstrated over a 100-span straight-line optical link. Performance of minimum-phase Kramers-Kronig 4-, 8-, 16-, 32-, and 64-QAM signals are evaluated at various distances.
http://w3id.org/mlsea/pwc/scientificWork/10-mega%20pixel%20snapshot%20compressive%20imaging%20with%20a%20hybrid%20coded%20aperture                                                                                  10-mega pixel snapshot compressive imaging with a hybrid coded aperture                                                                                  High resolution images are widely used in our daily life, whereas high-speed video capture is challenging due to the low frame rate of cameras working at the high resolution mode. Digging deeper, the main bottleneck lies in the low throughput of existing imaging systems. Towards this end, snapshot compressive imaging (SCI) was proposed as a promising solution to improve the throughput of imaging systems by compressive sampling and computational reconstruction. During acquisition, multiple high-speed images are encoded and collapsed to a single measurement. After this, algorithms are employed to retrieve the video frames from the coded snapshot. Recently developed Plug-and-Play (PnP) algorithms make it possible for SCI reconstruction in large-scale problems. However, the lack of high-resolution encoding systems still precludes SCI's wide application. In this paper, we build a novel hybrid coded aperture snapshot compressive imaging (HCA-SCI) system by incorporating a dynamic liquid crystal on silicon and a high-resolution lithography mask. We further implement a PnP reconstruction algorithm with cascaded denoisers for high quality reconstruction. Based on the proposed HCA-SCI system and algorithm, we achieve a 10-mega pixel SCI system to capture high-speed scenes, leading to a high throughput of 4.6G voxels per second. Both simulation and real data experiments verify the feasibility and performance of our proposed HCA-SCI scheme.
http://w3id.org/mlsea/pwc/scientificWork/13.16%20Tbit%2Fs%20Free-space%20Optical%20Transmission%20over%2010.45%20km%20for%20Geostationary%20Satellite%20Feeder-links                                                                                  13.16 Tbit/s Free-space Optical Transmission over 10.45 km for Geostationary Satellite Feeder-links                                                                                  We report a 13.16 Tbit/s 54-channel DWDM free-space optical data transmission field trial emulating the worst-case scenario for optical GEO satellite uplinks through the atmosphere, and show the system performance under turbulent conditions.
http://w3id.org/mlsea/pwc/scientificWork/1st%20Place%20Solution%20for%20YouTubeVOS%20Challenge%202021%3AVideo%20Instance%20Segmentation                                                                                  1st Place Solution for YouTubeVOS Challenge 2021:Video Instance Segmentation                                                                                  Video Instance Segmentation (VIS) is a multi-task problem performing detection, segmentation, and tracking simultaneously. Extended from image set applications, video data additionally induces the temporal information, which, if handled appropriately, is very useful to identify and predict object motions. In this work, we design a unified model to mutually learn these tasks. Specifically, we propose two modules, named Temporally Correlated Instance Segmentation (TCIS) and Bidirectional Tracking (BiTrack), to take the benefit of the temporal correlation between the object's instance masks across adjacent frames. On the other hand, video data is often redundant due to the frame's overlap. Our analysis shows that this problem is particularly severe for the YoutubeVOS-VIS2021 data. Therefore, we propose a Multi-Source Data (MSD) training mechanism to compensate for the data deficiency. By combining these techniques with a bag of tricks, the network performance is significantly boosted compared to the baseline, and outperforms other methods by a considerable margin on the YoutubeVOS-VIS 2019 and 2021 datasets.
http://w3id.org/mlsea/pwc/scientificWork/1st%20Place%20Solution%20to%20ICDAR%202021%20RRC-ICTEXT%20End-to-end%20Text%20Spotting%20and%20Aesthetic%20Assessment%20on%20Integrated%20Circuit                                                                                  1st Place Solution to ICDAR 2021 RRC-ICTEXT End-to-end Text Spotting and Aesthetic Assessment on Integrated Circuit                                                                                  This paper presents our proposed methods to ICDAR 2021 Robust Reading Challenge - Integrated Circuit Text Spotting and Aesthetic Assessment (ICDAR RRC-ICTEXT 2021). For the text spotting task, we detect the characters on integrated circuit and classify them based on yolov5 detection model. We balance the lowercase and non-lowercase by using SynthText, generated data and data sampler. We adopt semi-supervised algorithm and distillation to furtherly improve the model's accuracy. For the aesthetic assessment task, we add a classification branch of 3 classes to differentiate the aesthetic classes of each character. Finally, we make model deployment to accelerate inference speed and reduce memory consumption based on NVIDIA Tensorrt. Our methods achieve 59.1 mAP on task 3.1 with 31 FPS and 306M memory (rank 1), 78.7 % F2 score on task 3.2 with 30 FPS and 306M memory (rank 1).
http://w3id.org/mlsea/pwc/scientificWork/1st%20Place%20Solutions%20for%20UG2%2B%20Challenge%202021%20--%20%28Semi-%29supervised%20Face%20detection%20in%20the%20low%20light%20condition                                                                                  1st Place Solutions for UG2+ Challenge 2021 -- (Semi-)supervised Face detection in the low light condition                                                                                  In this technical report, we briefly introduce the solution of our team 'TAL-ai' for (Semi-) supervised Face detection in the low light condition in UG2+ Challenge in CVPR 2021. By conducting several experiments with popular image enhancement methods and image transfer methods, we pulled the low light image and the normal image to a more closer domain. And it is observed that using these data to training can achieve better performance. We also adapt several popular object detection frameworks, e.g., DetectoRS, Cascade-RCNN, and large backbone like Swin-transformer. Finally, we ensemble several models which achieved mAP 74.89 on the testing set, ranking 1st on the final leaderboard.
http://w3id.org/mlsea/pwc/scientificWork/1xN%20Pattern%20for%20Pruning%20Convolutional%20Neural%20Networks                                                                                  1xN Pattern for Pruning Convolutional Neural Networks                                                                                  Though network pruning receives popularity in reducing the complexity of convolutional neural networks (CNNs), it remains an open issue to concurrently maintain model accuracy as well as achieve significant speedups on general CPUs. In this paper, we propose a novel 1xN pruning pattern to break this limitation. In particular, consecutive N output kernels with the same input channel index are grouped into one block, which serves as a basic pruning granularity of our pruning pattern. Our 1xN pattern prunes these blocks considered unimportant. We also provide a workflow of filter rearrangement that first rearranges the weight matrix in the output channel dimension to derive more influential blocks for accuracy improvements and then applies similar rearrangement to the next-layer weights in the input channel dimension to ensure correct convolutional operations. Moreover, the output computation after our 1xN pruning can be realized via a parallelized block-wise vectorized operation, leading to significant speedups on general CPUs. The efficacy of our pruning pattern is proved with experiments on ILSVRC-2012. For example, given the pruning rate of 50% and N=4, our pattern obtains about 3.0% improvements over filter pruning in the top-1 accuracy of MobileNet-V2. Meanwhile, it obtains 56.04ms inference savings on Cortex-A7 CPU over weight pruning. Our project is made available at https://github.com/lmbxmu/1xN.
http://w3id.org/mlsea/pwc/scientificWork/2.5D%20Visual%20Relationship%20Detection                                                                                  2.5D Visual Relationship Detection                                                                                  Visual 2.5D perception involves understanding the semantics and geometry of a scene through reasoning about object relationships with respect to the viewer in an environment. However, existing works in visual recognition primarily focus on the semantics. To bridge this gap, we study 2.5D visual relationship detection (2.5VRD), in which the goal is to jointly detect objects and predict their relative depth and occlusion relationships. Unlike general VRD, 2.5VRD is egocentric, using the camera's viewpoint as a common reference for all 2.5D relationships. Unlike depth estimation, 2.5VRD is object-centric and not only focuses on depth. To enable progress on this task, we create a new dataset consisting of 220k human-annotated 2.5D relationships among 512K objects from 11K images. We analyze this dataset and conduct extensive experiments including benchmarking multiple state-of-the-art VRD models on this task. Our results show that existing models largely rely on semantic cues and simple heuristics to solve 2.5VRD, motivating further research on models for 2.5D perception. The new dataset is available at https://github.com/google-research-datasets/2.5vrd.
http://w3id.org/mlsea/pwc/scientificWork/20-fold%20Accelerated%207T%20fMRI%20Using%20Referenceless%20Self-Supervised%20Deep%20Learning%20Reconstruction                                                                                  20-fold Accelerated 7T fMRI Using Referenceless Self-Supervised Deep Learning Reconstruction                                                                                  High spatial and temporal resolution across the whole brain is essential to accurately resolve neural activities in fMRI. Therefore, accelerated imaging techniques target improved coverage with high spatio-temporal resolution. Simultaneous multi-slice (SMS) imaging combined with in-plane acceleration are used in large studies that involve ultrahigh field fMRI, such as the Human Connectome Project. However, for even higher acceleration rates, these methods cannot be reliably utilized due to aliasing and noise artifacts. Deep learning (DL) reconstruction techniques have recently gained substantial interest for improving highly-accelerated MRI. Supervised learning of DL reconstructions generally requires fully-sampled training datasets, which is not available for high-resolution fMRI studies. To tackle this challenge, self-supervised learning has been proposed for training of DL reconstruction with only undersampled datasets, showing similar performance to supervised learning. In this study, we utilize a self-supervised physics-guided DL reconstruction on a 5-fold SMS and 4-fold in-plane accelerated 7T fMRI data. Our results show that our self-supervised DL reconstruction produce high-quality images at this 20-fold acceleration, substantially improving on existing methods, while showing similar functional precision and temporal effects in the subsequent analysis compared to a standard 10-fold accelerated acquisition.
http://w3id.org/mlsea/pwc/scientificWork/2D%20vs.%203D%20LiDAR-based%20Person%20Detection%20on%20Mobile%20Robots                                                                                  2D vs. 3D LiDAR-based Person Detection on Mobile Robots                                                                                  Person detection is a crucial task for mobile robots navigating in human-populated environments. LiDAR sensors are promising for this task, thanks to their accurate depth measurements and large field of view. Two types of LiDAR sensors exist: the 2D LiDAR sensors, which scan a single plane, and the 3D LiDAR sensors, which scan multiple planes, thus forming a volume. How do they compare for the task of person detection? To answer this, we conduct a series of experiments, using the public, large-scale JackRabbot dataset and the state-of-the-art 2D and 3D LiDAR-based person detectors (DR-SPAAM and CenterPoint respectively). Our experiments include multiple aspects, ranging from the basic performance and speed comparison, to more detailed analysis on localization accuracy and robustness against distance and scene clutter. The insights from these experiments highlight the strengths and weaknesses of 2D and 3D LiDAR sensors as sources for person detection, and are especially valuable for designing mobile robots that will operate in close proximity to surrounding humans (e.g. service or social robot).
http://w3id.org/mlsea/pwc/scientificWork/2nd%20Place%20Solution%20for%20IJCAI-PRICAI%202020%203D%20AI%20Challenge%3A%203D%20Object%20Reconstruction%20from%20A%20Single%20Image                                                                                  2nd Place Solution for IJCAI-PRICAI 2020 3D AI Challenge: 3D Object Reconstruction from A Single Image                                                                                  In this paper, we present our solution for the { it IJCAI--PRICAI--20 3D AI Challenge: 3D Object Reconstruction from A Single Image}. We develop a variant of AtlasNet that consumes single 2D images and generates 3D point clouds through 2D to 3D mapping. To push the performance to the limit and present guidance on crucial implementation choices, we conduct extensive experiments to analyze the influence of decoder design and different settings on the normalization, projection, and sampling methods. Our method achieves 2nd place in the final track with a score of $70.88$, a chamfer distance of $36.87$, and a mean f-score of $59.18$. The source code of our method will be available at https://github.com/em-data/Enhanced_AtlasNet_3DReconstruction.
http://w3id.org/mlsea/pwc/scientificWork/2nd%20Place%20Solution%20for%20Waymo%20Open%20Dataset%20Challenge%20-%20Real-time%202D%20Object%20Detection                                                                                  2nd Place Solution for Waymo Open Dataset Challenge - Real-time 2D Object Detection                                                                                  In an autonomous driving system, it is essential to recognize vehicles, pedestrians and cyclists from images. Besides the high accuracy of the prediction, the requirement of real-time running brings new challenges for convolutional network models. In this report, we introduce a real-time method to detect the 2D objects from images. We aggregate several popular one-stage object detectors and train the models of variety input strategies independently, to yield better performance for accurate multi-scale detection of each category, especially for small objects. For model acceleration, we leverage TensorRT to optimize the inference time of our detection pipeline. As shown in the leaderboard, our proposed detection framework ranks the 2nd place with 75.00% L1 mAP and 69.72% L2 mAP in the real-time 2D detection track of the Waymo Open Dataset Challenges, while our framework achieves the latency of 45.8ms/frame on an Nvidia Tesla V100 GPU.
http://w3id.org/mlsea/pwc/scientificWork/2nd%20Place%20Solution%20for%20Waymo%20Open%20Dataset%20Challenge%20--%20Real-time%202D%20Object%20Detection                                                                                  2nd Place Solution for Waymo Open Dataset Challenge -- Real-time 2D Object Detection                                                                                  In an autonomous driving system, it is essential to recognize vehicles, pedestrians and cyclists from images. Besides the high accuracy of the prediction, the requirement of real-time running brings new challenges for convolutional network models. In this report, we introduce a real-time method to detect the 2D objects from images. We aggregate several popular one-stage object detectors and train the models of variety input strategies independently, to yield better performance for accurate multi-scale detection of each category, especially for small objects. For model acceleration, we leverage TensorRT to optimize the inference time of our detection pipeline. As shown in the leaderboard, our proposed detection framework ranks the 2nd place with 75.00% L1 mAP and 69.72% L2 mAP in the real-time 2D detection track of the Waymo Open Dataset Challenges, while our framework achieves the latency of 45.8ms/frame on an Nvidia Tesla V100 GPU.
http://w3id.org/mlsea/pwc/scientificWork/2nd-order%20Updates%20with%201st-order%20Complexity                                                                                  2nd-order Updates with 1st-order Complexity                                                                                  It has long been a goal to efficiently compute and use second order information on a function ($f$) to assist in numerical approximations. Here it is shown how, using only basic physics and a numerical approximation, such information can be accurately obtained at a cost of ${ cal O}(N)$ complexity, where $N$ is the dimensionality of the parameter space of $f$. In this paper, an algorithm ({ em VA-Flow}) is developed to exploit this second order information, and pseudocode is presented. It is applied to two classes of problems, that of inverse kinematics (IK) and gradient descent (GD). In the IK application, the algorithm is fast and robust, and is shown to lead to smooth behavior even near singularities. For GD the algorithm also works very well, provided the cost function is locally well-described by a polynomial.
http://w3id.org/mlsea/pwc/scientificWork/2rd%20Place%20Solutions%20in%20the%20HC-STVG%20track%20of%20Person%20in%20Context%20Challenge%202021                                                                                  2rd Place Solutions in the HC-STVG track of Person in Context Challenge 2021                                                                                  In this technical report, we present our solution to localize a spatio-temporal person in an untrimmed video based on a sentence. We achieve the second vIOU(0.30025) in the HC-STVG track of the 3rd Person in Context(PIC) Challenge. Our solution contains three parts: 1) human attributes information is extracted from the sentence, it is helpful to filter out tube proposals in the testing phase and supervise our classifier to learn appearance information in the training phase. 2) we detect humans with YoloV5 and track humans based on the DeepSort framework but replace the original ReID network with FastReID. 3) a visual transformer is used to extract cross-modal representations for localizing a spatio-temporal tube of the target person.
http://w3id.org/mlsea/pwc/scientificWork/3218IR%20at%20SemEval-2020%20Task%2011%3A%20Conv1D%20and%20Word%20Embedding%20in%20Propaganda%20Span%20Identification%20at%20News%20Articles                                                                                  3218IR at SemEval-2020 Task 11: Conv1D and Word Embedding in Propaganda Span Identification at News Articles                                                                                  In this paper, we present the result of our experiment with a variant of 1 Dimensional Convolutional Neural Network (Conv1D) hyper-parameters value. We describe the system entered by the team of Information Retrieval Lab. Universitas Indonesia (3218IR) in the SemEval 2020 Task 11 Sub Task 1 about propaganda span identification in news articles. The best model obtained an F1 score of 0.24 in the development set and 0.23 in the test set. We show that there is a potential for performance improvement through the use of models with appropriate hyper-parameters. Our system uses a combination of Conv1D and GloVe as Word Embedding to detect propaganda in the fragment text level.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Adversarial%20Attacks%20Beyond%20Point%20Cloud                                                                                  3D Adversarial Attacks Beyond Point Cloud                                                                                  Recently, 3D deep learning models have been shown to be susceptible to adversarial attacks like their 2D counterparts. Most of the state-of-the-art (SOTA) 3D adversarial attacks perform perturbation to 3D point clouds. To reproduce these attacks in the physical scenario, a generated adversarial 3D point cloud need to be reconstructed to mesh, which leads to a significant drop in its adversarial effect. In this paper, we propose a strong 3D adversarial attack named Mesh Attack to address this problem by directly performing perturbation on mesh of a 3D object. In order to take advantage of the most effective gradient-based attack, a differentiable sample module that back-propagate the gradient of point cloud to mesh is introduced. To further ensure the adversarial mesh examples without outlier and 3D printable, three mesh losses are adopted. Extensive experiments demonstrate that the proposed scheme outperforms SOTA 3D attacks by a significant margin. We also achieved SOTA performance under various defenses. Our code is available at: https://github.com/cuge1995/Mesh-Attack.
http://w3id.org/mlsea/pwc/scientificWork/3D%20AffordanceNet%3A%20A%20Benchmark%20for%20Visual%20Object%20Affordance%20Understanding                                                                                  3D AffordanceNet: A Benchmark for Visual Object Affordance Understanding                                                                                  The ability to understand the ways to interact with objects from visual cues, a.k.a. visual affordance, is essential to vision-guided robotic research. This involves categorizing, segmenting and reasoning of visual affordance. Relevant studies in 2D and 2.5D image domains have been made previously, however, a truly functional understanding of object affordance requires learning and prediction in the 3D physical domain, which is still absent in the community. In this work, we present a 3D AffordanceNet dataset, a benchmark of 23k shapes from 23 semantic object categories, annotated with 18 visual affordance categories. Based on this dataset, we provide three benchmarking tasks for evaluating visual affordance understanding, including full-shape, partial-view and rotation-invariant affordance estimations. Three state-of-the-art point cloud deep learning networks are evaluated on all tasks. In addition we also investigate a semi-supervised learning setup to explore the possibility to benefit from unlabeled data. Comprehensive results on our contributed dataset show the promise of visual affordance understanding as a valuable yet challenging benchmark.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Convolution%20Neural%20Network%20based%20Person%20Identification%20using%20Gait%20cycles                                                                                  3D Convolution Neural Network based Person Identification using Gait cycles                                                                                  Human identification plays a prominent role in terms of security. In modern times security is becoming the key term for an individual or a country, especially for countries which are facing internal or external threats. Gait analysis is interpreted as the systematic study of the locomotive in humans. It can be used to extract the exact walking features of individuals. Walking features depends on biological as well as the physical feature of the object; hence, it is unique to every individual. In this work, gait features are used to identify an individual. The steps involve object detection, background subtraction, silhouettes extraction, skeletonization, and training 3D Convolution Neural Network on these gait features. The model is trained and evaluated on the dataset acquired by CASIA B Gait, which consists of 15000 videos of 124 subjects walking pattern captured from 11 different angles carrying objects such as bag and coat. The proposed method focuses more on the lower body part to extract features such as the angle between knee and thighs, hip angle, angle of contact, and many other features. The experimental results are compared with amongst accuracies of silhouettes as datasets for training and skeletonized image as training data. The results show that extracting the information from skeletonized data yields improved accuracy.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Convolutional%20Neural%20Networks%20for%20Stalled%20Brain%20Capillary%20Detection                                                                                  3D Convolutional Neural Networks for Stalled Brain Capillary Detection                                                                                  Adequate blood supply is critical for normal brain function. Brain vasculature dysfunctions such as stalled blood flow in cerebral capillaries are associated with cognitive decline and pathogenesis in Alzheimer's disease. Recent advances in imaging technology enabled generation of high-quality 3D images that can be used to visualize stalled blood vessels. However, localization of stalled vessels in 3D images is often required as the first step for downstream analysis, which can be tedious, time-consuming and error-prone, when done manually. Here, we describe a deep learning-based approach for automatic detection of stalled capillaries in brain images based on 3D convolutional neural networks. Our networks employed custom 3D data augmentations and were used weight transfer from pre-trained 2D models for initialization. We used an ensemble of several 3D models to produce the winning submission to the Clog Loss: Advance Alzheimer's Research with Stall Catchers machine learning competition that challenged the participants with classifying blood vessels in 3D image stacks as stalled or flowing. In this setting, our approach outperformed other methods and demonstrated state-of-the-art results, achieving 0.85 Matthews correlation coefficient, 85% sensitivity, and 99.3% specificity. The source code for our solution is made publicly available.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Convolutional%20Neural%20Networks%20for%20Ultrasound-Based%20Silent%20Speech%20Interfaces                                                                                  3D Convolutional Neural Networks for Ultrasound-Based Silent Speech Interfaces                                                                                  Silent speech interfaces (SSI) aim to reconstruct the speech signal from a recording of the articulatory movement, such as an ultrasound video of the tongue. Currently, deep neural networks are the most successful technology for this task. The efficient solution requires methods that do not simply process single images, but are able to extract the tongue movement information from a sequence of video frames. One option for this is to apply recurrent neural structures such as the long short-term memory network (LSTM) in combination with 2D convolutional neural networks (CNNs). Here, we experiment with another approach that extends the CNN to perform 3D convolution, where the extra dimension corresponds to time. In particular, we apply the spatial and temporal convolutions in a decomposed form, which proved very successful recently in video action recognition. We find experimentally that our 3D network outperforms the CNN+LSTM model, indicating that 3D CNNs may be a feasible alternative to CNN+LSTM networks in SSI systems.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Human%20Action%20Representation%20Learning%20via%20Cross-View%20Consistency%20Pursuit                                                                                  3D Human Action Representation Learning via Cross-View Consistency Pursuit                                                                                  In this work, we propose a Cross-view Contrastive Learning framework for unsupervised 3D skeleton-based action Representation (CrosSCLR), by leveraging multi-view complementary supervision signal. CrosSCLR consists of both single-view contrastive learning (SkeletonCLR) and cross-view consistent knowledge mining (CVC-KM) modules, integrated in a collaborative learning manner. It is noted that CVC-KM works in such a way that high-confidence positive/negative samples and their distributions are exchanged among views according to their embedding similarity, ensuring cross-view consistency in terms of contrastive context, i.e., similar distributions. Extensive experiments show that CrosSCLR achieves remarkable action recognition results on NTU-60 and NTU-120 datasets under unsupervised settings, with observed higher-quality action representations. Our code is available at https://github.com/LinguoLi/CrosSCLR.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Human%20Body%20Reshaping%20with%20Anthropometric%20Modeling                                                                                  3D Human Body Reshaping with Anthropometric Modeling                                                                                  Reshaping accurate and realistic 3D human bodies from anthropometric parameters (e.g., height, chest size, etc.) poses a fundamental challenge for person identification, online shopping and virtual reality. Existing approaches for creating such 3D shapes often suffer from complex measurement by range cameras or high-end scanners, which either involve heavy expense cost or result in low quality. However, these high-quality equipments limit existing approaches in real applications, because the equipments are not easily accessible for common users. In this paper, we have designed a 3D human body reshaping system by proposing a novel feature-selection-based local mapping technique, which enables automatic anthropometric parameter modeling for each body facet. Note that the proposed approach can leverage limited anthropometric parameters (i.e., 3-5 measurements) as input, which avoids complex measurement, and thus better user-friendly experience can be achieved in real scenarios. Specifically, the proposed reshaping model consists of three steps. First, we calculate full-body anthropometric parameters from limited user inputs by imputation technique, and thus essential anthropometric parameters for 3D body reshaping can be obtained. Second, we select the most relevant anthropometric parameters for each facet by adopting relevance masks, which are learned offline by the proposed local mapping technique. Third, we generate the 3D body meshes by mapping matrices, which are learned by linear regression from the selected parameters to mesh-based body representation. We conduct experiments by anthropomorphic evaluation and a user study from 68 volunteers. Experiments show the superior results of the proposed system in terms of mean reconstruction error against the state-of-the-art approaches.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Human%20Pose%20Regression%20using%20Graph%20Convolutional%20Network                                                                                  3D Human Pose Regression using Graph Convolutional Network                                                                                  3D human pose estimation is a difficult task, due to challenges such as occluded body parts and ambiguous poses. Graph convolutional networks encode the structural information of the human skeleton in the form of an adjacency matrix, which is beneficial for better pose prediction. We propose one such graph convolutional network named PoseGraphNet for 3D human pose regression from 2D poses. Our network uses an adaptive adjacency matrix and kernels specific to neighbor groups. We evaluate our model on the Human3.6M dataset which is a standard dataset for 3D pose estimation. Our model's performance is close to the state-of-the-art, but with much fewer parameters. The model learns interesting adjacency relations between joints that have no physical connections, but are behaviorally similar.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Iterative%20Spatiotemporal%20Filtering%20for%20Classification%20of%20Multitemporal%20Satellite%20Data%20Sets                                                                                  3D Iterative Spatiotemporal Filtering for Classification of Multitemporal Satellite Data Sets                                                                                  The current practice in land cover/land use change analysis relies heavily on the individually classified maps of the multitemporal data set. Due to varying acquisition conditions (e.g., illumination, sensors, seasonal differences), the classification maps yielded are often inconsistent through time for robust statistical analysis. 3D geometric features have been shown to be stable for assessing differences across the temporal data set. Therefore, in this article we investigate he use of a multitemporal orthophoto and digital surface model derived from satellite data for spatiotemporal classification. Our approach consists of two major steps: generating per-class probability distribution maps using the random-forest classifier with limited training samples, and making spatiotemporal inferences using an iterative 3D spatiotemporal filter operating on per-class probability maps. Our experimental results demonstrate that the proposed methods can consistently improve the individual classification results by 2%-6% and thus can be an important postclassification refinement approach.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Near-Field%20Millimeter-Wave%20Synthetic%20Aperture%20Radar%20Imaging                                                                                  3D Near-Field Millimeter-Wave Synthetic Aperture Radar Imaging                                                                                  In this paper, we present 3D high resolution radar imaging process at millimeter-Wave (mmWave) frequencies by creating the effect of a large aperture synthetically. We use a low cost fully integrated Frequency Modulated Continuous Wave (FMCW) radar operating at $ rm 79 ;GHz$ and then perform Synthetic Aperture Radar (SAR) imaging in the near-filed zone. At the end, we conduct a real experiment and present the reconstructed image.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Object%20Detection%20for%20Autonomous%20Driving%3A%20A%20Survey                                                                                  3D Object Detection for Autonomous Driving: A Survey                                                                                  Autonomous driving is regarded as one of the most promising remedies to shield human beings from severe crashes. To this end, 3D object detection serves as the core basis of perception stack especially for the sake of path planning, motion prediction, and collision avoidance etc. Taking a quick glance at the progress we have made, we attribute challenges to visual appearance recovery in the absence of depth information from images, representation learning from partially occluded unstructured point clouds, and semantic alignments over heterogeneous features from cross modalities. Despite existing efforts, 3D object detection for autonomous driving is still in its infancy. Recently, a large body of literature have been investigated to address this 3D vision task. Nevertheless, few investigations have looked into collecting and structuring this growing knowledge. We therefore aim to fill this gap in a comprehensive survey, encompassing all the main concerns including sensors, datasets, performance metrics and the recent state-of-the-art detection methods, together with their pros and cons. Furthermore, we provide quantitative comparisons with the state of the art. A case study on fifteen selected representative methods is presented, involved with runtime analysis, error analysis, and robustness analysis. Finally, we provide concluding remarks after an in-depth analysis of the surveyed works and identify promising directions for future work.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Scene%20Compression%20through%20Entropy%20Penalized%20Neural%20Representation%20Functions                                                                                  3D Scene Compression through Entropy Penalized Neural Representation Functions                                                                                  Some forms of novel visual media enable the viewer to explore a 3D scene from arbitrary viewpoints, by interpolating between a discrete set of original views. Compared to 2D imagery, these types of applications require much larger amounts of storage space, which we seek to reduce. Existing approaches for compressing 3D scenes are based on a separation of compression and rendering: each of the original views is compressed using traditional 2D image formats; the receiver decompresses the views and then performs the rendering. We unify these steps by directly compressing an implicit representation of the scene, a function that maps spatial coordinates to a radiance vector field, which can then be queried to render arbitrary viewpoints. The function is implemented as a neural network and jointly trained for reconstruction as well as compressibility, in an end-to-end manner, with the use of an entropy penalty on the parameters. Our method significantly outperforms a state-of-the-art conventional approach for scene compression, achieving simultaneously higher quality reconstructions and lower bitrates. Furthermore, we show that the performance at lower bitrates can be improved by jointly representing multiple scenes using a soft form of parameter sharing.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Segmentation%20Learning%20from%20Sparse%20Annotations%20and%20Hierarchical%20Descriptors                                                                                  3D Segmentation Learning from Sparse Annotations and Hierarchical Descriptors                                                                                  One of the main obstacles to 3D semantic segmentation is the significant amount of endeavor required to generate expensive point-wise annotations for fully supervised training. To alleviate manual efforts, we propose GIDSeg, a novel approach that can simultaneously learn segmentation from sparse annotations via reasoning global-regional structures and individual-vicinal properties. GIDSeg depicts global- and individual- relation via a dynamic edge convolution network coupled with a kernelized identity descriptor. The ensemble effects are obtained by endowing a fine-grained receptive field to a low-resolution voxelized map. In our GIDSeg, an adversarial learning module is also designed to further enhance the conditional constraint of identity descriptors within the joint feature distribution. Despite the apparent simplicity, our proposed approach achieves superior performance over state-of-the-art for inferencing 3D dense segmentation with only sparse annotations. Particularly, with $5 %$ annotations of raw data, GIDSeg outperforms other 3D segmentation methods.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Semantic%20Mapping%20from%20Arthroscopy%20using%20Out-of-distribution%20Pose%20and%20Depth%20and%20In-distribution%20Segmentation%20Training                                                                                  3D Semantic Mapping from Arthroscopy using Out-of-distribution Pose and Depth and In-distribution Segmentation Training                                                                                  Minimally invasive surgery (MIS) has many documented advantages, but the surgeon's limited visual contact with the scene can be problematic. Hence, systems that can help surgeons navigate, such as a method that can produce a 3D semantic map, can compensate for the limitation above. In theory, we can borrow 3D semantic mapping techniques developed for robotics, but this requires finding solutions to the following challenges in MIS: 1) semantic segmentation, 2) depth estimation, and 3) pose estimation. In this paper, we propose the first 3D semantic mapping system from knee arthroscopy that solves the three challenges above. Using out-of-distribution non-human datasets, where pose could be labeled, we jointly train depth+pose estimators using selfsupervised and supervised losses. Using an in-distribution human knee dataset, we train a fully-supervised semantic segmentation system to label arthroscopic image pixels into femur, ACL, and meniscus. Taking testing images from human knees, we combine the results from these two systems to automatically create 3D semantic maps of the human knee. The result of this work opens the pathway to the generation of intraoperative 3D semantic mapping, registration with pre-operative data, and robotic-assisted arthroscopy
http://w3id.org/mlsea/pwc/scientificWork/3D%20Semantic%20Scene%20Completion%3A%20a%20Survey                                                                                  3D Semantic Scene Completion: a Survey                                                                                  Semantic Scene Completion (SSC) aims to jointly estimate the complete geometry and semantics of a scene, assuming partial sparse input. In the last years following the multiplication of large-scale 3D datasets, SSC has gained significant momentum in the research community because it holds unresolved challenges. Specifically, SSC lies in the ambiguous completion of large unobserved areas and the weak supervision signal of the ground truth. This led to a substantially increasing number of papers on the matter. This survey aims to identify, compare and analyze the techniques providing a critical analysis of the SSC literature on both methods and datasets. Throughout the paper, we provide an in-depth analysis of the existing works covering all choices made by the authors while highlighting the remaining avenues of research. SSC performance of the SoA on the most popular datasets is also evaluated and analyzed.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Shape%20Generation%20and%20Completion%20through%20Point-Voxel%20Diffusion                                                                                  3D Shape Generation and Completion through Point-Voxel Diffusion                                                                                  We propose a novel approach for probabilistic generative modeling of 3D shapes. Unlike most existing models that learn to deterministically translate a latent vector to a shape, our model, Point-Voxel Diffusion (PVD), is a unified, probabilistic formulation for unconditional shape generation and conditional, multi-modal shape completion. PVD marries denoising diffusion models with the hybrid, point-voxel representation of 3D shapes. It can be viewed as a series of denoising steps, reversing the diffusion process from observed point cloud data to Gaussian noise, and is trained by optimizing a variational lower bound to the (conditional) likelihood function. Experiments demonstrate that PVD is capable of synthesizing high-fidelity shapes, completing partial point clouds, and generating multiple completion results from single-view depth scans of real objects.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Shape%20Registration%20Using%20Spectral%20Graph%20Embedding%20and%20Probabilistic%20Matching                                                                                  3D Shape Registration Using Spectral Graph Embedding and Probabilistic Matching                                                                                  We address the problem of 3D shape registration and we propose a novel technique based on spectral graph theory and probabilistic matching. The task of 3D shape analysis involves tracking, recognition, registration, etc. Analyzing 3D data in a single framework is still a challenging task considering the large variability of the data gathered with different acquisition devices. 3D shape registration is one such challenging shape analysis task. The main contribution of this chapter is to extend the spectral graph matching methods to very large graphs by combining spectral graph matching with Laplacian embedding. Since the embedded representation of a graph is obtained by dimensionality reduction we claim that the existing spectral-based methods are not easily applicable. We discuss solutions for the exact and inexact graph isomorphism problems and recall the main spectral properties of the combinatorial graph Laplacian; We provide a novel analysis of the commute-time embedding that allows us to interpret the latter in terms of the PCA of a graph, and to select the appropriate dimension of the associated embedded metric space; We derive a unit hyper-sphere normalization for the commute-time embedding that allows us to register two shapes with different samplings; We propose a novel method to find the eigenvalue-eigenvector ordering and the eigenvector signs using the eigensignature (histogram) which is invariant to the isometric shape deformations and fits well in the spectral graph matching framework, and we present a probabilistic shape matching formulation using an expectation maximization point registration algorithm which alternates between aligning the eigenbases and finding a vertex-to-vertex assignment.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Spatial%20Recognition%20without%20Spatially%20Labeled%203D                                                                                  3D Spatial Recognition without Spatially Labeled 3D                                                                                  We introduce WyPR, a Weakly-supervised framework for Point cloud Recognition, requiring only scene-level class tags as supervision. WyPR jointly addresses three core 3D recognition tasks: point-level semantic segmentation, 3D proposal generation, and 3D object detection, coupling their predictions through self and cross-task consistency losses. We show that in conjunction with standard multiple-instance learning objectives, WyPR can detect and segment objects in point cloud data without access to any spatial labels at training time. We demonstrate its efficacy using the ScanNet and S3DIS datasets, outperforming prior state of the art on weakly-supervised segmentation by more than 6% mIoU. In addition, we set up the first benchmark for weakly-supervised 3D object detection on both datasets, where WyPR outperforms standard approaches and establishes strong baselines for future work.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Spectrum%20Mapping%20Based%20on%20ROI-Driven%20UAV%20Deployment                                                                                  3D Spectrum Mapping Based on ROI-Driven UAV Deployment                                                                                  Given the explosive growth of Internet of Things (IoT) devices ranging from the two-dimensional (2D) ground to the three-dimensional (3D) space, it is a necessity to establish a 3D spectrum map to comprehensively present and effectively manage the 3D spatial spectrum resources in smart city infrastructures. By leveraging the popularity and location flexibility of the unmanned aerial vehicles (UAVs), we are able to execute spatial sampling with these emerging flying spectrum-monitoring devices (SMDs) at will. In this paper, we first present a brief survey to show the state-of-the-art studies on spectrum mapping. Then, we introduce the 3D spectrum mapping model. Next, we propose a 3D spectrum mapping framework which is composed of pre-sampling, spectrum situation estimation, UAV deployment and spectrum recovery. Therein we develop a Region of Interest (ROI)-driven UAV deployment scheme, which selects new sampling points of the highest estimated interest and the lowest energy cost iteratively. Meanwhile, we slice the entire 3D spectrum map into a series of 'images' and 'repair' those unsampled locations. Furthermore, we provide an exemplary case study on the 3D spectrum mapping, where, for example, an important event is being held and the entire spectrum situation needs to be monitored in real time to deal with malicious interference sources. Lastly, the challenges and open issues are discussed.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Surfel%20Map-Aided%20Visual%20Relocalization%20with%20Learned%20Descriptors                                                                                  3D Surfel Map-Aided Visual Relocalization with Learned Descriptors                                                                                  In this paper, we introduce a method for visual relocalization using the geometric information from a 3D surfel map. A visual database is first built by global indices from the 3D surfel map rendering, which provides associations between image points and 3D surfels. Surfel reprojection constraints are utilized to optimize the keyframe poses and map points in the visual database. A hierarchical camera relocalization algorithm then utilizes the visual database to estimate 6-DoF camera poses. Learned descriptors are further used to improve the performance in challenging cases. We present evaluation under real-world conditions and simulation to show the effectiveness and efficiency of our method, and make the final camera poses consistently well aligned with the 3D environment.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Tensor-based%20Deep%20Learning%20Models%20for%20Predicting%20Option%20Price                                                                                  3D Tensor-based Deep Learning Models for Predicting Option Price                                                                                  Option pricing is a significant problem for option risk management and trading. In this article, we utilize a framework to present financial data from different sources. The data is processed and represented in a form of 2D tensors in three channels. Furthermore, we propose two deep learning models that can deal with 3D tensor data. Experiments performed on the Chinese market option dataset prove the practicability of the proposed strategies over commonly used ways, including B-S model and vector-based LSTM.
http://w3id.org/mlsea/pwc/scientificWork/3D%20U-NetR%3A%20Low%20Dose%20Computed%20Tomography%20Reconstruction%20via%20Deep%20Learning%20and%203%20Dimensional%20Convolutions                                                                                  3D U-NetR: Low Dose Computed Tomography Reconstruction via Deep Learning and 3 Dimensional Convolutions                                                                                  In this paper, we introduced a novel deep learning-based reconstruction technique for low-dose CT imaging using 3 dimensional convolutions to include the sagittal information unlike the existing 2 dimensional networks which exploits correlation only in transverse plane. In the proposed reconstruction technique, sparse and noisy sinograms are back-projected to the image domain with FBP operation, then the denoising process is applied with a U-Net like 3-dimensional network called 3D U-NetR. The proposed network is trained with synthetic and real chest CT images, and 2D U-Net is also trained with the same dataset to show the importance of the third dimension in terms of recovering the fine details. The proposed network shows better quantitative performance on SSIM and PSNR, especially in the real chest CT data. More importantly, 3D U-NetR captures medically critical visual details that cannot be visualized by a 2D network on the reconstruction of real CT images with 1/10 of the normal dose.
http://w3id.org/mlsea/pwc/scientificWork/3D%20UAV%20Trajectory%20and%20Data%20Collection%20Optimisation%20via%20Deep%20Reinforcement%20Learning                                                                                  3D UAV Trajectory and Data Collection Optimisation via Deep Reinforcement Learning                                                                                  Unmanned aerial vehicles (UAVs) are now beginning to be deployed for enhancing the network performance and coverage in wireless communication. However, due to the limitation of their on-board power and flight time, it is challenging to obtain an optimal resource allocation scheme for the UAV-assisted Internet of Things (IoT). In this paper, we design a new UAV-assisted IoT systems relying on the shortest flight path of the UAVs while maximising the amount of data collected from IoT devices. Then, a deep reinforcement learning-based technique is conceived for finding the optimal trajectory and throughput in a specific coverage area. After training, the UAV has the ability to autonomously collect all the data from user nodes at a significant total sum-rate improvement while minimising the associated resources used. Numerical results are provided to highlight how our techniques strike a balance between the throughput attained, trajectory, and the time spent. More explicitly, we characterise the attainable performance in terms of the UAV trajectory, the expected reward and the total sum-rate.
http://w3id.org/mlsea/pwc/scientificWork/3D%20Video%20Stabilization%20With%20Depth%20Estimation%20by%20CNN-Based%20Optimization                                                                                  3D Video Stabilization With Depth Estimation by CNN-Based Optimization                                                                                   Video stabilization is an essential component of visual quality enhancement. Early methods rely on feature tracking to recover either 2D or 3D frame motion, which suffer from the robustness of local feature extraction and tracking in shaky videos. Recently, learning-based methods seek to find frame transformations with high-level information via deep neural networks to overcome the robustness issue of feature tracking. Nevertheless, to our best knowledge, no learning-based methods leverage 3D cues for the transformation inference yet; hence they would lead to artifacts on complex scene-depth scenarios. In this paper, we propose Deep3D Stabilizer, a novel 3D depth-based learning method for video stabilization. We take advantage of the recent self-supervised framework on jointly learning depth and camera ego-motion estimation on raw videos. Our approach requires no data for pre-training but stabilizes the input video via 3D reconstruction directly. The rectification stage incorporates the 3D scene depth and camera motion to smooth the camera trajectory and synthesize the stabilized video. Unlike most one-size-fits-all learning-based methods, our smoothing algorithm allows users to manipulate the stability of a video efficiently. Experimental results on challenging benchmarks show that the proposed solution consistently outperforms the state-of-the-art methods on almost all motion categories. 
http://w3id.org/mlsea/pwc/scientificWork/3D%20beamforming%20and%20handover%20analysis%20for%20UAV%20networks                                                                                  3D beamforming and handover analysis for UAV networks                                                                                  In future drone applications fast moving unmanned aerial vehicles (UAVs) will need to be connected via a high throughput ultra reliable wireless link. MmWave communication is assumed to be a promising technology for UAV communication, as the narrow beams cause little interference to and from the ground. A challenge for such networks is the beamforming requirement, and the fact that frequent handovers are required as the cells are small. In the UAV communication research community, mobility and especially handovers are often neglected, however when considering beamforming, antenna array sizes start to matter and the effect of azimuth and elevation should be studied, especially their impact on handover rate and outage capacity. This paper aims to fill some of this knowledge gap and to shed some light on the existing problems. This work will analyse the performance of 3D beamforming and handovers for UAV networks through a case study of a realistic 5G deployment using mmWave. We will look at the performance of a UAV flying over a city utilizing a beamformed mmWave link.
http://w3id.org/mlsea/pwc/scientificWork/3D%20map%20creation%20using%20crowdsourced%20GNSS%20data                                                                                  3D map creation using crowdsourced GNSS data                                                                                  3D maps are increasingly useful for many applications such as drone navigation, emergency services, and urban planning. However, creating 3D maps and keeping them up-to-date using existing technologies, such as laser scanners, is expensive. This paper proposes and implements a novel approach to generate 2.5D (otherwise known as 3D level-of-detail (LOD) 1) maps for free using Global Navigation Satellite Systems (GNSS) signals, which are globally available and are blocked only by obstacles between the satellites and the receivers. This enables us to find the patterns of GNSS signal availability and create 3D maps. The paper applies algorithms to GNSS signal strength patterns based on a boot-strapped technique that iteratively trains the signal classifiers while generating the map. Results of the proposed technique demonstrate the ability to create 3D maps using automatically processed GNSS data. The results show that the third dimension, i.e. height of the buildings, can be estimated with below 5 metre accuracy, which is the benchmark recommended by the CityGML standard.
http://w3id.org/mlsea/pwc/scientificWork/3D%2F2D%20regularized%20CNN%20feature%20hierarchy%20for%20Hyperspectral%20image%20classification                                                                                  3D/2D regularized CNN feature hierarchy for Hyperspectral image classification                                                                                  Convolutional Neural Networks (CNN) have been rigorously studied for Hyperspectral Image Classification (HSIC) and are known to be effective in exploiting joint spatial-spectral information with the expense of lower generalization performance and learning speed due to the hard labels and non-uniform distribution over labels. Several regularization techniques have been used to overcome the aforesaid issues. However, sometimes models learn to predict the samples extremely confidently which is not good from a generalization point of view. Therefore, this paper proposed an idea to enhance the generalization performance of a hybrid CNN for HSIC using soft labels that are a weighted average of the hard labels and uniform distribution over ground labels. The proposed method helps to prevent CNN from becoming over-confident. We empirically show that in improving generalization performance, label smoothing also improves model calibration which significantly improves beam-search. Several publicly available Hyperspectral datasets are used to validate the experimental evaluation which reveals improved generalization performance, statistical significance, and computational complexity as compared to the state-of-the-art models. The code will be made available at https://github.com/mahmad00.
http://w3id.org/mlsea/pwc/scientificWork/3D-Aware%20Ellipse%20Prediction%20for%20Object-Based%20Camera%20Pose%20Estimation                                                                                  3D-Aware Ellipse Prediction for Object-Based Camera Pose Estimation                                                                                  In this paper, we propose a method for coarse camera pose computation which is robust to viewing conditions and does not require a detailed model of the scene. This method meets the growing need of easy deployment of robotics or augmented reality applications in any environments, especially those for which no accurate 3D model nor huge amount of ground truth data are available. It exploits the ability of deep learning techniques to reliably detect objects regardless of viewing conditions. Previous works have also shown that abstracting the geometry of a scene of objects by an ellipsoid cloud allows to compute the camera pose accurately enough for various application needs. Though promising, these approaches use the ellipses fitted to the detection bounding boxes as an approximation of the imaged objects. In this paper, we go one step further and propose a learning-based method which detects improved elliptic approximations of objects which are coherent with the 3D ellipsoid in terms of perspective projection. Experiments prove that the accuracy of the computed pose significantly increases thanks to our method and is more robust to the variability of the boundaries of the detection boxes. This is achieved with very little effort in terms of training data acquisition -- a few hundred calibrated images of which only three need manual object annotation. Code and models are released at https://github.com/zinsmatt/3D-Aware-Ellipses-for-Visual-Localization.
http://w3id.org/mlsea/pwc/scientificWork/3D-CNN%20for%20Facial%20Micro-%20and%20Macro-expression%20Spotting%20on%20Long%20Video%20Sequences%20using%20Temporal%20Oriented%20Reference%20Frame                                                                                  3D-CNN for Facial Micro- and Macro-expression Spotting on Long Video Sequences using Temporal Oriented Reference Frame                                                                                  Facial expression spotting is the preliminary step for micro- and macro-expression analysis. The task of reliably spotting such expressions in video sequences is currently unsolved. The current best systems depend upon optical flow methods to extract regional motion features, before categorisation of that motion into a specific class of facial movement. Optical flow is susceptible to drift error, which introduces a serious problem for motions with long-term dependencies, such as high frame-rate macro-expression. We propose a purely deep learning solution which, rather than tracking frame differential motion, compares via a convolutional model, each frame with two temporally local reference frames. Reference frames are sampled according to calculated micro- and macro-expression duration. As baseline for MEGC2021 using leave-one-subject-out evaluation method, we show that our solution achieves F1-score of 0.105 in a high frame-rate (200 fps) SAMM long videos dataset (SAMM-LV) and is competitive in a low frame-rate (30 fps) (CAS(ME)2) dataset. On unseen MEGC2022 challenge dataset, the baseline results are 0.1176 on SAMM Challenge dataset, 0.1739 on CAS(ME)3 and overall performance of 0.1531 on both dataset.
http://w3id.org/mlsea/pwc/scientificWork/3D-FFS%3A%20Faster%203D%20object%20detection%20with%20Focused%20Frustum%20Search%20in%20sensor%20fusion%20based%20networks                                                                                  3D-FFS: Faster 3D object detection with Focused Frustum Search in sensor fusion based networks                                                                                  In this work we propose 3D-FFS, a novel approach to make sensor fusion based 3D object detection networks significantly faster using a class of computationally inexpensive heuristics. Existing sensor fusion based networks generate 3D region proposals by leveraging inferences from 2D object detectors. However, as images have no depth information, these networks rely on extracting semantic features of points from the entire scene to locate the object. By leveraging aggregated intrinsic properties (e.g. point density) of point cloud data, 3D-FFS can substantially constrain the 3D search space and thereby significantly reduce training time, inference time and memory consumption without sacrificing accuracy. To demonstrate the efficacy of 3D-FFS, we have integrated it with Frustum ConvNet (F-ConvNet), a prominent sensor fusion based 3D object detection model. We assess the performance of 3D-FFS on the KITTI dataset. Compared to F-ConvNet, we achieve improvements in training and inference times by up to 62.80% and 58.96%, respectively, while reducing the memory usage by up to 58.53%. Additionally, we achieve 0.36%, 0.59% and 2.19% improvements in accuracy for the Car, Pedestrian and Cyclist classes, respectively. 3D-FFS shows a lot of promise in domains with limited computing power, such as autonomous vehicles, drones and robotics where LiDAR-Camera based sensor fusion perception systems are widely used.
http://w3id.org/mlsea/pwc/scientificWork/3D-MAN%3A%203D%20Multi-frame%20Attention%20Network%20for%20Object%20Detection                                                                                  3D-MAN: 3D Multi-frame Attention Network for Object Detection                                                                                  3D object detection is an important module in autonomous driving and robotics. However, many existing methods focus on using single frames to perform 3D detection, and do not fully utilize information from multiple frames. In this paper, we present 3D-MAN: a 3D multi-frame attention network that effectively aggregates features from multiple perspectives and achieves state-of-the-art performance on Waymo Open Dataset. 3D-MAN first uses a novel fast single-frame detector to produce box proposals. The box proposals and their corresponding feature maps are then stored in a memory bank. We design a multi-view alignment and aggregation module, using attention networks, to extract and aggregate the temporal features stored in the memory bank. This effectively combines the features coming from different perspectives of the scene. We demonstrate the effectiveness of our approach on the large-scale complex Waymo Open Dataset, achieving state-of-the-art results compared to published single-frame and multi-frame methods.
http://w3id.org/mlsea/pwc/scientificWork/3D-TalkEmo%3A%20Learning%20to%20Synthesize%203D%20Emotional%20Talking%20Head                                                                                  3D-TalkEmo: Learning to Synthesize 3D Emotional Talking Head                                                                                  Impressive progress has been made in audio-driven 3D facial animation recently, but synthesizing 3D talking-head with rich emotion is still unsolved. This is due to the lack of 3D generative models and available 3D emotional dataset with synchronized audios. To address this, we introduce 3D-TalkEmo, a deep neural network that generates 3D talking head animation with various emotions. We also create a large 3D dataset with synchronized audios and videos, rich corpus, as well as various emotion states of different persons with the sophisticated 3D face reconstruction methods. In the emotion generation network, we propose a novel 3D face representation structure - geometry map by classical multi-dimensional scaling analysis. It maps the coordinates of vertices on a 3D face to a canonical image plane, while preserving the vertex-to-vertex geodesic distance metric in a least-square sense. This maintains the adjacency relationship of each vertex and holds the effective convolutional structure for the 3D facial surface. Taking a neutral 3D mesh and a speech signal as inputs, the 3D-TalkEmo is able to generate vivid facial animations. Moreover, it provides access to change the emotion state of the animated speaker. We present extensive quantitative and qualitative evaluation of our method, in addition to user studies, demonstrating the generated talking-heads of significantly higher quality compared to previous state-of-the-art methods.
http://w3id.org/mlsea/pwc/scientificWork/3D-to-2D%20Distillation%20for%20Indoor%20Scene%20Parsing                                                                                  3D-to-2D Distillation for Indoor Scene Parsing                                                                                  Indoor scene semantic parsing from RGB images is very challenging due to occlusions, object distortion, and viewpoint variations. Going beyond prior works that leverage geometry information, typically paired depth maps, we present a new approach, a 3D-to-2D distillation framework, that enables us to leverage 3D features extracted from large-scale 3D data repository (e.g., ScanNet-v2) to enhance 2D features extracted from RGB images. Our work has three novel contributions. First, we distill 3D knowledge from a pretrained 3D network to supervise a 2D network to learn simulated 3D features from 2D features during the training, so the 2D network can infer without requiring 3D data. Second, we design a two-stage dimension normalization scheme to calibrate the 2D and 3D features for better integration. Third, we design a semantic-aware adversarial training model to extend our framework for training with unpaired 3D data. Extensive experiments on various datasets, ScanNet-V2, S3DIS, and NYU-v2, demonstrate the superiority of our approach. Also, experimental results show that our 3D-to-2D distillation improves the model generalization.
http://w3id.org/mlsea/pwc/scientificWork/3D3L%3A%20Deep%20Learned%203D%20Keypoint%20Detection%20and%20Description%20for%20LiDARs                                                                                  3D3L: Deep Learned 3D Keypoint Detection and Description for LiDARs                                                                                  With the advent of powerful, light-weight 3D LiDARs, they have become the hearth of many navigation and SLAM algorithms on various autonomous systems. Pointcloud registration methods working with unstructured pointclouds such as ICP are often computationally expensive or require a good initial guess. Furthermore, 3D feature-based registration methods have never quite reached the robustness of 2D methods in visual SLAM. With the continuously increasing resolution of LiDAR range images, these 2D methods not only become applicable but should exploit the illumination-independent modalities that come with it, such as depth and intensity. In visual SLAM, deep learned 2D features and descriptors perform exceptionally well compared to traditional methods. In this publication, we use a state-of-the-art 2D feature network as a basis for 3D3L, exploiting both intensity and depth of LiDAR range images to extract powerful 3D features. Our results show that these keypoints and descriptors extracted from LiDAR scan images outperform state-of-the-art on different benchmark metrics and allow for robust scan-to-scan alignment as well as global localization.
http://w3id.org/mlsea/pwc/scientificWork/3DB%3A%20A%20Framework%20for%20Debugging%20Computer%20Vision%20Models                                                                                  3DB: A Framework for Debugging Computer Vision Models                                                                                  We introduce 3DB: an extendable, unified framework for testing and debugging vision models using photorealistic simulation. We demonstrate, through a wide range of use cases, that 3DB allows users to discover vulnerabilities in computer vision systems and gain insights into how models make decisions. 3DB captures and generalizes many robustness analyses from prior work, and enables one to study their interplay. Finally, we find that the insights generated by the system transfer to the physical world. We are releasing 3DB as a library (https://github.com/3db/3db) alongside a set of example analyses, guides, and documentation: https://3db.github.io/3db/ .
http://w3id.org/mlsea/pwc/scientificWork/3DBodyNet%3A%20Fast%20Reconstruction%20of%203D%20Animatable%20Human%20Body%20Shape%20from%20a%20Single%20Commodity%20Depth%20Camera                                                                                  3DBodyNet: Fast Reconstruction of 3D Animatable Human Body Shape from a Single Commodity Depth Camera                                                                                  Knowledge about individual body shape has numerous applications in various domains such as healthcare, fashion and personalized entertainment. Most of the depth based whole body scanners need multiple cameras surrounding the user and requiring the user to keep a canonical pose strictly during capturing depth images. These scanning devices are expensive and need professional knowledge for operation. In order to make 3D scanning as easy-to-use and fast as possible, there is a great demand to simplify the process and to reduce the hardware requirements. In this paper, we propose a deep learning algorithm, dubbed 3DBodyNet, to rapidly reconstruct the 3D shape of human bodies using a single commodity depth camera. As easy-to-use as taking a photo using a mobile phone, our algorithm only needs two depth images of the front-facing and back-facing bodies. The proposed algorithm has strong operability since it is insensitive to the pose and the pose variations between the two depth images. It can also reconstruct an accurate body shape for users under tight/loose clothing. Another advantage of our method is the ability to generate an animatable human body model. Extensive experimental results show that the proposed method enables robust and easy-to-use animatable human body reconstruction, and outperforms the state-of-the-art methods with respect to running time and accuracy.
http://w3id.org/mlsea/pwc/scientificWork/3KG%3A%20Contrastive%20Learning%20of%2012-Lead%20Electrocardiograms%20using%20Physiologically-Inspired%20Augmentations                                                                                  3KG: Contrastive Learning of 12-Lead Electrocardiograms using Physiologically-Inspired Augmentations                                                                                  We propose 3KG, a physiologically-inspired contrastive learning approach that generates views using 3D augmentations of the 12-lead electrocardiogram. We evaluate representation quality by fine-tuning a linear layer for the downstream task of 23-class diagnosis on the PhysioNet 2020 challenge training data and find that 3KG achieves a $9.1 %$ increase in mean AUC over the best self-supervised baseline when trained on $1 %$ of labeled data. Our empirical analysis shows that combining spatial and temporal augmentations produces the strongest representations. In addition, we investigate the effect of this physiologically-inspired pretraining on downstream performance on different disease subgroups and find that 3KG makes the greatest gains for conduction and rhythm abnormalities. Our method allows for flexibility in incorporating other self-supervised strategies and highlights the potential for similar modality-specific augmentations for other biomedical signals.
http://w3id.org/mlsea/pwc/scientificWork/3U-EdgeAI%3A%20Ultra-Low%20Memory%20Training%2C%20Ultra-Low%20BitwidthQuantization%2C%20and%20Ultra-Low%20Latency%20Acceleration                                                                                  3U-EdgeAI: Ultra-Low Memory Training, Ultra-Low BitwidthQuantization, and Ultra-Low Latency Acceleration                                                                                  The deep neural network (DNN) based AI applications on the edge require both low-cost computing platforms and high-quality services. However, the limited memory, computing resources, and power budget of the edge devices constrain the effectiveness of the DNN algorithms. Developing edge-oriented AI algorithms and implementations (e.g., accelerators) is challenging. In this paper, we summarize our recent efforts for efficient on-device AI development from three aspects, including both training and inference. First, we present on-device training with ultra-low memory usage. We propose a novel rank-adaptive tensor-based tensorized neural network model, which offers orders-of-magnitude memory reduction during training. Second, we introduce an ultra-low bitwidth quantization method for DNN model compression, achieving the state-of-the-art accuracy under the same compression ratio. Third, we introduce an ultra-low latency DNN accelerator design, practicing the software/hardware co-design methodology. This paper emphasizes the importance and efficacy of training, quantization and accelerator design, and calls for more research breakthroughs in the area for AI on the edge.
http://w3id.org/mlsea/pwc/scientificWork/3rd%20Place%20Solution%20for%20Short-video%20Face%20Parsing%20Challenge                                                                                  3rd Place Solution for Short-video Face Parsing Challenge                                                                                  This is a short technical report introducing the solution of Team Rat for Short-video Parsing Face Parsing Track of The 3rd Person in Context (PIC) Workshop and Challenge at CVPR 2021. In this report, we propose an Edge-Aware Network (EANet) that uses edge information to refine the segmentation edge. To further obtain the finer edge results, we introduce edge attention loss that only compute cross entropy on the edges, it can effectively reduce the classification error around edge and get more smooth boundary. Benefiting from the edge information and edge attention loss, the proposed EANet achieves 86.16 % accuracy in the Short-video Face Parsing track of the 3rd Person in Context (PIC) Workshop and Challenge, ranked the third place.
http://w3id.org/mlsea/pwc/scientificWork/4C%3A%20A%20Computation%2C%20Communication%2C%20and%20Control%20Co-Design%20Framework%20for%20CAVs                                                                                  4C: A Computation, Communication, and Control Co-Design Framework for CAVs                                                                                  Connected and autonomous vehicles (CAVs) are promising due to their potential safety and efficiency benefits and have attracted massive investment and interest from government agencies, industry, and academia. With more computing and communication resources are available, both vehicles and edge servers are equipped with a set of camera-based vision sensors, also known as Visual IoT (V-IoT) techniques, for sensing and perception. Tremendous efforts have been made for achieving programmable communication, computation, and control. However, they are conducted mainly in the silo mode, limiting the responsiveness and efficiency of handling challenging scenarios in the real world. To improve the end-to-end performance, we envision that future CAVs require the co-design of communication, computation, and control. This paper presents our vision of the end-to-end design principle for CAVs, called 4C, which extends the V-IoT system by providing a unified communication, computation, and control co-design framework. With programmable communications, fine-grained heterogeneous computation, and efficient vehicle controls in 4C, CAVs can handle critical scenarios and achieve energy-efficient autonomous driving. Finally, we present several challenges to achieving the vision of the 4C framework.
http://w3id.org/mlsea/pwc/scientificWork/4D%20Hyperspectral%20Photoacoustic%20Data%20Restoration%20With%20Reliability%20Analysis                                                                                  4D Hyperspectral Photoacoustic Data Restoration With Reliability Analysis                                                                                   Hyperspectral photoacoustic (HSPA) spectroscopy is an emerging bi-modal imaging technology that is able to show the wavelength-dependent absorption distribution of the interior of a 3D volume. However, HSPA devices have to scan an object exhaustively in the spatial and spectral domains; and the acquired data tend to suffer from complex noise. This time-consuming scanning process and noise severely affects the usability of HSPA. It is therefore critical to examine the feasibility of 4D HSPA data restoration from an incomplete and noisy observation. In this work, we present a data reliability analysis for the depth and spectral domain. On the basis of this analysis, we explore the inherent data correlations and develop a restoration algorithm to recover 4D HSPA cubes. Experiments on real data verify that the proposed method achieves satisfactory restoration results. 
http://w3id.org/mlsea/pwc/scientificWork/4DComplete%3A%20Non-Rigid%20Motion%20Estimation%20Beyond%20the%20Observable%20Surface                                                                                  4DComplete: Non-Rigid Motion Estimation Beyond the Observable Surface                                                                                  Tracking non-rigidly deforming scenes using range sensors has numerous applications including computer vision, AR/VR, and robotics. However, due to occlusions and physical limitations of range sensors, existing methods only handle the visible surface, thus causing discontinuities and incompleteness in the motion field. To this end, we introduce 4DComplete, a novel data-driven approach that estimates the non-rigid motion for the unobserved geometry. 4DComplete takes as input a partial shape and motion observation, extracts 4D time-space embedding, and jointly infers the missing geometry and motion field using a sparse fully-convolutional network. For network training, we constructed a large-scale synthetic dataset called DeformingThings4D, which consists of 1972 animation sequences spanning 31 different animals or humanoid categories with dense 4D annotation. Experiments show that 4DComplete 1) reconstructs high-resolution volumetric shape and motion field from a partial observation, 2) learns an entangled 4D feature representation that benefits both shape and motion estimation, 3) yields more accurate and natural deformation than classic non-rigid priors such as As-Rigid-As-Possible (ARAP) deformation, and 4) generalizes well to unseen objects in real-world sequences.
http://w3id.org/mlsea/pwc/scientificWork/56%20GBaud%20PAM-4%20100%20km%20Transmission%20System%20with%20Photonic%20Processing%20Schemes                                                                                  56 GBaud PAM-4 100 km Transmission System with Photonic Processing Schemes                                                                                  Analog photonic computing has been proposed and tested in recent years as an alternative approach for data recovery in fiber transmission systems. Photonic reservoir computing, performing nonlinear transformations of the transmitted signals and exhibiting internal fading memory, has been found advantageous for this kind of processing. In this work, we show that the effectiveness of the internal fading memory depends significantly on the properties of the signal to be processed. Specifically, we demonstrate two experimental photonic post-processing schemes for a 56 GBaud PAM-4 experimental transmission system, with 100 km uncompensated standard single-mode fiber and direct detection. We show that, for transmission systems with significant chromatic dispersion, the contribution of a photonic reservoir's fading memory to the computational performance is limited. In a comparison between the data recovery performances between a reservoir computing and an extreme learning machine fiber-based configuration, we find that both offer equivalent data recovery. The extreme learning machine approach eliminates the necessity of external recurrent connectivity, which simplifies the system and increases the computation speed. Above 31 dB OSNR, the photonics-based equalization exhibits a lower BER than the respective offline DSP-based KK receiver.
http://w3id.org/mlsea/pwc/scientificWork/5G%20MIMO%20Data%20for%20Machine%20Learning%3A%20Application%20to%20Beam-Selection%20using%20Deep%20Learning                                                                                  5G MIMO Data for Machine Learning: Application to Beam-Selection using Deep Learning                                                                                  The increasing complexity of configuring cellular networks suggests that machine learning (ML) can effectively improve 5G technologies. Deep learning has proven successful in ML tasks such as speech processing and computational vision, with a performance that scales with the amount of available data. The lack of large datasets inhibits the flourish of deep learning applications in wireless communications. This paper presents a methodology that combines a vehicle traffic simulator with a raytracing simulator, to generate channel realizations representing 5G scenarios with mobility of both transceivers and objects. The paper then describes a specific dataset for investigating beams-election techniques on vehicle-to-infrastructure using millimeter waves. Experiments using deep learning in classification, regression and reinforcement learning problems illustrate the use of datasets generated with the proposed methodology
http://w3id.org/mlsea/pwc/scientificWork/64%20GBd%20DP-Bipolar-8ASK%20Transmission%20over%20120%20km%20SSMF%20Employing%20a%20Monolithically%20Integrated%20Driver%20and%20MZM%20in%200.25-um%20SiGe%20BiCMOS%20Technology                                                                                  64 GBd DP-Bipolar-8ASK Transmission over 120 km SSMF Employing a Monolithically Integrated Driver and MZM in 0.25-um SiGe BiCMOS Technology                                                                                  We demonstrate 64 GBd signal generation up to bipolar-8-ASK utilizing a single MZM, monolithically integrated with segmented drivers in SiGe. Using polarization multiplexing, 300-Gb/s net data rate transmission over 120 km SSMF is shown.
http://w3id.org/mlsea/pwc/scientificWork/6G%20Cellular%20Networks%20and%20Connected%20Autonomous%20Vehicles                                                                                  6G Cellular Networks and Connected Autonomous Vehicles                                                                                  With 5G mobile communication systems been commercially rolled out, research discussions on next generation mobile systems, i.e., 6G, have started. On the other hand, vehicular technologies are also evolving rapidly, from connected vehicles as coined by V2X (vehicle to everything) to autonomous vehicles to the combination of the two, i.e., the networks of connected autonomous vehicles (CAV). How fast the evolution of these two areas will go head-in-head is of great importance, which is the focus of this paper. Based on a brief overview on the technological evolution of V2X to CAV and 6G key technologies, this paper explores two complementary research directions, namely, 6G for CAVs versus CAVs for 6G. The former investigates how various 6G key enablers, such as THz, cell free communication and artificial intelligence (AI), can be utilized to provide CAV mission-critical services. The latter discusses how CAVs can facilitate effective deployment and operation of 6G systems. This paper attempts to investigate the interactions between the two technologies to spark more research efforts in these areas.
http://w3id.org/mlsea/pwc/scientificWork/6G%20Communication%3A%20Envisioning%20the%20Key%20Issues%20and%20Challenges                                                                                  6G Communication: Envisioning the Key Issues and Challenges                                                                                  In 2030, we are going to evidence the 6G mobile communication technology, which will enable the Internet of Everything. Yet 5G has to be experienced by people worldwide and B5G has to be developed; the researchers have already started planning, visioning, and gathering requirements of the 6G. Moreover, many countries have already initiated the research on 6G. 6G promises connecting every smart device to the Internet from smartphone to intelligent vehicles. 6G will provide sophisticated and high QoS such as holographic communication, augmented reality/virtual reality and many more. Also, it will focus on Quality of Experience (QoE) to provide rich experiences from 6G technology. Notably, it is very important to vision the issues and challenges of 6G technology, otherwise, promises may not be delivered on time. The requirements of 6G poses new challenges to the research community. To achieve desired parameters of 6G, researchers are exploring various alternatives. Hence, there are diverse research challenges to envision, from devices to softwarization. Therefore, in this article, we discuss the future issues and challenges to be faced by the 6G technology. We have discussed issues and challenges from every aspect from hardware to the enabling technologies which will be utilized by 6G.
http://w3id.org/mlsea/pwc/scientificWork/6G%20White%20Paper%20on%20Localization%20and%20Sensing                                                                                  6G White Paper on Localization and Sensing                                                                                  This white paper explores future localization and sensing opportunities for beyond 5G wireless communication systems by identifying key technology enablers and discussing their underlying challenges, implementation issues, and identifying potential solutions. In addition, we present exciting new opportunities for localization and sensing applications, which will disrupt traditional design principles and revolutionize the way we live, interact with our environment, and do business. Following the trend initiated in the 5G NR systems, 6G will continue to develop towards even higher frequency ranges, wider bandwidths, and massive antenna arrays. In turn, this will enable sensing solutions with very fine range, Doppler and angular resolutions, as well as localization to cm-level degree of accuracy. Moreover, new materials, device types, and reconfigurable surfaces will allow network operators to reshape and control the electromagnetic response of the environment. At the same time, machine learning and artificial intelligence will leverage the unprecedented availability of data and computing resources to tackle the biggest and hardest problems in wireless communication systems. 6G will be truly intelligent wireless systems that will not only provide ubiquitous communication but also empower high accuracy localization and high-resolution sensing services. They will become the catalyst for this revolution by bringing about a unique new set of features and service capabilities, where localization and sensing will coexist with communication, continuously sharing the available resources in time, frequency and space. This white paper concludes by highlighting foundational research challenges, as well as implications and opportunities related to privacy, security, and trust. Addressing these challenges will undoubtedly require an inter-disciplinary and concerted effort from the research community.
http://w3id.org/mlsea/pwc/scientificWork/6G%20Wireless%20Channel%20Measurements%20and%20Models%3A%20Trends%20and%20Challenges                                                                                  6G Wireless Channel Measurements and Models: Trends and Challenges                                                                                  In this article, we first present our vision on the application scenarios, performance metrics, and potential key technologies of the sixth generation (6G) wireless communication networks. Then, 6G wireless channel measurements, characteristics, and models are comprehensively surveyed for all frequency bands and all scenarios, focusing on millimeter wave (mmWave), terahertz (THz), and optical wireless communication channels under all spectrums, satellite, unmanned aerial vehicle (UAV), maritime, and underwater acoustic communication channels under global coverage scenarios, and high-speed train (HST), vehicle-to-vehicle (V2V), ultra-massive multiple-input multiple-output (MIMO), orbital angular momentum (OAM), and industry Internet of things (IoT) communication channels under full application scenarios. Future research challenges on 6G channel measurements, a general standard 6G channel model framework, channel measurements and models for intelligent reflection surface (IRS) based 6G technologies, and artificial intelligence (AI) enabled channel measurements and models are also given.
http://w3id.org/mlsea/pwc/scientificWork/96%20dB%20Linear%20High%20Dynamic%20Range%20CAOS%20Spectrometer%20Demonstration                                                                                  96 dB Linear High Dynamic Range CAOS Spectrometer Demonstration                                                                                  For the first time, a CAOS (i.e., Coded Access Optical Sensor) spectrometer is demonstrated. The design implemented uses a reflective diffraction grating and a time-frequency CAOS mode operations Digital Micromirror Device (DMD) in combination with a large area point photo-detector to enable highly programmable linear High Dynamic Range (HDR) spectrometry. Experiments are conducted with a 2850 K color temperature light bulb source and visible band color bandpass and high-pass filters as well as neutral density (ND) attenuation filters. A ~369 nm to ~715 nm input light source spectrum is measured with a designed ~1 nm spectral resolution. Using the optical filters and different CAOS modes, namely, Code Division Multiple Access (CDMA), Frequency Modulation (FM)-CDMA and FM-Time Division Multiple Access (TDMA) modes, measured are improving spectrometer linear dynamic ranges of 28 dB, 50 dB, and 96.2 dB, respectively. Applications for the linear HDR CAOS spectrometer includes materials inspection in biomedicine, foods, forensics, and pharmaceuticals.
http://w3id.org/mlsea/pwc/scientificWork/A%20%240.11-0.38%24%20pJ%2Fcycle%20Differential%20Ring%20Oscillator%20in%20%2465%24%20nm%20CMOS%20for%20Robust%20Neurocomputing                                                                                  A $0.11-0.38$ pJ/cycle Differential Ring Oscillator in $65$ nm CMOS for Robust Neurocomputing                                                                                  This paper presents a low-area and low-power consumption CMOS differential current controlled oscillator (CCO) for neuromorphic applications. The oscillation frequency is improved over the conventional one by reducing the number of MOS transistors thus lowering the load capacitor in each stage. The analysis shows that for the same power consumption, the oscillation frequency can be increased about $11 %$ compared with the conventional one without degrading the phase noise. Alternatively, the power consumption can be reduced $15 %$ at the same frequency. The prototype structures are fabricated in a standard $65$ nm CMOS technology and measurements demonstrate that the proposed CCO operates from $0.7-1.2$ V supply with maximum frequencies of $80$ MHz and energy/cycle ranging from $0.11-0.38$ pJ over the tuning range. Further, system level simulations show that the nonlinearity in current-frequency conversion by the CCO does not affect its use as a neuron in a Deep Neural Network if accounted for during training.
http://w3id.org/mlsea/pwc/scientificWork/A%20%24t%24-test%20for%20synthetic%20controls                                                                                  A $t$-test for synthetic controls                                                                                  We propose a practical and robust method for making inferences on average treatment effects estimated by synthetic controls. We develop a $K$-fold cross-fitting procedure for bias-correction. To avoid the difficult estimation of the long-run variance, inference is based on a self-normalized $t$-statistic, which has an asymptotically pivotal $t$-distribution. Our $t$-test is easy to implement, provably robust against misspecification, valid with non-stationary data, and demonstrates an excellent small sample performance. Compared to difference-in-differences, our method often yields more than 50% shorter confidence intervals and is robust to violations of parallel trends assumptions. An $ texttt{R}$-package for implementing our methods is available.
http://w3id.org/mlsea/pwc/scientificWork/A%20%24t%24-test%20for%20synthetic%20controls                                                                                  A $t$-test for synthetic controls                                                                                  We propose a practical and robust method for making inferences on average treatment effects estimated by synthetic controls. We develop a $K$-fold cross-fitting procedure for bias correction. To avoid the difficult estimation of the long-run variance, inference is based on a self-normalized $t$-statistic, which has an asymptotically pivotal $t$-distribution. Our $t$-test is easy to implement, provably robust against misspecification, and valid with stationary and non-stationary data. It demonstrates an excellent small sample performance in application-based simulations and performs well relative to alternative methods. We illustrate the usefulness of the $t$-test by revisiting the effect of carbon taxes on emissions.
http://w3id.org/mlsea/pwc/scientificWork/A%201D-CNN%20Based%20Deep%20Learning%20Technique%20for%20Sleep%20Apnea%20Detection%20in%20IoT%20Sensors                                                                                  A 1D-CNN Based Deep Learning Technique for Sleep Apnea Detection in IoT Sensors                                                                                  Internet of Things (IoT) enabled wearable sensors for health monitoring are widely used to reduce the cost of personal healthcare and improve quality of life. The sleep apnea-hypopnea syndrome, characterized by the abnormal reduction or pause in breathing, greatly affects the quality of sleep of an individual. This paper introduces a novel method for apnea detection (pause in breathing) from electrocardiogram (ECG) signals obtained from wearable devices. The novelty stems from the high resolution of apnea detection on a second-by-second basis, and this is achieved using a 1-dimensional convolutional neural network for feature extraction and detection of sleep apnea events. The proposed method exhibits an accuracy of 99.56% and a sensitivity of 96.05%. This model outperforms several lower resolution state-of-the-art apnea detection methods. The complexity of the proposed model is analyzed. We also analyze the feasibility of model pruning and binarization to reduce the resource requirements on a wearable IoT device. The pruned model with 80 % sparsity exhibited an accuracy of 97.34% and a sensitivity of 86.48%. The binarized model exhibited an accuracy of 75.59% and sensitivity of 63.23%. The performance of low complexity patient-specific models derived from the generic model is also studied to analyze the feasibility of retraining existing models to fit patient-specific requirements. The patient-specific models on average exhibited an accuracy of 97.79% and sensitivity of 92.23%. The source code for this work is made publicly available.
http://w3id.org/mlsea/pwc/scientificWork/A%202.5D%20Vehicle%20Odometry%20Estimation%20for%20Vision%20Applications                                                                                  A 2.5D Vehicle Odometry Estimation for Vision Applications                                                                                  This paper proposes a method to estimate the pose of a sensor mounted on a vehicle as the vehicle moves through the world, an important topic for autonomous driving systems. Based on a set of commonly deployed vehicular odometric sensors, with outputs available on automotive communication buses (e.g. CAN or FlexRay), we describe a set of steps to combine a planar odometry based on wheel sensors with a suspension model based on linear suspension sensors. The aim is to determine a more accurate estimate of the camera pose. We outline its usage for applications in both visualisation and computer vision.
http://w3id.org/mlsea/pwc/scientificWork/A%202020%20taxonomy%20of%20algorithms%20inspired%20on%20living%20beings%20behavior                                                                                  A 2020 taxonomy of algorithms inspired on living beings behavior                                                                                  Taking the role of a computer naturalist, a journey is taken through bio inspired algorithms taking account on algorithms which are inspired on living being behaviors. A compilation of algorithms is made considering several reviews or surveys of bio-inspired heuristics and swarm intelligence until 2020 year. A classification is made considering kingdoms as used by biologists generating several branches for animalia, bacteria, plants, fungi and protista to develop a taxonomy.
http://w3id.org/mlsea/pwc/scientificWork/A%203D%20CNN%20Network%20with%20BERT%20For%20Automatic%20COVID-19%20Diagnosis%20From%20CT-Scan%20Images                                                                                  A 3D CNN Network with BERT For Automatic COVID-19 Diagnosis From CT-Scan Images                                                                                  We present an automatic COVID1-19 diagnosis framework from lung CT-scan slice images. In this framework, the slice images of a CT-scan volume are first proprocessed using segmentation techniques to filter out images of closed lung, and to remove the useless background. Then a resampling method is used to select one or multiple sets of a fixed number of slice images for training and validation. A 3D CNN network with BERT is used to classify this set of selected slice images. In this network, an embedding feature is also extracted. In cases where there are more than one set of slice images in a volume, the features of all sets are extracted and pooled into a global feature vector for the whole CT-scan volume. A simple multiple-layer perceptron (MLP) network is used to further classify the aggregated feature vector. The models are trained and evaluated on the provided training and validation datasets. On the validation dataset, the accuracy is 0.9278 and the F1 score is 0.9261.
http://w3id.org/mlsea/pwc/scientificWork/A%203D%20Non-Stationary%20Channel%20Model%20for%206G%20Wireless%20Systems%20Employing%20Intelligent%20Reflecting%20Surface                                                                                  A 3D Non-Stationary Channel Model for 6G Wireless Systems Employing Intelligent Reflecting Surface                                                                                  As one of the key technologies for the sixth generation (6G) mobile communications, intelligent reflecting surface IRS) has the advantages of low power consumption, low cost, and simple design methods. But channel modeling is still an open issue in this field currently. In this paper, we propose a three-dimensional (3D) geometry based stochastic model (GBSM) for a massive multiple-input multiple-output (MIMO) communication system employing IRS. The model supports the movements of the transmitter, the receiver, and clusters. The evolution of clusters on the linear array and planar array is also considered in the proposed model. In addition, the generation of reflecting coefficient is incorporated into the model and the path loss of the sub-channel assisted by IRS is also proposed. The steering vector is set up at the base station for the cooperation with IRS. Through studying statistical properties such as the temporal autocorrelation function and space correlation function, the nonstationary properties are verified. The good agreement between the simulation results and the analytical results illustrates the correctness of the proposed channel model.
http://w3id.org/mlsea/pwc/scientificWork/A%203D%20Non-Stationary%20Channel%20Model%20for%206G%20Wireless%20Systems%20Employing%20Intelligent%20Reflecting%20Surfaces%20with%20Practical%20Phase%20Shifts                                                                                  A 3D Non-Stationary Channel Model for 6G Wireless Systems Employing Intelligent Reflecting Surfaces with Practical Phase Shifts                                                                                  In this paper, a three-dimensional (3D) geometry based stochastic model (GBSM) for a massive multiple-input multiple-output (MIMO) communication system employing practical discrete intelligent reflecting surface (IRS) is proposed. The proposed channel model supports the scenario where both transceivers and environments move. The evolution of clusters in the space domain and the practical discrete phase shifts are considered in the channel model. The steering vector is set at the base station for the cooperation with IRS. Through studying statistical properties, the non-stationary properties are verified. We find that IRS plays a role in separating the whole channel and make the absolute value of time autocorrelation function (ACF) larger than the situation without employing IRS. Time ACF of the case using discrete phase shifts is also compared with the continuous case.
http://w3id.org/mlsea/pwc/scientificWork/A%203D%20Non-stationary%20MmWave%20Channel%20Model%20for%20Vacuum%20Tube%20Ultra-High-Speed%20Train%20Channels                                                                                  A 3D Non-stationary MmWave Channel Model for Vacuum Tube Ultra-High-Speed Train Channels                                                                                  As a potential development direction of future transportation, the vacuum tube ultra-high-speed train (UHST) wireless communication systems have newly different channel characteristics from existing high-speed train (HST) scenarios. In this paper, a three-dimensional non-stationary millimeter wave (mmWave) geometry-based stochastic model (GBSM) is proposed to investigate the channel characteristics of UHST channels in vacuum tube scenarios, taking into account the waveguide effect and the impact of tube wall roughness on channel. Then, based on the proposed model, some important time-variant channel statistical properties are studied and compared with those in existing HST and tunnel channels. The results obtained show that the multipath effect in vacuum tube scenarios will be more obvious than tunnel scenarios but less than existing HST scenarios, which will provide some insights for future research on vacuum tube UHST wireless communications.
http://w3id.org/mlsea/pwc/scientificWork/A%204-Element%20800MHz-BW%2029mW%20True-Time-Delay%20Spatial%20Signal%20Processor%20Enabling%20Fast%20Beam-Training%20with%20Data%20Communications                                                                                  A 4-Element 800MHz-BW 29mW True-Time-Delay Spatial Signal Processor Enabling Fast Beam-Training with Data Communications                                                                                  Spatial signal processors (SSP) for emerging millimeter-wave wireless networks are critically dependent on link discovery. To avoid loss in communication, mobile devices need to locate narrow directional beams with millisecond latency. In this work, we demonstrate a true-time-delay (TTD) array with digitally reconfigurable delay elements enabling both fast beam-training at the receiver with wideband data communications. In beam-training mode, large delay-bandwidth products are implemented to accelerate beam training using frequency-dependent probing beams. In data communications mode, precise beam alignment is achieved to mitigate spatial effects during beam-forming for wideband signals. The 4-element switched-capacitor based time-interleaved array uses a compact closed-loop integrator for signal combining with the delay compensation implemented in the clock domain to achieve high precision and large delay range. Prototyped in TSMC 65nm CMOS, the TTD SSP successfully demonstrates unique frequency-to-angle mapping with 3.8ns maximum delay and 800MHz bandwidth in the beam-training mode. In the data communications mode, nearly 12dB uniform beamforming gain is achieved from 80MHz to 800MHz. The TTD SSP consumes 29mW at 1V supply achieving 122MB/s with 16-QAM at 9.8% EVM.
http://w3id.org/mlsea/pwc/scientificWork/A%205G-NR%20Satellite%20Extension%20for%20the%20QuaDRiGa%20Channel%20Model                                                                                  A 5G-NR Satellite Extension for the QuaDRiGa Channel Model                                                                                  Low Earth orbit (LEO) satellite networks will become an integral part of the global telecommunication infrastructure. Modeling the radio-links of these networks and their interaction with existing terrestrial systems is crucial for the design, planning and scaling of these networks. The 3rd generation partnership project (3GPP) addressed this by providing guideline for such a radio-channel model. However, the proposed model lacks a satellite orbit model and has some inconsistencies in the provided parameters. This is addressed in this paper. We provide a non-geostationary-satellite model that can be integrated into geometry-based stochastic channel models (GSCMs) such as QuaDRiGa. We then use this model to obtain the GSCM parameters from a simplified environment model and compare the results to the 3GPP parameter-set. This solves the inconsistencies, but our simplified approach does not consider many propagation effects. Future work must therefore rely on measurements or accurate Ray-tracing models to obtain the parameters.
http://w3id.org/mlsea/pwc/scientificWork/A%20BERT-based%20Distractor%20Generation%20Scheme%20with%20Multi-tasking%20and%20Negative%20Answer%20Training%20Strategies.                                                                                  A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies.                                                                                  In this paper, we investigate the following two limitations for the existing distractor generation (DG) methods. First, the quality of the existing DG methods are still far from practical use. There are still room for DG quality improvement. Second, the existing DG designs are mainly for single distractor generation. However, for practical MCQ preparation, multiple distractors are desired. Aiming at these goals, in this paper, we present a new distractor generation scheme with multi-tasking and negative answer training strategies for effectively generating textit{multiple} distractors. The experimental results show that (1) our model advances the state-of-the-art result from 28.65 to 39.81 (BLEU 1 score) and (2) the generated multiple distractors are diverse and shows strong distracting power for multiple choice question.
http://w3id.org/mlsea/pwc/scientificWork/A%20BFS-Tree%20of%20Ranking%20References%20for%20Unsupervised%20Manifold%20Learning                                                                                  A BFS-Tree of Ranking References for Unsupervised Manifold Learning                                                                                  Contextual information, defined in terms of the proximity of feature vectors in a feature space, has been successfully used in the construction of search services. These search systems aim to exploit such information to effectively improve ranking results, by taking into account the manifold distribution of features usually encoded. In this paper, a novel unsupervised manifold learning is proposed through a similarity representation based on ranking references. A breadth-first tree is used to represent similarity information given by ranking references and is exploited to discovery underlying similarity relationships. As a result, a more effective similarity measure is computed, which leads to more relevant objects in the returned ranked lists of search sessions. Several experiments conducted on eight public datasets, commonly used for image retrieval benchmarking, demonstrated that the proposed method achieves very high effectiveness results, which are comparable or superior to the ones produced by state-of-the-art approaches.
http://w3id.org/mlsea/pwc/scientificWork/A%20BIC-based%20Mixture%20Model%20Defense%20against%20Data%20Poisoning%20Attacks%20on%20Classifiers                                                                                  A BIC-based Mixture Model Defense against Data Poisoning Attacks on Classifiers                                                                                  Data Poisoning (DP) is an effective attack that causes trained classifiers to misclassify their inputs. DP attacks significantly degrade a classifier's accuracy by covertly injecting attack samples into the training set. Broadly applicable to different classifier structures, without strong assumptions about the attacker, an { it unsupervised} Bayesian Information Criterion (BIC)-based mixture model defense against 'error generic' DP attacks is herein proposed that: 1) addresses the most challenging { it embedded} DP scenario wherein, if DP is present, the poisoned samples are an { it a priori} unknown subset of the training set, and with no clean validation set available; 2) applies a mixture model both to well-fit potentially multi-modal class distributions and to capture poisoned samples within a small subset of the mixture components; 3) jointly identifies poisoned components and samples by minimizing the BIC cost defined over the whole training set, with the identified poisoned data removed prior to classifier training. Our experimental results, for various classifier structures and benchmark datasets, demonstrate the effectiveness and universality of our defense under strong DP attacks, as well as its superiority over other works.
http://w3id.org/mlsea/pwc/scientificWork/A%20Bagging%20and%20Boosting%20Based%20Convexly%20Combined%20Optimum%20Mixture%20Probabilistic%20Model                                                                                  A Bagging and Boosting Based Convexly Combined Optimum Mixture Probabilistic Model                                                                                  Unlike previous studies on mixture distributions, a bagging and boosting based convexly combined mixture probabilistic model has been suggested. This model is a result of iteratively searching for obtaining the optimum probabilistic model that provides the maximum p value.
http://w3id.org/mlsea/pwc/scientificWork/A%20Basket%20Half%20Full%3A%20Sparse%20Portfolios                                                                                  A Basket Half Full: Sparse Portfolios                                                                                  The existing approaches to sparse wealth allocations (1) are limited to low-dimensional setup when the number of assets is less than the sample size; (2) lack theoretical analysis of sparse wealth allocations and their impact on portfolio exposure; (3) are suboptimal due to the bias induced by an $ ell_1$-penalty. We address these shortcomings and develop an approach to construct sparse portfolios in high dimensions. Our contribution is twofold: from the theoretical perspective, we establish the oracle bounds of sparse weight estimators and provide guidance regarding their distribution. From the empirical perspective, we examine the merit of sparse portfolios during different market scenarios. We find that in contrast to non-sparse counterparts, our strategy is robust to recessions and can be used as a hedging vehicle during such times.
http://w3id.org/mlsea/pwc/scientificWork/A%20Bayesian%20Approach%20to%20Reinforcement%20Learning%20of%20Vision-Based%20Vehicular%20Control                                                                                  A Bayesian Approach to Reinforcement Learning of Vision-Based Vehicular Control                                                                                  In this paper, we present a state-of-the-art reinforcement learning method for autonomous driving. Our approach employs temporal difference learning in a Bayesian framework to learn vehicle control signals from sensor data. The agent has access to images from a forward facing camera, which are preprocessed to generate semantic segmentation maps. We trained our system using both ground truth and estimated semantic segmentation input. Based on our observations from a large set of experiments, we conclude that training the system on ground truth input data leads to better performance than training the system on estimated input even if estimated input is used for evaluation. The system is trained and evaluated in a realistic simulated urban environment using the CARLA simulator. The simulator also contains a benchmark that allows for comparing to other systems and methods. The required training time of the system is shown to be lower and the performance on the benchmark superior to competing approaches.
http://w3id.org/mlsea/pwc/scientificWork/A%20Bayesian%20Convolutional%20Neural%20Network%20for%20Robust%20Galaxy%20Ellipticity%20Regression                                                                                  A Bayesian Convolutional Neural Network for Robust Galaxy Ellipticity Regression                                                                                  Cosmic shear estimation is an essential scientific goal for large galaxy surveys. It refers to the coherent distortion of distant galaxy images due to weak gravitational lensing along the line of sight. It can be used as a tracer of the matter distribution in the Universe. The unbiased estimation of the local value of the cosmic shear can be obtained via Bayesian analysis which relies on robust estimation of the galaxies ellipticity (shape) posterior distribution. This is not a simple problem as, among other things, the images may be corrupted with strong background noise. For current and coming surveys, another central issue in galaxy shape determination is the treatment of statistically dominant overlapping (blended) objects. We propose a Bayesian Convolutional Neural Network based on Monte-Carlo Dropout to reliably estimate the ellipticity of galaxies and the corresponding measurement uncertainties. We show that while a convolutional network can be trained to correctly estimate well calibrated aleatoric uncertainty, -- the uncertainty due to the presence of noise in the images -- it is unable to generate a trustworthy ellipticity distribution when exposed to previously unseen data (i.e. here, blended scenes). By introducing a Bayesian Neural Network, we show how to reliably estimate the posterior predictive distribution of ellipticities along with robust estimation of epistemic uncertainties. Experiments also show that epistemic uncertainty can detect inconsistent predictions due to unknown blended scenes.
http://w3id.org/mlsea/pwc/scientificWork/A%20Bayesian%20Graphical%20Approach%20for%20Large-Scale%20Portfolio%20Management%20with%20Fewer%20Historical%20Data                                                                                  A Bayesian Graphical Approach for Large-Scale Portfolio Management with Fewer Historical Data                                                                                  Managing a large-scale portfolio with many assets is one of the most challenging tasks in the field of finance. It is partly because estimation of either covariance or precision matrix of asset returns tends to be unstable or even infeasible when the number of assets $p$ exceeds the number of observations $n$. For this reason, most of the previous studies on portfolio management have focused on the case of$ p < n$. To deal with the case of $p > n$, we propose to use a new Bayesian framework based on adaptive graphical LASSO for estimating the precision matrix of asset returns in a large-scale portfolio. Unlike the previous studies on graphical LASSO in the literature, our approach utilizes a Bayesian estimation method for the precision matrix proposed by Oya and Nakatsuma (2022) so that the positive definiteness of the precision matrix should be always guaranteed. As an empirical application, we construct the global minimum variance portfolio of $p = 100$ for various values of n with the proposed approach as well as the non-Bayesian graphical LASSO approach, and compare their out-of-sample performance with the equal weight portfolio as the benchmark. In this comparison, the proposed approach produces more stable results than the non-Bayesian approach in terms of Sharpe ratio, portfolio composition and turnover. Furthermore, the proposed approach succeeds in estimating the precision matrix even if $n$ is much smaller than $p$ and the non-Bayesian approach fails to do so.
http://w3id.org/mlsea/pwc/scientificWork/A%20Bayesian%20analysis%20of%20gain-loss%20asymmetry                                                                                  A Bayesian analysis of gain-loss asymmetry                                                                                  We perform a quantitative analysis of the gain/loss asymmetry for financial time series by using a Bayesian approach. In particular, we focus on some selected indices and analyze the statistical significance of the asymmetry amount through a Bayesian generalization of the t-Test, which relaxes the normality assumption on the underlying distribution. We propose two different models for data distribution, we study the convergence of our method and we provide several graphical representations of our numerical results. Finally, we perform a sensitivity analysis with respect to model parameters in order to study the reliability and robustness of our results.
http://w3id.org/mlsea/pwc/scientificWork/A%20Bayesian%20realized%20threshold%20measurement%20GARCH%20framework%20for%20financial%20tail%20risk%20forecasting                                                                                  A Bayesian realized threshold measurement GARCH framework for financial tail risk forecasting                                                                                  This paper proposes an innovative threshold measurement equation to be employed in a Realized-GARCH framework. The proposed framework incorporates a nonlinear threshold regression specification to consider the leverage effect and model the contemporaneous dependence between the observed realized measure and hidden volatility. A Bayesian Markov Chain Monte Carlo method is adapted and employed for model estimation, with its validity assessed via a simulation study. The validity of incorporating the proposed measurement equation in Realized-GARCH type models is evaluated via an empirical study, forecasting the 1% and 2.5% Value-at-Risk and Expected Shortfall on six market indices with two different out-of-sample sizes. The proposed framework is shown to be capable of producing competitive tail risk forecasting results in comparison to the GARCH and Realized-GARCH type models.
http://w3id.org/mlsea/pwc/scientificWork/A%20Behavior-aware%20Graph%20Convolution%20Network%20Model%20for%20Video%20Recommendation                                                                                  A Behavior-aware Graph Convolution Network Model for Video Recommendation                                                                                  Interactions between users and videos are the major data source of performing video recommendation. Despite lots of existing recommendation methods, user behaviors on videos, which imply the complex relations between users and videos, are still far from being fully explored. In the paper, we present a model named Sagittarius. Sagittarius adopts a graph convolutional neural network to capture the influence between users and videos. In particular, Sagittarius differentiates between different user behaviors by weighting and fuses the semantics of user behaviors into the embeddings of users and videos. Moreover, Sagittarius combines multiple optimization objectives to learn user and video embeddings and then achieves the video recommendation by the learned user and video embeddings. The experimental results on multiple datasets show that Sagittarius outperforms several state-of-the-art models in terms of recall, unique recall and NDCG.
http://w3id.org/mlsea/pwc/scientificWork/A%20Benchmarking%20on%20Cloud%20based%20Speech-To-Text%20Services%20for%20French%20Speech%20and%20Background%20Noise%20Effect                                                                                  A Benchmarking on Cloud based Speech-To-Text Services for French Speech and Background Noise Effect                                                                                  This study presents a large scale benchmarking on cloud based Speech-To-Text systems: {Google Cloud Speech-To-Text}, {Microsoft Azure Cognitive Services}, {Amazon Transcribe}, {IBM Watson Speech to Text}. For each systems, 40158 clean and noisy speech files about 101 hours are tested. Effect of background noise on STT quality is also evaluated with 5 different Signal-to-noise ratios from 40dB to 0dB. Results showed that {Microsoft Azure} provided lowest transcription error rate $9.09 %$ on clean speech, with high robustness to noisy environment. {Google Cloud} and {Amazon Transcribe} gave similar performance, but the latter is very limited for time-constraint usage. Though {IBM Watson} could work correctly in quiet conditions, it is highly sensible to noisy speech which could strongly limit its application in real life situations.
http://w3id.org/mlsea/pwc/scientificWork/A%20Bi-Encoder%20LSTM%20Model%20For%20Learning%20Unstructured%20Dialogs                                                                                  A Bi-Encoder LSTM Model For Learning Unstructured Dialogs                                                                                  Creating a data-driven model that is trained on a large dataset of unstructured dialogs is a crucial step in developing Retrieval-based Chatbot systems. This paper presents a Long Short Term Memory (LSTM) based architecture that learns unstructured multi-turn dialogs and provides results on the task of selecting the best response from a collection of given responses. Ubuntu Dialog Corpus Version 2 was used as the corpus for training. We show that our model achieves 0.8%, 1.0% and 0.3% higher accuracy for Recall@1, Recall@2 and Recall@5 respectively than the benchmark model. We also show results on experiments performed by using several similarity functions, model hyper-parameters and word embeddings on the proposed architecture
http://w3id.org/mlsea/pwc/scientificWork/A%20Bi-Level%20Framework%20for%20Learning%20to%20Solve%20Combinatorial%20Optimization%20on%20Graphs                                                                                  A Bi-Level Framework for Learning to Solve Combinatorial Optimization on Graphs                                                                                  Combinatorial Optimization (CO) has been a long-standing challenging research topic featured by its NP-hard nature. Traditionally such problems are approximately solved with heuristic algorithms which are usually fast but may sacrifice the solution quality. Currently, machine learning for combinatorial optimization (MLCO) has become a trending research topic, but most existing MLCO methods treat CO as a single-level optimization by directly learning the end-to-end solutions, which are hard to scale up and mostly limited by the capacity of ML models given the high complexity of CO. In this paper, we propose a hybrid approach to combine the best of the two worlds, in which a bi-level framework is developed with an upper-level learning method to optimize the graph (e.g. add, delete or modify edges in a graph), fused with a lower-level heuristic algorithm solving on the optimized graph. Such a bi-level approach simplifies the learning on the original hard CO and can effectively mitigate the demand for model capacity. The experiments and results on several popular CO problems like Directed Acyclic Graph scheduling, Graph Edit Distance and Hamiltonian Cycle Problem show its effectiveness over manually designed heuristics and single-level learning methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20Big%20Data%20Analysis%20of%20the%20Ethereum%20Network%3A%20from%20Blockchain%20to%20Google%20Trends                                                                                  A Big Data Analysis of the Ethereum Network: from Blockchain to Google Trends                                                                                  First, a big data analysis of the transactions and smart contracts made on the Ethereum blockchain is performed, revealing interesting trends in motion. Next, these trends are compared with the public's interest in Ether and Bitcoin, measured by the volume of online searches. An analysis of the crypto prices and search trends suggests the existence of big players (and not the regular users), manipulating the market after a drop in prices. Lastly, a cross-correlation study of crypto prices and search trends reveals the pairs providing more accurate and timely predictions of Ether prices.
http://w3id.org/mlsea/pwc/scientificWork/A%20Binarizing%20NUV%20Prior%20and%20its%20Use%20for%20M-Level%20Control%20and%20Digital-to-Analog%20Conversion                                                                                  A Binarizing NUV Prior and its Use for M-Level Control and Digital-to-Analog Conversion                                                                                  Priors with a NUV representation (normal with unknown variance) have mostly been used for sparsity. In this paper, a novel NUV prior is proposed that effectively binarizes. While such a prior may have many uses, in this paper, we explore its use for discrete-level control (with M $ geq$ 2 levels) including, in particular, a practical scheme for digital-to-analog conversion. The resulting computations, for each planning period, amount to iterating forward-backward Gaussian message passing recursions (similar to Kalman smoothing), with a complexity (per iteration) that is linear in the planning horizon. In consequence, the proposed method is not limited to a short planning horizon and can therefore outperform 'optimal' methods. A preference for sparse level switches can easily be incorporated.
http://w3id.org/mlsea/pwc/scientificWork/A%20Birds-eye%20%28Re%29View%20of%20Acid-suppression%20Drugs%2C%20COVID-19%2C%20and%20the%20Highly%20Variable%20Literature                                                                                  A Birds-eye (Re)View of Acid-suppression Drugs, COVID-19, and the Highly Variable Literature                                                                                  We consider the recent surge of information on the potential benefits of acid-suppression drugs in the context of COVID-19, with an eye on the variability (and confusion) across the reported findings--at least as regards the popular antacid famotidine. The inconsistencies reflect contradictory conclusions from independent clinical-based studies that took roughly similar approaches, in terms of experimental design (retrospective, cohort-based, etc.) and statistical analyses (propensity-score matching and stratification, etc.). The confusion has significant ramifications in choosing therapeutic interventions: e.g., do potential benefits of famotidine indicate its use in a particular COVID-19 case? Beyond this pressing therapeutic issue, conflicting information on famotidine must be resolved before its integration in ontological and knowledge graph-based frameworks, which in turn are useful in drug repurposing efforts. To begin systematically structuring the rapidly accumulating information, in the hopes of clarifying and reconciling the discrepancies, we consider the contradictory information along three proposed 'axes': (1) a context-of-disease axis, (2) a degree-of-[therapeutic]-benefit axis, and (3) a mechanism-of-action axis. We suspect that incongruencies in how these axes have been (implicitly) treated in past studies has led to the contradictory indications for famotidine and COVID-19. We also trace the evolution of information on acid-suppression agents as regards the transmission, severity, and mortality of COVID-19, given the many literature reports that have accumulated. By grouping the studies conceptually and thematically, we identify three eras in the progression of our understanding of famotidine and COVID-19. Harmonizing these findings is a key goal for both clinical standards-of-care (COVID and beyond) as well as ontological and knowledge graph-based approaches.
http://w3id.org/mlsea/pwc/scientificWork/A%20Bit%20More%20Bayesian%3A%20Domain-Invariant%20Learning%20with%20Uncertainty                                                                                  A Bit More Bayesian: Domain-Invariant Learning with Uncertainty                                                                                  Domain generalization is challenging due to the domain shift and the uncertainty caused by the inaccessibility of target domain data. In this paper, we address both challenges with a probabilistic framework based on variational Bayesian inference, by incorporating uncertainty into neural network weights. We couple domain invariance in a probabilistic formula with the variational Bayesian inference. This enables us to explore domain-invariant learning in a principled way. Specifically, we derive domain-invariant representations and classifiers, which are jointly established in a two-layer Bayesian neural network. We empirically demonstrate the effectiveness of our proposal on four widely used cross-domain visual recognition benchmarks. Ablation studies validate the synergistic benefits of our Bayesian treatment when jointly learning domain-invariant representations and classifiers for domain generalization. Further, our method consistently delivers state-of-the-art mean accuracy on all benchmarks.
http://w3id.org/mlsea/pwc/scientificWork/A%20Black-Scholes%20user%27s%20guide%20to%20the%20Bachelier%20model                                                                                  A Black-Scholes user's guide to the Bachelier model                                                                                  To cope with the negative oil futures price caused by the COVID-19 recession, global commodity futures exchanges temporarily switched the option model from Black--Scholes to Bachelier in 2020. This study reviews the literature on Bachelier's pioneering option pricing model and summarizes the practical results on volatility conversion, risk management, stochastic volatility, and barrier options pricing to facilitate the model transition. In particular, using the displaced Black-Scholes model as a model family with the Black-Scholes and Bachelier models as special cases, we not only connect the two models but also present a continuous spectrum of model choices.
http://w3id.org/mlsea/pwc/scientificWork/A%20Blueprint%20for%20the%20Study%20of%20the%20Brain%27s%20Spatiotemporal%20Patterns                                                                                  A Blueprint for the Study of the Brain's Spatiotemporal Patterns                                                                                  The functioning of an organ such as the brain emerges from interactions between its constituent parts. Further, this interaction is not immutable in time but rather unfolds in a succession of patterns, thereby allowing the brain to adapt to constantly changing exterior and interior milieus. This calls for a framework able to study patterned spatiotemporal interactions between components of the brain. A theoretical and methodological framework is developed to study the brain's coordination dynamics. Here we present a toolset designed to decipher the continuous dynamics of electrophysiological data and its relation to (dys-) function. Understanding the spatiotemporal organization of brain patterns and their association with behavioral, cognitive and clinically-relevant variables is an important challenge for the fields of neuroscience and biologically-inspired engineering. It is hoped that such a comprehensive framework will shed light not only on human behavior and the human mind but also help in understanding the growing number of pathologies that are linked to disorders of brain connectivity.
http://w3id.org/mlsea/pwc/scientificWork/A%20Bop%20and%20Beyond%3A%20A%20Second%20Order%20Optimizer%20for%20Binarized%20Neural%20Networks                                                                                  A Bop and Beyond: A Second Order Optimizer for Binarized Neural Networks                                                                                  The optimization of Binary Neural Networks (BNNs) relies on approximating the real-valued weights with their binarized representations. Current techniques for weight-updating use the same approaches as traditional Neural Networks (NNs) with the extra requirement of using an approximation to the derivative of the sign function - as it is the Dirac-Delta function - for back-propagation; thus, efforts are focused adapting full-precision techniques to work on BNNs. In the literature, only one previous effort has tackled the problem of directly training the BNNs with bit-flips by using the first raw moment estimate of the gradients and comparing it against a threshold for deciding when to flip a weight (Bop). In this paper, we take an approach parallel to Adam which also uses the second raw moment estimate to normalize the first raw moment before doing the comparison with the threshold, we call this method Bop2ndOrder. We present two versions of the proposed optimizer: a biased one and a bias-corrected one, each with its own applications. Also, we present a complete ablation study of the hyperparameters space, as well as the effect of using schedulers on each of them. For these studies, we tested the optimizer in CIFAR10 using the BinaryNet architecture. Also, we tested it in ImageNet 2012 with the XnorNet and BiRealNet architectures for accuracy. In both datasets our approach proved to converge faster, was robust to changes of the hyperparameters, and achieved better accuracy values.
http://w3id.org/mlsea/pwc/scientificWork/A%20Bregman%20Learning%20Framework%20for%20Sparse%20Neural%20Networks                                                                                  A Bregman Learning Framework for Sparse Neural Networks                                                                                  We propose a learning framework based on stochastic Bregman iterations, also known as mirror descent, to train sparse neural networks with an inverse scale space approach. We derive a baseline algorithm called LinBreg, an accelerated version using momentum, and AdaBreg, which is a Bregmanized generalization of the Adam algorithm. In contrast to established methods for sparse training the proposed family of algorithms constitutes a regrowth strategy for neural networks that is solely optimization-based without additional heuristics. Our Bregman learning framework starts the training with very few initial parameters, successively adding only significant ones to obtain a sparse and expressive network. The proposed approach is extremely easy and efficient, yet supported by the rich mathematical theory of inverse scale space methods. We derive a statistically profound sparse parameter initialization strategy and provide a rigorous stochastic convergence analysis of the loss decay and additional convergence proofs in the convex regime. Using only 3.4% of the parameters of ResNet-18 we achieve 90.2% test accuracy on CIFAR-10, compared to 93.6% using the dense network. Our algorithm also unveils an autoencoder architecture for a denoising task. The proposed framework also has a huge potential for integrating sparse backpropagation and resource-friendly training.
http://w3id.org/mlsea/pwc/scientificWork/A%20Brief%20Study%20on%20the%20Effects%20of%20Training%20Generative%20Dialogue%20Models%20with%20a%20Semantic%20loss                                                                                  A Brief Study on the Effects of Training Generative Dialogue Models with a Semantic loss                                                                                  Neural models trained for next utterance generation in dialogue task learn to mimic the n-gram sequences in the training set with training objectives like negative log-likelihood (NLL) or cross-entropy. Such commonly used training objectives do not foster generating alternate responses to a context. But, the effects of minimizing an alternate training objective that fosters a model to generate alternate response and score it on semantic similarity has not been well studied. We hypothesize that a language generation model can improve on its diversity by learning to generate alternate text during training and minimizing a semantic loss as an auxiliary objective. We explore this idea on two different sized data sets on the task of next utterance generation in goal oriented dialogues. We make two observations (1) minimizing a semantic objective improved diversity in responses in the smaller data set (Frames) but only as-good-as minimizing the NLL in the larger data set (MultiWoZ) (2) large language model embeddings can be more useful as a semantic loss objective than as initialization for token embeddings.
http://w3id.org/mlsea/pwc/scientificWork/A%20Bytecode-based%20Approach%20for%20Smart%20Contract%20Classification                                                                                  A Bytecode-based Approach for Smart Contract Classification                                                                                  With the development of blockchain technologies, the number of smart contracts deployed on blockchain platforms is growing exponentially, which makes it difficult for users to find desired services by manual screening. The automatic classification of smart contracts can provide blockchain users with keyword-based contract searching and helps to manage smart contracts effectively. Current research on smart contract classification focuses on Natural Language Processing (NLP) solutions which are based on contract source code. However, more than 94% of smart contracts are not open-source, so the application scenarios of NLP methods are very limited. Meanwhile, NLP models are vulnerable to adversarial attacks. This paper proposes a classification model based on features from contract bytecode instead of source code to solve these problems. We also use feature selection and ensemble learning to optimize the model. Our experimental studies on over 3,300 real-world Ethereum smart contracts show that our model can classify smart contracts without source code and has better performance than baseline models. Our model also has good resistance to adversarial attacks compared with NLP-based models. In addition, our analysis reveals that account features used in many smart contract classification models have little effect on classification and can be excluded.
http://w3id.org/mlsea/pwc/scientificWork/A%20C-V2X%20Platform%20Using%20Transportation%20Data%20and%20Spectrum-Aware%20Sidelink%20Access                                                                                  A C-V2X Platform Using Transportation Data and Spectrum-Aware Sidelink Access                                                                                  Intelligent transportation systems and autonomous vehicles are expected to bring new experiences with enhanced efficiency and safety to road users in the near future. However, an efficient and robust vehicular communication system should act as a strong backbone to offer the needed infrastructure connectivity. Deep learning (DL)-based algorithms are widely adopted recently in various vehicular communication applications due to their achieved low latency and fast reconfiguration properties. Yet, collecting actual and sufficient transportation data to train DL-based vehicular communication models is costly and complex. This paper introduces a cellular vehicle-to-everything (C-V2X) verification platform based on an actual traffic simulator and spectrum-aware access. This integrated platform can generate realistic transportation and communication data, benefiting the development and adaptivity of DL-based solutions. Accordingly, vehicular spectrum recognition and management are further investigated to demonstrate the potentials of dynamic slidelink access. Numerical results show that our platform can effectively train and realize DL-based C-V2X algorithms. The developed slidelink communication can adopt different operating bands with remarkable spectrum detection performance, validating its practicality in real-world vehicular environments.
http://w3id.org/mlsea/pwc/scientificWork/A%20CCG-Based%20Version%20of%20the%20DisCoCat%20Framework                                                                                  A CCG-Based Version of the DisCoCat Framework                                                                                  While the DisCoCat model (Coecke et al., 2010) has been proved a valuable tool for studying compositional aspects of language at the level of semantics, its strong dependency on pregroup grammars poses important restrictions: first, it prevents large-scale experimentation due to the absence of a pregroup parser; and second, it limits the expressibility of the model to context-free grammars. In this paper we solve these problems by reformulating DisCoCat as a passage from Combinatory Categorial Grammar (CCG) to a category of semantics. We start by showing that standard categorial grammars can be expressed as a biclosed category, where all rules emerge as currying/uncurrying the identity; we then proceed to model permutation-inducing rules by exploiting the symmetry of the compact closed category encoding the word meaning. We provide a proof of concept for our method, converting 'Alice in Wonderland' into DisCoCat form, a corpus that we make available to the community.
http://w3id.org/mlsea/pwc/scientificWork/A%20CNN%20Segmentation-Based%20Approach%20to%20Object%20Detection%20and%20Tracking%20in%20Ultrasound%20Scans%20with%20Application%20to%20the%20Vagus%20Nerve%20Detection                                                                                  A CNN Segmentation-Based Approach to Object Detection and Tracking in Ultrasound Scans with Application to the Vagus Nerve Detection                                                                                  Ultrasound scanning is essential in several medical diagnostic and therapeutic applications. It is used to visualize and analyze anatomical features and structures that influence treatment plans. However, it is both labor intensive, and its effectiveness is operator dependent. Real-time accurate and robust automatic detection and tracking of anatomical structures while scanning would significantly impact diagnostic and therapeutic procedures to be consistent and efficient. In this paper, we propose a deep learning framework to automatically detect and track a specific anatomical target structure in ultrasound scans. Our framework is designed to be accurate and robust across subjects and imaging devices, to operate in real-time, and to not require a large training set. It maintains a localization precision and recall higher than 90% when trained on training sets that are as small as 20% in size of the original training set. The framework backbone is a weakly trained segmentation neural network based on U-Net. We tested the framework on two different ultrasound datasets with the aim to detect and track the Vagus nerve, where it outperformed current state-of-the-art real-time object detection networks.
http://w3id.org/mlsea/pwc/scientificWork/A%20CNN%E2%80%93LSTM%20model%20for%20gold%20price%20time-series%20forecasting                                                                                  A CNN–LSTM model for gold price time-series forecasting                                                                                  Gold price volatilities have a significant impact on many financial activities of the world. The development of a reliable prediction model could offer insights in gold price fluctuations, behavior and dynamics and ultimately could provide the opportunity of gaining significant profits. In this work, we propose a new deep learning forecasting model for the accurate prediction of gold price and movement. The proposed model exploits the ability of convolutional layers for extracting useful knowledge and learning the internal representation of time-series data as well as the effectiveness of long short-term memory (LSTM) layers for identifying short-term and long-term dependencies. We conducted a series of experiments and evaluated the proposed model against state-of-the-art deep learning and machine learning models. The preliminary experimental analysis illustrated that the utilization of LSTM layers along with additional convolutional layers could provide a significant boost in increasing the forecasting performance.
http://w3id.org/mlsea/pwc/scientificWork/A%20CNN-based%20Prediction-Aware%20Quality%20Enhancement%20Framework%20for%20VVC                                                                                  A CNN-based Prediction-Aware Quality Enhancement Framework for VVC                                                                                  This paper presents a framework for Convolutional Neural Network (CNN)-based quality enhancement task, by taking advantage of coding information in the compressed video signal. The motivation is that normative decisions made by the encoder can significantly impact the type and strength of artifacts in the decoded images. In this paper, the main focus has been put on decisions defining the prediction signal in intra and inter frames. This information has been used in the training phase as well as input to help the process of learning artifacts that are specific to each coding type. Furthermore, to retain a low memory requirement for the proposed method, one model is used for all Quantization Parameters (QPs) with a QP-map, which is also shared between luma and chroma components. In addition to the Post Processing (PP) approach, the In-Loop Filtering (ILF) codec integration has also been considered, where the characteristics of the Group of Pictures (GoP) are taken into account to boost the performance. The proposed CNN-based Quality Enhancement(QE) framework has been implemented on top of the VVC Test Model (VTM-10). Experiments show that the prediction-aware aspect of the proposed method improves the coding efficiency gain of the default CNN-based QE method by 1.52%, in terms of BD-BR, at the same network complexity compared to the default CNN-based QE filter.
http://w3id.org/mlsea/pwc/scientificWork/A%20Caputo%20fractional%20derivative-based%20algorithm%20for%20optimization                                                                                  A Caputo fractional derivative-based algorithm for optimization                                                                                  We propose a novel Caputo fractional derivative-based optimization algorithm. Upon defining the Caputo fractional gradient with respect to the Cartesian coordinate, we present a generic Caputo fractional gradient descent (CFGD) method. We prove that the CFGD yields the steepest descent direction of a locally smoothed objective function. The generic CFGD requires three parameters to be specified, and a choice of the parameters yields a version of CFGD. We propose three versions -- non-adaptive, adaptive terminal and adaptive order. By focusing on quadratic objective functions, we provide a convergence analysis. We prove that the non-adaptive CFGD converges to a Tikhonov regularized solution. For the two adaptive versions, we derive error bounds, which show convergence to integer-order stationary point under some conditions. We derive an explicit formula of CFGD for quadratic functions. We computationally found that the adaptive terminal (AT) CFGD mitigates the dependence on the condition number in the rate of convergence and results in significant acceleration over gradient descent (GD). For non-quadratic functions, we develop an efficient implementation of CFGD using the Gauss-Jacobi quadrature, whose computational cost is approximately proportional to the number of the quadrature points and the cost of GD. Our numerical examples show that AT-CFGD results in acceleration over GD, even when a small number of the Gauss-Jacobi quadrature points (including a single point) is used.
http://w3id.org/mlsea/pwc/scientificWork/A%20Cardinality%20Minimization%20Approach%20to%20Security-Constrained%20Economic%20Dispatch                                                                                  A Cardinality Minimization Approach to Security-Constrained Economic Dispatch                                                                                  We present a threshold-based cardinality minimization formulation to model the security-constrained economic dispatch problem. The model aims to minimize the operating cost of the system while simultaneously reducing the number of lines operating in emergency operating zones during contingency events. The model allows the system operator to monitor the duration for which lines operate in emergency zones and ensure that they are within the acceptable reliability standards determined by the system operators. We develop a continuous difference-of-convex approximation of the cardinality minimization problem and a solution method to solve the problem. Our numerical experiments demonstrate that the cardinality minimization approach reduces the overall system operating cost as well as avoids prolonged periods of high electricity prices during contingency events.
http://w3id.org/mlsea/pwc/scientificWork/A%20Case%20Study%20of%20LLVM-Based%20Analysis%20for%20Optimizing%20SIMD%20Code%20Generation                                                                                  A Case Study of LLVM-Based Analysis for Optimizing SIMD Code Generation                                                                                  This paper presents a methodology for using LLVM-based tools to tune the DCA++ (dynamical clusterapproximation) application that targets the new ARM A64FX processor. The goal is to describethe changes required for the new architecture and generate efficient single instruction/multiple data(SIMD) instructions that target the new Scalable Vector Extension instruction set. During manualtuning, the authors used the LLVM tools to improve code parallelization by using OpenMP SIMD,refactored the code and applied transformation that enabled SIMD optimizations, and ensured thatthe correct libraries were used to achieve optimal performance. By applying these code changes, codespeed was increased by 1.98X and 78 GFlops were achieved on the A64FX processor. The authorsaim to automatize parts of the efforts in the OpenMP Advisor tool, which is built on top of existingand newly introduced LLVM tooling.
http://w3id.org/mlsea/pwc/scientificWork/A%20Case%20Study%20of%20Spanish%20Text%20Transformations%20for%20Twitter%20Sentiment%20Analysis                                                                                  A Case Study of Spanish Text Transformations for Twitter Sentiment Analysis                                                                                  Sentiment analysis is a text mining task that determines the polarity of a given text, i.e., its positiveness or negativeness. Recently, it has received a lot of attention given the interest in opinion mining in micro-blogging platforms. These new forms of textual expressions present new challenges to analyze text given the use of slang, orthographic and grammatical errors, among others. Along with these challenges, a practical sentiment classifier should be able to handle efficiently large workloads. The aim of this research is to identify which text transformations (lemmatization, stemming, entity removal, among others), tokenizers (e.g., words $n$-grams), and tokens weighting schemes impact the most the accuracy of a classifier (Support Vector Machine) trained on two Spanish corpus. The methodology used is to exhaustively analyze all the combinations of the text transformations and their respective parameters to find out which characteristics the best performing classifiers have in common. Furthermore, among the different text transformations studied, we introduce a novel approach based on the combination of word based $n$-grams and character based $q$-grams. The results show that this novel combination of words and characters produces a classifier that outperforms the traditional word based combination by $11.17 %$ and $5.62 %$ on the INEGI and TASS'15 dataset, respectively.
http://w3id.org/mlsea/pwc/scientificWork/A%20Case%20Study%20on%20Pros%20and%20Cons%20of%20Regular%20Expression%20Detection%20and%20Dependency%20Parsing%20for%20Negation%20Extraction%20from%20German%20Medical%20Documents.%20Technical%20Report                                                                                  A Case Study on Pros and Cons of Regular Expression Detection and Dependency Parsing for Negation Extraction from German Medical Documents. Technical Report                                                                                  We describe our work on information extraction in medical documents written in German, especially detecting negations using an architecture based on the UIMA pipeline. Based on our previous work on software modules to cover medical concepts like diagnoses, examinations, etc. we employ a version of the NegEx regular expression algorithm with a large set of triggers as a baseline. We show how a significantly smaller trigger set is sufficient to achieve similar results, in order to reduce adaptation times to new text types. We elaborate on the question whether dependency parsing (based on the Stanford CoreNLP model) is a good alternative and describe the potentials and shortcomings of both approaches.
http://w3id.org/mlsea/pwc/scientificWork/A%20Case%20Study%3A%20Using%20Genetic%20Algorithm%20for%20Job%20Scheduling%20Problem                                                                                  A Case Study: Using Genetic Algorithm for Job Scheduling Problem                                                                                  Nowadays, DevOps pipelines of huge projects are getting more and more complex. Each job in the pipeline might need different requirements including specific hardware specifications and dependencies. To achieve minimal makespan, developers always apply as much machines as possible. Consequently, others may be stalled for waiting resource released. Minimizing the makespan of each job using a few resource is a challenging problem. In this study, it is aimed to 1) automatically determine the priority of jobs to reduce the waiting time in the line, 2) automatically allocate the machine resource to each job. In this work, the problem is formulated as a multi-objective optimization problem. We use GA algorithm to automatically determine job priorities and resource demand for minimizing individual makespan and resource usage. Finally, the experimental results show that our proposed priority list generation algorithm is more effective than current priority list producing method in the aspects of makespan and allocated machine count.
http://w3id.org/mlsea/pwc/scientificWork/A%20Case%20for%203D%20Integrated%20System%20Design%20for%20Neuromorphic%20Computing%20%26%20AI%20Applications                                                                                  A Case for 3D Integrated System Design for Neuromorphic Computing & AI Applications                                                                                  Over the last decade, artificial intelligence has found many applications areas in the society. As AI solutions have become more sophistication and the use cases grew, they highlighted the need to address performance and energy efficiency challenges faced during the implementation process. To address these challenges, there has been growing interest in neuromorphic chips. Neuromorphic computing relies on non von Neumann architectures as well as novel devices, circuits and manufacturing technologies to mimic the human brain. Among such technologies, 3D integration is an important enabler for AI hardware and the continuation of the scaling laws. In this paper, we overview the unique opportunities 3D integration provides in neuromorphic chip design, discuss the emerging opportunities in next generation neuromorphic architectures and review the obstacles. Neuromorphic architectures, which relied on the brain for inspiration and emulation purposes, face grand challenges due to the limited understanding of the functionality and the architecture of the human brain. Yet, high-levels of investments are dedicated to develop neuromorphic chips. We argue that 3D integration not only provides strategic advantages to the cost-effective and flexible design of neuromorphic chips, it may provide design flexibility in incorporating advanced capabilities to further benefits the designs in the future.
http://w3id.org/mlsea/pwc/scientificWork/A%20Category%20for%20Extensive-Form%20Games                                                                                  A Category for Extensive-Form Games                                                                                  This paper introduces Gm, which is a category for extensive-form games. It also provides some applications. The category's objects are games, which are understood to be sets of nodes which have been endowed with edges, information sets, actions, players, and utility functions. Its arrows are functions from source nodes to target nodes that preserve the additional structure. For instance, a game's information-set collection is newly regarded as a topological basis for the game's decision-node set, and thus a morphism's continuity serves to preserve information sets. Given these definitions, a game monomorphism is characterized by the property of not mapping two source runs (plays) to the same target run. Further, a game isomorphism is characterized as a bijection whose restriction to decision nodes is a homeomorphism, whose induced player transformation is injective, and which strictly preserves the ordinal content of the utility functions. The category is then applied to some game-theoretic concepts beyond the definition of a game. A Selten subgame is characterized as a special kind of categorical subgame, and game isomorphisms are shown to preserve strategy sets, Nash equilibria, Selten subgames, subgame-perfect equilibria, perfect-information, and no-absentmindedness. Further, it is shown that the full subcategory for distinguished-action sequence games is essentially wide in the category of all games, and that the full subcategory of action-set games is essentially wide in the full subcategory for games with no-absentmindedness.
http://w3id.org/mlsea/pwc/scientificWork/A%20Central%20Limit%20Theorem%20for%20Differentially%20Private%20Query%20Answering                                                                                  A Central Limit Theorem for Differentially Private Query Answering                                                                                  Perhaps the single most important use case for differential privacy is to privately answer numerical queries, which is usually achieved by adding noise to the answer vector. The central question, therefore, is to understand which noise distribution optimizes the privacy-accuracy trade-off, especially when the dimension of the answer vector is high. Accordingly, extensive literature has been dedicated to the question and the upper and lower bounds have been matched up to constant factors [BUV18, SU17]. In this paper, we take a novel approach to address this important optimality question. We first demonstrate an intriguing central limit theorem phenomenon in the high-dimensional regime. More precisely, we prove that a mechanism is approximately Gaussian Differentially Private [DRS21] if the added noise satisfies certain conditions. In particular, densities proportional to $ mathrm{e}^{- |x |_p^ alpha}$, where $ |x |_p$ is the standard $ ell_p$-norm, satisfies the conditions. Taking this perspective, we make use of the Cramer--Rao inequality and show an 'uncertainty principle'-style result: the product of the privacy parameter and the $ ell_2$-loss of the mechanism is lower bounded by the dimension. Furthermore, the Gaussian mechanism achieves the constant-sharp optimal privacy-accuracy trade-off among all such noises. Our findings are corroborated by numerical experiments.
http://w3id.org/mlsea/pwc/scientificWork/A%20Central%20Limit%20Theorem%2C%20Loss%20Aversion%20and%20Multi-Armed%20Bandits                                                                                  A Central Limit Theorem, Loss Aversion and Multi-Armed Bandits                                                                                  This paper studies a multi-armed bandit problem where the decision-maker is loss averse, in particular she is risk averse in the domain of gains and risk loving in the domain of losses. The focus is on large horizons. Consequences of loss aversion for asymptotic (large horizon) properties are derived in a number of analytical results. The analysis is based on a new central limit theorem for a set of measures under which conditional variances can vary in a largely unstructured history-dependent way subject only to the restriction that they lie in a fixed interval.
http://w3id.org/mlsea/pwc/scientificWork/A%20Centralized%20Optimization%20Approach%20for%20Bidirectional%20PEV%20Impacts%20Analysis%20in%20a%20Commercial%20Building-Integrated%20Microgrid                                                                                  A Centralized Optimization Approach for Bidirectional PEV Impacts Analysis in a Commercial Building-Integrated Microgrid                                                                                  Building sector is the largest energy user in the United States. Conventional building energy studies mostly involve Heating, Ventilation, and Air Conditioning (HVAC), and lighting energy consumptions. Recent additions of solar Photovoltaics (PV) along with other Distributed Energy Resources (DER), particularly Plug-in Electric Vehicles (PEV) have added a new dimension to this problem and made it more complex. This paper presents an avant-garde framework for selecting the best charging/discharging level of PEV for a commercial building-integrated microgrid. A typical commercial building is used as a microgrid testbed incorporating all the DERs presented in a smart building. A Mixed Integer Linear Programming (MILP) problem is formulated to optimize the energy and demand cost associated with this building operation. The cost function is solved in conjunction with real data and modified to assess the bidirectional PEV impacts on the flexible building loads that are contributing factors in making energy usage decisions. Finally, the impacts of optimized DERs are investigated on a Distribution System (DS) to show the necessity of a holistic approach for selecting the suitable PEV strategies. The results show that bidirectional fast PEV activities can provide higher cost reduction and less voltage deviation in comparison to slow PEV activities.
http://w3id.org/mlsea/pwc/scientificWork/A%20Century%20of%20Economic%20Policy%20Uncertainty%20Through%20the%20French-Canadian%20Lens                                                                                  A Century of Economic Policy Uncertainty Through the French-Canadian Lens                                                                                  A novel token-distance-based triple approach is proposed for identifying EPU mentions in textual documents. The method is applied to a corpus of French-language news to construct a century-long historical EPU index for the Canadian province of Quebec. The relevance of the index is shown in a macroeconomic nowcasting experiment.
http://w3id.org/mlsea/pwc/scientificWork/A%20Characterization%20of%20All%20Passivizing%20Input-Output%20Transformations%20of%20a%20Passive-Short%20System                                                                                  A Characterization of All Passivizing Input-Output Transformations of a Passive-Short System                                                                                  Passivity theory is one of the cornerstones of control theory, as it allows one to prove stability of a large-scale system while treating each component separately. In practice, many systems are not passive, and must be passivized in order to be included in the framework of passivity theory. Input-output transformations are the most general tool for passivizing systems, generalizing output-feedback and input-feedthrough. In this paper, we classify all possible input-output transformations that map a system with given shortage of passivity to a system with prescribed excess of passivity. We do so by using the connection between passivity theory and cones for SISO systems, and using the S-lemma for MIMO systems. We also present several possible applications of our results, including simultaneous passivation of multiple systems or with respect to multiple equilibria, as well as optimization problems such as $ mathcal{L}_2$-gain minimization. We also exhibit our results in a case study about synchronization in a network of non-passive faulty agents.
http://w3id.org/mlsea/pwc/scientificWork/A%20Chirp%20Spread%20Spectrum%20Modulation%20Scheme%20for%20Robust%20Power%20Line%20Communication                                                                                  A Chirp Spread Spectrum Modulation Scheme for Robust Power Line Communication                                                                                  This paper proposes the use of a LoRa like chirp spread spectrum physical layer as the basis for a new Power Line Communication modulation scheme suited for low-bandwidth communication. It is shown that robust communication can be established even in channels exhibiting both extreme multipath interference and low SNR (-40dB), with synchronisation requirements significantly reduced compared to conventional LoRa. ATP-EMTP simulations using frequency dependent line and transformer models, and simulations using artificial Rayleigh channels demonstrate the effectiveness of the new scheme in providing load data from LV feeders back to the MV primary substation. We further present experimental results based on a Field Programmable Gate Array hardware implementation of the proposed scheme.
http://w3id.org/mlsea/pwc/scientificWork/A%20Chirplet%20Transform-based%20Mode%20Retrieval%20Method%20for%20Multicomponent%20Signals%20with%20Crossover%20Instantaneous%20Frequencies                                                                                  A Chirplet Transform-based Mode Retrieval Method for Multicomponent Signals with Crossover Instantaneous Frequencies                                                                                  In nature and engineering world, the acquired signals are usually affected by multiple complicated factors and appear as multicomponent nonstationary modes. In such and many other situations, it is necessary to separate these signals into a finite number of monocomponents to represent the intrinsic modes and underlying dynamics implicated in the source signals. In this paper, we consider the mode retrieval of a multicomponent signal which has crossing instantaneous frequencies (IFs), meaning that some of the components of the signal overlap in the time-frequency domain. We use the chirplet transform (CT) to represent a multicomponent signal in the three-dimensional space of time, frequency and chirp rate and introduce a CT-based signal separation scheme (CT3S) to retrieve modes. In addition, we analyze the error bounds for IF estimation and component recovery with this scheme. We also propose a matched-filter along certain specific time-frequency lines with respect to the chirp rate to make nonstationary signals be further separated and more concentrated in the three-dimensional space of CT. Furthermore, based on the approximation of source signals with linear chirps at any local time, we propose an innovative signal reconstruction algorithm, called the group filter-matched CT3S (GFCT3S), which also takes a group of components into consideration simultaneously. GFCT3S is suitable for signals with crossing IFs. It also decreases component recovery errors when the IFs curves of different components are not crossover, but fast-varying and close to one and other. Numerical experiments on synthetic and real signals show our method is more accurate and consistent in signal separation than the empirical mode decomposition, synchrosqueezing transform, and other approaches
http://w3id.org/mlsea/pwc/scientificWork/A%20Circular-Structured%20Representation%20for%20Visual%20Emotion%20Distribution%20Learning                                                                                  A Circular-Structured Representation for Visual Emotion Distribution Learning                                                                                  Visual Emotion Analysis (VEA) has attracted increasing attention recently with the prevalence of sharing images on social networks. Since human emotions are ambiguous and subjective, it is more reasonable to address VEA in a label distribution learning (LDL) paradigm rather than a single-label classification task. Different from other LDL tasks, there exist intrinsic relationships between emotions and unique characteristics within them, as demonstrated in psychological theories. Inspired by this, we propose a well-grounded circular-structured representation to utilize the prior knowledge for visual emotion distribution learning. To be specific, we first construct an Emotion Circle to unify any emotional state within it. On the proposed Emotion Circle, each emotion distribution is represented with an emotion vector, which is defined with three attributes (i.e., emotion polarity, emotion type, emotion intensity) as well as two properties (i.e., similarity, additivity). Besides, we design a novel Progressive Circular (PC) loss to penalize the dissimilarities between predicted emotion vector and labeled one in a coarse-to-fine manner, which further boosts the learning process in an emotion-specific way. Extensive experiments and comparisons are conducted on public visual emotion distribution datasets, and the results demonstrate that the proposed method outperforms the state-of-the-art methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20Clarification%20of%20the%20Nuances%20in%20the%20Fairness%20Metrics%20Landscape                                                                                  A Clarification of the Nuances in the Fairness Metrics Landscape                                                                                  In recent years, the problem of addressing fairness in Machine Learning (ML) and automatic decision-making has attracted a lot of attention in the scientific communities dealing with Artificial Intelligence. A plethora of different definitions of fairness in ML have been proposed, that consider different notions of what is a 'fair decision' in situations impacting individuals in the population. The precise differences, implications and 'orthogonality' between these notions have not yet been fully analyzed in the literature. In this work, we try to make some order out of this zoo of definitions.
http://w3id.org/mlsea/pwc/scientificWork/A%20Class%20of%20Dimension-free%20Metrics%20for%20the%20Convergence%20of%20Empirical%20Measures                                                                                  A Class of Dimension-free Metrics for the Convergence of Empirical Measures                                                                                  This paper concerns the convergence of empirical measures in high dimensions. We propose a new class of probability metrics and show that under such metrics, the convergence is free of the curse of dimensionality (CoD). Such a feature is critical for high-dimensional analysis and stands in contrast to classical metrics ({ it e.g.}, the Wasserstein metric). The proposed metrics fall into the category of integral probability metrics, for which we specify criteria of test function spaces to guarantee the property of being free of CoD. Examples of the selected test function spaces include the reproducing kernel Hilbert spaces, Barron space, and flow-induced function spaces. Three applications of the proposed metrics are presented: 1. The convergence of empirical measure in the case of random variables; 2. The convergence of $n$-particle system to the solution to McKean-Vlasov stochastic differential equation; 3. The construction of an $ varepsilon$-Nash equilibrium for a homogeneous $n$-player game by its mean-field limit. As a byproduct, we prove that, given a distribution close to the target distribution measured by our metric and a certain representation of the target distribution, we can generate a distribution close to the target one in terms of the Wasserstein metric and relative entropy. Overall, we show that the proposed class of metrics is a powerful tool to analyze the convergence of empirical measures in high dimensions without CoD.
http://w3id.org/mlsea/pwc/scientificWork/A%20Clinically%20Inspired%20Approach%20for%20Melanoma%20classification                                                                                  A Clinically Inspired Approach for Melanoma classification                                                                                  Melanoma is a leading cause of deaths due to skin cancer deaths and hence, early and effective diagnosis of melanoma is of interest. Current approaches for automated diagnosis of melanoma either use pattern recognition or analytical recognition like ABCDE (asymmetry, border, color, diameter and evolving) criterion. In practice however, a differential approach wherein outliers (ugly duckling) are detected and used to evaluate nevi/lesions. Incorporation of differential recognition in Computer Aided Diagnosis (CAD) systems has not been explored but can be beneficial as it can provide a clinical justification for the derived decision. We present a method for identifying and quantifying ugly ducklings by performing Intra-Patient Comparative Analysis (IPCA) of neighboring nevi. This is then incorporated in a CAD system design for melanoma detection. This design ensures flexibility to handle cases where IPCA is not possible. Our experiments on a public dataset show that the outlier information helps boost the sensitivity of detection by at least 4.1 % and specificity by 4.0 % to 8.9 %, depending on the use of a strong (EfficientNet) or moderately strong (VGG or ResNet) classifier.
http://w3id.org/mlsea/pwc/scientificWork/A%20Closed-form%20Localization%20Method%20Utilizing%20Pseudorange%20Measurements%20from%20Two%20Non-synchronized%20Positioning%20Systems                                                                                  A Closed-form Localization Method Utilizing Pseudorange Measurements from Two Non-synchronized Positioning Systems                                                                                  In a time-of-arrival (TOA) or pseudorange based positioning system, user location is obtained by observing multiple anchor nodes (AN) at known positions. Utilizing more than one positioning systems, e.g., combining Global Positioning System (GPS) and BeiDou Navigation Satellite System (BDS), brings better positioning accuracy. However, ANs from two systems are usually synchronized to two different clock sources. Different from single-system localization, an extra user-to-system clock offset needs to be handled. Existing dual-system methods either have high computational complexity or sub-optimal positioning accuracy. In this paper, we propose a new closed-form dual-system localization (CDL) approach that has low complexity and optimal localization accuracy. We first convert the nonlinear problem into a linear one by squaring the distance equations and employing intermediate variables. Then, a weighted least squares (WLS) method is used to optimize the positioning accuracy. We prove that the positioning error of the new method reaches Cramer-Rao Lower Bound (CRLB) in far field conditions with small measurement noise. Simulations on 2D and 3D positioning scenes are conducted. Results show that, compared with the iterative approach, which has high complexity and requires a good initialization, the new CDL method does not require initialization and has lower computational complexity with comparable positioning accuracy. Numerical results verify the theoretical analysis on positioning accuracy, and show that the new CDL method has superior performance over the state-of-the-art closed-form method. Experiments using real GPS and BDS data verify the applicability of the new CDL method and the superiority of its performance in the real world.
http://w3id.org/mlsea/pwc/scientificWork/A%20Closer%20Look%20at%20Fourier%20Spectrum%20Discrepancies%20for%20CNN-generated%20Images%20Detection                                                                                  A Closer Look at Fourier Spectrum Discrepancies for CNN-generated Images Detection                                                                                  CNN-based generative modelling has evolved to produce synthetic images indistinguishable from real images in the RGB pixel space. Recent works have observed that CNN-generated images share a systematic shortcoming in replicating high frequency Fourier spectrum decay attributes. Furthermore, these works have successfully exploited this systematic shortcoming to detect CNN-generated images reporting up to 99% accuracy across multiple state-of-the-art GAN models. In this work, we investigate the validity of assertions claiming that CNN-generated images are unable to achieve high frequency spectral decay consistency. We meticulously construct a counterexample space of high frequency spectral decay consistent CNN-generated images emerging from our handcrafted experiments using DCGAN, LSGAN, WGAN-GP and StarGAN, where we empirically show that this frequency discrepancy can be avoided by a minor architecture change in the last upsampling operation. We subsequently use images from this counterexample space to successfully bypass the recently proposed forensics detector which leverages on high frequency Fourier spectrum decay attributes for CNN-generated image detection. Through this study, we show that high frequency Fourier spectrum decay discrepancies are not inherent characteristics for existing CNN-based generative models--contrary to the belief of some existing work--, and such features are not robust to perform synthetic image detection. Our results prompt re-thinking of using high frequency Fourier spectrum decay attributes for CNN-generated image detection. Code and models are available at https://keshik6.github.io/Fourier-Discrepancies-CNN-Detection/
http://w3id.org/mlsea/pwc/scientificWork/A%20Closer%20Look%20at%20How%20Fine-tuning%20Changes%20BERT                                                                                  A Closer Look at How Fine-tuning Changes BERT                                                                                  Given the prevalence of pre-trained contextualized representations in today's NLP, there have been many efforts to understand what information they contain, and why they seem to be universally successful. The most common approach to use these representations involves fine-tuning them for an end task. Yet, how fine-tuning changes the underlying embedding space is less studied. In this work, we study the English BERT family and use two probing techniques to analyze how fine-tuning changes the space. We hypothesize that fine-tuning affects classification performance by increasing the distances between examples associated with different labels. We confirm this hypothesis with carefully designed experiments on five different NLP tasks. Via these experiments, we also discover an exception to the prevailing wisdom that 'fine-tuning always improves performance'. Finally, by comparing the representations before and after fine-tuning, we discover that fine-tuning does not introduce arbitrary changes to representations; instead, it adjusts the representations to downstream tasks while largely preserving the original spatial structure of the data points.
http://w3id.org/mlsea/pwc/scientificWork/A%20Closer%20Look%20at%20Self-training%20for%20Zero-Label%20Semantic%20Segmentation                                                                                  A Closer Look at Self-training for Zero-Label Semantic Segmentation                                                                                  Being able to segment unseen classes not observed during training is an important technical challenge in deep learning, because of its potential to reduce the expensive annotation required for semantic segmentation. Prior zero-label semantic segmentation works approach this task by learning visual-semantic embeddings or generative models. However, they are prone to overfitting on the seen classes because there is no training signal for them. In this paper, we study the challenging generalized zero-label semantic segmentation task where the model has to segment both seen and unseen classes at test time. We assume that pixels of unseen classes could be present in the training images but without being annotated. Our idea is to capture the latent information on unseen classes by supervising the model with self-produced pseudo-labels for unlabeled pixels. We propose a consistency regularizer to filter out noisy pseudo-labels by taking the intersections of the pseudo-labels generated from different augmentations of the same image. Our framework generates pseudo-labels and then retrain the model with human-annotated and pseudo-labelled data. This procedure is repeated for several iterations. As a result, our approach achieves the new state-of-the-art on PascalVOC12 and COCO-stuff datasets in the challenging generalized zero-label semantic segmentation setting, surpassing other existing methods addressing this task with more complex strategies.
http://w3id.org/mlsea/pwc/scientificWork/A%20Closer%20Look%20at%20the%20Worst-case%20Behavior%20of%20Multi-armed%20Bandit%20Algorithms                                                                                  A Closer Look at the Worst-case Behavior of Multi-armed Bandit Algorithms                                                                                  One of the key drivers of complexity in the classical (stochastic) multi-armed bandit (MAB) problem is the difference between mean rewards in the top two arms, also known as the instance gap. The celebrated Upper Confidence Bound (UCB) policy is among the simplest optimism-based MAB algorithms that naturally adapts to this gap: for a horizon of play n, it achieves optimal O(log n) regret in instances with 'large' gaps, and a near-optimal O( sqrt{n log n}) minimax regret when the gap can be arbitrarily 'small.' This paper provides new results on the arm-sampling behavior of UCB, leading to several important insights. Among these, it is shown that arm-sampling rates under UCB are asymptotically deterministic, regardless of the problem complexity. This discovery facilitates new sharp asymptotics and a novel alternative proof for the O( sqrt{n log n}) minimax regret of UCB. Furthermore, the paper also provides the first complete process-level characterization of the MAB problem under UCB in the conventional diffusion scaling. Among other things, the 'small' gap worst-case lens adopted in this paper also reveals profound distinctions between the behavior of UCB and Thompson Sampling, such as an 'incomplete learning' phenomenon characteristic of the latter.
http://w3id.org/mlsea/pwc/scientificWork/A%20Cloud-based%20Deep%20Learning%20Framework%20for%20Remote%20Detection%20of%20Diabetic%20Foot%20Ulcers                                                                                  A Cloud-based Deep Learning Framework for Remote Detection of Diabetic Foot Ulcers                                                                                  This research proposes a mobile and cloud-based framework for the automatic detection of diabetic foot ulcers and conducts an investigation of its performance. The system uses a cross-platform mobile framework which enables the deployment of mobile apps to multiple platforms using a single TypeScript code base. A deep convolutional neural network was deployed to a cloud-based platform where the mobile app could send photographs of patient's feet for inference to detect the presence of diabetic foot ulcers. The functionality and usability of the system were tested in two clinical settings: Salford Royal NHS Foundation Trust and Lancashire Teaching Hospitals NHS Foundation Trust. The benefits of the system, such as the potential use of the app by patients to identify and monitor their condition are discussed.
http://w3id.org/mlsea/pwc/scientificWork/A%20Cluster-based%20Approach%20for%20Improving%20Isotropy%20in%20Contextual%20Embedding%20Space                                                                                  A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space                                                                                  The representation degeneration problem in Contextual Word Representations (CWRs) hurts the expressiveness of the embedding space by forming an anisotropic cone where even unrelated words have excessively positive correlations. Existing techniques for tackling this issue require a learning process to re-train models with additional objectives and mostly employ a global assessment to study isotropy. Our quantitative analysis over isotropy shows that a local assessment could be more accurate due to the clustered structure of CWRs. Based on this observation, we propose a local cluster-based method to address the degeneration issue in contextual embedding spaces. We show that in clusters including punctuations and stop words, local dominant directions encode structural information, removing which can improve CWRs performance on semantic tasks. Moreover, we find that tense information in verb representations dominates sense semantics. We show that removing dominant directions of verb representations can transform the space to better suit semantic applications. Our experiments demonstrate that the proposed cluster-based method can mitigate the degeneration problem on multiple tasks.
http://w3id.org/mlsea/pwc/scientificWork/A%20Clustering%20Framework%20for%20Residential%20Electric%20Demand%20Profiles                                                                                  A Clustering Framework for Residential Electric Demand Profiles                                                                                  The availability of residential electric demand profiles data, enabled by the large-scale deployment of smart metering infrastructure, has made it possible to perform more accurate analysis of electricity consumption patterns. This paper analyses the electric demand profiles of individual households located in the city Amsterdam, the Netherlands. A comprehensive clustering framework is defined to classify households based on their electricity consumption pattern. This framework consists of two main steps, namely a dimensionality reduction step of input electricity consumption data, followed by an unsupervised clustering algorithm of the reduced subspace. While any algorithm, which has been used in the literature for the aforementioned clustering task, can be used for the corresponding step, the more important question is to deduce which particular combination of algorithms is the best for a given dataset and a clustering task. This question is addressed in this paper by proposing a novel objective validation strategy, whose recommendations are then cross-verified by performing subjective validation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Clustering-based%20Framework%20for%20Classifying%20Data%20Streams                                                                                  A Clustering-based Framework for Classifying Data Streams                                                                                  The non-stationary nature of data streams strongly challenges traditional machine learning techniques. Although some solutions have been proposed to extend traditional machine learning techniques for handling data streams, these approaches either require an initial label set or rely on specialized design parameters. The overlap among classes and the labeling of data streams constitute other major challenges for classifying data streams. In this paper, we proposed a clustering-based data stream classification framework to handle non-stationary data streams without utilizing an initial label set. A density-based stream clustering procedure is used to capture novel concepts with a dynamic threshold and an effective active label querying strategy is introduced to continuously learn the new concepts from the data streams. The sub-cluster structure of each cluster is explored to handle the overlap among classes. Experimental results and quantitative comparison studies reveal that the proposed method provides statistically better or comparable performance than the existing methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20Coarse%20to%20Fine%20Question%20Answering%20System%20based%20on%20Reinforcement%20Learning                                                                                  A Coarse to Fine Question Answering System based on Reinforcement Learning                                                                                  In this paper, we present a coarse to fine question answering (CFQA) system based on reinforcement learning which can efficiently processes documents with different lengths by choosing appropriate actions. The system is designed using an actor-critic based deep reinforcement learning model to achieve multi-step question answering. Compared to previous QA models targeting on datasets mainly containing either short or long documents, our multi-step coarse to fine model takes the merits from multiple system modules, which can handle both short and long documents. The system hence obtains a much better accuracy and faster trainings speed compared to the current state-of-the-art models. We test our model on four QA datasets, WIKEREADING, WIKIREADING LONG, CNN and SQuAD, and demonstrate 1.3$ %$-1.7$ %$ accuracy improvements with 1.5x-3.4x training speed-ups in comparison to the baselines using state-of-the-art models.
http://w3id.org/mlsea/pwc/scientificWork/A%20Coarse-to-Fine%20Instance%20Segmentation%20Network%20with%20Learning%20Boundary%20Representation                                                                                  A Coarse-to-Fine Instance Segmentation Network with Learning Boundary Representation                                                                                  Boundary-based instance segmentation has drawn much attention since of its attractive efficiency. However, existing methods suffer from the difficulty in long-distance regression. In this paper, we propose a coarse-to-fine module to address the problem. Approximate boundary points are generated at the coarse stage and then features of these points are sampled and fed to a refined regressor for fine prediction. It is end-to-end trainable since differential sampling operation is well supported in the module. Furthermore, we design a holistic boundary-aware branch and introduce instance-agnostic supervision to assist regression. Equipped with ResNet-101, our approach achieves 31.7 % mask AP on COCO dataset with single-scale training and testing, outperforming the baseline 1.3 % mask AP with less than 1 % additional parameters and GFLOPs. Experiments also show that our proposed method achieves competitive performance compared to existing boundary-based methods with a lightweight design and a simple pipeline.
http://w3id.org/mlsea/pwc/scientificWork/A%20Cognitive%20Regularizer%20for%20Language%20Modeling                                                                                  A Cognitive Regularizer for Language Modeling                                                                                  The uniform information density (UID) hypothesis, which posits that speakers behaving optimally tend to distribute information uniformly across a linguistic signal, has gained traction in psycholinguistics as an explanation for certain syntactic, morphological, and prosodic choices. In this work, we explore whether the UID hypothesis can be operationalized as an inductive bias for statistical language modeling. Specifically, we augment the canonical MLE objective for training language models with a regularizer that encodes UID. In experiments on ten languages spanning five language families, we find that using UID regularization consistently improves perplexity in language models, having a larger effect when training data is limited. Moreover, via an analysis of generated sequences, we find that UID-regularized language models have other desirable properties, e.g., they generate text that is more lexically diverse. Our results not only suggest that UID is a reasonable inductive bias for language modeling, but also provide an alternative validation of the UID hypothesis using modern-day NLP tools.
http://w3id.org/mlsea/pwc/scientificWork/A%20Cognitive%20Science%20perspective%20for%20learning%20how%20to%20design%20meaningful%20user%20experiences%20and%20human-centered%20technology                                                                                  A Cognitive Science perspective for learning how to design meaningful user experiences and human-centered technology                                                                                  This paper reviews literature in cognitive science, human-computer interaction (HCI) and natural-language processing (NLP) to consider how analogical reasoning (AR) could help inform the design of communication and learning technologies, as well as online communities and digital platforms. First, analogical reasoning (AR) is defined, and use-cases of AR in the computing sciences are presented. The concept of schema is introduced, along with use-cases in computing. Finally, recommendations are offered for future work on using analogical reasoning and schema methods in the computing sciences.
http://w3id.org/mlsea/pwc/scientificWork/A%20Combination%205-DOF%20Active%20Magnetic%20Bearing%20For%20Energy%20Storage%20Flywheel                                                                                  A Combination 5-DOF Active Magnetic Bearing For Energy Storage Flywheel                                                                                  Conventional active magnetic bearing (AMB) systems use several separate radial and thrust bearings to provide a 5 degree of freedom (DOF) levitation control. This paper presents a novel combination 5-DOF active magnetic bearing (C5AMB) designed for a shaft-less, hub-less, high-strength steel energy storage flywheel (SHFES), which achieves doubled energy density compared to prior technologies. As a single device, the C5AMB provides radial, axial, and tilting levitations simultaneously. In addition, it utilizes low-cost and more available materials to replace silicon steels and laminations, which results in reduced costs and more convenient assemblies. Apart from the unique structure and the use of low magnetic grade material, other design challenges include shared flux paths, large dimensions, and relatively small air gaps. The finite element method (FEM) is too computationally intensive for early-stage analysis. An equivalent magnetic circuit method (EMCM) is developed for modeling and analysis. Nonlinear FEM is then used for detailed simulations. Both permanent magnets (PM) and electromagnetic control currents provide the weight-balancing lifting force. During the full-scale prototype testing, the C5AMB successfully levitates a 5440 kg and 2 m diameter flywheel at an air gap of 1.14 mm. Its current and position stiffnesses are verified experimentally.
http://w3id.org/mlsea/pwc/scientificWork/A%20Combined%20Deep%20Learning%20based%20End-to-End%20Video%20Coding%20Architecture%20for%20YUV%20Color%20Space                                                                                  A Combined Deep Learning based End-to-End Video Coding Architecture for YUV Color Space                                                                                  Most of the existing deep learning based end-to-end video coding (DLEC) architectures are designed specifically for RGB color format, yet the video coding standards, including H.264/AVC, H.265/HEVC and H.266/VVC developed over past few decades, have been designed primarily for YUV 4:2:0 format, where the chrominance (U and V) components are subsampled to achieve superior compression performances considering the human visual system. While a broad number of papers on DLEC compare these two distinct coding schemes in RGB domain, it is ideal to have a common evaluation framework in YUV 4:2:0 domain for a more fair comparison. This paper introduces a new DLEC architecture for video coding to effectively support YUV 4:2:0 and compares its performance against the HEVC standard under a common evaluation framework. The experimental results on YUV 4:2:0 video sequences show that the proposed architecture can outperform HEVC in intra-frame coding, however inter-frame coding is not as efficient on contrary to the RGB coding results reported in recent papers.
http://w3id.org/mlsea/pwc/scientificWork/A%20Compact%20Model%20for%20SiC%20Power%20MOSFETs%20for%20Large%20Current%20and%20High%20Voltage%20Operation                                                                                  A Compact Model for SiC Power MOSFETs for Large Current and High Voltage Operation                                                                                  This work presents a physics based compact model for SiC power MOSFETs that accurately describes the I-V characteristics up to large voltages and currents. Charge-based formulations accounting for the different physics of SiC power MOSFETs are presented. The formulations account for the effect of the large SiC/SiO2 interface traps density characteristic of SiC MOSFETs and its dependence with temperature. The modeling of interface charge density is found to be necessary to describe the electrostatics of SiC power MOSFETs when operating at simultaneous high current and high voltage regions. The proposed compact model accurately fits the measurement data extracted of a 160 milli ohms, 1200V SiC power MOSFET in the complete IV plane from drain-voltage $V_d$ = 5mV up to 800 V and current ranges from few mA to 30 A.
http://w3id.org/mlsea/pwc/scientificWork/A%20Compact%20and%20Interpretable%20Convolutional%20Neural%20Network%20for%20Cross-Subject%20Driver%20Drowsiness%20Detection%20from%20Single-Channel%20EEG                                                                                  A Compact and Interpretable Convolutional Neural Network for Cross-Subject Driver Drowsiness Detection from Single-Channel EEG                                                                                  Driver drowsiness is one of main factors leading to road fatalities and hazards in the transportation industry. Electroencephalography (EEG) has been considered as one of the best physiological signals to detect drivers drowsy states, since it directly measures neurophysiological activities in the brain. However, designing a calibration-free system for driver drowsiness detection with EEG is still a challenging task, as EEG suffers from serious mental and physical drifts across different subjects. In this paper, we propose a compact and interpretable Convolutional Neural Network (CNN) to discover shared EEG features across different subjects for driver drowsiness detection. We incorporate the Global Average Pooling (GAP) layer in the model structure, allowing the Class Activation Map (CAM) method to be used for localizing regions of the input signal that contribute most for classification. Results show that the proposed model can achieve an average accuracy of 73.22% on 11 subjects for 2-class cross-subject EEG signal classification, which is higher than conventional machine learning methods and other state-of-art deep learning methods. It is revealed by the visualization technique that the model has learned biologically explainable features, e.g., Alpha spindles and Theta burst, as evidence for the drowsy state. It is also interesting to see that the model uses artifacts that usually dominate the wakeful EEG, e.g., muscle artifacts and sensor drifts, to recognize the alert state. The proposed model illustrates a potential direction to use CNN models as a powerful tool to discover shared features related to different mental states across different subjects from EEG signals.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comparative%20Analysis%20of%20Machine%20Learning%20and%20Grey%20Models                                                                                  A Comparative Analysis of Machine Learning and Grey Models                                                                                  Artificial Intelligence (AI) has recently shown its capabilities for almost every field of life. Machine Learning, which is a subset of AI, is a `HOT' topic for researchers. Machine Learning outperforms other classical forecasting techniques in almost all-natural applications. It is a crucial part of modern research. As per this statement, Modern Machine Learning algorithms are hungry for big data. Due to the small datasets, the researchers may not prefer to use Machine Learning algorithms. To tackle this issue, the main purpose of this survey is to illustrate, demonstrate related studies for significance of a semi-parametric Machine Learning framework called Grey Machine Learning (GML). This kind of framework is capable of handling large datasets as well as small datasets for time series forecasting likely outcomes. This survey presents a comprehensive overview of the existing semi-parametric machine learning techniques for time series forecasting. In this paper, a primer survey on the GML framework is provided for researchers. To allow an in-depth understanding for the readers, a brief description of Machine Learning, as well as various forms of conventional grey forecasting models are discussed. Moreover, a brief description on the importance of GML framework is presented.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comparative%20Evaluation%20of%20Predominant%20Deep%20Learning%20Quantified%20Stock%20Trading%20Strategies                                                                                  A Comparative Evaluation of Predominant Deep Learning Quantified Stock Trading Strategies                                                                                  This study first reconstructs three deep learning powered stock trading models and their associated strategies that are representative of distinct approaches to the problem and established upon different aspects of the many theories evolved around deep learning. It then seeks to compare the performance of these strategies from different perspectives through trading simulations ran on three scenarios when the benchmarks are kept at historical low points for extended periods of time. The results show that in extremely adverse market climates, investment portfolios managed by deep learning powered algorithms are able to avert accumulated losses by generating return sequences that shift the constantly negative CSI 300 benchmark return upward. Among the three, the LSTM model's strategy yields the best performance when the benchmark sustains continued loss.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comparative%20Study%20of%20Using%20Spatial-Temporal%20Graph%20Convolutional%20Networks%20for%20Predicting%20Availability%20in%20Bike%20Sharing%20Schemes                                                                                  A Comparative Study of Using Spatial-Temporal Graph Convolutional Networks for Predicting Availability in Bike Sharing Schemes                                                                                  Accurately forecasting transportation demand is crucial for efficient urban traffic guidance, control and management. One solution to enhance the level of prediction accuracy is to leverage graph convolutional networks (GCN), a neural network based modelling approach with the ability to process data contained in graph based structures. As a powerful extension of GCN, a spatial-temporal graph convolutional network (ST-GCN) aims to capture the relationship of data contained in the graphical nodes across both spatial and temporal dimensions, which presents a novel deep learning paradigm for the analysis of complex time-series data that also involves spatial information as present in transportation use cases. In this paper, we present an Attention-based ST-GCN (AST-GCN) for predicting the number of available bikes in bike-sharing systems in cities, where the attention-based mechanism is introduced to further improve the performance of an ST-GCN. Furthermore, we also discuss the impacts of different modelling methods of adjacency matrices on the proposed architecture. Our experimental results are presented using two real-world datasets, Dublinbikes and NYC-Citi Bike, to illustrate the efficacy of our proposed model which outperforms the majority of existing approaches.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comparative%20Study%20on%20Collecting%20High-Quality%20Implicit%20Reasonings%20at%20a%20Large-scale                                                                                  A Comparative Study on Collecting High-Quality Implicit Reasonings at a Large-scale                                                                                  Explicating implicit reasoning (i.e. warrants) in arguments is a long-standing challenge for natural language understanding systems. While recent approaches have focused on explicating warrants via crowdsourcing or expert annotations, the quality of warrants has been questionable due to the extreme complexity and subjectivity of the task. In this paper, we tackle the complex task of warrant explication and devise various methodologies for collecting warrants. We conduct an extensive study with trained experts to evaluate the resulting warrants of each methodology and find that our methodologies allow for high-quality warrants to be collected. We construct a preliminary dataset of 6,000 warrants annotated over 600 arguments for 3 debatable topics. To facilitate research in related downstream tasks, we release our guidelines and preliminary dataset.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comparative%20Study%20on%20Neural%20Architectures%20and%20Training%20Methods%20for%20Japanese%20Speech%20Recognition                                                                                  A Comparative Study on Neural Architectures and Training Methods for Japanese Speech Recognition                                                                                  End-to-end (E2E) modeling is advantageous for automatic speech recognition (ASR) especially for Japanese since word-based tokenization of Japanese is not trivial, and E2E modeling is able to model character sequences directly. This paper focuses on the latest E2E modeling techniques, and investigates their performances on character-based Japanese ASR by conducting comparative experiments. The results are analyzed and discussed in order to understand the relative advantages of long short-term memory (LSTM), and Conformer models in combination with connectionist temporal classification, transducer, and attention-based loss functions. Furthermore, the paper investigates on effectivity of the recent training techniques such as data augmentation (SpecAugment), variational noise injection, and exponential moving average. The best configuration found in the paper achieved the state-of-the-art character error rates of 4.1%, 3.2%, and 3.5% for Corpus of Spontaneous Japanese (CSJ) eval1, eval2, and eval3 tasks, respectively. The system is also shown to be computationally efficient thanks to the efficiency of Conformer transducers.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comparative%20Study%20on%20Schema-Guided%20Dialogue%20State%20Tracking                                                                                  A Comparative Study on Schema-Guided Dialogue State Tracking                                                                                  Frame-based state representation is widely used in modern task-oriented dialog systems to model user intentions and slot values. However, a fixed design of domain ontology makes it difficult to extend to new services and APIs. Recent work proposed to use natural language descriptions to define the domain ontology instead of tag names for each intent or slot, thus offering a dynamic set of schema. In this paper, we conduct in-depth comparative studies to understand the use of natural language description for schema in dialog state tracking. Our discussion mainly covers three aspects: encoder architectures, impact of supplementary training, and effective schema description styles. We introduce a set of newly designed bench-marking descriptions and reveal the model robustness on both homogeneous and heterogeneous description styles in training and evaluation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comparison%20for%20Anti-noise%20Robustness%20of%20Deep%20Learning%20Classification%20Methods%20on%20a%20Tiny%20Object%20Image%20Dataset%3A%20from%20Convolutional%20Neural%20Network%20to%20Visual%20Transformer%20and%20Performer                                                                                  A Comparison for Anti-noise Robustness of Deep Learning Classification Methods on a Tiny Object Image Dataset: from Convolutional Neural Network to Visual Transformer and Performer                                                                                  Image classification has achieved unprecedented advance with the the rapid development of deep learning. However, the classification of tiny object images is still not well investigated. In this paper, we first briefly review the development of Convolutional Neural Network and Visual Transformer in deep learning, and introduce the sources and development of conventional noises and adversarial attacks. Then we use various models of Convolutional Neural Network and Visual Transformer to conduct a series of experiments on the image dataset of tiny objects (sperms and impurities), and compare various evaluation metrics in the experimental results to obtain a model with stable performance. Finally, we discuss the problems in the classification of tiny objects and make a prospect for the classification of tiny objects in the future.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comparison%20for%20Patch-level%20Classification%20of%20Deep%20Learning%20Methods%20on%20Transparent%20Environmental%20Microorganism%20Images%3A%20from%20Convolutional%20Neural%20Networks%20to%20Visual%20Transformers                                                                                  A Comparison for Patch-level Classification of Deep Learning Methods on Transparent Environmental Microorganism Images: from Convolutional Neural Networks to Visual Transformers                                                                                  Nowadays, analysis of Transparent Environmental Microorganism Images (T-EM images) in the field of computer vision has gradually become a new and interesting spot. This paper compares different deep learning classification performance for the problem that T-EM images are challenging to analyze. We crop the T-EM images into 8 * 8 and 224 * 224 pixel patches in the same proportion and then divide the two different pixel patches into foreground and background according to ground truth. We also use four convolutional neural networks and a novel ViT network model to compare the foreground and background classification experiments. We conclude that ViT performs the worst in classifying 8 * 8 pixel patches, but it outperforms most convolutional neural networks in classifying 224 * 224 pixel patches.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comparison%20of%20Indonesia%20E-Commerce%20Sentiment%20Analysis%20for%20Marketing%20Intelligence%20Effort                                                                                  A Comparison of Indonesia E-Commerce Sentiment Analysis for Marketing Intelligence Effort                                                                                  The rapid growth of the e-commerce market in Indonesia, making various e-commerce companies appear and there has been high competition among them. Marketing intelligence is an important activity to measure competitive position. One element of marketing intelligence is to assess customer satisfaction. Many Indonesian customers express their sense of satisfaction or dissatisfaction towards the company through social media. Hence, using social media data provides a new practical way to measure marketing intelligence effort. This research performs sentiment analysis using the naive bayes classifier classification method with TF-IDF weighting. We compare the sentiments towards of top-3 e-commerce sites visited companies, are Bukalapak, Tokopedia, and Elevenia. We use Twitter data for sentiment analysis because it's faster, cheaper, and easier from both the customer and the researcher side. The purpose of this research is to find out how to process the huge customer sentiment Twitter to become useful information for the e-commerce company, and which of those top-3 e-commerce companies has the highest level of customer satisfaction. The experiment results show the method can be used to classify customer sentiments in social media Twitter automatically and Elevenia is the highest e-commerce with customer satisfaction.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comparison%20of%20Multi-View%20Learning%20Strategies%20for%20Satellite%20Image-Based%20Real%20Estate%20Appraisal                                                                                  A Comparison of Multi-View Learning Strategies for Satellite Image-Based Real Estate Appraisal                                                                                  In the house credit process, banks and lenders rely on a fast and accurate estimation of a real estate price to determine the maximum loan value. Real estate appraisal is often based on relational data, capturing the hard facts of the property. Yet, models benefit strongly from including image data, capturing additional soft factors. The combination of the different data types requires a multi-view learning method. Therefore, the question arises which strengths and weaknesses different multi-view learning strategies have. In our study, we test multi-kernel learning, multi-view concatenation and multi-view neural networks on real estate data and satellite images from Asheville, NC. Our results suggest that multi-view learning increases the predictive performance up to 13% in MAE. Multi-view neural networks perform best, however result in intransparent black-box models. For users seeking interpretability, hybrid multi-view neural networks or a boosting strategy are a suitable alternative.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comparison%20of%20Reward%20Functions%20in%20Q-Learning%20Applied%20to%20a%20Cart%20Position%20Problem                                                                                  A Comparison of Reward Functions in Q-Learning Applied to a Cart Position Problem                                                                                  Growing advancements in reinforcement learning has led to advancements in control theory. Reinforcement learning has effectively solved the inverted pendulum problem and more recently the double inverted pendulum problem. In reinforcement learning, our agents learn by interacting with the control system with the goal of maximizing rewards. In this paper, we explore three such reward functions in the cart position problem. This paper concludes that a discontinuous reward function that gives non-zero rewards to agents only if they are within a given distance from the desired position gives the best results.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comparison%20of%20Similarity%20Based%20Instance%20Selection%20Methods%20for%20Cross%20Project%20Defect%20Prediction                                                                                  A Comparison of Similarity Based Instance Selection Methods for Cross Project Defect Prediction                                                                                  Context: Previous studies have shown that training data instance selection based on nearest neighborhood (NN) information can lead to better performance in cross project defect prediction (CPDP) by reducing heterogeneity in training datasets. However, neighborhood calculation is computationally expensive and approximate methods such as Locality Sensitive Hashing (LSH) can be as effective as exact methods. Aim: We aim at comparing instance selection methods for CPDP, namely LSH, NN-filter, and Genetic Instance Selection (GIS). Method: We conduct experiments with five base learners, optimizing their hyper parameters, on 13 datasets from PROMISE repository in order to compare the performance of LSH with benchmark instance selection methods NN-Filter and GIS. Results: The statistical tests show six distinct groups for F-measure performance. The top two group contains only LSH and GIS benchmarks whereas the bottom two groups contain only NN-Filter variants. LSH and GIS favor recall more than precision. In fact, for precision performance only three significantly distinct groups are detected by the tests where the top group is comprised of NN-Filter variants only. Recall wise, 16 different groups are identified where the top three groups contain only LSH methods, four of the next six are GIS only and the bottom five contain only NN-Filter. Finally, NN-Filter benchmarks never outperform the LSH counterparts with the same base learner, tuned or non-tuned. Further, they never even belong to the same rank group, meaning that LSH is always significantly better than NN-Filter with the same learner and settings. Conclusions: The increase in performance and the decrease in computational overhead and runtime make LSH a promising approach. However, the performance of LSH is based on high recall and in environments where precision is considered more important NN-Filter should be considered.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comparison%20of%20the%20Delta%20Method%20and%20the%20Bootstrap%20in%20Deep%20Learning%20Classification                                                                                  A Comparison of the Delta Method and the Bootstrap in Deep Learning Classification                                                                                  We validate the recently introduced deep learning classification adapted Delta method by a comparison with the classical Bootstrap. We show that there is a strong linear relationship between the quantified predictive epistemic uncertainty levels obtained from the two methods when applied on two LeNet-based neural network classifiers using the MNIST and CIFAR-10 datasets. Furthermore, we demonstrate that the Delta method offers a five times computation time reduction compared to the Bootstrap.
http://w3id.org/mlsea/pwc/scientificWork/A%20Competitive%20Analysis%20of%20Online%20Multi-Agent%20Path%20Finding                                                                                  A Competitive Analysis of Online Multi-Agent Path Finding                                                                                  We study online Multi-Agent Path Finding (MAPF), where new agents are constantly revealed over time and all agents must find collision-free paths to their given goal locations. We generalize existing complexity results of (offline) MAPF to online MAPF. We classify online MAPF algorithms into different categories based on (1) controllability (the set of agents that they can plan paths for at each time) and (2) rationality (the quality of paths they plan) and study the relationships between them. We perform a competitive analysis for each category of online MAPF algorithms with respect to commonly-used objective functions. We show that a naive algorithm that routes newly-revealed agents one at a time in sequence achieves a competitive ratio that is asymptotically bounded from both below and above by the number of agents with respect to flowtime and makespan. We then show a counter-intuitive result that, if rerouting of previously-revealed agents is not allowed, any rational online MAPF algorithms, including ones that plan optimal paths for all newly-revealed agents, have the same asymptotic competitive ratio as the naive algorithm, even on 2D 4-neighbor grids. We also derive constant lower bounds on the competitive ratio of any rational online MAPF algorithms that allow rerouting. The results thus provide theoretical insights into the effectiveness of using MAPF algorithms in an online setting for the first time.
http://w3id.org/mlsea/pwc/scientificWork/A%20Competitive%20Method%20to%20VIPriors%20Object%20Detection%20Challenge                                                                                  A Competitive Method to VIPriors Object Detection Challenge                                                                                  In this report, we introduce the technical details of our submission to the VIPriors object detection challenge. Our solution is based on mmdetction of a strong baseline open-source detection toolbox. Firstly, we introduce an effective data augmentation method to address the lack of data problem, which contains bbox-jitter, grid-mask, and mix-up. Secondly, we present a robust region of interest (ROI) extraction method to learn more significant ROI features via embedding global context features. Thirdly, we propose a multi-model integration strategy to refinement the prediction box, which weighted boxes fusion (WBF). Experimental results demonstrate that our approach can significantly improve the average precision (AP) of object detection on the subset of the COCO2017 dataset.
http://w3id.org/mlsea/pwc/scientificWork/A%20Complementarity%20Analysis%20of%20the%20COCO%20Benchmark%20Problems%20and%20Artificially%20Generated%20Problems                                                                                  A Complementarity Analysis of the COCO Benchmark Problems and Artificially Generated Problems                                                                                  When designing a benchmark problem set, it is important to create a set of benchmark problems that are a good generalization of the set of all possible problems. One possible way of easing this difficult task is by using artificially generated problems. In this paper, one such single-objective continuous problem generation approach is analyzed and compared with the COCO benchmark problem set, a well know problem set for benchmarking numerical optimization algorithms. Using Exploratory Landscape Analysis and Singular Value Decomposition, we show that such representations allow us to further explore the relations between the problems by applying visualization and correlation analysis techniques, with the goal of decreasing the bias in benchmark problem assessment.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comprehensive%20Assessment%20of%20Dialog%20Evaluation%20Metrics                                                                                  A Comprehensive Assessment of Dialog Evaluation Metrics                                                                                  Automatic evaluation metrics are a crucial component of dialog systems research. Standard language evaluation metrics are known to be ineffective for evaluating dialog. As such, recent research has proposed a number of novel, dialog-specific metrics that correlate better with human judgements. Due to the fast pace of research, many of these metrics have been assessed on different datasets and there has as yet been no time for a systematic comparison between them. To this end, this paper provides a comprehensive assessment of recently proposed dialog evaluation metrics on a number of datasets. In this paper, 23 different automatic evaluation metrics are evaluated on 10 different datasets. Furthermore, the metrics are assessed in different settings, to better qualify their respective strengths and weaknesses. Metrics are assessed (1) on both the turn level and the dialog level, (2) for different dialog lengths, (3) for different dialog qualities (e.g., coherence, engaging), (4) for different types of response generation models (i.e., generative, retrieval, simple models and state-of-the-art models), (5) taking into account the similarity of different metrics and (6) exploring combinations of different metrics. This comprehensive assessment offers several takeaways pertaining to dialog evaluation metrics in general. It also suggests how to best assess evaluation metrics and indicates promising directions for future work.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comprehensive%20Attempt%20to%20Research%20Statement%20Generation                                                                                  A Comprehensive Attempt to Research Statement Generation                                                                                  For a researcher, writing a good research statement is crucial but costs a lot of time and effort. To help researchers, in this paper, we propose the research statement generation (RSG) task which aims to summarize one's research achievements and help prepare a formal research statement. For this task, we conduct a comprehensive attempt including corpus construction, method design, and performance evaluation. First, we construct an RSG dataset with 62 research statements and the corresponding 1,203 publications. Due to the limitation of our resources, we propose a practical RSG method which identifies a researcher's research directions by topic modeling and clustering techniques and extracts salient sentences by a neural text summarizer. Finally, experiments show that our method outperforms all the baselines with better content coverage and coherence.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comprehensive%20Investigation%20on%20Range-free%20Localization%20Algorithms%20with%20Mobile%20Anchors%20at%20Different%20Altitudes                                                                                  A Comprehensive Investigation on Range-free Localization Algorithms with Mobile Anchors at Different Altitudes                                                                                  In this work, the problem of localizing ground devices (GDs) is studied comparing the performance of four range-free (RF) localization algorithms that use a mobile anchor (MA). All the investigated algorithms are based on the so-called heard/not-heard (HnH) method, which allows the GDs to detect the MA at the border of their antenna communication radius. Despite the simplicity of this method, its efficacy in terms of accuracy is poor because it relies on the antenna radius that continuously varies under different conditions. Usually, the antenna radius declared by the manufacturer does not fully characterize the actual antenna radiation pattern. In this paper, the radiation pattern of the commercial DecaWave DWM1001 Ultra-Wide-Band (UWB) antennas is observed in a real test-bed at different altitudes for collecting more information and insights on the antenna radius. The compared algorithms are then tested using both the observed and the manufacturer radii. The experimental accuracy is close to the expected theoretical one only when the antenna pattern is actually omnidirectional. However, typical antennas have strong pattern irregularities that decrease the accuracy. For improving the performance, we propose range-based (RB) variants of the compared algorithms in which, instead of using the observed or the manufacturer radii, the actual measured distances between the MA and the GD are used. The localization accuracy tremendously improves confirming that the knowledge of the exact antenna pattern is essential for any RF algorithm.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comprehensive%20Optimization%20Method%20for%20Commercial%20Building%20Loads%20with%20Renewable%20Generation%20and%20Energy%20Storage%20from%20Utility%20Rate%20Structure%20Perspective                                                                                  A Comprehensive Optimization Method for Commercial Building Loads with Renewable Generation and Energy Storage from Utility Rate Structure Perspective                                                                                  To accommodate the changes in the nature and pattern of electricity consumption with the available resources, utility companies have introduced a variety of rate structures over the years. This paper develops a comprehensive optimization method that addresses the diversity of utility rate structure of a commercial building. It includes a general set of constraints that can be used for any system with a building load, a renewable source, and a battery energy storage system (BESS). A cost function is formulated for each type of rate structure that can be exercised by a utility on a commercial building. A novel algorithm is developed to apply the optimization model and generate the desired optimal outputs by using the appropriate cost function. The results for several building loads and rate structure types were obtained and compared. A sensitivity analysis was done on the optimization model based on the changes in the rates using historical data. The results exhibit that adding BESS is more effective for buildings with lower load factor and CPP rate structures in comparison to the buildings with flat energy rates. Savings from adding renewables such as solar is primarily influenced by energy charges whereas additional benefits from BESS are dominated by demand charges. These results can help a customer with deciding on the different rate structure options and resource planning of their renewable generation and energy storage. Utilities may also benefit from this work by designing a unified rate structure, considering the increasing renewable penetration and BESS deployment in the grid.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comprehensive%20Review%20on%20Non-Neural%20Networks%20Collaborative%20Filtering%20Recommendation%20Systems                                                                                  A Comprehensive Review on Non-Neural Networks Collaborative Filtering Recommendation Systems                                                                                  Over the past two decades, recommender systems have attracted a lot of interest due to the explosion in the amount of data in online applications. A particular attention has been paid to collaborative filtering, which is the most widely used in applications that involve information recommendations. Collaborative filtering (CF) uses the known preference of a group of users to make predictions and recommendations about the unknown preferences of other users (recommendations are made based on the past behavior of users). First introduced in the 1990s, a wide variety of increasingly successful models have been proposed. Due to the success of machine learning techniques in many areas, there has been a growing emphasis on the application of such algorithms in recommendation systems. In this article, we present an overview of the CF approaches for recommender systems, their two main categories, and their evaluation metrics. We focus on the application of classical Machine Learning algorithms to CF recommender systems by presenting their evolution from their first use-cases to advanced Machine Learning models. We attempt to provide a comprehensive and comparative overview of CF systems (with python implementations) that can serve as a guideline for research and practice in this area.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comprehensive%20Survey%20and%20Taxonomy%20on%20Single%20Image%20Dehazing%20Based%20on%20Deep%20Learning                                                                                  A Comprehensive Survey and Taxonomy on Single Image Dehazing Based on Deep Learning                                                                                  With the development of convolutional neural networks, hundreds of deep learning based dehazing methods have been proposed. In this paper, we provide a comprehensive survey on supervised, semi-supervised, and unsupervised single image dehazing. We first discuss the physical model, datasets, network modules, loss functions, and evaluation metrics that are commonly used. Then, the main contributions of various dehazing algorithms are categorized and summarized. Further, quantitative and qualitative experiments of various baseline methods are carried out. Finally, the unsolved issues and challenges that can inspire the future research are pointed out. A collection of useful dehazing materials is available at url{https://github.com/Xiaofeng-life/AwesomeDehazing}.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comprehensive%20Survey%20of%206G%20Wireless%20Communications                                                                                  A Comprehensive Survey of 6G Wireless Communications                                                                                  While fifth-generation (5G) communications are being rolled out worldwide, sixth-generation (6G) communications have attracted much attention from both the industry and the academia. Compared with 5G, 6G will have a wider frequency band, higher transmission rate, spectrum efficiency, greater connection capacity, shorter delay, broader coverage, and more robust anti-interference capability to satisfy various network requirements. This survey presents an insightful understanding of 6G wireless communications by introducing requirements, features, critical technologies, challenges, and applications. First, we give an overview of 6G from perspectives of technologies, security and privacy, and applications. Subsequently, we introduce various 6G technologies and their existing challenges in detail, e.g., artificial intelligence (AI), intelligent surfaces, THz, space-air-ground-sea integrated network, cell-free massive MIMO, etc. Because of these technologies, 6G is expected to outperform existing wireless communication systems regarding the transmission rate, latency, global coverage, etc. Next, we discuss security and privacy techniques that can be applied to protect data in 6G. Since edge devices are expected to gain popularity soon, the vast amount of generated data and frequent data exchange make the leakage of data easily. Finally, we predict real-world applications built on the technologies and features of 6G; for example, smart healthcare, smart city, and smart manufacturing will be implemented by taking advantage of AI.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comprehensive%20Survey%20of%20Image-Based%20Food%20Recognition%20and%20Volume%20Estimation%20Methods%20for%20Dietary%20Assessment                                                                                  A Comprehensive Survey of Image-Based Food Recognition and Volume Estimation Methods for Dietary Assessment                                                                                  Dietary studies showed that dietary-related problem such as obesity is associated with other chronic diseases like hypertension, irregular blood sugar levels, and increased risk of heart attacks. The primary cause of these problems is poor lifestyle choices and unhealthy dietary habits, which are manageable using interactive mHealth apps. However, traditional dietary monitoring systems using manual food logging suffer from imprecision, underreporting, time consumption, and low adherence. Recent dietary monitoring systems tackle these challenges by automatic assessment of dietary intake through machine learning methods. This survey discusses the most performing methodologies that have been developed so far for automatic food recognition and volume estimation. First, we will present the rationale of visual-based methods for food recognition. The core of the paper is the presentation, discussion and evaluation of these methods on popular food image databases. Following that, we discussed the mobile applications that are implementing these methods. The survey ends with a discussion of research gaps and open issues in this area.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comprehensive%20Survey%20of%20Incentive%20Mechanism%20for%20Federated%20Learning                                                                                  A Comprehensive Survey of Incentive Mechanism for Federated Learning                                                                                  Federated learning utilizes various resources provided by participants to collaboratively train a global model, which potentially address the data privacy issue of machine learning. In such promising paradigm, the performance will be deteriorated without sufficient training data and other resources in the learning process. Thus, it is quite crucial to inspire more participants to contribute their valuable resources with some payments for federated learning. In this paper, we present a comprehensive survey of incentive schemes for federate learning. Specifically, we identify the incentive problem in federated learning and then provide a taxonomy for various schemes. Subsequently, we summarize the existing incentive mechanisms in terms of the main techniques, such as Stackelberg game, auction, contract theory, Shapley value, reinforcement learning, blockchain. By reviewing and comparing some impressive results, we figure out three directions for the future study.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comprehensive%20Survey%20of%20Scene%20Graphs%3A%20Generation%20and%20Application                                                                                  A Comprehensive Survey of Scene Graphs: Generation and Application                                                                                  Scene graph is a structured representation of a scene that can clearly express the objects, attributes, and relationships between objects in the scene. As computer vision technology continues to develop, people are no longer satisfied with simply detecting and recognizing objects in images; instead, people look forward to a higher level of understanding and reasoning about visual scenes. For example, given an image, we want to not only detect and recognize objects in the image, but also know the relationship between objects (visual relationship detection), and generate a text description (image captioning) based on the image content. Alternatively, we might want the machine to tell us what the little girl in the image is doing (Visual Question Answering (VQA)), or even remove the dog from the image and find similar images (image editing and retrieval), etc. These tasks require a higher level of understanding and reasoning for image vision tasks. The scene graph is just such a powerful tool for scene understanding. Therefore, scene graphs have attracted the attention of a large number of researchers, and related research is often cross-modal, complex, and rapidly developing. However, no relatively systematic survey of scene graphs exists at present. To this end, this survey conducts a comprehensive investigation of the current scene graph research. More specifically, we first summarized the general definition of the scene graph, then conducted a comprehensive and systematic discussion on the generation method of the scene graph (SGG) and the SGG with the aid of prior knowledge. We then investigated the main applications of scene graphs and summarized the most commonly used datasets. Finally, we provide some insights into the future development of scene graphs. We believe this will be a very helpful foundation for future research on scene graphs.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comprehensive%20Survey%20on%20Community%20Detection%20with%20Deep%20Learning                                                                                  A Comprehensive Survey on Community Detection with Deep Learning                                                                                  A community reveals the features and connections of its members that are different from those in other communities in a network. Detecting communities is of great significance in network analysis. Despite the classical spectral clustering and statistical inference methods, we notice a significant development of deep learning techniques for community detection in recent years with their advantages in handling high dimensional network data. Hence, a comprehensive overview of community detection's latest progress through deep learning is timely to academics and practitioners. This survey devises and proposes a new taxonomy covering different state-of-the-art methods, including deep learning-based models upon deep neural networks, deep nonnegative matrix factorization and deep sparse filtering. The main category, i.e., deep neural networks, is further divided into convolutional networks, graph attention networks, generative adversarial networks and autoencoders. The survey also summarizes the popular benchmark data sets, evaluation metrics, and open-source implementations to address experimentation settings. We then discuss the practical applications of community detection in various domains and point to implementation scenarios. Finally, we outline future directions by suggesting challenging topics in this fast-growing deep learning field.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comprehensive%20Survey%20on%20Graph%20Anomaly%20Detection%20with%20Deep%20Learning                                                                                  A Comprehensive Survey on Graph Anomaly Detection with Deep Learning                                                                                  Anomalies represent rare observations (e.g., data records or events) that deviate significantly from others. Over several decades, research on anomaly mining has received increasing interests due to the implications of these occurrences in a wide range of disciplines. Anomaly detection, which aims to identify rare observations, is among the most vital tasks in the world, and has shown its power in preventing detrimental events, such as financial fraud, network intrusion, and social spam. The detection task is typically solved by identifying outlying data points in the feature space and inherently overlooks the relational information in real-world data. Graphs have been prevalently used to represent the structural information, which raises the graph anomaly detection problem - identifying anomalous graph objects (i.e., nodes, edges and sub-graphs) in a single graph, or anomalous graphs in a database/set of graphs. However, conventional anomaly detection techniques cannot tackle this problem well because of the complexity of graph data. For the advent of deep learning, graph anomaly detection with deep learning has received a growing attention recently. In this survey, we aim to provide a systematic and comprehensive review of the contemporary deep learning techniques for graph anomaly detection. We compile open-sourced implementations, public datasets, and commonly-used evaluation metrics to provide affluent resources for future studies. More importantly, we highlight twelve extensive future research directions according to our survey results covering unsolved and emerging research problems and real-world applications. With this survey, our goal is to create a 'one-stop-shop' that provides a unified understanding of the problem categories and existing approaches, publicly available hands-on resources, and high-impact open challenges for graph anomaly detection using deep learning.
http://w3id.org/mlsea/pwc/scientificWork/A%20Comprehensive%20Taxonomy%20for%20Explainable%20Artificial%20Intelligence%3A%20A%20Systematic%20Survey%20of%20Surveys%20on%20Methods%20and%20Concepts                                                                                  A Comprehensive Taxonomy for Explainable Artificial Intelligence: A Systematic Survey of Surveys on Methods and Concepts                                                                                  In the meantime, a wide variety of terminologies, motivations, approaches, and evaluation criteria have been developed within the research field of explainable artificial intelligence (XAI). With the amount of XAI methods vastly growing, a taxonomy of methods is needed by researchers as well as practitioners: To grasp the breadth of the topic, compare methods, and to select the right XAI method based on traits required by a specific use-case context. Many taxonomies for XAI methods of varying level of detail and depth can be found in the literature. While they often have a different focus, they also exhibit many points of overlap. This paper unifies these efforts and provides a complete taxonomy of XAI methods with respect to notions present in the current state of research. In a structured literature analysis and meta-study, we identified and reviewed more than 50 of the most cited and current surveys on XAI methods, metrics, and method traits. After summarizing them in a survey of surveys, we merge terminologies and concepts of the articles into a unified structured taxonomy. Single concepts therein are illustrated by more than 50 diverse selected example methods in total, which we categorize accordingly. The taxonomy may serve both beginners, researchers, and practitioners as a reference and wide-ranging overview of XAI method traits and aspects. Hence, it provides foundations for targeted, use-case-oriented, and context-sensitive future research.
http://w3id.org/mlsea/pwc/scientificWork/A%20Compression-Compilation%20Framework%20for%20On-mobile%20Real-time%20BERT%20Applications                                                                                  A Compression-Compilation Framework for On-mobile Real-time BERT Applications                                                                                  Transformer-based deep learning models have increasingly demonstrated high accuracy on many natural language processing (NLP) tasks. In this paper, we propose a compression-compilation co-design framework that can guarantee the identified model to meet both resource and real-time specifications of mobile devices. Our framework applies a compiler-aware neural architecture optimization method (CANAO), which can generate the optimal compressed model that balances both accuracy and latency. We are able to achieve up to 7.8x speedup compared with TensorFlow-Lite with only minor accuracy loss. We present two types of BERT applications on mobile devices: Question Answering (QA) and Text Generation. Both can be executed in real-time with latency as low as 45ms. Videos for demonstrating the framework can be found on https://www.youtube.com/watch?v=_WIRvK_2PZI
http://w3id.org/mlsea/pwc/scientificWork/A%20Computational%20Framework%20for%20Modeling%20Complex%20Sensor%20Network%20Data%20Using%20Graph%20Signal%20Processing%20and%20Graph%20Neural%20Networks%20in%20Structural%20Health%20Monitoring                                                                                  A Computational Framework for Modeling Complex Sensor Network Data Using Graph Signal Processing and Graph Neural Networks in Structural Health Monitoring                                                                                  Complex networks lend themselves to the modeling of multidimensional data, such as relational and/or temporal data. In particular, when such complex data and their inherent relationships need to be formalized, complex network modeling and its resulting graph representations enable a wide range of powerful options. In this paper, we target this - connected to specific machine learning approaches on graphs for structural health monitoring on an analysis and predictive (maintenance) perspective. Specifically, we present a framework based on Complex Network Modeling, integrating Graph Signal Processing (GSP) and Graph Neural Network (GNN) approaches. We demonstrate this framework in our targeted application domain of Structural Health Monitoring (SHM). In particular, we focus on a prominent real-world structural health monitoring use case, i.e., modeling and analyzing sensor data (strain, vibration) of a large bridge in the Netherlands. In our experiments, we show that GSP enables the identification of the most important sensors, for which we investigate a set of search and optimization approaches. Furthermore, GSP enables the detection of specific graph signal patterns (mode shapes), capturing physical functional properties of the sensors in the applied complex network. In addition, we show the efficacy of applying GNNs for strain prediction on this kind of data.
http://w3id.org/mlsea/pwc/scientificWork/A%20Computational%20Model%20of%20Representation%20Learning%20in%20the%20Brain%20Cortex%2C%20Integrating%20Unsupervised%20and%20Reinforcement%20Learning                                                                                  A Computational Model of Representation Learning in the Brain Cortex, Integrating Unsupervised and Reinforcement Learning                                                                                  A common view on the brain learning processes proposes that the three classic learning paradigms -- unsupervised, reinforcement, and supervised -- take place in respectively the cortex, the basal-ganglia, and the cerebellum. However, dopamine outbursts, usually assumed to encode reward, are not limited to the basal ganglia but also reach prefrontal, motor, and higher sensory cortices. We propose that in the cortex the same reward-based trial-and-error processes might support not only the acquisition of motor representations but also of sensory representations. In particular, reward signals might guide trial-and-error processes that mix with associative learning processes to support the acquisition of representations better serving downstream action selection. We tested the soundness of this hypothesis with a computational model that integrates unsupervised learning (Contrastive Divergence) and reinforcement learning (REINFORCE). The model was tested with a task requiring different responses to different visual images grouped in categories involving either colour, shape, or size. Results show that a balanced mix of unsupervised and reinforcement learning processes leads to the best performance. Indeed, excessive unsupervised learning tends to under-represent task-relevant features while excessive reinforcement learning tends to initially learn slowly and then to incur in local minima. These results stimulate future empirical studies on category learning directed to investigate similar effects in the extrastriate visual cortices. Moreover, they prompt further computational investigations directed to study the possible advantages of integrating unsupervised and reinforcement learning processes.
http://w3id.org/mlsea/pwc/scientificWork/A%20Computational%20Model%20of%20the%20Institutional%20Analysis%20and%20Development%20Framework                                                                                  A Computational Model of the Institutional Analysis and Development Framework                                                                                  The Institutional Analysis and Development (IAD) framework is a conceptual toolbox put forward by Elinor Ostrom and colleagues in an effort to identify and delineate the universal common variables that structure the immense variety of human interactions. The framework identifies rules as one of the core concepts to determine the structure of interactions, and acknowledges their potential to steer a community towards more beneficial and socially desirable outcomes. This work presents the first attempt to turn the IAD framework into a computational model to allow communities of agents to formally perform what-if analysis on a given rule configuration. To do so, we define the Action Situation Language -- or ASL -- whose syntax is hgighly tailored to the components of the IAD framework and that we use to write descriptions of social interactions. ASL is complemented by a game engine that generates its semantics as an extensive-form game. These models, then, can be analyzed with the standard tools of game theory to predict which outcomes are being most incentivized, and evaluated according to their socially relevant properties.
http://w3id.org/mlsea/pwc/scientificWork/A%20Computationally%20Efficient%202D%20MUSIC%20Approach%20for%205G%20and%206G%20Sensing%20Networks                                                                                  A Computationally Efficient 2D MUSIC Approach for 5G and 6G Sensing Networks                                                                                  Future cellular networks are intended to have the ability to sense the environment by utilizing reflections of transmitted signals. Multi-dimensional sensing brings along the crucial advantage of being able to resort to multiple domains to resolve targets, enhancing detection capabilities compared to 1D estimation. However, estimating parameters jointly in 5G New Radio (NR) systems poses the challenge of limiting the computational complexity while preserving a high resolution. To that end, we make us of channel state information (CSI) decimation for MUltiple SIgnal Classification (MUSIC)-based joint range-angle of arrival (AoA) estimation. We further introduce multi-peak search routines to achieve additional detection capability improvements. Simulation results with orthogonal frequency-division multiplexing (OFDM) signals show that we attain higher detection probabilities for closely spaced targets than with 1D range-only estimation. Moreover, we demonstrate that for our considered 5G setup, we are able to significantly reduce the required number of computations due to CSI decimation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Computationally%20Efficient%20Approach%20to%20Non-cooperative%20Target%20Detection%20and%20Tracking%20with%20Almost%20No%20A-priori%20Information                                                                                  A Computationally Efficient Approach to Non-cooperative Target Detection and Tracking with Almost No A-priori Information                                                                                  This paper addresses the problem of real-time detection and tracking of a non-cooperative target in the challenging scenario with almost no a-priori information about target birth, death, dynamics and detection probability. Furthermore, there are false and missing data at unknown yet low rates in the measurements. The only information given in advance is about the target-measurement model and the constraint that there is no more than one target in the scenario. To solve these challenges, we model the movement of the target by using a trajectory function of time (T-FoT). Data-driven T-FoT initiation and termination strategies are proposed for identifying the (re-)appearance and disappearance of the target. During the existence of the target, real target measurements are distinguished from clutter if the target indeed exists and is detected, in order to update the T-FoT at each scan for which we design a least-squares estimator. Simulations using either linear or nonlinear systems are conducted to demonstrate the effectiveness of our approach in comparison with the Bayes optimal Bernoulli filters. The results show that our approach is comparable to the perfectly-modeled filters, even outperforms them in some cases while requiring much less a-prior information and computing much faster.
http://w3id.org/mlsea/pwc/scientificWork/A%20Computationally%20Efficient%20Hamilton-Jacobi-based%20Formula%20for%20State-Constrained%20Optimal%20Control%20Problems                                                                                  A Computationally Efficient Hamilton-Jacobi-based Formula for State-Constrained Optimal Control Problems                                                                                  This paper investigates a Hamilton-Jacobi (HJ) analysis to solve finite-horizon optimal control problems for high-dimensional systems. Although grid-based methods, such as the level-set method [1], numerically solve a general class of HJ partial differential equations, the computational complexity is exponential in the dimension of the continuous state. To manage this computational complexity, methods based on Lax-Hopf theory have been developed for the state-unconstrained optimal control problem under certain assumptions, such as affine dynamics and state-independent stage cost. Based on the Lax formula [2], this paper proposes an HJ formula for the state-constrained optimal control problem for nonlinear systems. We call this formula textit{the generalized Lax formula} for the optimal control problem. The HJ formula provides both the optimal cost and an optimal control signal. We also provide an efficient computational method for a class of problems for which the dynamics is affine in the state, and for which the stage and terminal cost, as well as the state constraints, are convex in the state. This class of problems does not require affine dynamics and convex stage cost in the control. This paper also provides three practical examples.
http://w3id.org/mlsea/pwc/scientificWork/A%20Concavification%20Approach%20to%20Ambiguous%20Persuasion                                                                                  A Concavification Approach to Ambiguous Persuasion                                                                                  This note shows that the value of ambiguous persuasion characterized in Beauchene, Li and Li(2019) can be given by a concavification program as in Bayesian persuasion (Kamenica and Gentzkow, 2011). In addition, it implies that an ambiguous persuasion game can be equivalently formalized as a Bayesian persuasion game by distorting the utility functions. This result is obtained under a novel construction of ambiguous persuasion.
http://w3id.org/mlsea/pwc/scientificWork/A%20Conceptual%20Framework%20for%20Establishing%20Trust%20in%20Real%20World%20Intelligent%20Systems                                                                                  A Conceptual Framework for Establishing Trust in Real World Intelligent Systems                                                                                  Intelligent information systems that contain emergent elements often encounter trust problems because results do not get sufficiently explained and the procedure itself can not be fully retraced. This is caused by a control flow depending either on stochastic elements or on the structure and relevance of the input data. Trust in such algorithms can be established by letting users interact with the system so that they can explore results and find patterns that can be compared with their expected solution. Reflecting features and patterns of human understanding of a domain against algorithmic results can create awareness of such patterns and may increase the trust that a user has in the solution. If expectations are not met, close inspection can be used to decide whether a solution conforms to the expectations or whether it goes beyond the expected. By either accepting or rejecting a solution, the user's set of expectations evolves and a learning process for the users is established. In this paper we present a conceptual framework that reflects and supports this process. The framework is the result of an analysis of two exemplary case studies from two different disciplines with information systems that assist experts in their complex tasks.
http://w3id.org/mlsea/pwc/scientificWork/A%20Conceptual%20Framework%20for%20Implicit%20Evaluation%20of%20Conversational%20Search%20Interfaces                                                                                  A Conceptual Framework for Implicit Evaluation of Conversational Search Interfaces                                                                                  Conversational search (CS) has recently become a significant focus of the information retrieval (IR) research community. Multiple studies have been conducted which explore the concept of conversational search. Understanding and advancing research in CS requires careful and detailed evaluation. Existing CS studies have been limited to evaluation based on simple user feedback on task completion. We propose a CS evaluation framework which includes multiple dimensions: search experience, knowledge gain, software usability, cognitive load and user experience, based on studies of conversational systems and IR. We introduce these evaluation criteria and propose their use in a framework for the evaluation of CS systems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Concise%20Guide%20on%20the%20Integration%20of%20Battery%20Electric%20Buses%20into%20Urban%20Bus%20Networks                                                                                  A Concise Guide on the Integration of Battery Electric Buses into Urban Bus Networks                                                                                  With the increasing market penetration of battery-electric buses into urban bus networks, practitioners face many novel planning problems. As a result, the interest in optimization-based decision-making for these planning problems increases but practitioners' requirements on planning solutions and current academic approaches often diverge. Against this background, this survey aims to provide a concise guide on optimization-based planning approaches for integrating battery-electric buses into urban bus networks for both practitioners and academics. First, we derive practitioners' requirements for integrating battery-electric buses from state-of-the-art specifications, project reports, and expert knowledge. Second, we analyze whether existing optimization-based planning models fulfill these practitioners' requirements. Based on this analysis, we carve out the existing gap between practice and research and discuss how to address these in future research.
http://w3id.org/mlsea/pwc/scientificWork/A%20Concise%20Review%20of%20Transfer%20Learning                                                                                  A Concise Review of Transfer Learning                                                                                  The availability of abundant labeled data in recent years led the researchers to introduce a methodology called transfer learning, which utilizes existing data in situations where there are difficulties in collecting new annotated data. Transfer learning aims to boost the performance of a target learner by applying another related source data. In contrast to the traditional machine learning and data mining techniques, which assume that the training and testing data lie from the same feature space and distribution, transfer learning can handle situations where there is a discrepancy between domains and distributions. These characteristics give the model the potential to utilize the available related source data and extend the underlying knowledge to the target task achieving better performance. This survey paper aims to give a concise review of traditional and current transfer learning settings, existing challenges, and related approaches.
http://w3id.org/mlsea/pwc/scientificWork/A%20Conditional%20Splitting%20Framework%20for%20Efficient%20Constituency%20Parsing                                                                                  A Conditional Splitting Framework for Efficient Constituency Parsing                                                                                  We introduce a generic seq2seq parsing framework that casts constituency parsing problems (syntactic and discourse parsing) into a series of conditional splitting decisions. Our parsing model estimates the conditional probability distribution of possible splitting points in a given text span and supports efficient top-down decoding, which is linear in number of nodes. The conditional splitting formulation together with efficient beam search inference facilitate structural consistency without relying on expensive structured inference. Crucially, for discourse analysis we show that in our formulation, discourse segmentation can be framed as a special case of parsing which allows us to perform discourse parsing without requiring segmentation as a pre-requisite. Experiments show that our model achieves good results on the standard syntactic parsing tasks under settings with/without pre-trained representations and rivals state-of-the-art (SoTA) methods that are more computationally expensive than ours. In discourse parsing, our method outperforms SoTA by a good margin.
http://w3id.org/mlsea/pwc/scientificWork/A%20Conditional%20Value-at-Risk%20Based%20Planning%20Model%20for%20Integrated%20Energy%20System%20with%20Energy%20Storage%20and%20Renewables                                                                                  A Conditional Value-at-Risk Based Planning Model for Integrated Energy System with Energy Storage and Renewables                                                                                  Owing to the potential higher energy supply efficiency and operation flexibility, integrated energy system (IES), which usually includes electric power, gas and heating/cooling systems, is considered as one of the primary forms of energy carrier in the future. However, with the increasing complexity of multiple energy devices and systems integration, IES planning is facing a significant challenge in terms of risk assessment. To this end, an energy hub (EH) planning model considering renewable energy sources (RES) and energy storage system (ESS) integration is proposed in this paper, in which the risk is measured by Conditional Value-at-Risk (CVaR). The proposed IES planning model includes two stages: 1) investment planning on equipment types and capacity (e.g., energy converters, distributed RES and ESS) and 2) optimizing the potential risk loss in operation scenarios along with confidence level and risk preference. The problem solving is accelerated by Benders Decomposition and Improved Backward Scenario Reduction Method. The numerical results illustrate the effectiveness of proposed method in balancing the potential operation risk and investment cost. Moreover, the effectiveness of reducing potential operation risk by introducing ESS and RES are also verified.
http://w3id.org/mlsea/pwc/scientificWork/A%20Configurable%20BNN%20ASIC%20using%20a%20Network%20of%20Programmable%20Threshold%20Logic%20Standard%20Cells                                                                                  A Configurable BNN ASIC using a Network of Programmable Threshold Logic Standard Cells                                                                                  This paper presents TULIP, a new architecture for a binary neural network (BNN) that uses an optimal schedule for executing the operations of an arbitrary BNN. It was constructed with the goal of maximizing energy efficiency per classification. At the top-level, TULIP consists of a collection of unique processing elements (TULIP-PEs) that are organized in a SIMD fashion. Each TULIP-PE consists of a small network of binary neurons, and a small amount of local memory per neuron. The unique aspect of the binary neuron is that it is implemented as a mixed-signal circuit that natively performs the inner-product and thresholding operation of an artificial binary neuron. Moreover, the binary neuron, which is implemented as a single CMOS standard cell, is reconfigurable, and with a change in a single parameter, can implement all standard operations involved in a BNN. We present novel algorithms for mapping arbitrary nodes of a BNN onto the TULIP-PEs. TULIP was implemented as an ASIC in TSMC 40nm-LP technology. To provide a fair comparison, a recently reported BNN that employs a conventional MAC-based arithmetic processor was also implemented in the same technology. The results show that TULIP is consistently 3X more energy-efficient than the conventional design, without any penalty in performance, area, or accuracy.
http://w3id.org/mlsea/pwc/scientificWork/A%20Connected%20Component%20Labelling%20algorithm%20for%20multi-pixel%20per%20clock%20cycle%20video%20strea                                                                                  A Connected Component Labelling algorithm for multi-pixel per clock cycle video strea                                                                                  This work describes the hardware implementation of a connected component labelling (CCL) module in reprogammable logic. The main novelty of the design is the 'full', i.e. without any simplifications, support of a 4 pixel per clock format (4 ppc) and real-time processing of a 4K/UltraHD video stream (3840 x 2160 pixels) at 60 frames per second. To achieve this, a special labelling method was designed and a functionality that stops the input data stream in order to process pixel groups which require writing more than one merger into the equivalence table. The proposed module was verified in simulation and in hardware on the Xilinx Zynq Ultrascale+ MPSoC chip on the ZCU104 evaluation board.
http://w3id.org/mlsea/pwc/scientificWork/A%20Consciousness-Inspired%20Planning%20Agent%20for%20Model-Based%20Reinforcement%20Learning                                                                                  A Consciousness-Inspired Planning Agent for Model-Based Reinforcement Learning                                                                                  We present an end-to-end, model-based deep reinforcement learning agent which dynamically attends to relevant parts of its state during planning. The agent uses a bottleneck mechanism over a set-based representation to force the number of entities to which the agent attends at each planning step to be small. In experiments, we investigate the bottleneck mechanism with several sets of customized environments featuring different challenges. We consistently observe that the design allows the planning agents to generalize their learned task-solving abilities in compatible unseen environments by attending to the relevant objects, leading to better out-of-distribution generalization performance.
http://w3id.org/mlsea/pwc/scientificWork/A%20Consensual%20Collaborative%20Learning%20Method%20for%20Remote%20Sensing%20Image%20Classification%20Under%20Noisy%20Multi-Labels                                                                                  A Consensual Collaborative Learning Method for Remote Sensing Image Classification Under Noisy Multi-Labels                                                                                  Collecting a large number of reliable training images annotated by multiple land-cover class labels in the framework of multi-label classification is time-consuming and costly in remote sensing (RS). To address this problem, publicly available thematic products are often used for annotating RS images with zero-labeling-cost. However, such an approach may result in constructing a training set with noisy multi-labels, distorting the learning process. To address this problem, we propose a Consensual Collaborative Multi-Label Learning (CCML) method. The proposed CCML identifies, ranks and corrects training images with noisy multi-labels through four main modules: 1) discrepancy module; 2) group lasso module; 3) flipping module; and 4) swap module. The discrepancy module ensures that the two networks learn diverse features, while obtaining the same predictions. The group lasso module detects the potentially noisy labels by estimating the label uncertainty based on the aggregation of two collaborative networks. The flipping module corrects the identified noisy labels, whereas the swap module exchanges the ranking information between the two networks. The experimental results confirm the success of the proposed CCML under high (synthetically added) multi-label noise rates. The code of the proposed method is publicly available at https://noisy-labels-in-rs.org
http://w3id.org/mlsea/pwc/scientificWork/A%20Consistency-Based%20Loss%20for%20Deep%20Odometry%20Through%20Uncertainty%20Propagation                                                                                  A Consistency-Based Loss for Deep Odometry Through Uncertainty Propagation                                                                                  The incremental poses computed through odometry can be integrated over time to calculate the pose of a device with respect to an initial location. The resulting global pose may be used to formulate a second, consistency based, loss term in a deep odometry setting. In such cases where multiple losses are imposed on a network, the uncertainty over each output can be derived to weigh the different loss terms in a maximum likelihood setting. However, when imposing a constraint on the integrated transformation, due to how only odometry is estimated at each iteration of the algorithm, there is no information about the uncertainty associated with the global pose to weigh the global loss term. In this paper, we associate uncertainties with the output poses of a deep odometry network and propagate the uncertainties through each iteration. Our goal is to use the estimated covariance matrix at each incremental step to weigh the loss at the corresponding step while weighting the global loss term using the compounded uncertainty. This formulation provides an adaptive method to weigh the incremental and integrated loss terms against each other, noting the increase in uncertainty as new estimates arrive. We provide quantitative and qualitative analysis of pose estimates and show that our method surpasses the accuracy of the state-of-the-art Visual Odometry approaches. Then, uncertainty estimates are evaluated and comparisons against fixed baselines are provided. Finally, the uncertainty values are used in a realistic example to show the effectiveness of uncertainty quantification for localization.
http://w3id.org/mlsea/pwc/scientificWork/A%20Constrained%20Convex%20Optimization%20Approach%20to%20Hyperspectral%20Image%20Restoration%20with%20Hybrid%20Spatio-Spectral%20Regularization                                                                                  A Constrained Convex Optimization Approach to Hyperspectral Image Restoration with Hybrid Spatio-Spectral Regularization                                                                                  We propose a new constrained optimization approach to hyperspectral (HS) image restoration. Most existing methods restore a desirable HS image by solving some optimization problem, which consists of a regularization term(s) and a data-fidelity term(s). The methods have to handle a regularization term(s) and a data-fidelity term(s) simultaneously in one objective function, and so we need to carefully control the hyperparameter(s) that balances these terms. However, the setting of such hyperparameters is often a troublesome task because their suitable values depend strongly on the regularization terms adopted and the noise intensities on a given observation. Our proposed method is formulated as a convex optimization problem, where we utilize a novel hybrid regularization technique named Hybrid Spatio-Spectral Total Variation (HSSTV) and incorporate data-fidelity as hard constraints. HSSTV has a strong ability of noise and artifact removal while avoiding oversmoothing and spectral distortion, without combining other regularizations such as low-rank modeling-based ones. In addition, the constraint-type data-fidelity enables us to translate the hyperparameters that balance between regularization and data-fidelity to the upper bounds of the degree of data-fidelity that can be set in a much easier manner. We also develop an efficient algorithm based on the alternating direction method of multipliers (ADMM) to efficiently solve the optimization problem. Through comprehensive experiments, we illustrate the advantages of the proposed method over various HS image restoration methods including state-of-the-art ones.
http://w3id.org/mlsea/pwc/scientificWork/A%20Construction%20Kit%20for%20Efficient%20Low%20Power%20Neural%20Network%20Accelerator%20Designs                                                                                  A Construction Kit for Efficient Low Power Neural Network Accelerator Designs                                                                                  Implementing embedded neural network processing at the edge requires efficient hardware acceleration that couples high computational performance with low power consumption. Driven by the rapid evolution of network architectures and their algorithmic features, accelerator designs are constantly updated and improved. To evaluate and compare hardware design choices, designers can refer to a myriad of accelerator implementations in the literature. Surveys provide an overview of these works but are often limited to system-level and benchmark-specific performance metrics, making it difficult to quantitatively compare the individual effect of each utilized optimization technique. This complicates the evaluation of optimizations for new accelerator designs, slowing-down the research progress. This work provides a survey of neural network accelerator optimization approaches that have been used in recent works and reports their individual effects on edge processing performance. It presents the list of optimizations and their quantitative effects as a construction kit, allowing to assess the design choices for each building block separately. Reported optimizations range from up to 10'000x memory savings to 33x energy reductions, providing chip designers an overview of design choices for implementing efficient low power neural network accelerators.
http://w3id.org/mlsea/pwc/scientificWork/A%20Context-Dependent%20Gated%20Module%20for%20Incorporating%20Symbolic%20Semantics%20into%20Event%20Coreference%20Resolution                                                                                  A Context-Dependent Gated Module for Incorporating Symbolic Semantics into Event Coreference Resolution                                                                                  Event coreference resolution is an important research problem with many applications. Despite the recent remarkable success of pretrained language models, we argue that it is still highly beneficial to utilize symbolic features for the task. However, as the input for coreference resolution typically comes from upstream components in the information extraction pipeline, the automatically extracted symbolic features can be noisy and contain errors. Also, depending on the specific context, some features can be more informative than others. Motivated by these observations, we propose a novel context-dependent gated module to adaptively control the information flows from the input symbolic features. Combined with a simple noisy training method, our best models achieve state-of-the-art results on two datasets: ACE 2005 and KBP 2016.
http://w3id.org/mlsea/pwc/scientificWork/A%20Continuized%20View%20on%20Nesterov%20Acceleration%20for%20Stochastic%20Gradient%20Descent%20and%20Randomized%20Gossip                                                                                  A Continuized View on Nesterov Acceleration for Stochastic Gradient Descent and Randomized Gossip                                                                                  We introduce the continuized Nesterov acceleration, a close variant of Nesterov acceleration whose variables are indexed by a continuous time parameter. The two variables continuously mix following a linear ordinary differential equation and take gradient steps at random times. This continuized variant benefits from the best of the continuous and the discrete frameworks: as a continuous process, one can use differential calculus to analyze convergence and obtain analytical expressions for the parameters; and a discretization of the continuized process can be computed exactly with convergence rates similar to those of Nesterov original acceleration. We show that the discretization has the same structure as Nesterov acceleration, but with random parameters. We provide continuized Nesterov acceleration under deterministic as well as stochastic gradients, with either additive or multiplicative noise. Finally, using our continuized framework and expressing the gossip averaging problem as the stochastic minimization of a certain energy function, we provide the first rigorous acceleration of asynchronous gossip algorithms.
http://w3id.org/mlsea/pwc/scientificWork/A%20Contraction%20Theory%20Approach%20to%20Optimization%20Algorithms%20from%20Acceleration%20Flows                                                                                  A Contraction Theory Approach to Optimization Algorithms from Acceleration Flows                                                                                  Much recent interest has focused on the design of optimization algorithms from the discretization of an associated optimization flow, i.e., a system of differential equations (ODEs) whose trajectories solve an associated optimization problem. Such a design approach poses an important problem: how to find a principled methodology to design and discretize appropriate ODEs. This paper aims to provide a solution to this problem through the use of contraction theory. We first introduce general mathematical results that explain how contraction theory guarantees the stability of the implicit and explicit Euler integration methods. Then, we propose a novel system of ODEs, namely the Accelerated-Contracting-Nesterov flow, and use contraction theory to establish it is an optimization flow with exponential convergence rate, from which the linear convergence rate of its associated optimization algorithm is immediately established. Remarkably, a simple explicit Euler discretization of this flow corresponds to the Nesterov acceleration method. Finally, we present how our approach leads to performance guarantees in the design of optimization algorithms for time-varying optimization problems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Convergence%20Theory%20for%20SVGD%20in%20the%20Population%20Limit%20under%20Talagrand%27s%20Inequality%20T1                                                                                  A Convergence Theory for SVGD in the Population Limit under Talagrand's Inequality T1                                                                                  Stein Variational Gradient Descent (SVGD) is an algorithm for sampling from a target density which is known up to a multiplicative constant. Although SVGD is a popular algorithm in practice, its theoretical study is limited to a few recent works. We study the convergence of SVGD in the population limit, (i.e., with an infinite number of particles) to sample from a non-logconcave target distribution satisfying Talagrand's inequality T1. We first establish the convergence of the algorithm. Then, we establish a dimension-dependent complexity bound in terms of the Kernelized Stein Discrepancy (KSD). Unlike existing works, we do not assume that the KSD is bounded along the trajectory of the algorithm. Our approach relies on interpreting SVGD as a gradient descent over a space of probability measures.
http://w3id.org/mlsea/pwc/scientificWork/A%20Conversational%20Agent%20System%20for%20Dietary%20Supplements%20Use                                                                                  A Conversational Agent System for Dietary Supplements Use                                                                                  Dietary supplements (DS) have been widely used by consumers, but the information around the efficacy and safety of DS is disparate or incomplete, thus creating barriers for consumers to find information effectively. Conversational agent (CA) systems have been applied to the healthcare domain, but there is no such a system to answer consumers regarding DS use, although widespread use of DS. In this study, we develop the first CA system for DS use
http://w3id.org/mlsea/pwc/scientificWork/A%20Cooperative%20Game%20Theory-based%20Approach%20to%20Under-frequency%20Load%20Shedding%20Control                                                                                  A Cooperative Game Theory-based Approach to Under-frequency Load Shedding Control                                                                                  This paper proposes a cooperative game theory-based under-frequency load shedding (UFLS) approach for frequency stability and control in power systems. UFLS is a crucial factor for frequency stability and control especially in power grids with high penetration of renewable energy sources and restructured power systems. Conventional UFLS methods, most of which are off-line, usually shed fixed amounts of predetermined loads based on a predetermined schedule which can lead to over or under curtailment of load. This paper presents a co-operative game theory-based two-stage strategy to effectively and precisely determine locations and amounts of loads to be shed for UFLS control. In the first stage, the total amount of loads to be shed, also referred to as deficit in generation or the disturbance power, is computed using the initial rate of change of frequency (ROCOF) referred to the equivalent inertial center. In the second stage, the Shapley value, one of the solution concepts of cooperative game theory, is used to determine load shedding amounts and locations. The proposed method is implemented on the reduced 9-bus 3-machine Western Electricity Coordinating Council (WECC) system and simulated on Real-time Digital Simulators (RTDS). The results show that the proposed UFLS approach can effectively return the system to normal state after disturbances.
http://w3id.org/mlsea/pwc/scientificWork/A%20Cooperative-Competitive%20Multi-Agent%20Framework%20for%20Auto-bidding%20in%20Online%20Advertising                                                                                  A Cooperative-Competitive Multi-Agent Framework for Auto-bidding in Online Advertising                                                                                  In online advertising, auto-bidding has become an essential tool for advertisers to optimize their preferred ad performance metrics by simply expressing high-level campaign objectives and constraints. Previous works designed auto-bidding tools from the view of single-agent, without modeling the mutual influence between agents. In this paper, we instead consider this problem from a distributed multi-agent perspective, and propose a general $ underline{M}$ulti-$ underline{A}$gent reinforcement learning framework for $ underline{A}$uto-$ underline{B}$idding, namely MAAB, to learn the auto-bidding strategies. First, we investigate the competition and cooperation relation among auto-bidding agents, and propose a temperature-regularized credit assignment to establish a mixed cooperative-competitive paradigm. By carefully making a competition and cooperation trade-off among agents, we can reach an equilibrium state that guarantees not only individual advertiser's utility but also the system performance (i.e., social welfare). Second, to avoid the potential collusion behaviors of bidding low prices underlying the cooperation, we further propose bar agents to set a personalized bidding bar for each agent, and then alleviate the revenue degradation due to the cooperation. Third, to deploy MAAB in the large-scale advertising system with millions of advertisers, we propose a mean-field approach. By grouping advertisers with the same objective as a mean auto-bidding agent, the interactions among the large-scale advertisers are greatly simplified, making it practical to train MAAB efficiently. Extensive experiments on the offline industrial dataset and Alibaba advertising platform demonstrate that our approach outperforms several baseline methods in terms of social welfare and revenue.
http://w3id.org/mlsea/pwc/scientificWork/A%20Coupled%20Design%20of%20Exploiting%20Record%20Similarity%20for%20Practical%20Vertical%20Federated%20Learning                                                                                  A Coupled Design of Exploiting Record Similarity for Practical Vertical Federated Learning                                                                                  Federated learning is a learning paradigm to enable collaborative learning across different parties without revealing raw data. Notably, vertical federated learning (VFL), where parties share the same set of samples but only hold partial features, has a wide range of real-world applications. However, most existing studies in VFL disregard the 'record linkage' process. They design algorithms either assuming the data from different parties can be exactly linked or simply linking each record with its most similar neighboring record. These approaches may fail to capture the key features from other less similar records. Moreover, such improper linkage cannot be corrected by training since existing approaches provide no feedback on linkage during training. In this paper, we design a novel coupled training paradigm, FedSim, that integrates one-to-many linkage into the training process. Besides enabling VFL in many real-world applications with fuzzy identifiers, FedSim also achieves better performance in traditional VFL tasks. Moreover, we theoretically analyze the additional privacy risk incurred by sharing similarities. Our experiments on eight datasets with various similarity metrics show that FedSim outperforms other state-of-the-art baselines. The codes of FedSim are available at https://github.com/Xtra-Computing/FedSim.
http://w3id.org/mlsea/pwc/scientificWork/A%20Coupled%20Random%20Projection%20Approach%20to%20Large-Scale%20Canonical%20Polyadic%20Decomposition                                                                                  A Coupled Random Projection Approach to Large-Scale Canonical Polyadic Decomposition                                                                                  We propose a novel algorithm for the computation of canonical polyadic decomposition (CPD) of large-scale tensors. The proposed algorithm generalizes the random projection (RAP) technique, which is often used to compute large-scale decompositions, from one single projection to multiple but coupled random projections (CoRAP). The proposed CoRAP technique yields a set of tensors that together admits a coupled CPD (C-CPD) and a C-CPD algorithm is then used to jointly decompose these tensors. The results of C-CPD are finally fused to obtain factor matrices of the original large-scale data tensor. As more data samples are jointly exploited via C-CPD, the proposed CoRAP based CPD is more accurate than RAP based CPD. Experiments are provided to illustrate the performance of the proposed approach.
http://w3id.org/mlsea/pwc/scientificWork/A%20Crash%20Course%20on%20Reinforcement%20Learning                                                                                  A Crash Course on Reinforcement Learning                                                                                  The emerging field of Reinforcement Learning (RL) has led to impressive results in varied domains like strategy games, robotics, etc. This handout aims to give a simple introduction to RL from control perspective and discuss three possible approaches to solve an RL problem: Policy Gradient, Policy Iteration, and Model-building. Dynamical systems might have discrete action-space like cartpole where two possible actions are +1 and -1 or continuous action space like linear Gaussian systems. Our discussion covers both cases.
http://w3id.org/mlsea/pwc/scientificWork/A%20Critical%20Review%20of%20Information%20Bottleneck%20Theory%20and%20its%20Applications%20to%20Deep%20Learning                                                                                  A Critical Review of Information Bottleneck Theory and its Applications to Deep Learning                                                                                  In the past decade, deep neural networks have seen unparalleled improvements that continue to impact every aspect of today's society. With the development of high performance GPUs and the availability of vast amounts of data, learning capabilities of ML systems have skyrocketed, going from classifying digits in a picture to beating world-champions in games with super-human performance. However, even as ML models continue to achieve new frontiers, their practical success has been hindered by the lack of a deep theoretical understanding of their inner workings. Fortunately, a known information-theoretic method called the information bottleneck theory has emerged as a promising approach to better understand the learning dynamics of neural networks. In principle, IB theory models learning as a trade-off between the compression of the data and the retainment of information. The goal of this survey is to provide a comprehensive review of IB theory covering it's information theoretic roots and the recently proposed applications to understand deep learning models.
http://w3id.org/mlsea/pwc/scientificWork/A%20Crossover%20That%20Matches%20Diverse%20Parents%20Together%20in%20Evolutionary%20Algorithms                                                                                  A Crossover That Matches Diverse Parents Together in Evolutionary Algorithms                                                                                  Crossover and mutation are the two main operators that lead to new solutions in evolutionary approaches. In this article, a new method of performing the crossover phase is presented. The problem of choice is evolutionary decision tree construction. The method aims at finding such individuals that together complement each other. Hence we say that they are diversely specialized. We propose the way of calculating the so-called complementary fitness. In several empirical experiments, we evaluate the efficacy of the method proposed in four variants and compare it to a fitness-rank-based approach. One variant emerges clearly as the best approach, whereas the remaining ones are below the baseline.
http://w3id.org/mlsea/pwc/scientificWork/A%20DEEP%20ADVERSARIAL%20LEARNING%20METHODOLOGY%20FOR%20DESIGNING%20MICROSTRUCTURAL%20MATERIAL%20SYSTEMS                                                                                  A DEEP ADVERSARIAL LEARNING METHODOLOGY FOR DESIGNING MICROSTRUCTURAL MATERIAL SYSTEMS                                                                                  In Computational Materials Design (CMD), it is well recognized that identifying key microstructure characteristics is crucial for determining material design variables. However, existing microstructure characterization and reconstruction (MCR) techniques have limitations to be applied for materials design. Some MCR approaches are not applicable for material microstructural design because no parameters are available to serve as design variables, while others introduce significant information loss in either microstructure representation and/or dimensionality reduction. In this work, we present a deep adversarial learning methodology that overcomes the limitations of existing MCR techniques. In the proposed methodology, generative adversarial networks (GAN) are trained to learn the mapping between latent variables and microstructures. Thereafter, the low-dimensional latent variables serve as design variables, and a Bayesian optimization framework is applied to obtain microstructures with desired material property. Due to the special design of the network architecture, the proposed methodology is able to identify the latent (design) variables with desired dimensionality, as well as capturing complex material microstructural characteristics. The validity of the proposed methodology is tested numerically on a synthetic microstructure dataset and its effectiveness for materials design is evaluated through a case study of optimizing optical performance for energy absorption. Additional features, such as scalability and transferability, are also demonstrated in this work. In essence, the proposed methodology provides an end-toend solution for microstructural design, in which GAN reduces information loss and preserves more microstructural characteristics, and the GP-Hedge optimization improves the efficiency of design exploration
http://w3id.org/mlsea/pwc/scientificWork/A%20Dashboard%20for%20Mitigating%20the%20COVID-19%20Misinfodemic                                                                                  A Dashboard for Mitigating the COVID-19 Misinfodemic                                                                                  This paper describes the current milestones achieved in our ongoing project that aims to understand the surveillance of, impact of and intervention on COVID-19 misinfodemic on Twitter. Specifically, it introduces a public dashboard which, in addition to displaying case counts in an interactive map and a navigational panel, also provides some unique features not found in other places. Particularly, the dashboard uses a curated catalog of COVID-19 related facts and debunks of misinformation, and it displays the most prevalent information from the catalog among Twitter users in user-selected U.S. geographic regions. The paper explains how to use BERT models to match tweets with the facts and misinformation and to detect their stance towards such information. The paper also discusses the results of preliminary experiments on analyzing the spatio-temporal spread of misinformation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Data-Adaptive%20Loss%20Function%20for%20Incomplete%20Data%20and%20Incremental%20Learning%20in%20Semantic%20Image%20Segmentation                                                                                  A Data-Adaptive Loss Function for Incomplete Data and Incremental Learning in Semantic Image Segmentation                                                                                  In the last years, deep learning has dramatically improved the performances in a variety of medical image analysis applications. Among different types of deep learning models, convolutional neural networks have been among the most successful and they have been used in many applications in medical imaging. Training deep convolutional neural networks often requires large amounts of image data to generalize well to new unseen images. It is often time-consuming and expensive to collect large amounts of data in the medical image domain due to expensive imaging systems, and the need for experts to manually make ground truth annotations. A potential problem arises if new structures are added when a decision support system is already deployed and in use. Since the field of radiation therapy is constantly developing, the new structures would also have to be covered by the decision support system. In the present work, we propose a novel loss function, that adapts to the available data in order to utilize all available data, even when some have missing annotations. We demonstrate that the proposed loss function also works well in an incremental learning setting, where it can automatically incorporate new structures as they appear. Experiments on a large in-house data set show that the proposed method performs on par with baseline models, while greatly reducing the training time.
http://w3id.org/mlsea/pwc/scientificWork/A%20Data-Driven%20Approach%20to%20Full-Field%20Damage%20and%20Failure%20Pattern%20Prediction%20in%20Microstructure-Dependent%20Composites%20using%20Deep%20Learning                                                                                  A Data-Driven Approach to Full-Field Damage and Failure Pattern Prediction in Microstructure-Dependent Composites using Deep Learning                                                                                  An image-based deep learning framework is developed in this paper to predict damage and failure in microstructure-dependent composite materials. The work is motivated by the complexity and computational cost of high-fidelity simulations of such materials. The proposed deep learning framework predicts the post-failure full-field stress distribution and crack pattern in two-dimensional representations of the composites based on the geometry of microstructures. The material of interest is selected to be a high-performance unidirectional carbon fiber-reinforced polymer composite. The deep learning framework contains two stacked fully-convolutional networks, namely, Generator 1 and Generator 2, trained sequentially. First, Generator 1 learns to translate the microstructural geometry to the full-field post-failure stress distribution. Then, Generator 2 learns to translate the output of Generator 1 to the failure pattern. A physics-informed loss function is also designed and incorporated to further improve the performance of the proposed framework and facilitate the validation process. In order to provide a sufficiently large data set for training and validating the deep learning framework, 4500 microstructural representations are synthetically generated and simulated in an efficient finite element framework. It is shown that the proposed deep learning approach can effectively predict the composites' post-failure full-field stress distribution and failure pattern, two of the most complex phenomena to simulate in computational solid mechanics.
http://w3id.org/mlsea/pwc/scientificWork/A%20Data-Efficient%20Approach%20to%20Behind-the-Meter%20Solar%20Generation%20Disaggregation                                                                                  A Data-Efficient Approach to Behind-the-Meter Solar Generation Disaggregation                                                                                  With the emergence of cost effective battery storage and the decline in the solar photovoltaic (PV) levelized cost of energy (LCOE), the number of behind-the-meter solar PV systems is expected to increase steadily. The ability to estimate solar generation from these latent systems is crucial for a range of applications, including distribution system planning and operation, demand response, and non-intrusive load monitoring (NILM). This paper investigates the problem of disaggregating solar generation from smart meter data when historical disaggregated data from the target home is unavailable, and deployment characteristics of the PV system are unknown. The proposed approach entails inferring the physical characteristics from smart meter data and disaggregating solar generation using an iterative algorithm. This algorithm takes advantage of solar generation data (aka proxy measurements) from a few sites that are located in the same area as the target home, and solar generation data synthesized using a physical PV model. We evaluate our methods with 4 different proxy settings on around 160 homes in the United States and Australia, and show that the solar disaggregation accuracy is improved by 32.31% and 15.66% over two state-of-the-art methods using only one real proxy along with three synthetic proxies. Furthermore, we demonstrate that using the disaggregated home load rather than the net load data could improve the overall accuracy of three popular NILM methods by at least 22%.
http://w3id.org/mlsea/pwc/scientificWork/A%20Data-Fusion-Assisted%20Telemetry%20Layer%20for%20Autonomous%20Optical%20Networks                                                                                  A Data-Fusion-Assisted Telemetry Layer for Autonomous Optical Networks                                                                                  For further improving the capacity and reliability of optical networks, a closed-loop autonomous architecture is preferred. Considering a large number of optical components in an optical network and many digital signal processing modules in each optical transceiver, massive real-time data can be collected. However, for a traditional monitoring structure, collecting, storing and processing a large size of data are challenging tasks. Moreover, strong correlations and similarities between data from different sources and regions are not properly considered, which may limit function extension and accuracy improvement. To address abovementioned issues, a data-fusion-assisted telemetry layer between the physical layer and control layer is proposed in this paper. The data fusion methodologies are elaborated on three different levels: Source Level, Space Level and Model Level. For each level, various data fusion algorithms are introduced and relevant works are reviewed. In addition, proof-of-concept use cases for each level are provided through simulations, where the benefits of the data-fusion-assisted telemetry layer are shown.
http://w3id.org/mlsea/pwc/scientificWork/A%20Data-driven%20Optimization%20of%20First-order%20Regular%20Perturbation%20Coefficients%20for%20Fiber%20Nonlinearities                                                                                  A Data-driven Optimization of First-order Regular Perturbation Coefficients for Fiber Nonlinearities                                                                                  We study the performance of gradient-descent optimization to estimate the coefficients of the discrete-time first-order regular perturbation (FRP). With respect to numerically computed coefficients, the optimized coefficients yield a model that (i) extends the FRP range of validity, and (ii) reduces the model's complexity.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dataset%20And%20Benchmark%20Of%20Underwater%20Object%20Detection%20For%20Robot%20Picking                                                                                  A Dataset And Benchmark Of Underwater Object Detection For Robot Picking                                                                                  Underwater object detection for robot picking has attracted a lot of interest. However, it is still an unsolved problem due to several challenges. We take steps towards making it more realistic by addressing the following challenges. Firstly, the currently available datasets basically lack the test set annotations, causing researchers must compare their method with other SOTAs on a self-divided test set (from the training set). Training other methods lead to an increase in workload and different researchers divide different datasets, resulting there is no unified benchmark to compare the performance of different algorithms. Secondly, these datasets also have other shortcomings, e.g., too many similar images or incomplete labels. Towards these challenges we introduce a dataset, Detecting Underwater Objects (DUO), and a corresponding benchmark, based on the collection and re-annotation of all relevant datasets. DUO contains a collection of diverse underwater images with more rational annotations. The corresponding benchmark provides indicators of both efficiency and accuracy of SOTAs (under the MMDtection framework) for academic research and industrial applications, where JETSON AGX XAVIER is used to assess detector speed to simulate the robot-embedded environment.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dataset%20and%20Baselines%20for%20Multilingual%20Reply%20Suggestion                                                                                  A Dataset and Baselines for Multilingual Reply Suggestion                                                                                  Reply suggestion models help users process emails and chats faster. Previous work only studies English reply suggestion. Instead, we present MRS, a multilingual reply suggestion dataset with ten languages. MRS can be used to compare two families of models: 1) retrieval models that select the reply from a fixed set and 2) generation models that produce the reply from scratch. Therefore, MRS complements existing cross-lingual generalization benchmarks that focus on classification and sequence labeling tasks. We build a generation model and a retrieval model as baselines for MRS. The two models have different strengths in the monolingual setting, and they require different strategies to generalize across languages. MRS is publicly available at https://github.com/zhangmozhi/mrs.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dataset%20and%20System%20for%20Real-Time%20Gun%20Detection%20in%20Surveillance%20Video%20Using%20Deep%20Learning                                                                                  A Dataset and System for Real-Time Gun Detection in Surveillance Video Using Deep Learning                                                                                  Gun violence is a severe problem in the world, particularly in the United States. Deep learning methods have been studied to detect guns in surveillance video cameras or smart IP cameras and to send a real-time alert to security personals. One problem for the development of gun detection algorithms is the lack of large public datasets. In this work, we first publish a dataset with 51K annotated gun images for gun detection and other 51K cropped gun chip images for gun classification we collect from a few different sources. To our knowledge, this is the largest dataset for the study of gun detection. This dataset can be downloaded at www.linksprite.com/gun-detection-datasets. We present a gun detection system using a smart IP camera as an embedded edge device, and a cloud server as a manager for device, data, alert, and to further reduce the false positive rate. We study to find solutions for gun detection in an embedded device, and for gun classification on the edge device and the cloud server. This edge/cloud framework makes the deployment of gun detection in the real world possible.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dataset%20for%20Linguistic%20Understanding%2C%20Visual%20Evaluation%2C%20and%20Recognition%20of%20Sign%20Languages%3A%20The%20K-RSL                                                                                  A Dataset for Linguistic Understanding, Visual Evaluation, and Recognition of Sign Languages: The K-RSL                                                                                  The paper presents the first dataset that aims to serve interdisciplinary purposes for the utility of computer vision community and sign language linguistics. To date, a majority of Sign Language Recognition (SLR) approaches focus on recognising sign language as a manual gesture recognition problem. However, signers use other articulators: facial expressions, head and body position and movement to convey linguistic information. Given the important role of non-manual markers, this paper proposes a dataset and presents a use case to stress the importance of including non-manual features to improve the recognition accuracy of signs. To the best of our knowledge no prior publicly available dataset exists that explicitly focuses on non-manual components responsible for the grammar of sign languages. To this end, the proposed dataset contains 28250 videos of signs of high resolution and quality, with annotation of manual and non-manual components. We conducted a series of evaluations in order to investigate whether non-manual components would improve signs{'} recognition accuracy. We release the dataset to encourage SLR researchers and help advance current progress in this area toward real-time sign language interpretation. Our dataset will be made publicly available at https://krslproject.github.io/krsl-corpus
http://w3id.org/mlsea/pwc/scientificWork/A%20Dataset%20for%20Provident%20Vehicle%20Detection%20at%20Night                                                                                  A Dataset for Provident Vehicle Detection at Night                                                                                  In current object detection, algorithms require the object to be directly visible in order to be detected. As humans, however, we intuitively use visual cues caused by the respective object to already make assumptions about its appearance. In the context of driving, such cues can be shadows during the day and often light reflections at night. In this paper, we study the problem of how to map this intuitive human behavior to computer vision algorithms to detect oncoming vehicles at night just from the light reflections they cause by their headlights. For that, we present an extensive open-source dataset containing 59746 annotated grayscale images out of 346 different scenes in a rural environment at night. In these images, all oncoming vehicles, their corresponding light objects (e.g., headlamps), and their respective light reflections (e.g., light reflections on guardrails) are labeled. In this context, we discuss the characteristics of the dataset and the challenges in objectively describing visual cues such as light reflections. We provide different metrics for different ways to approach the task and report the results we achieved using state-of-the-art and custom object detection models as a first benchmark. With that, we want to bring attention to a new and so far neglected field in computer vision research, encourage more researchers to tackle the problem, and thereby further close the gap between human performance and computer vision systems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dataset%20of%20Dynamic%20Reverberant%20Sound%20Scenes%20with%20Directional%20Interferers%20for%20Sound%20Event%20Localization%20and%20Detection                                                                                  A Dataset of Dynamic Reverberant Sound Scenes with Directional Interferers for Sound Event Localization and Detection                                                                                  This report presents the dataset and baseline of Task 3 of the DCASE2021 Challenge on Sound Event Localization and Detection (SELD). The dataset is based on emulation of real recordings of static or moving sound events under real conditions of reverberation and ambient noise, using spatial room impulse responses captured in a variety of rooms and delivered in two spatial formats. The acoustical synthesis remains the same as in the previous iteration of the challenge, however the new dataset brings more challenging conditions of polyphony and overlapping instances of the same class. The most important difference of the new dataset is the introduction of directional interferers, meaning sound events that are localized in space but do not belong to the target classes to be detected and are not annotated. Since such interfering events are expected in every real-world scenario of SELD, the new dataset aims to promote systems that deal with this condition effectively. A modified SELDnet baseline employing the recent ACCDOA representation of SELD problems accompanies the dataset and it is shown to outperform the previous one. The new dataset is shown to be significantly more challenging for both baselines according to all considered metrics. To investigate the individual and combined effects of ambient noise, interferers, and reverberation, we study the performance of the baseline on different versions of the dataset excluding or including combinations of these factors. The results indicate that by far the most detrimental effects are caused by directional interferers.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dataset%20of%20Information-Seeking%20Questions%20and%20Answers%20Anchored%20in%20Research%20Papers                                                                                  A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers                                                                                  Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present QASPER, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dataset-Level%20Geometric%20Framework%20for%20Ensemble%20Classifiers                                                                                  A Dataset-Level Geometric Framework for Ensemble Classifiers                                                                                  Ensemble classifiers have been investigated by many in the artificial intelligence and machine learning community. Majority voting and weighted majority voting are two commonly used combination schemes in ensemble learning. However, understanding of them is incomplete at best, with some properties even misunderstood. In this paper, we present a group of properties of these two schemes formally under a dataset-level geometric framework. Two key factors, every component base classifier's performance and dissimilarity between each pair of component classifiers are evaluated by the same metric - the Euclidean distance. Consequently, ensembling becomes a deterministic problem and the performance of an ensemble can be calculated directly by a formula. We prove several theorems of interest and explain their implications for ensembles. In particular, we compare and contrast the effect of the number of component classifiers on these two types of ensemble schemes. Empirical investigation is also conducted to verify the theoretical results when other metrics such as accuracy are used. We believe that the results from this paper are very useful for us to understand the fundamental properties of these two combination schemes and the principles of ensemble classifiers in general. The results are also helpful for us to investigate some issues in ensemble classifiers, such as ensemble performance prediction, selecting a small number of base classifiers to obtain efficient and effective ensembles.
http://w3id.org/mlsea/pwc/scientificWork/A%20De-raining%20semantic%20segmentation%20network%20for%20real-time%20foreground%20segmentation                                                                                  A De-raining semantic segmentation network for real-time foreground segmentation                                                                                  Few researches have been proposed specifically for real-time semantic segmentation in rainy environments. However, the demand in this area is huge and it is challenging for lightweight networks. Therefore, this paper proposes a lightweight network which is specially designed for the foreground segmentation in rainy environments, named De-raining Semantic Segmentation Network (DRSNet). By analyzing the characteristics of raindrops, the MultiScaleSE Block is targetedly designed to encode the input image, it uses multi-scale dilated convolutions to increase the receptive field, and SE attention mechanism to learn the weights of each channels. In order to combine semantic information between different encoder and decoder layers, it is proposed to use Asymmetric Skip, that is, the higher semantic layer of encoder employs bilinear interpolation and the output passes through pointwise convolution, then added element-wise to the lower semantic layer of decoder. According to the control experiments, the performances of MultiScaleSE Block and Asymmetric Skip compared with SEResNet18 and Symmetric Skip respectively are improved to a certain degree on the Foreground Accuracy index. The parameters and the floating point of operations (FLOPs) of DRSNet is only 0.54M and 0.20GFLOPs separately. The state-of-the-art results and real-time performances are achieved on both the UESTC all-day Scenery add rain (UAS-add-rain) and the Baidu People Segmentation add rain (BPS-add-rain) benchmarks with the input sizes of 192*128, 384*256 and 768*512. The speed of DRSNet exceeds all the networks within 1GFLOPs, and Foreground Accuracy index is also the best among the similar magnitude networks on both benchmarks.
http://w3id.org/mlsea/pwc/scientificWork/A%20Decade%20of%20Research%20for%20Image%20Compression%20In%20Multimedia%20Laboratory                                                                                  A Decade of Research for Image Compression In Multimedia Laboratory                                                                                  With the advancement of technology, we have supercomputers with high processing power and affordable prices. In addition, using multimedia expanded all around the world. This caused a vast use of images and videos in different fields. As this kind of data consists of a large amount of information, there is a need to use compression methods to store, manage or transfer them better and faster. One effective technique, which was introduced is variable resolution. This technique stimulates human vision and divides regions in pictures into two different parts, including the area of interest that needs more detail and periphery parts with less detail. This results in better compression. The variable resolution was used for image, video, and 3D motion data compression. This paper investigates the mentioned technique and some other research in this regard.
http://w3id.org/mlsea/pwc/scientificWork/A%20Decentralized%20Adaptive%20Momentum%20Method%20for%20Solving%20a%20Class%20of%20Min-Max%20Optimization%20Problems                                                                                  A Decentralized Adaptive Momentum Method for Solving a Class of Min-Max Optimization Problems                                                                                  Min-max saddle point games have recently been intensely studied, due to their wide range of applications, including training Generative Adversarial Networks (GANs). However, most of the recent efforts for solving them are limited to special regimes such as convex-concave games. Further, it is customarily assumed that the underlying optimization problem is solved either by a single machine or in the case of multiple machines connected in centralized fashion, wherein each one communicates with a central node. The latter approach becomes challenging, when the underlying communications network has low bandwidth. In addition, privacy considerations may dictate that certain nodes can communicate with a subset of other nodes. Hence, it is of interest to develop methods that solve min-max games in a decentralized manner. To that end, we develop a decentralized adaptive momentum (ADAM)-type algorithm for solving min-max optimization problem under the condition that the objective function satisfies a Minty Variational Inequality condition, which is a generalization to convex-concave case. The proposed method overcomes shortcomings of recent non-adaptive gradient-based decentralized algorithms for min-max optimization problems that do not perform well in practice and require careful tuning. In this paper, we obtain non-asymptotic rates of convergence of the proposed algorithm (coined DADAM$^3$) for finding a (stochastic) first-order Nash equilibrium point and subsequently evaluate its performance on training GANs. The extensive empirical evaluation shows that DADAM$^3$ outperforms recently developed methods, including decentralized optimistic stochastic gradient for solving such min-max problems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Decentralized%20Multi-UAV%20Spatio-Temporal%20Multi-Task%20Allocation%20Approach%20for%20Perimeter%20Defense                                                                                  A Decentralized Multi-UAV Spatio-Temporal Multi-Task Allocation Approach for Perimeter Defense                                                                                  This paper provides a new solution approach to a multi-player perimeter defense game, in which the intruders' team tries to enter the territory, and a team of defenders protects the territory by capturing intruders on the perimeter of the territory. The objective of the defenders is to detect and capture the intruders before the intruders enter the territory. Each defender independently senses the intruder and computes his trajectory to capture the assigned intruders in a cooperative fashion. The intruder is estimated to reach a specific location on the perimeter at a specific time. Each intruder is viewed as a spatio-temporal task, and the defenders are assigned to execute these spatio-temporal tasks. At any given time, the perimeter defense problem is converted into a Decentralized Multi-UAV Spatio-Temporal Multi-Task Allocation (DMUST-MTA) problem. The cost of executing a task for a trajectory is defined by a composite cost function of both the spatial and temporal components. In this paper, a decentralized consensus-based bundle algorithm has been modified to solve the spatio-temporal multi-task allocation problem, and the performance evaluation of the proposed approach is carried out based on Monte-Carlo simulations. The simulation results show the effectiveness of the proposed approach to solve the perimeter defense game under different scenarios. Performance comparison with a state-of-the-art centralized approach with full observability, clearly indicates that DMUST-MTA achieves similar performance in a decentralized way with partial observability conditions with a lesser computational time and easy scaling up.
http://w3id.org/mlsea/pwc/scientificWork/A%20Decomposition%20Model%20for%20Stereo%20Matching                                                                                  A Decomposition Model for Stereo Matching                                                                                  In this paper, we present a decomposition model for stereo matching to solve the problem of excessive growth in computational cost (time and memory cost) as the resolution increases. In order to reduce the huge cost of stereo matching at the original resolution, our model only runs dense matching at a very low resolution and uses sparse matching at different higher resolutions to recover the disparity of lost details scale-by-scale. After the decomposition of stereo matching, our model iteratively fuses the sparse and dense disparity maps from adjacent scales with an occlusion-aware mask. A refinement network is also applied to improving the fusion result. Compared with high-performance methods like PSMNet and GANet, our method achieves $10-100 times$ speed increase while obtaining comparable disparity estimation results.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Deterministic%20Policy%20Gradient-based%20Strategy%20for%20Stocks%20Portfolio%20Management                                                                                  A Deep Deterministic Policy Gradient-based Strategy for Stocks Portfolio Management                                                                                  With the improvement of computer performance and the development of GPU-accelerated technology, trading with machine learning algorithms has attracted the attention of many researchers and practitioners. In this research, we propose a novel portfolio management strategy based on the framework of Deep Deterministic Policy Gradient, a policy-based reinforcement learning framework, and compare its performance to that of other trading strategies. In our framework, two Long Short-Term Memory neural networks and two fully connected neural networks are constructed. We also investigate the performance of our strategy with and without transaction costs. Experimentally, we choose eight US stocks consisting of four low-volatility stocks and four high-volatility stocks. We compare the compound annual return rate of our strategy against seven other strategies, e.g., Uniform Buy and Hold, Exponential Gradient and Universal Portfolios. In our case, the compound annual return rate is 14.12%, outperforming all other strategies. Furthermore, in terms of Sharpe Ratio (0.5988), our strategy is nearly 33% higher than that of the second-best performing strategy.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Dive%20into%20Conflict%20Generating%20Decisions                                                                                  A Deep Dive into Conflict Generating Decisions                                                                                  Boolean Satisfiability (SAT) is a well-known NP-complete problem. Despite this theoretical hardness, SAT solvers based on Conflict Driven Clause Learning (CDCL) can solve large SAT instances from many important domains. CDCL learns clauses from conflicts, a technique that allows a solver to prune its search space. The selection heuristics in CDCL prioritize variables that are involved in recent conflicts. While only a fraction of decisions generate any conflicts, many generate multiple conflicts. In this paper, we study conflict-generating decisions in CDCL in detail. We investigate the impact of single conflict (sc) decisions, which generate only one conflict, and multi-conflict (mc) decisions which generate two or more. We empirically characterize these two types of decisions based on the quality of the learned clauses produced by each type of decision. We also show an important connection between consecutive clauses learned within the same mc decision, where one learned clause triggers the learning of the next one forming a chain of clauses. This leads to the consideration of similarity between conflicts, for which we formulate the notion of conflictsproximity as a similarity measure. We show that conflicts in mc decisions are more closely related than consecutive conflicts generated from sc decisions. Finally, we develop Common Reason Variable Reduction (CRVR) as a new decision strategy that reduces the selection priority of some variables from the learned clauses of mc decisions. Our empirical evaluation of CRVR implemented in three leading solvers demonstrates performance gains in benchmarks from the main track of SAT Competition-2020.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Ensemble-based%20Wireless%20Receiver%20Architecture%20for%20Mitigating%20Adversarial%20Attacks%20in%20Automatic%20Modulation%20Classification                                                                                  A Deep Ensemble-based Wireless Receiver Architecture for Mitigating Adversarial Attacks in Automatic Modulation Classification                                                                                  Deep learning-based automatic modulation classification (AMC) models are susceptible to adversarial attacks. Such attacks inject specifically crafted wireless interference into transmitted signals to induce erroneous classification predictions. Furthermore, adversarial interference is transferable in black box environments, allowing an adversary to attack multiple deep learning models with a single perturbation crafted for a particular classification model. In this work, we propose a novel wireless receiver architecture to mitigate the effects of adversarial interference in various black box attack environments. We begin by evaluating the architecture uncertainty environment, where we show that adversarial attacks crafted to fool specific AMC DL architectures are not directly transferable to different DL architectures. Next, we consider the domain uncertainty environment, where we show that adversarial attacks crafted on time domain and frequency domain features to not directly transfer to the altering domain. Using these insights, we develop our Assorted Deep Ensemble (ADE) defense, which is an ensemble of deep learning architectures trained on time and frequency domain representations of received signals. Through evaluation on two wireless signal datasets under different sources of uncertainty, we demonstrate that our ADE obtains substantial improvements in AMC classification performance compared with baseline defenses across different adversarial attacks and potencies.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Latent%20Space%20Model%20for%20Graph%20Representation%20Learning                                                                                  A Deep Latent Space Model for Graph Representation Learning                                                                                  Graph representation learning is a fundamental problem for modeling relational data and benefits a number of downstream applications. Traditional Bayesian-based graph models and recent deep learning based GNN either suffer from impracticability or lack interpretability, thus combined models for undirected graphs have been proposed to overcome the weaknesses. As a large portion of real-world graphs are directed graphs (of which undirected graphs are special cases), in this paper, we propose a Deep Latent Space Model (DLSM) for directed graphs to incorporate the traditional latent variable based generative model into deep learning frameworks. Our proposed model consists of a graph convolutional network (GCN) encoder and a stochastic decoder, which are layer-wise connected by a hierarchical variational auto-encoder architecture. By specifically modeling the degree heterogeneity using node random factors, our model possesses better interpretability in both community structure and degree heterogeneity. For fast inference, the stochastic gradient variational Bayes (SGVB) is adopted using a non-iterative recognition model, which is much more scalable than traditional MCMC-based methods. The experiments on real-world datasets show that the proposed model achieves the state-of-the-art performances on both link prediction and community detection tasks while learning interpretable node embeddings. The source code is available at https://github.com/upperr/DLSM.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Learning%20Approach%20to%20Mapping%20Irrigation%3A%20IrrMapper-U-Net                                                                                  A Deep Learning Approach to Mapping Irrigation: IrrMapper-U-Net                                                                                  Accurate maps of irrigation are essential for understanding and managing water resources. We present a new method of mapping irrigation and demonstrate its accuracy for the state of Montana from years 2000-2019. The method is based off of an ensemble of convolutional neural networks that use reflectance information from Landsat imagery to classify irrigated pixels, that we call IrrMapper-U-Net. The methodology does not rely on extensive feature engineering and does not condition the classification with land use information from existing geospatial datasets. The ensemble does not need exhaustive hyperparameter tuning and the analysis pipeline is lightweight enough to be implemented on a personal computer. Furthermore, the proposed methodology provides an estimate of the uncertainty associated with classification. We evaluated our methodology and the resulting irrigation maps using a highly accurate novel spatially-explicit ground truth data set, using county-scale USDA surveys of irrigation extent, and using cadastral surveys. We found that that our method outperforms other methods of mapping irrigation in Montana in terms of overall accuracy and precision. We found that our method agrees better statewide with the USDA National Agricultural Statistics Survey estimates of irrigated area compared to other methods, and has far fewer errors of commission in rainfed agriculture areas. The method learns to mask clouds and ignore Landsat 7 scan-line failures without supervision, reducing the need for preprocessing data. This methodology has the potential to be applied across the entire United States and for the complete Landsat record.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Learning%20Approach%20to%20Private%20Data%20Sharing%20of%20Medical%20Images%20Using%20Conditional%20GANs                                                                                  A Deep Learning Approach to Private Data Sharing of Medical Images Using Conditional GANs                                                                                  Sharing data from clinical studies can facilitate innovative data-driven research and ultimately lead to better public health. However, sharing biomedical data can put sensitive personal information at risk. This is usually solved by anonymization, which is a slow and expensive process. An alternative to anonymization is sharing a synthetic dataset that bears a behaviour similar to the real data but preserves privacy. As part of the collaboration between Novartis and the Oxford Big Data Institute, we generate a synthetic dataset based on COSENTYX (secukinumab) Ankylosing Spondylitis clinical study. We apply an Auxiliary Classifier GAN to generate synthetic MRIs of vertebral units. The images are conditioned on the VU location (cervical, thoracic and lumbar). In this paper, we present a method for generating a synthetic dataset and conduct an in-depth analysis on its properties along three key metrics: image fidelity, sample diversity and dataset privacy.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Learning%20Based%20Cost%20Model%20for%20Automatic%20Code%20Optimization                                                                                  A Deep Learning Based Cost Model for Automatic Code Optimization                                                                                  Enabling compilers to automatically optimize code has been a longstanding goal for the compiler community. Efficiently solving this problem requires using precise cost models. These models predict whether applying a sequence of code transformations reduces the execution time of the program. Building an analytical cost model to do so is hard in modern x86 architectures due to the complexity of the microarchitecture. In this paper, we present a novel deep learning based cost model for automatic code optimization. This model was integrated in a search method and implemented in the Tiramisu compiler to select the best code transformations. The input of the proposed model is a set of simple features representing the unoptimized code and a sequence of code transformations. The model predicts the speedup expected when the code transformations are applied. Unlike previous models, the proposed one works on full programs and does not rely on any heavy feature engineering. The proposed model has only 16% of mean absolute percentage error in predicting speedups on full programs. The proposed model enables Tiramisu to automatically find code transformations that match or are better than state-of-the-art compilers without requiring the same level of heavy feature engineering required by those compilers.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Learning%20Framework%20for%20Lifelong%20Machine%20Learning                                                                                  A Deep Learning Framework for Lifelong Machine Learning                                                                                  Humans can learn a variety of concepts and skills incrementally over the course of their lives while exhibiting many desirable properties, such as continual learning without forgetting, forward transfer and backward transfer of knowledge, and learning a new concept or task with only a few examples. Several lines of machine learning research, such as lifelong machine learning, few-shot learning, and transfer learning attempt to capture these properties. However, most previous approaches can only demonstrate subsets of these properties, often by different complex mechanisms. In this work, we propose a simple yet powerful unified deep learning framework that supports almost all of these properties and approaches through one central mechanism. Experiments on toy examples support our claims. We also draw connections between many peculiarities of human learning (such as memory loss and 'rain man') and our framework. As academics, we often lack resources required to build and train, deep neural networks with billions of parameters on hundreds of TPUs. Thus, while our framework is still conceptual, and our experiment results are surely not SOTA, we hope that this unified lifelong learning framework inspires new work towards large-scale experiments and understanding human learning in general. This paper is summarized in two short YouTube videos: https://youtu.be/gCuUyGETbTU (part 1) and https://youtu.be/XsaGI01b-1o (part 2).
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Learning%20Object%20Detection%20Method%20for%20an%20Efficient%20Clusters%20Initialization                                                                                  A Deep Learning Object Detection Method for an Efficient Clusters Initialization                                                                                  Clustering is an unsupervised machine learning method grouping data samples into clusters of similar objects. In practice, clustering has been used in numerous applications such as banking customers profiling, document retrieval, image segmentation, and e-commerce recommendation engines. However, the existing clustering techniques present significant limitations, from which is the dependability of their stability on the initialization parameters (e.g. number of clusters, centroids). Different solutions were presented in the literature to overcome this limitation (i.e. internal and external validation metrics). However, these solutions require high computational complexity and memory consumption, especially when dealing with big data. In this paper, we apply the recent object detection Deep Learning (DL) model, named YOLO-v5, to detect the initial clustering parameters such as the number of clusters with their sizes and centroids. Mainly, the proposed solution consists of adding a DL-based initialization phase making the clustering algorithms free of initialization. Two model solutions are provided in this work, one for isolated clusters and the other one for overlapping clusters. The features of the incoming dataset determine which model to use. Moreover, The results show that the proposed solution can provide near-optimal clusters initialization parameters with low computational and resources overhead compared to existing solutions.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Learning-Accelerated%20Data%20Assimilation%20and%20Forecasting%20Workflow%20for%20Commercial-Scale%20Geologic%20Carbon%20Storage                                                                                  A Deep Learning-Accelerated Data Assimilation and Forecasting Workflow for Commercial-Scale Geologic Carbon Storage                                                                                  Fast assimilation of monitoring data to update forecasts of pressure buildup and carbon dioxide (CO2) plume migration under geologic uncertainties is a challenging problem in geologic carbon storage. The high computational cost of data assimilation with a high-dimensional parameter space impedes fast decision-making for commercial-scale reservoir management. We propose to leverage physical understandings of porous medium flow behavior with deep learning techniques to develop a fast history matching-reservoir response forecasting workflow. Applying an Ensemble Smoother Multiple Data Assimilation framework, the workflow updates geologic properties and predicts reservoir performance with quantified uncertainty from pressure history and CO2 plumes interpreted through seismic inversion. As the most computationally expensive component in such a workflow is reservoir simulation, we developed surrogate models to predict dynamic pressure and CO2 plume extents under multi-well injection. The surrogate models employ deep convolutional neural networks, specifically, a wide residual network and a residual U-Net. The workflow is validated against a flat three-dimensional reservoir model representative of a clastic shelf depositional environment. Intelligent treatments are applied to bridge between quantities in a true-3D reservoir model and those in a single-layer reservoir model. The workflow can complete history matching and reservoir forecasting with uncertainty quantification in less than one hour on a mainstream personal workstation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Learning-based%20Multimodal%20Depth-Aware%20Dynamic%20Hand%20Gesture%20Recognition%20System                                                                                  A Deep Learning-based Multimodal Depth-Aware Dynamic Hand Gesture Recognition System                                                                                  The dynamic hand gesture recognition task has seen studies on various unimodal and multimodal methods. Previously, researchers have explored depth and 2D-skeleton-based multimodal fusion CRNNs (Convolutional Recurrent Neural Networks) but have had limitations in getting expected recognition results. In this paper, we revisit this approach to hand gesture recognition and suggest several improvements. We observe that raw depth images possess low contrast in the hand regions of interest (ROI). They do not highlight important fine details, such as finger orientation, overlap between the finger and palm, or overlap between multiple fingers. We thus propose quantizing the depth values into several discrete regions, to create a higher contrast between several key parts of the hand. In addition, we suggest several ways to tackle the high variance problem in existing multimodal fusion CRNN architectures. We evaluate our method on two benchmarks: the DHG-14/28 dataset and the SHREC'17 track dataset. Our approach shows a significant improvement in accuracy and parameter efficiency over previous similar multimodal methods, with a comparable result to the state-of-the-art.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Local%20and%20Global%20Scene-Graph%20Matching%20for%20Image-Text%20Retrieval                                                                                  A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval                                                                                  Conventional approaches to image-text retrieval mainly focus on indexing visual objects appearing in pictures but ignore the interactions between these objects. Such objects occurrences and interactions are equivalently useful and important in this field as they are usually mentioned in the text. Scene graph presentation is a suitable method for the image-text matching challenge and obtained good results due to its ability to capture the inter-relationship information. Both images and text are represented in scene graph levels and formulate the retrieval challenge as a scene graph matching challenge. In this paper, we introduce the Local and Global Scene Graph Matching (LGSGM) model that enhances the state-of-the-art method by integrating an extra graph convolution network to capture the general information of a graph. Specifically, for a pair of scene graphs of an image and its caption, two separate models are used to learn the features of each graph's nodes and edges. Then a Siamese-structure graph convolution model is employed to embed graphs into vector forms. We finally combine the graph-level and the vector-level to calculate the similarity of this image-text pair. The empirical experiments show that our enhancement with the combination of levels can improve the performance of the baseline method by increasing the recall by more than 10% on the Flickr30k dataset.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Metric%20Learning%20Approach%20to%20Account%20Linking                                                                                  A Deep Metric Learning Approach to Account Linking                                                                                  We consider the task of linking social media accounts that belong to the same author in an automated fashion on the basis of the content and metadata of their corresponding document streams. We focus on learning an embedding that maps variable-sized samples of user activity -- ranging from single posts to entire months of activity -- to a vector space, where samples by the same author map to nearby points. The approach does not require human-annotated data for training purposes, which allows us to leverage large amounts of social media content. The proposed model outperforms several competitive baselines under a novel evaluation framework modeled after established recognition benchmarks in other domains. Our method achieves high linking accuracy, even with small samples from accounts not seen at training time, a prerequisite for practical applications of the proposed linking framework.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Reinforcement%20Learning%20Approach%20for%20the%20Meal%20Delivery%20Problem                                                                                  A Deep Reinforcement Learning Approach for the Meal Delivery Problem                                                                                  We consider a meal delivery service fulfilling dynamic customer requests given a set of couriers over the course of a day. A courier's duty is to pick-up an order from a restaurant and deliver it to a customer. We model this service as a Markov decision process and use deep reinforcement learning as the solution approach. We experiment with the resulting policies on synthetic and real-world datasets and compare those with the baseline policies. We also examine the courier utilization for different numbers of couriers. In our analysis, we specifically focus on the impact of the limited available resources in the meal delivery problem. Furthermore, we investigate the effect of intelligent order rejection and re-positioning of the couriers. Our numerical experiments show that, by incorporating the geographical locations of the restaurants, customers, and the depot, our model significantly improves the overall service quality as characterized by the expected total reward and the delivery times. Our results present valuable insights on both the courier assignment process and the optimal number of couriers for different order frequencies on a given day. The proposed model also shows a robust performance under a variety of scenarios for real-world implementation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Reinforcement%20Learning%20Approach%20to%20Audio-Based%20Navigation%20in%20a%20Multi-Speaker%20Environment                                                                                  A Deep Reinforcement Learning Approach to Audio-Based Navigation in a Multi-Speaker Environment                                                                                  In this work we use deep reinforcement learning to create an autonomous agent that can navigate in a two-dimensional space using only raw auditory sensory information from the environment, a problem that has received very little attention in the reinforcement learning literature. Our experiments show that the agent can successfully identify a particular target speaker among a set of $N$ predefined speakers in a room and move itself towards that speaker, while avoiding collision with other speakers or going outside the room boundaries. The agent is shown to be robust to speaker pitch shifting and it can learn to navigate the environment, even when a limited number of training utterances are available for each speaker.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Reinforcement%20Learning%20Approach%20to%20Marginalized%20Importance%20Sampling%20with%20the%20Successor%20Representation                                                                                  A Deep Reinforcement Learning Approach to Marginalized Importance Sampling with the Successor Representation                                                                                  Marginalized importance sampling (MIS), which measures the density ratio between the state-action occupancy of a target policy and that of a sampling distribution, is a promising approach for off-policy evaluation. However, current state-of-the-art MIS methods rely on complex optimization tricks and succeed mostly on simple toy problems. We bridge the gap between MIS and deep reinforcement learning by observing that the density ratio can be computed from the successor representation of the target policy. The successor representation can be trained through deep reinforcement learning methodology and decouples the reward optimization from the dynamics of the environment, making the resulting algorithm stable and applicable to high-dimensional domains. We evaluate the empirical performance of our approach on a variety of challenging Atari and MuJoCo environments.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Reinforcement%20Learning%20Approach%20towards%20Pendulum%20Swing-up%20Problem%20based%20on%20TF-Agents                                                                                  A Deep Reinforcement Learning Approach towards Pendulum Swing-up Problem based on TF-Agents                                                                                  Adapting the idea of training CartPole with Deep Q-learning agent, we are able to find a promising result that prevent the pole from falling down. The capacity of reinforcement learning (RL) to learn from the interaction between the environment and agent provides an optimal control strategy. In this paper, we aim to solve the classic pendulum swing-up problem that making the learned pendulum to be in upright position and balanced. Deep Deterministic Policy Gradient algorithm is introduced to operate over continuous action domain in this problem. Salient results of optimal pendulum are proved with increasing average return, decreasing loss, and live video in the code part.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Transfer%20Learning%20Approach%20on%20Identifying%20Glitch%20Wave-form%20in%20Gravitational%20Wave%20Data                                                                                  A Deep Transfer Learning Approach on Identifying Glitch Wave-form in Gravitational Wave Data                                                                                  LIGO interferometer is considered the most sensitive and complicated gravitational experimental equipment ever built. Its main objective is to detect the gravitational wave from the strongest events in the universe by observing if the length of its 4-kilometer arms change by a distance 10,000 times smaller than the diameter of a proton. Due to its sensitivity, interferometer is prone to the disturbance of external noises which affects the data being collected to detect the gravitational wave. These noises are commonly called by the gravitational-wave community as glitches. This study focuses on identifying those glitches using different deep transfer learning algorithms. The extensive experiment shows that algorithm with architecture VGG19 recorded the highest AUC-ROC among other experimented algorithm with 0.9898. While all of the experimented algorithm achieved a considerably high AUC-ROC, some of the algorithm suffered from class imbalance of the dataset which has a detrimental effect when identifying other classes.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Transfer%20Learning-based%20Edge%20Computing%20Method%20for%20Home%20Health%20Monitoring                                                                                  A Deep Transfer Learning-based Edge Computing Method for Home Health Monitoring                                                                                  The health-care gets huge stress in a pandemic or epidemic situation. Some diseases such as COVID-19 that causes a pandemic is highly spreadable from an infected person to others. Therefore, providing health services at home for non-critical infected patients with isolation shall assist to mitigate this kind of stress. In addition, this practice is also very useful for monitoring the health-related activities of elders who live at home. The home health monitoring, a continuous monitoring of a patient or elder at home using visual sensors is one such non-intrusive sub-area of health services at home. In this article, we propose a transfer learning-based edge computing method for home health monitoring. Specifically, a pre-trained convolutional neural network-based model can leverage edge devices with a small amount of ground-labeled data and fine-tuning method to train the model. Therefore, on-site computing of visual data captured by RGB, depth, or thermal sensor could be possible in an affordable way. As a result, raw data captured by these types of sensors is not required to be sent outside from home. Therefore, privacy, security, and bandwidth scarcity shall not be issues. Moreover, real-time computing for the above-mentioned purposes shall be possible in an economical way.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Value-network%20Based%20Approach%20for%20Multi-Driver%20Order%20Dispatching                                                                                  A Deep Value-network Based Approach for Multi-Driver Order Dispatching                                                                                  Recent works on ride-sharing order dispatching have highlighted the importance of taking into account both the spatial and temporal dynamics in the dispatching process for improving the transportation system efficiency. At the same time, deep reinforcement learning has advanced to the point where it achieves superhuman performance in a number of fields. In this work, we propose a deep reinforcement learning based solution for order dispatching and we conduct large scale online A/B tests on DiDi's ride-dispatching platform to show that the proposed method achieves significant improvement on both total driver income and user experience related metrics. In particular, we model the ride dispatching problem as a Semi Markov Decision Process to account for the temporal aspect of the dispatching actions. To improve the stability of the value iteration with nonlinear function approximators like neural networks, we propose Cerebellar Value Networks (CVNet) with a novel distributed state representation layer. We further derive a regularized policy evaluation scheme for CVNet that penalizes large Lipschitz constant of the value network for additional robustness against adversarial perturbation and noises. Finally, we adapt various transfer learning methods to CVNet for increased learning adaptability and efficiency across multiple cities. We conduct extensive offline simulations based on real dispatching data as well as online AB tests through the DiDi's platform. Results show that CVNet consistently outperforms other recently proposed dispatching methods. We finally show that the performance can be further improved through the efficient use of transfer learning.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Variational%20Approach%20to%20Clustering%20Survival%20Data                                                                                  A Deep Variational Approach to Clustering Survival Data                                                                                  In this work, we study the problem of clustering survival data $-$ a challenging and so far under-explored task. We introduce a novel semi-supervised probabilistic approach to cluster survival data by leveraging recent advances in stochastic gradient variational inference. In contrast to previous work, our proposed method employs a deep generative model to uncover the underlying distribution of both the explanatory variables and censored survival times. We compare our model to the related work on clustering and mixture models for survival data in comprehensive experiments on a wide range of synthetic, semi-synthetic, and real-world datasets, including medical imaging data. Our method performs better at identifying clusters and is competitive at predicting survival times. Relying on novel generative assumptions, the proposed model offers a holistic perspective on clustering survival data and holds a promise of discovering subpopulations whose survival is regulated by different generative mechanisms.
http://w3id.org/mlsea/pwc/scientificWork/A%20Deep%20Variational%20Bayesian%20Framework%20for%20Blind%20Image%20Deblurring                                                                                  A Deep Variational Bayesian Framework for Blind Image Deblurring                                                                                  Blind image deblurring is an important yet very challenging problem in low-level vision. Traditional optimization based methods generally formulate this task as a maximum-a-posteriori estimation or variational inference problem, whose performance highly relies on the handcraft priors for both the latent image and the blur kernel. In contrast, recent deep learning methods generally learn, from a large collection of training images, deep neural networks (DNNs) directly mapping the blurry image to the clean one or to the blur kernel, paying less attention to the physical degradation process of the blurry image. In this paper, we present a deep variational Bayesian framework for blind image deblurring. Under this framework, the posterior of the latent clean image and blur kernel can be jointly estimated in an amortized inference fashion with DNNs, and the involved inference DNNs can be trained by fully considering the physical blur model, together with the supervision of data driven priors for the clean image and blur kernel, which is naturally led to by the evidence lower bound objective. Comprehensive experiments are conducted to substantiate the effectiveness of the proposed framework. The results show that it can not only achieve a promising performance with relatively simple networks, but also enhance the performance of existing DNNs for deblurring.
http://w3id.org/mlsea/pwc/scientificWork/A%20Derivation%20of%20Identifiable%20Condition%20for%20Non-Uniform%20Linear%20Array%20DOA%20Estimation                                                                                  A Derivation of Identifiable Condition for Non-Uniform Linear Array DOA Estimation                                                                                  Phase ambiguity happens in uniform linear arrays (ULAs) when the sensor distance is greater than $ lambda/2$. This problem in direction of arrival (DOA) estimation and can be solved by designing a proper sensor configuration. In this work, we derive the identifiable condition for ULA DOA estimation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Description%20Logic%20for%20Analogical%20Reasoning                                                                                  A Description Logic for Analogical Reasoning                                                                                  Ontologies formalise how the concepts from a given domain are interrelated. Despite their clear potential as a backbone for explainable AI, existing ontologies tend to be highly incomplete, which acts as a significant barrier to their more widespread adoption. To mitigate this issue, we present a mechanism to infer plausible missing knowledge, which relies on reasoning by analogy. To the best of our knowledge, this is the first paper that studies analogical reasoning within the setting of description logic ontologies. After showing that the standard formalisation of analogical proportion has important limitations in this setting, we introduce an alternative semantics based on bijective mappings between sets of features. We then analyse the properties of analogies under the proposed semantics, and show among others how it enables two plausible inference patterns: rule translation and rule extrapolation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Design%20Space%20Study%20for%20LISTA%20and%20Beyond                                                                                  A Design Space Study for LISTA and Beyond                                                                                  In recent years, great success has been witnessed in building problem-specific deep networks from unrolling iterative algorithms, for solving inverse problems and beyond. Unrolling is believed to incorporate the model-based prior with the learning capacity of deep learning. This paper revisits the role of unrolling as a design approach for deep networks: to what extent its resulting special architecture is superior, and can we find better? Using LISTA for sparse recovery as a representative example, we conduct the first thorough design space study for the unrolled models. Among all possible variations, we focus on extensively varying the connectivity patterns and neuron types, leading to a gigantic design space arising from LISTA. To efficiently explore this space and identify top performers, we leverage the emerging tool of neural architecture search (NAS). We carefully examine the searched top architectures in a number of settings, and are able to discover networks that are consistently better than LISTA. We further present more visualization and analysis to 'open the black box', and find that the searched top architectures demonstrate highly consistent and potentially transferable patterns. We hope our study to spark more reflections and explorations on how to better mingle model-based optimization prior and data-driven learning.
http://w3id.org/mlsea/pwc/scientificWork/A%20Design-Based%20Perspective%20on%20Synthetic%20Control%20Methods                                                                                  A Design-Based Perspective on Synthetic Control Methods                                                                                  Since their introduction in Abadie and Gardeazabal (2003), Synthetic Control (SC) methods have quickly become one of the leading methods for estimating causal effects in observational studies in settings with panel data. Formal discussions often motivate SC methods by the assumption that the potential outcomes were generated by a factor model. Here we study SC methods from a design-based perspective, assuming a model for the selection of the treated unit(s) and period(s). We show that the standard SC estimator is generally biased under random assignment. We propose a Modified Unbiased Synthetic Control (MUSC) estimator that guarantees unbiasedness under random assignment and derive its exact, randomization-based, finite-sample variance. We also propose an unbiased estimator for this variance. We document in settings with real data that under random assignment, SC-type estimators can have root mean-squared errors that are substantially lower than that of other common estimators. We show that such an improvement is weakly guaranteed if the treated period is similar to the other periods, for example, if the treated period was randomly selected. While our results only directly apply in settings where treatment is assigned randomly, we believe that they can complement model-based approaches even for observational studies.
http://w3id.org/mlsea/pwc/scientificWork/A%20Detector-oblivious%20Multi-arm%20Network%20for%20Keypoint%20Matching                                                                                  A Detector-oblivious Multi-arm Network for Keypoint Matching                                                                                  This paper presents a matching network to establish point correspondence between images. We propose a Multi-Arm Network (MAN) to learn region overlap and depth, which can greatly improve the keypoint matching robustness while bringing little computational cost during the inference stage. Another design that makes this framework different from many existing learning based pipelines that require re-training when a different keypoint detector is adopted, our network can directly work with different keypoint detectors without such a time-consuming re-training process. Comprehensive experiments conducted on outdoor and indoor datasets demonstrated that our proposed MAN outperforms state-of-the-art methods. Code will be made publicly available.
http://w3id.org/mlsea/pwc/scientificWork/A%20Diffeomorphic%20Aging%20Model%20for%20Adult%20Human%20Brain%20from%20Cross-Sectional%20Data                                                                                  A Diffeomorphic Aging Model for Adult Human Brain from Cross-Sectional Data                                                                                  Normative aging trends of the brain can serve as an important reference in the assessment of neurological structural disorders. Such models are typically developed from longitudinal brain image data -- follow-up data of the same subject over different time points. In practice, obtaining such longitudinal data is difficult. We propose a method to develop an aging model for a given population, in the absence of longitudinal data, by using images from different subjects at different time points, the so-called cross-sectional data. We define an aging model as a diffeomorphic deformation on a structural template derived from the data and propose a method that develops topology preserving aging model close to natural aging. The proposed model is successfully validated on two public cross-sectional datasets which provide templates constructed from different sets of subjects at different age points.
http://w3id.org/mlsea/pwc/scientificWork/A%20Differentiable%20Point%20Process%20with%20Its%20Application%20to%20Spiking%20Neural%20Networks                                                                                  A Differentiable Point Process with Its Application to Spiking Neural Networks                                                                                  This paper is concerned about a learning algorithm for a probabilistic model of spiking neural networks (SNNs). Jimenez Rezende & Gerstner (2014) proposed a stochastic variational inference algorithm to train SNNs with hidden neurons. The algorithm updates the variational distribution using the score function gradient estimator, whose high variance often impedes the whole learning algorithm. This paper presents an alternative gradient estimator for SNNs based on the path-wise gradient estimator. The main technical difficulty is a lack of a general method to differentiate a realization of an arbitrary point process, which is necessary to derive the path-wise gradient estimator. We develop a differentiable point process, which is the technical highlight of this paper, and apply it to derive the path-wise gradient estimator for SNNs. We investigate the effectiveness of our gradient estimator through numerical simulation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Differential-Cascaded%20Approach%20for%20Adaptive%20Control%20of%20Robot%20Manipulators                                                                                  A Differential-Cascaded Approach for Adaptive Control of Robot Manipulators                                                                                  This paper investigates adaptive control of nonlinear robot manipulators with parametric uncertainty. Motivated by generating closed-loop robot dynamics with enhanced transmission capability of a reference torque and with connection to linear dynamics, we develop a new adaptive approach by exploiting forwardstepping design and inertia invariance, yielding differential-cascaded closed-loop dynamics. With this approach, we propose a new class of adaptive controllers for nonlinear robot manipulators. Our particular study concerning adaptive control of robots exhibits a design methodology towards establishing the connection between adaptive control of highly nonlinear uncertain systems (e.g., with a variable inertia matrix) and linear dynamics (typically with the same or increased order), which is a long-standing intractable issue in the literature.
http://w3id.org/mlsea/pwc/scientificWork/A%20Digital%20Twin%20of%20a%20Compartmental%20Epidemiological%20Model%20based%20on%20a%20Stieltjes%20Differential%20Equation                                                                                  A Digital Twin of a Compartmental Epidemiological Model based on a Stieltjes Differential Equation                                                                                  We introduce a digital twin of the classical compartmental SIR (Susceptible, Infected, Recovered) epidemic model and study the interrelation between the digital twin and the system. In doing so, we use Stieltjes derivatives to feed the data from the real system to the virtual model which, in return, improves it in real time. As a byproduct of the model, we present a precise mathematical definition of solution to the problem. We also analyze the existence and uniqueness of solutions, introduce the concept of Main Digital Twin and present some numerical simulations with real data of the COVID-19 epidemic, showing the accuracy of the proposed ideas.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dimension-Insensitive%20Algorithm%20for%20Stochastic%20Zeroth-Order%20Optimization                                                                                  A Dimension-Insensitive Algorithm for Stochastic Zeroth-Order Optimization                                                                                  This paper concerns a convex, stochastic zeroth-order optimization (S-ZOO) problem. The objective is to minimize the expectation of a cost function whose gradient is not directly accessible. For this problem, traditional optimization algorithms mostly yield query complexities that grow polynomially with dimensionality (the number of decision variables). Consequently, these methods may not perform well in solving massive-dimensional problems arising in many modern applications. Although more recent methods can be provably dimension-insensitive, almost all of them require arguably more stringent conditions such as everywhere sparse or compressible gradient. In this paper, we propose a sparsity-inducing stochastic gradient-free (SI-SGF) algorithm, which provably yields a dimension-free (up to a logarithmic term) query complexity in both convex and strongly convex cases. Such insensitivity to the dimensionality growth is proven, for the first time, to be achievable when neither gradient sparsity nor gradient compressibility is satisfied. Our numerical results demonstrate a consistency between our theoretical prediction and the empirical performance.
http://w3id.org/mlsea/pwc/scientificWork/A%20Direct%20Slip%20Ratio%20Estimation%20Method%20based%20on%20an%20Intelligent%20Tire%20and%20Machine%20Learning                                                                                  A Direct Slip Ratio Estimation Method based on an Intelligent Tire and Machine Learning                                                                                  Accurate estimation of the tire slip ratio is critical for vehicle safety, as it is necessary for vehicle control purposes. In this paper, an intelligent tire system is presented to develop a novel slip ratio estimation model using machine learning algorithms. The accelerations, generated by a triaxial accelerometer installed onto the inner liner of the tire, are varied when the tire rotates to update the contact patch. Meanwhile, the slip ratio reference value can be measured by the MTS Flat-Trac tire test platform. Then, by analyzing the variation between the accelerations and slip ratio, highly useful features are discovered, which are especially promising for assessing vertical acceleration. For these features, machine learning (ML) algorithms are trained to build the slip ratio estimation model, in which the ML algorithms include artificial neural networks (ANNs), gradient boosting machines (GBMs), random forests (RFs), and support vector machines (SVMs). Finally, the estimated NRMS errors are evaluated using 10-fold cross-validation (CV). The proposed estimation model is able to estimate the slip ratio continuously and stably using only the acceleration from the intelligent tire system, and the estimated slip ratio range can reach 30%. The estimation results have high robustness to vehicle velocity and load, where the best NRMS errors can reach 4.88%. In summary, the present study with the fusion of an intelligent tire system and machine learning paves the way for the accurate estimation of the tire slip ratio under different driving conditions, which create new opportunities for autonomous vehicles, intelligent tires, and tire slip ratio estimation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dirichlet%20Multinomial%20Mixture%20Model-based%20Approach%20for%20Short%20Text%20Clustering                                                                                  A Dirichlet Multinomial Mixture Model-based Approach for Short Text Clustering                                                                                  Short text clustering has become an increasingly important task with the popularity of social media like Twitter,Google+, and Facebook. It is a challenging problem due to its sparse, high-dimensional, and large-volume characteristics. In this paper, we proposed a collapsed Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture model for short text clustering (abbr. to GSDMM). We found that GSDMM can infer the number of clusters automatically with a good balance between the completeness and homogeneity of the clustering results, and is fast to converge. GSDMM can also cope with the sparse and high-dimensional problem of short texts, and can obtain the representative words of each cluster. Our extensive experimental study shows that GSDMM can achieve significantly better performance than three other clustering models.
http://w3id.org/mlsea/pwc/scientificWork/A%20Discontinuity%20Capturing%20Shallow%20Neural%20Network%20for%20Elliptic%20Interface%20Problems                                                                                  A Discontinuity Capturing Shallow Neural Network for Elliptic Interface Problems                                                                                  In this paper, a new Discontinuity Capturing Shallow Neural Network (DCSNN) for approximating $d$-dimensional piecewise continuous functions and for solving elliptic interface problems is developed. There are three novel features in the present network; namely, (i) jump discontinuities are accurately captured, (ii) it is completely shallow, comprising only one hidden layer, (iii) it is completely mesh-free for solving partial differential equations. The crucial idea here is that a $d$-dimensional piecewise continuous function can be extended to a continuous function defined in $(d+1)$-dimensional space, where the augmented coordinate variable labels the pieces of each sub-domain. We then construct a shallow neural network to express this new function. Since only one hidden layer is employed, the number of training parameters (weights and biases) scales linearly with the dimension and the neurons used in the hidden layer. For solving elliptic interface problems, the network is trained by minimizing the mean square error loss that consists of the residual of the governing equation, boundary condition, and the interface jump conditions. We perform a series of numerical tests to demonstrate the accuracy of the present network. Our DCSNN model is efficient due to only a moderate number of parameters needed to be trained (a few hundred parameters used throughout all numerical examples), and the results indicate good accuracy. Compared with the results obtained by the traditional grid-based immersed interface method (IIM), which is designed particularly for elliptic interface problems, our network model shows a better accuracy than IIM. We conclude by solving a six-dimensional problem to demonstrate the capability of the present network for high-dimensional applications.
http://w3id.org/mlsea/pwc/scientificWork/A%20Discrete%20Variational%20Derivation%20of%20Accelerated%20Methods%20in%20Optimization                                                                                  A Discrete Variational Derivation of Accelerated Methods in Optimization                                                                                  Many of the new developments in machine learning are connected with gradient-based optimization methods. Recently, these methods have been studied using a variational perspective. This has opened up the possibility of introducing variational and symplectic methods using geometric integration. In particular, in this paper, we introduce variational integrators which allow us to derive different methods for optimization. Using both, Hamilton's and Lagrange-d'Alembert's principle, we derive two families of respective optimization methods in one-to-one correspondence that generalize Polyak's heavy ball and the well known Nesterov accelerated gradient method, the second of which mimics the behavior of the first reducing the oscillations of classical momentum methods. However, since the systems considered are explicitly time-dependent, the preservation of symplecticity of autonomous systems occurs here solely on the fibers. Several experiments exemplify the result.
http://w3id.org/mlsea/pwc/scientificWork/A%20Discrete-time%20Reputation-based%20Resilient%20Consensus%20Algorithm%20for%20Synchronous%20or%20Asynchronous%20Communications                                                                                  A Discrete-time Reputation-based Resilient Consensus Algorithm for Synchronous or Asynchronous Communications                                                                                  We tackle the problem of a set of agents achieving resilient consensus in the presence of attacked agents. We present a discrete-time reputation-based consensus algorithm for synchronous and asynchronous networks by developing a local strategy where, at each time, each agent assigns a reputation (between zero and one) to each neighbor. The reputation is then used to weigh the neighbors' values in the update of its state. Under mild assumptions, we show that: (i) the proposed method converges exponentially to the consensus of the regular agents; (ii) if a regular agent identifies a neighbor as an attacked node, then it is indeed an attacked node; (iii) if the consensus value of the normal nodes differs from that of any of the attacked nodes' values, then the reputation that a regular agent assigns to the attacked neighbors goes to zero. Further, we extend our method to achieve resilience in the scenarios where there are noisy nodes, dynamic networks and stochastic node selection. Finally, we illustrate our algorithm with several examples, and we delineate some attacking scenarios that can be dealt by the current proposal but not by the state-of-the-art approaches.
http://w3id.org/mlsea/pwc/scientificWork/A%20Discriminative%20Entity-Aware%20Language%20Model%20for%20Virtual%20Assistants                                                                                  A Discriminative Entity-Aware Language Model for Virtual Assistants                                                                                  High-quality automatic speech recognition (ASR) is essential for virtual assistants (VAs) to work well. However, ASR often performs poorly on VA requests containing named entities. In this work, we start from the observation that many ASR errors on named entities are inconsistent with real-world knowledge. We extend previous discriminative n-gram language modeling approaches to incorporate real-world knowledge from a Knowledge Graph (KG), using features that capture entity type-entity and entity-entity relationships. We apply our model through an efficient lattice rescoring process, achieving relative sentence error rate reductions of more than 25% on some synthesized test sets covering less popular entities, with minimal degradation on a uniformly sampled VA test set.
http://w3id.org/mlsea/pwc/scientificWork/A%20Discussion%20On%20the%20Validity%20of%20Manifold%20Learning                                                                                  A Discussion On the Validity of Manifold Learning                                                                                  Dimensionality reduction (DR) and manifold learning (ManL) have been applied extensively in many machine learning tasks, including signal processing, speech recognition, and neuroinformatics. However, the understanding of whether DR and ManL models can generate valid learning results remains unclear. In this work, we investigate the validity of learning results of some widely used DR and ManL methods through the chart mapping function of a manifold. We identify a fundamental problem of these methods: the mapping functions induced by these methods violate the basic settings of manifolds, and hence they are not learning manifold in the mathematical sense. To address this problem, we provide a provably correct algorithm called fixed points Laplacian mapping (FPLM), that has the geometric guarantee to find a valid manifold representation (up to a homeomorphism). Combining one additional condition(orientation preserving), we discuss a sufficient condition for an algorithm to be bijective for any d-simplex decomposition result on a d-manifold. However, constructing such a mapping function and its computational method satisfying these conditions is still an open problem in mathematics.
http://w3id.org/mlsea/pwc/scientificWork/A%20Discussion%20on%20Building%20Practical%20NLP%20Leaderboards%3A%20The%20Case%20of%20Machine%20Translation                                                                                  A Discussion on Building Practical NLP Leaderboards: The Case of Machine Translation                                                                                  Recent advances in AI and ML applications have benefited from rapid progress in NLP research. Leaderboards have emerged as a popular mechanism to track and accelerate progress in NLP through competitive model development. While this has increased interest and participation, the over-reliance on single, and accuracy-based metrics have shifted focus from other important metrics that might be equally pertinent to consider in real-world contexts. In this paper, we offer a preliminary discussion of the risks associated with focusing exclusively on accuracy metrics and draw on recent discussions to highlight prescriptive suggestions on how to develop more practical and effective leaderboards that can better reflect the real-world utility of models.
http://w3id.org/mlsea/pwc/scientificWork/A%20Distance%20Covariance-based%20Estimator                                                                                  A Distance Covariance-based Estimator                                                                                  Weak instruments present a major setback to empirical work. This paper introduces an estimator that admits weak, uncorrelated, or mean-independent instruments that are non-independent of endogenous covariates. Relative to conventional instrumental variable methods, the proposed estimator weakens the relevance condition considerably without imposing a stronger exclusion restriction. Identification mainly rests on (1) a weak conditional median exclusion restriction imposed on pairwise differences in disturbances and (2) non-independence between covariates and instruments. Under mild conditions, the estimator is consistent and asymptotically normal. Monte Carlo experiments showcase an excellent performance of the estimator, and two empirical examples illustrate its practical utility.
http://w3id.org/mlsea/pwc/scientificWork/A%20Distance%20Covariance-based%20Kernel%20for%20Nonlinear%20Causal%20Clustering%20in%20Heterogeneous%20Populations                                                                                  A Distance Covariance-based Kernel for Nonlinear Causal Clustering in Heterogeneous Populations                                                                                  We consider the problem of causal structure learning in the setting of heterogeneous populations, i.e., populations in which a single causal structure does not adequately represent all population members, as is common in biological and social sciences. To this end, we introduce a distance covariance-based kernel designed specifically to measure the similarity between the underlying nonlinear causal structures of different samples. Indeed, we prove that the corresponding feature map is a statistically consistent estimator of nonlinear independence structure, rendering the kernel itself a statistical test for the hypothesis that sets of samples come from different generating causal structures. Even stronger, we prove that the kernel space is isometric to the space of causal ancestral graphs, so that distance between samples in the kernel space is guaranteed to correspond to distance between their generating causal structures. This kernel thus enables us to perform clustering to identify the homogeneous subpopulations, for which we can then learn causal structures using existing methods. Though we focus on the theoretical aspects of the kernel, we also evaluate its performance on synthetic data and demonstrate its use on a real gene expression data set.
http://w3id.org/mlsea/pwc/scientificWork/A%20Distance-based%20Separability%20Measure%20for%20Internal%20Cluster%20Validation                                                                                  A Distance-based Separability Measure for Internal Cluster Validation                                                                                  To evaluate clustering results is a significant part of cluster analysis. Since there are no true class labels for clustering in typical unsupervised learning, many internal cluster validity indices (CVIs), which use predicted labels and data, have been created. Without true labels, to design an effective CVI is as difficult as to create a clustering method. And it is crucial to have more CVIs because there are no universal CVIs that can be used to measure all datasets and no specific methods of selecting a proper CVI for clusters without true labels. Therefore, to apply a variety of CVIs to evaluate clustering results is necessary. In this paper, we propose a novel internal CVI -- the Distance-based Separability Index (DSI), based on a data separability measure. We compared the DSI with eight internal CVIs including studies from early Dunn (1974) to most recent CVDD (2019) and an external CVI as ground truth, by using clustering results of five clustering algorithms on 12 real and 97 synthetic datasets. Results show DSI is an effective, unique, and competitive CVI to other compared CVIs. We also summarized the general process to evaluate CVIs and created the rank-difference metric for comparison of CVIs' results.
http://w3id.org/mlsea/pwc/scientificWork/A%20Distributed%20Active%20Perception%20Strategy%20for%20Source%20Seeking%20and%20Level%20Curve%20Tracking                                                                                  A Distributed Active Perception Strategy for Source Seeking and Level Curve Tracking                                                                                  Algorithms for multi-agent systems to locate a source or to follow a desired level curve of spatially distributed scalar fields generally require sharing field measurements among the agents for gradient estimation. Yet, in this paper, we propose a distributed active perception strategy that enables swarms of various sizes and graph structures to perform source seeking and level curve tracking without the need to explicitly estimate the field gradient or explicitly share measurements. The proposed method utilizes a consensus-like Principal Component Analysis perception algorithm that does not require explicit communication in order to compute a local body frame. This body frame is used to design a distributed control law where each agent modulates its motion based only on its instantaneous field measurement. Several stability results are obtained within a singular perturbation framework which justifies the convergence and robustness of the strategy. Additionally, efficiency is validated through various computer simulations and robots implementation in $2$-D scalar fields. The active perception strategy leverages the available local information and has the potential to be used in various applications such as modeling information propagation in biological and robotic swarms.
http://w3id.org/mlsea/pwc/scientificWork/A%20Distributed%20Power%20Control%20Algorithm%20for%20Energy%20Efficiency%20Maximization%20in%20Wireless%20Cellular%20Networks                                                                                  A Distributed Power Control Algorithm for Energy Efficiency Maximization in Wireless Cellular Networks                                                                                  In this paper, we propose a distributed power control algorithm for addressing the global energy efficiency (GEE) maximization problem subject to satisfying a minimum target SINR for all user equipments (UEs) in wireless cellular networks. We state the problem as a multi-objective optimization problem which targets minimizing total power consumption and maximizing total throughput, simultaneously, while a minimum target SINR is guaranteed for all UEs. We propose an iterative scheme executed in the UEs to control their transmit power using individual channel state information (CSI) such that the GEE is maximized in a distributed manner. We prove the convergence of the proposed iterative algorithm to its corresponding unique fixed point also shown by our numerical results. Additionally, simulation results demonstrate that our proposed scheme outperforms other algorithms in the literature and performs like the centralized algorithm executed in the base station and maximizes the GEE using the global CSI.
http://w3id.org/mlsea/pwc/scientificWork/A%20DoA%20Estimation%20Based%20Robust%20Beam%20Forming%20Method%20for%20UAV-BS%20Communication                                                                                  A DoA Estimation Based Robust Beam Forming Method for UAV-BS Communication                                                                                  High data rate communication with Unmanned Aerial Vehicles (UAV) is of growing demand among industrial and commercial applications since the last decade. In this paper, we investigate enhancing beam forming performance based on signal Direction of Arrival (DoA) estimation to support UAV-cellular network communication. We first study UAV fast moving scenario where we found that drone's mobility cause degradation of beam forming algorithm performance. Then, we propose a DoA estimation algorithm and a steering vector adaptive receiving beam forming method. The DoA estimation algorithm is of high precision with low computational complexity. Also it enables a beam former to timely adjust steering vector value in calculating beam forming weight. Simulation results show higher SINR performance and more stability of proposed method than traditional method based on Multiple Signal Classification (MUSIC) DoA estimation algorithm.
http://w3id.org/mlsea/pwc/scientificWork/A%20Domain-Oblivious%20Approach%20for%20Learning%20Concise%20Representations%20of%20Filtered%20Topological%20Spaces%20for%20Clustering                                                                                  A Domain-Oblivious Approach for Learning Concise Representations of Filtered Topological Spaces for Clustering                                                                                  Persistence diagrams have been widely used to quantify the underlying features of filtered topological spaces in data visualization. In many applications, computing distances between diagrams is essential; however, computing these distances has been challenging due to the computational cost. In this paper, we propose a persistence diagram hashing framework that learns a binary code representation of persistence diagrams, which allows for fast computation of distances. This framework is built upon a generative adversarial network (GAN) with a diagram distance loss function to steer the learning process. Instead of using standard representations, we hash diagrams into binary codes, which have natural advantages in large-scale tasks. The training of this model is domain-oblivious in that it can be computed purely from synthetic, randomly created diagrams. As a consequence, our proposed method is directly applicable to various datasets without the need for retraining the model. These binary codes, when compared using fast Hamming distance, better maintain topological similarity properties between datasets than other vectorized representations. To evaluate this method, we apply our framework to the problem of diagram clustering and we compare the quality and performance of our approach to the state-of-the-art. In addition, we show the scalability of our approach on a dataset with 10k persistence diagrams, which is not possible with current techniques. Moreover, our experimental results demonstrate that our method is significantly faster with the potential of less memory usage, while retaining comparable or better quality comparisons.
http://w3id.org/mlsea/pwc/scientificWork/A%20Downlink%20Puncturing%20Scheme%20for%20Simultaneous%20Transmission%20of%20URLLC%20and%20eMBB%20Traffic%20by%20Exploiting%20Data%20Similarity                                                                                  A Downlink Puncturing Scheme for Simultaneous Transmission of URLLC and eMBB Traffic by Exploiting Data Similarity                                                                                  Ultra Reliable and Low Latency Communications (URLLC) is deemed to be an essential service in 5G systems and beyond to accommodate a wide range of emerging applications with stringent latency and reliability requirements. Coexistence of URLLC alongside other service categories calls for developing spectrally efficient multiplexing techniques. Specifically, coupling URLLC and conventional enhanced Mobile BroadBand (eMBB) through superposition/puncturing naturally arises as a promising option due to the tolerance of the latter in terms of latency and reliability. The idea here is to transmit URLLC packets over resources occupied by ongoing eMBB transmissions while minimizing the impact on the eMBB transmissions. In this paper, we propose a novel downlink URLLC-eMBB multiplexing technique that exploits possible similarities among URLLC and eMBB symbols, with the objective of reducing the size of the punctured eMBB symbols. We propose that the base station scans the eMBB traffic' symbol sequences and punctures those that have the highest symbol similarity with that of the URLLC users to be served. As the eMBB and URLLC may use different constellation sizes, we introduce the concept of symbol region similarity to accommodate the different constellations. We assess the performance of the proposed scheme analytically, where we derive closed-form expressions for the symbol error rate (SER) of the eMBB and URLLC services. {We also derive an expression for the eMBB loss function due to puncturing in terms of the eMBB SER}. We demonstrate through numerical and simulation results the efficacy of the proposed scheme where we show that 1) the eMBB spectral efficiency is improved by puncturing fewer symbols, 2) the SER and reliability performance of eMBB are improved, and 3) the URLLC data is accommodated within the specified delay constraint while maintaining its reliability.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dual%20Process%20Model%20for%20Optimizing%20Cross%20Entropy%20in%20Neural%20Networks                                                                                  A Dual Process Model for Optimizing Cross Entropy in Neural Networks                                                                                  Minimizing cross-entropy is a widely used method for training artificial neural networks. Many training procedures based on backpropagation use cross-entropy directly as their loss function. Instead, this theoretical essay investigates a dual process model with two processes, in which one process minimizes the Kullback-Leibler divergence while its dual counterpart minimizes the Shannon entropy. Postulating that learning consists of two dual processes complementing each other, the model defines an equilibrium state for both processes in which the loss function assumes its minimum. An advantage of the proposed model is that it allows deriving the optimal learning rate and momentum weight to update network weights for backpropagation. Furthermore, the model introduces the golden ratio and complex numbers as important new concepts in machine learning.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dual-Attention%20Network%20for%20Joint%20Named%20Entity%20Recognition%20and%20Sentence%20Classification%20of%20Adverse%20Drug%20Events                                                                                  A Dual-Attention Network for Joint Named Entity Recognition and Sentence Classification of Adverse Drug Events                                                                                  An adverse drug event (ADE) is an injury resulting from medical intervention related to a drug. Automatic ADE detection from text is either fine-grained (ADE entity recognition) or coarse-grained (ADE assertive sentence classification), with limited efforts leveraging inter-dependencies among the two granularities. We instead propose a multi-grained joint deep network to concurrently learn the ADE entity recognition and ADE sentence classification tasks. Our joint approach takes advantage of their symbiotic relationship, with a transfer of knowledge between the two levels of granularity. Our dual-attention mechanism constructs multiple distinct representations of a sentence that capture both task-specific and semantic information in the sentence, providing stronger emphasis on the key elements essential for sentence classification. Our model improves state-of- art F1-score for both tasks: (i) entity recognition of ADE words (12.5{ %} increase) and (ii) ADE sentence classification (13.6{ %} increase) on MADE 1.0 benchmark of EHR notes.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dual-Critic%20Reinforcement%20Learning%20Framework%20for%20Frame-level%20Bit%20Allocation%20in%20HEVC%2FH.265                                                                                  A Dual-Critic Reinforcement Learning Framework for Frame-level Bit Allocation in HEVC/H.265                                                                                  This paper introduces a dual-critic reinforcement learning (RL) framework to address the problem of frame-level bit allocation in HEVC/H.265. The objective is to minimize the distortion of a group of pictures (GOP) under a rate constraint. Previous RL-based methods tackle such a constrained optimization problem by maximizing a single reward function that often combines a distortion and a rate reward. However, the way how these rewards are combined is usually ad hoc and may not generalize well to various coding conditions and video sequences. To overcome this issue, we adapt the deep deterministic policy gradient (DDPG) reinforcement learning algorithm for use with two critics, with one learning to predict the distortion reward and the other the rate reward. In particular, the distortion critic works to update the agent when the rate constraint is satisfied. By contrast, the rate critic makes the rate constraint a priority when the agent goes over the bit budget. Experimental results on commonly used datasets show that our method outperforms the bit allocation scheme in x265 and the single-critic baseline by a significant margin in terms of rate-distortion performance while offering fairly precise rate control.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dual-Function%20Radar%20Communication%20System%20With%20OFDM%20Waveforms%20and%20Subcarrier%20Sharing                                                                                  A Dual-Function Radar Communication System With OFDM Waveforms and Subcarrier Sharing                                                                                  A novel monostatic multiple-input multiple-output (MIMO) dual-function radar communication (DFRC) system is proposed, that uses the available bandwidth efficiently for both sensing and communication. The proposed system, referred to as Shared-Subcarriers DFRC (SS-DFRC) transmits wideband, orthogonal frequency division multiplexing (OFDM) waveforms and allows the transmit antennas to use subcarriers in a shared fashion. A novel, low complexity target estimation approach is proposed to overcome the coupling of radar target parameters and transmitted symbols that arises in that case. When all subcarriers are used in a shared fashion, the proposed system achieves high communication rate but its sensing performance is limited by the size of the receive array. We show that when some of the subcarriers are reserved for exclusive use by one transmit antenna each (private subcarriers), the communication rate can be traded off for improved sensing performance. The latter can be achieved by using the private subcarriers to construct a larger aperture virtual array that yields higher resolution angle estimates. The system is endowed with beamforming capability, via waveform precoding, where the precoding matrix is optimally designed to meet a joint sensing-communication system performance metric.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dual-Questioning%20Attention%20Network%20for%20Emotion-Cause%20Pair%20Extraction%20with%20Context%20Awareness                                                                                  A Dual-Questioning Attention Network for Emotion-Cause Pair Extraction with Context Awareness                                                                                  Emotion-cause pair extraction (ECPE), an emerging task in sentiment analysis, aims at extracting pairs of emotions and their corresponding causes in documents. This is a more challenging problem than emotion cause extraction (ECE), since it requires no emotion signals which are demonstrated as an important role in the ECE task. Existing work follows a two-stage pipeline which identifies emotions and causes at the first step and pairs them at the second step. However, error propagation across steps and pair combining without contextual information limits the effectiveness. Therefore, we propose a Dual-Questioning Attention Network to alleviate these limitations. Specifically, we question candidate emotions and causes to the context independently through attention networks for a contextual and semantical answer. Also, we explore how weighted loss functions in controlling error propagation between steps. Empirical results show that our method performs better than baselines in terms of multiple evaluation metrics. The source code can be obtained at https://github.com/QixuanSun/DQAN.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dynamic%20Battery%20State-of-Health%20Forecasting%20Model%20for%20Electric%20Trucks%3A%20Li-Ion%20Batteries%20Case-Study                                                                                  A Dynamic Battery State-of-Health Forecasting Model for Electric Trucks: Li-Ion Batteries Case-Study                                                                                  It is of extreme importance to monitor and manage the battery health to enhance the performance and decrease the maintenance cost of operating electric vehicles. This paper concerns the machine-learning-enabled state-of-health (SoH) prognosis for Li-ion batteries in electric trucks, where they are used as energy sources. The paper proposes methods to calculate SoH and cycle life for the battery packs. We propose autoregressive integrated modeling average (ARIMA) and supervised learning (bagging with decision tree as the base estimator; BAG) for forecasting the battery SoH in order to maximize the battery availability for forklift operations. As the use of data-driven methods for battery prognostics is increasing, we demonstrate the capabilities of ARIMA and under circumstances when there is little prior information available about the batteries. For this work, we had a unique data set of 31 lithium-ion battery packs from forklifts in commercial operations. On the one hand, results indicate that the developed ARIMA model provided relevant tools to analyze the data from several batteries. On the other hand, BAG model results suggest that the developed supervised learning model using decision trees as base estimator yields better forecast accuracy in the presence of large variation in data for one battery.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dynamic%20Response%20Recovery%20Framework%20Using%20Ambient%20Synchrophasor%20Data                                                                                  A Dynamic Response Recovery Framework Using Ambient Synchrophasor Data                                                                                  Wide-area dynamic studies are of paramount importance to ensure the stability and reliability of power grids. The rising deployment synchrophasor and other sensing technologies has made data-driven modeling and analysis possible using the synchronized fast-rate dynamic measurements. This paper presents a general model-free framework of inferring the grid dynamic responses using the ubiquitous ambient data collected during normal grid operations. Building upon the second-order dynamic model, we have established the connection from the cross-correlation of various types of angle, frequency, and line flow data at any two locations, to their corresponding dynamic responses. The theoretical results enabled a fully data-driven framework for estimating the latter using real-time ambient data. Numerical results using the WSCC 9-bus system and a synthetic 2000-bus Texas system have demonstrated the effectiveness of proposed approaches for dynamic modeling of realistic power systems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dynamic%20Spatial-temporal%20Attention%20Network%20for%20Early%20Anticipation%20of%20Traffic%20Accidents                                                                                  A Dynamic Spatial-temporal Attention Network for Early Anticipation of Traffic Accidents                                                                                  The rapid advancement of sensor technologies and artificial intelligence are creating new opportunities for traffic safety enhancement. Dashboard cameras (dashcams) have been widely deployed on both human driving vehicles and automated driving vehicles. A computational intelligence model that can accurately and promptly predict accidents from the dashcam video will enhance the preparedness for accident prevention. The spatial-temporal interaction of traffic agents is complex. Visual cues for predicting a future accident are embedded deeply in dashcam video data. Therefore, the early anticipation of traffic accidents remains a challenge. Inspired by the attention behavior of humans in visually perceiving accident risks, this paper proposes a Dynamic Spatial-Temporal Attention (DSTA) network for the early accident anticipation from dashcam videos. The DSTA-network learns to select discriminative temporal segments of a video sequence with a Dynamic Temporal Attention (DTA) module. It also learns to focus on the informative spatial regions of frames with a Dynamic Spatial Attention (DSA) module. A Gated Recurrent Unit (GRU) is trained jointly with the attention modules to predict the probability of a future accident. The evaluation of the DSTA-network on two benchmark datasets confirms that it has exceeded the state-of-the-art performance. A thorough ablation study that assesses the DSTA-network at the component level reveals how the network achieves such performance. Furthermore, this paper proposes a method to fuse the prediction scores from two complementary models and verifies its effectiveness in further boosting the performance of early accident anticipation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dynamical%20Systems%20Perspective%20on%20Chimeric%20Antigen%20Receptor%20T-Cell%20Dosing                                                                                  A Dynamical Systems Perspective on Chimeric Antigen Receptor T-Cell Dosing                                                                                  Chimeric antigen receptor T cells (CAR T cells) are dosed similarly to donor lymphocyte infusions following hematopoietic cell transplantation. In this perspective paper a mathematical basis for personalized dosing of CAR T cells is introduced.
http://w3id.org/mlsea/pwc/scientificWork/A%20Dynamics%20Perspective%20of%20Pursuit-Evasion%20Games%20of%20Intelligent%20Agents%20with%20the%20Ability%20to%20Learn                                                                                  A Dynamics Perspective of Pursuit-Evasion Games of Intelligent Agents with the Ability to Learn                                                                                  Pursuit-evasion games are ubiquitous in nature and in an artificial world. In nature, pursuer(s) and evader(s) are intelligent agents that can learn from experience, and dynamics (i.e., Newtonian or Lagrangian) is vital for the pursuer and the evader in some scenarios. To this end, this paper addresses the pursuit-evasion game of intelligent agents from the perspective of dynamics. A bio-inspired dynamics formulation of a pursuit-evasion game and baseline pursuit and evasion strategies are introduced at first. Then, reinforcement learning techniques are used to mimic the ability of intelligent agents to learn from experience. Based on the dynamics formulation and reinforcement learning techniques, the effects of improving both pursuit and evasion strategies based on experience on pursuit-evasion games are investigated at two levels 1) individual runs and 2) ranges of the parameters of pursuit-evasion games. Results of the investigation are consistent with nature observations and the natural law - survival of the fittest. More importantly, with respect to the result of a pursuit-evasion game of agents with baseline strategies, this study achieves a different result. It is shown that, in a pursuit-evasion game with a dynamics formulation, an evader is not able to escape from a slightly faster pursuer with an effective learned pursuit strategy, based on agile maneuvers and an effective learned evasion strategy.
http://w3id.org/mlsea/pwc/scientificWork/A%20Facial%20Feature%20Discovery%20Framework%20for%20Race%20Classification%20Using%20Deep%20Learning                                                                                  A Facial Feature Discovery Framework for Race Classification Using Deep Learning                                                                                  Race classification is a long-standing challenge in the field of face image analysis. The investigation of salient facial features is an important task to avoid processing all face parts. Face segmentation strongly benefits several face analysis tasks, including ethnicity and race classification. We propose a raceclassification algorithm using a prior face segmentation framework. A deep convolutional neural network (DCNN) was used to construct a face segmentation model. For training the DCNN, we label face images according to seven different classes, that is, nose, skin, hair, eyes, brows, back, and mouth. The DCNN model developed in the first phase was used to create segmentation results. The probabilistic classification method is used, and probability maps (PMs) are created for each semantic class. We investigated five salient facial features from among seven that help in race classification. Features are extracted from the PMs of five classes, and a new model is trained based on the DCNN. We assessed the performance of the proposed race classification method on four standard face datasets, reporting superior results compared with previous studies.
http://w3id.org/mlsea/pwc/scientificWork/A%20Falta%20de%20Pan%2C%20Buenas%20Son%20Tortas%3A%20The%20Efficacy%20of%20Predicted%20UPOS%20Tags%20for%20Low%20Resource%20UD%20Parsing                                                                                  A Falta de Pan, Buenas Son Tortas: The Efficacy of Predicted UPOS Tags for Low Resource UD Parsing                                                                                  We evaluate the efficacy of predicted UPOS tags as input features for dependency parsers in lower resource settings to evaluate how treebank size affects the impact tagging accuracy has on parsing performance. We do this for real low resource universal dependency treebanks, artificially low resource data with varying treebank sizes, and for very small treebanks with varying amounts of augmented data. We find that predicted UPOS tags are somewhat helpful for low resource treebanks, especially when fewer fully-annotated trees are available. We also find that this positive impact diminishes as the amount of data increases.
http://w3id.org/mlsea/pwc/scientificWork/A%20Fast%20Deep%20Learning%20Network%20for%20Automatic%20Image%20Auto-Straightening                                                                                  A Fast Deep Learning Network for Automatic Image Auto-Straightening                                                                                  Rectifying the orientation of images represents a daily task for every photographer. This task may be complicated even for the human eye, especially when the horizon or other horizontal and vertical lines in the image are missing. In this paper we address this problem and propose a new deep learning network specially adapted for image rotation correction: we introduce the rectangle-shaped depthwise convolutions which are specialized in detecting long lines from the image and a new adapted loss function that addresses the problem of orientation errors. Compared to other methods that are able to detect rotation errors only on few image categories, like man-made structures, the proposed method can be used on a larger variety of photographs e.g., portraits, landscapes, sport, night photos etc. Moreover, the model is adapted to mobile devices and can be run in real time, both for pictures and for videos. An extensive evaluation of our model on different datasets shows that it remarkably generalizes, not being dependent on any particular type of image. Finally, we significantly outperform the state-of-the-art methods, providing superior results.
http://w3id.org/mlsea/pwc/scientificWork/A%20Fast%20Evidential%20Approach%20for%20Stock%20Forecasting                                                                                  A Fast Evidential Approach for Stock Forecasting                                                                                  Within the framework of evidence theory, the confidence functions of different information can be combined into a combined confidence function to solve uncertain problems. The Dempster combination rule is a classic method of fusing different information. This paper proposes a similar confidence function for the time point in the time series. The Dempster combination rule can be used to fuse the growth rate of the last time point, and finally a relatively accurate forecast data can be obtained. Stock price forecasting is a concern of economics. The stock price data is large in volume, and more accurate forecasts are required at the same time. The classic methods of time series, such as ARIMA, cannot balance forecasting efficiency and forecasting accuracy at the same time. In this paper, the fusion method of evidence theory is applied to stock price prediction. Evidence theory deals with the uncertainty of stock price prediction and improves the accuracy of prediction. At the same time, the fusion method of evidence theory has low time complexity and fast prediction processing speed.
http://w3id.org/mlsea/pwc/scientificWork/A%20Fast%20MR%20Fingerprinting%20Simulator%20for%20Direct%20Error%20Estimation%20and%20Sequence%20Optimization                                                                                  A Fast MR Fingerprinting Simulator for Direct Error Estimation and Sequence Optimization                                                                                  MR Fingerprinting is a novel quantitative MR technique that could simultaneously provide multiple tissue property maps. When optimizing MRF scans, modeling undersampling errors and field imperfections in cost functions will make the optimization results more practical and robust. However, this process is computationally expensive and impractical for sequence optimization algorithms when MRF signal evolutions need to be generated for each optimization iteration. Here, we introduce a fast MRF simulator to simulate aliased images from actual scan scenarios including undersampling and system imperfections, which substantially reduces computational time and allows for direct error estimation and efficient sequence optimization. By constraining the total number of tissues present in a brain phantom, MRF signals from highly undersampled scans can be simulated as the product of the spatial response functions based on sampling patterns and sequence-dependent temporal functions. During optimization, the spatial response function is independent of sequence design and does not need to be recalculated. We evaluate the performance and computational speed of the proposed approach by simulations and in vivo experiments. We also demonstrate the power of applying the simulator in MRF sequence optimization. The simulation results from the proposed method closely approximate the signals and MRF maps from in vivo scans, with 158 times shorter processing time than the conventional simulation method using Non-uniform Fourier transform. Incorporating the proposed simulator in the MRF optimization framework makes direct estimation of undersampling errors during the optimization process feasible, and provide optimized MRF sequences that are robust against undersampling factors and system inhomogeneity.
http://w3id.org/mlsea/pwc/scientificWork/A%20Fast%20Partial%20Video%20Copy%20Detection%20Using%20KNN%20and%20Global%20Feature%20Database                                                                                  A Fast Partial Video Copy Detection Using KNN and Global Feature Database                                                                                  We propose a fast partial video copy detection framework in this paper. In this framework all frame features of the reference videos are organized in a KNN searchable database. Instead of scanning all reference videos, the query video segment does a fast KNN search in the global feature database. The returned results are used to generate a short list of candidate videos. A modified temporal network is then used to localize the copy segment in the candidate videos. We evaluate different choice of CNN features on the VCDB dataset. Our benchmark F1 score exceeds the state of the art by a big margin.
http://w3id.org/mlsea/pwc/scientificWork/A%20Feature%20Fusion-Net%20Using%20Deep%20Spatial%20Context%20Encoder%20and%20Nonstationary%20Joint%20Statistical%20Model%20for%20High%20Resolution%20SAR%20Image%20Classification                                                                                  A Feature Fusion-Net Using Deep Spatial Context Encoder and Nonstationary Joint Statistical Model for High Resolution SAR Image Classification                                                                                  Convolutional neural networks (CNNs) have been applied to learn spatial features for high-resolution (HR) synthetic aperture radar (SAR) image classification. However, there has been little work on integrating the unique statistical distributions of SAR images which can reveal physical properties of terrain objects, into CNNs in a supervised feature learning framework. To address this problem, a novel end-to-end supervised classification method is proposed for HR SAR images by considering both spatial context and statistical features. First, to extract more effective spatial features from SAR images, a new deep spatial context encoder network (DSCEN) is proposed, which is a lightweight structure and can be effectively trained with a small number of samples. Meanwhile, to enhance the diversity of statistics, the nonstationary joint statistical model (NS-JSM) is adopted to form the global statistical features. Specifically, SAR images are transformed into the Gabor wavelet domain and the produced multi-subbands magnitudes and phases are modeled by the log-normal and uniform distribution. The covariance matrix is further utilized to capture the inter-scale and intra-scale nonstationary correlation between the statistical subbands and make the joint statistical features more compact and distinguishable. Considering complementary advantages, a feature fusion network (Fusion-Net) base on group compression and smooth normalization is constructed to embed the statistical features into the spatial features and optimize the fusion feature representation. As a result, our model can learn the discriminative features and improve the final classification performance. Experiments on four HR SAR images validate the superiority of the proposed method over other related algorithms.
http://w3id.org/mlsea/pwc/scientificWork/A%20Feature%20Selection%20Method%20for%20Multi-Dimension%20Time-Series%20Data                                                                                  A Feature Selection Method for Multi-Dimension Time-Series Data                                                                                  Time-series data in application areas such as motion capture and activity recognition is often multi-dimension. In these application areas data typically comes from wearable sensors or is extracted from video. There is a lot of redundancy in these data streams and good classification accuracy will often be achievable with a small number of features (dimensions). In this paper we present a method for feature subset selection on multidimensional time-series data based on mutual information. This method calculates a merit score (MSTS) based on correlation patterns of the outputs of classifiers trained on single features and the `best' subset is selected accordingly. MSTS was found to be significantly more efficient in terms of computational cost while also managing to maintain a good overall accuracy when compared to Wrapper-based feature selection, a feature selection strategy that is popular elsewhere in Machine Learning. We describe the motivations behind this feature selection strategy and evaluate its effectiveness on six time series datasets.
http://w3id.org/mlsea/pwc/scientificWork/A%20Federated%20Data-Driven%20Evolutionary%20Algorithm%20for%20Expensive%20Multi%2FMany-objective%20Optimization                                                                                  A Federated Data-Driven Evolutionary Algorithm for Expensive Multi/Many-objective Optimization                                                                                  Data-driven optimization has found many successful applications in the real world and received increased attention in the field of evolutionary optimization. Most existing algorithms assume that the data used for optimization is always available on a central server for construction of surrogates. This assumption, however, may fail to hold when the data must be collected in a distributed way and is subject to privacy restrictions. This paper aims to propose a federated data-driven evolutionary multi-/many-objective optimization algorithm. To this end, we leverage federated learning for surrogate construction so that multiple clients collaboratively train a radial-basis-function-network as the global surrogate. Then a new federated acquisition function is proposed for the central server to approximate the objective values using the global surrogate and estimate the uncertainty level of the approximated objective values based on the local models. The performance of the proposed algorithm is verified on a series of multi/many-objective benchmark problems by comparing it with two state-of-the-art surrogate-assisted multi-objective evolutionary algorithms.
http://w3id.org/mlsea/pwc/scientificWork/A%20Federated%20Learning%20Framework%20for%20Non-Intrusive%20Load%20Monitoring                                                                                  A Federated Learning Framework for Non-Intrusive Load Monitoring                                                                                  Non-intrusive load monitoring (NILM) aims at decomposing the total reading of the household power consumption into appliance-wise ones, which is beneficial for consumer behavior analysis as well as energy conservation. NILM based on deep learning has been a focus of research. To train a better neural network, it is necessary for the network to be fed with massive data containing various appliances and reflecting consumer behavior habits. Therefore, data cooperation among utilities and DNOs (distributed network operators) who own the NILM data has been increasingly significant. During the cooperation, however, risks of consumer privacy leakage and losses of data control rights arise. To deal with the problems above, a framework to improve the performance of NILM with federated learning (FL) has been set up. In the framework, model weights instead of the local data are shared among utilities. The global model is generated by weighted averaging the locally-trained model weights to gather the locally-trained model information. Optimal model selection help choose the model which adapts to the data from different domains best. Experiments show that this proposal improves the performance of local NILM runners. The performance of this framework is close to that of the centrally-trained model obtained by the convergent data without privacy protection.
http://w3id.org/mlsea/pwc/scientificWork/A%20Federated%20Learning%20Framework%20for%20Smart%20Grids%3A%20Securing%20Power%20Traces%20in%20Collaborative%20Learning                                                                                  A Federated Learning Framework for Smart Grids: Securing Power Traces in Collaborative Learning                                                                                  With the deployment of smart sensors and advancements in communication technologies, big data analytics have become vastly popular in the smart grid domain, informing stakeholders of the best power utilization strategy. However, these power-related data are stored and owned by different parties. For example, power consumption data are stored in numerous transformer stations across cities; mobility data of the population, which are important indicators of power consumption, are held by mobile companies. Direct data sharing might compromise party benefits, individual privacy and even national security. Inspired by the federated learning scheme from Google AI, we propose a federated learning framework for smart grids, which enables collaborative learning of power consumption patterns without leaking individual power traces. Horizontal federated learning is employed when data are scattered in the sample space; vertical federated learning, on the other hand, is designed for the case with data scattered in the feature space. Case studies show that, with proper encryption schemes such as Paillier encryption, the machine learning models constructed from the proposed framework are lossless, privacy-preserving and effective. Finally, the promising future of federated learning in other facets of the smart grid is discussed, including electric vehicles, distributed generation/consumption and integrated energy systems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Few%20Brief%20Notes%20on%20DeepImpact%2C%20COIL%2C%20and%20a%20Conceptual%20Framework%20for%20Information%20Retrieval%20Techniques                                                                                  A Few Brief Notes on DeepImpact, COIL, and a Conceptual Framework for Information Retrieval Techniques                                                                                  Recent developments in representational learning for information retrieval can be organized in a conceptual framework that establishes two pairs of contrasts: sparse vs. dense representations and unsupervised vs. learned representations. Sparse learned representations can further be decomposed into expansion and term weighting components. This framework allows us to understand the relationship between recently proposed techniques such as DPR, ANCE, DeepCT, DeepImpact, and COIL, and furthermore, gaps revealed by our analysis point to 'low hanging fruit' in terms of techniques that have yet to be explored. We present a novel technique dubbed 'uniCOIL', a simple extension of COIL that achieves to our knowledge the current state-of-the-art in sparse retrieval on the popular MS MARCO passage ranking dataset. Our implementation using the Anserini IR toolkit is built on the Lucene search library and thus fully compatible with standard inverted indexes.
http://w3id.org/mlsea/pwc/scientificWork/A%20Fine-Grained%20Visual%20Attention%20Approach%20for%20Fingerspelling%20Recognition%20in%20the%20Wild                                                                                  A Fine-Grained Visual Attention Approach for Fingerspelling Recognition in the Wild                                                                                  Fingerspelling in sign language has been the means of communicating technical terms and proper nouns when they do not have dedicated sign language gestures. Automatic recognition of fingerspelling can help resolve communication barriers when interacting with deaf people. The main challenges prevalent in fingerspelling recognition are the ambiguity in the gestures and strong articulation of the hands. The automatic recognition model should address high inter-class visual similarity and high intra-class variation in the gestures. Most of the existing research in fingerspelling recognition has focused on the dataset collected in a controlled environment. The recent collection of a large-scale annotated fingerspelling dataset in the wild, from social media and online platforms, captures the challenges in a real-world scenario. In this work, we propose a fine-grained visual attention mechanism using the Transformer model for the sequence-to-sequence prediction task in the wild dataset. The fine-grained attention is achieved by utilizing the change in motion of the video frames (optical flow) in sequential context-based attention along with a Transformer encoder model. The unsegmented continuous video dataset is jointly trained by balancing the Connectionist Temporal Classification (CTC) loss and the maximum-entropy loss. The proposed approach can capture better fine-grained attention in a single iteration. Experiment evaluations show that it outperforms the state-of-the-art approaches.
http://w3id.org/mlsea/pwc/scientificWork/A%20Finer%20Calibration%20Analysis%20for%20Adversarial%20Robustness                                                                                  A Finer Calibration Analysis for Adversarial Robustness                                                                                  We present a more general analysis of $H$-calibration for adversarially robust classification. By adopting a finer definition of calibration, we can cover settings beyond the restricted hypothesis sets studied in previous work. In particular, our results hold for most common hypothesis sets used in machine learning. We both fix some previous calibration results (Bao et al., 2020) and generalize others (Awasthi et al., 2021). Moreover, our calibration results, combined with the previous study of consistency by Awasthi et al. (2021), also lead to more general $H$-consistency results covering common hypothesis sets.
http://w3id.org/mlsea/pwc/scientificWork/A%20First%20Look%3A%20Towards%20Explainable%20TextVQA%20Models%20via%20Visual%20and%20Textual%20Explanations                                                                                  A First Look: Towards Explainable TextVQA Models via Visual and Textual Explanations                                                                                  Explainable deep learning models are advantageous in many situations. Prior work mostly provide unimodal explanations through post-hoc approaches not part of the original system design. Explanation mechanisms also ignore useful textual information present in images. In this paper, we propose MTXNet, an end-to-end trainable multimodal architecture to generate multimodal explanations, which focuses on the text in the image. We curate a novel dataset TextVQA-X, containing ground truth visual and multi-reference textual explanations that can be leveraged during both training and evaluation. We then quantitatively show that training with multimodal explanations complements model performance and surpasses unimodal baselines by up to 7% in CIDEr scores and 2% in IoU. More importantly, we demonstrate that the multimodal explanations are consistent with human interpretations, help justify the models' decision, and provide useful insights to help diagnose an incorrect prediction. Finally, we describe a real-world e-commerce application for using the generated multimodal explanations.
http://w3id.org/mlsea/pwc/scientificWork/A%20Flawed%20Dataset%20for%20Symbolic%20Equation%20Verification                                                                                  A Flawed Dataset for Symbolic Equation Verification                                                                                  Arabshahi, Singh, and Anandkumar (2018) propose a method for creating a dataset of symbolic mathematical equations for the tasks of symbolic equation verification and equation completion. Unfortunately, a dataset constructed using the method they propose will suffer from two serious flaws. First, the class of true equations that the procedure can generate will be very limited. Second, because true and false equations are generated in completely different ways, there are likely to be artifactual features that allow easy discrimination. Moreover, over the class of equations they consider, there is an extremely simple probabilistic procedure that solves the problem of equation verification with extremely high reliability. The usefulness of this problem in general as a testbed for AI systems is therefore doubtful.
http://w3id.org/mlsea/pwc/scientificWork/A%20Flexible%20Agent-Based%20Model%20to%20Study%20COVID-19%20Outbreak%20--%20A%20Generic%20Approach                                                                                  A Flexible Agent-Based Model to Study COVID-19 Outbreak -- A Generic Approach                                                                                  Understanding dynamics of an outbreak like that of COVID-19 is important in designing effective control measures. This study aims to develop an agent based model that compares changes in infection progression by manipulating different parameters in a synthetic population. Model input includes population characteristics like age, sex, working status etc. of each individual and other factors influencing disease dynamics. Depending on number of epicentres of infection, location of primary cases, sensitivity, proportion of asymptomatic and frequency or duration of lockdown, our simulator tracks every individual and hence infection progression through community over time. In a closed community of 10000 people, it is seen that without any lockdown, number of cases peak around 6th week and wanes off around 15th week. If primary case is located inside dense population cluster like slums, cases peak early and wane off slowly. With introduction of lockdown, cases peak at slower rate. If sensitivity of identifying infection decreases, cases and deaths increase. Number of cases declines with increase in proportion of asymptomatic cases. The model is robust and provides reproducible estimates with realistic parameter values. It also guides in identifying measures to control outbreak in a community. It is flexible in accommodating different parameters like infectivity period, yield of testing, socio-economic strata, daily travel, awareness level, population density, social distancing, lockdown etc. and can be tailored to study other infections with similar transmission pattern.
http://w3id.org/mlsea/pwc/scientificWork/A%20Flow-Based%20Neural%20Network%20for%20Time%20Domain%20Speech%20Enhancement                                                                                  A Flow-Based Neural Network for Time Domain Speech Enhancement                                                                                  Speech enhancement involves the distinction of a target speech signal from an intrusive background. Although generative approaches using Variational Autoencoders or Generative Adversarial Networks (GANs) have increasingly been used in recent years, normalizing flow (NF) based systems are still scarse, despite their success in related fields. Thus, in this paper we propose a NF framework to directly model the enhancement process by density estimation of clean speech utterances conditioned on their noisy counterpart. The WaveGlow model from speech synthesis is adapted to enable direct enhancement of noisy utterances in time domain. In addition, we demonstrate that nonlinear input companding benefits the model performance by equalizing the distribution of input samples. Experimental evaluation on a publicly available dataset shows comparable results to current state-of-the-art GAN-based approaches, while surpassing the chosen baselines using objective evaluation metrics.
http://w3id.org/mlsea/pwc/scientificWork/A%20Formal%20Framework%20for%20Reasoning%20about%20Agents%27%20Independence%20in%20Self-organizing%20Multi-agent%20Systems                                                                                  A Formal Framework for Reasoning about Agents' Independence in Self-organizing Multi-agent Systems                                                                                  Self-organization is a process where a stable pattern is formed by the cooperative behavior between parts of an initially disordered system without external control or influence. It has been introduced to multi-agent systems as an internal control process or mechanism to solve difficult problems spontaneously. However, because a self-organizing multi-agent system has autonomous agents and local interactions between them, it is difficult to predict the behavior of the system from the behavior of the local agents we design. This paper proposes a logic-based framework of self-organizing multi-agent systems, where agents interact with each other by following their prescribed local rules. The dependence relation between coalitions of agents regarding their contributions to the global behavior of the system is reasoned about from the structural and semantic perspectives. We show that the computational complexity of verifying such a self-organizing multi-agent system is in exponential time. We then combine our framework with graph theory to decompose a system into different coalitions located in different layers, which allows us to verify agents' full contributions more efficiently. The resulting information about agents' full contributions allows us to understand the complex link between local agent behavior and system level behavior in a self-organizing multi-agent system. Finally, we show how we can use our framework to model a constraint satisfaction problem.
http://w3id.org/mlsea/pwc/scientificWork/A%20Fourier-based%20Framework%20for%20Domain%20Generalization                                                                                  A Fourier-based Framework for Domain Generalization                                                                                  Modern deep neural networks suffer from performance degradation when evaluated on testing data under different distributions from training data. Domain generalization aims at tackling this problem by learning transferable knowledge from multiple source domains in order to generalize to unseen target domains. This paper introduces a novel Fourier-based perspective for domain generalization. The main assumption is that the Fourier phase information contains high-level semantics and is not easily affected by domain shifts. To force the model to capture phase information, we develop a novel Fourier-based data augmentation strategy called amplitude mix which linearly interpolates between the amplitude spectrums of two images. A dual-formed consistency loss called co-teacher regularization is further introduced between the predictions induced from original and augmented images. Extensive experiments on three benchmarks have demonstrated that the proposed method is able to achieve state-of-the-arts performance for domain generalization.
http://w3id.org/mlsea/pwc/scientificWork/A%20Framework%20for%20Automatic%20Monitoring%20of%20Norms%20that%20regulate%20Time%20Constrained%20Actions                                                                                  A Framework for Automatic Monitoring of Norms that regulate Time Constrained Actions                                                                                  This paper addresses the problem of proposing a model of norms and a framework for automatically computing their violation or fulfilment. The proposed T-NORM model can be used to express abstract norms able to regulate classes of actions that should or should not be performed in a temporal interval. We show how the model can be used to formalize obligations and prohibitions and for inhibiting them by introducing permissions and exemptions. The basic building blocks for norm specification consists of rules with suitably nested components. The activation condition, the regulated actions, and the temporal constrains of norms are specified using the W3C Web Ontology Language (OWL 2). Thanks to this choice, it is possible to use OWL reasoning for computing the effects that the logical implication between actions has on norms fulfilment or violation. The operational semantics of the T-NORM model is specified by providing an unambiguous procedure for translating every norm and every exception into production rules.
http://w3id.org/mlsea/pwc/scientificWork/A%20Framework%20for%20Ethical%20AI%20at%20the%20United%20Nations                                                                                  A Framework for Ethical AI at the United Nations                                                                                  This paper aims to provide an overview of the ethical concerns in artificial intelligence (AI) and the framework that is needed to mitigate those risks, and to suggest a practical path to ensure the development and use of AI at the United Nations (UN) aligns with our ethical values. The overview discusses how AI is an increasingly powerful tool with potential for good, albeit one with a high risk of negative side-effects that go against fundamental human rights and UN values. It explains the need for ethical principles for AI aligned with principles for data governance, as data and AI are tightly interwoven. It explores different ethical frameworks that exist and tools such as assessment lists. It recommends that the UN develop a framework consisting of ethical principles, architectural standards, assessment methods, tools and methodologies, and a policy to govern the implementation and adherence to this framework, accompanied by an education program for staff.
http://w3id.org/mlsea/pwc/scientificWork/A%20Framework%20for%20Evaluating%20Post%20Hoc%20Feature-Additive%20Explainers                                                                                  A Framework for Evaluating Post Hoc Feature-Additive Explainers                                                                                  Many applications of data-driven models demand transparency of decisions, especially in health care, criminal justice, and other high-stakes environments. Modern trends in machine learning research have led to algorithms that are increasingly intricate to the degree that they are considered to be black boxes. In an effort to reduce the opacity of decisions, methods have been proposed to construe the inner workings of such models in a human-comprehensible manner. These post hoc techniques are described as being universal explainers - capable of faithfully augmenting decisions with algorithmic insight. Unfortunately, there is little agreement about what constitutes a 'good' explanation. Moreover, current methods of explanation evaluation are derived from either subjective or proxy means. In this work, we propose a framework for the evaluation of post hoc explainers on ground truth that is directly derived from the additive structure of a model. We demonstrate the efficacy of the framework in understanding explainers by evaluating popular explainers on thousands of synthetic and several real-world tasks. The framework unveils that explanations may be accurate but misattribute the importance of individual features.
http://w3id.org/mlsea/pwc/scientificWork/A%20Framework%20for%20Explainable%20Concept%20Drift%20Detection%20in%20Process%20Mining                                                                                  A Framework for Explainable Concept Drift Detection in Process Mining                                                                                  Rapidly changing business environments expose companies to high levels of uncertainty. This uncertainty manifests itself in significant changes that tend to occur over the lifetime of a process and possibly affect its performance. It is important to understand the root causes of such changes since this allows us to react to change or anticipate future changes. Research in process mining has so far only focused on detecting, locating and characterizing significant changes in a process and not on finding root causes of such changes. In this paper, we aim to close this gap. We propose a framework that adds an explainability level onto concept drift detection in process mining and provides insights into the cause-effect relationships behind significant changes. We define different perspectives of a process, detect concept drifts in these perspectives and plug the perspectives into a causality check that determines whether these concept drifts can be causal to each other. We showcase the effectiveness of our framework by evaluating it on both synthetic and real event data. Our experiments show that our approach unravels cause-effect relationships and provides novel insights into executed processes.
http://w3id.org/mlsea/pwc/scientificWork/A%20Framework%20for%20Knowledge%20Integrated%20Evolutionary%20Algorithms                                                                                  A Framework for Knowledge Integrated Evolutionary Algorithms                                                                                  One of the main reasons for the success of Evolutionary Algorithms (EAs) is their general-purposeness, i.e., the fact that they can be applied straightforwardly to a broad range of optimization problems, without any specific prior knowledge. On the other hand, it has been shown that incorporating a priori knowledge, such as expert knowledge or empirical findings, can significantly improve the performance of an EA. However, integrating knowledge in EAs poses numerous challenges. It is often the case that the features of the search space are unknown, hence any knowledge associated with the search space properties can be hardly used. In addition, a priori knowledge is typically problem-specific and hard to generalize. In this paper, we propose a framework, called Knowledge Integrated Evolutionary Algorithm (KIEA), which facilitates the integration of existing knowledge into EAs. Notably, the KIEA framework is EA-agnostic (i.e., it works with any evolutionary algorithm), problem-independent (i.e., it is not dedicated to a specific type of problems), expandable (i.e., its knowledge base can grow over time). Furthermore, the framework integrates knowledge while the EA is running, thus optimizing the use of the needed computational power. In the preliminary experiments shown here, we observe that the KIEA framework produces in the worst case an 80% improvement on the converge time, w.r.t. the corresponding 'knowledge-free' EA counterpart.
http://w3id.org/mlsea/pwc/scientificWork/A%20Framework%20for%20Recognizing%20and%20Estimating%20Human%20Concentration%20Levels                                                                                  A Framework for Recognizing and Estimating Human Concentration Levels                                                                                  One of the major tasks in online education is to estimate the concentration levels of each student. Previous studies have a limitation of classifying the levels using discrete states only. The purpose of this paper is to estimate the subtle levels as specified states by using the minimum amount of body movement data. This is done by a framework composed of a Deep Neural Network and Kalman Filter. Using this framework, we successfully extracted the concentration levels, which can be used to aid lecturers and expand to other areas.
http://w3id.org/mlsea/pwc/scientificWork/A%20Framework%20for%20Unsupervised%20Classificiation%20and%20Data%20Mining%20of%20Tweets%20about%20Cyber%20Vulnerabilities                                                                                  A Framework for Unsupervised Classificiation and Data Mining of Tweets about Cyber Vulnerabilities                                                                                  Many cyber network defense tools rely on the National Vulnerability Database (NVD) to provide timely information on known vulnerabilities that exist within systems on a given network. However, recent studies have indicated that the NVD is not always up to date, with known vulnerabilities being discussed publicly on social media platforms, like Twitter and Reddit, months before they are published to the NVD. To that end, we present a framework for unsupervised classification to filter tweets for relevance to cyber security. We consider and evaluate two unsupervised machine learning techniques for inclusion in our framework, and show that zero-shot classification using a Bidirectional and Auto-Regressive Transformers (BART) model outperforms the other technique with 83.52% accuracy and a F1 score of 83.88, allowing for accurate filtering of tweets without human intervention or labelled data for training. Additionally, we discuss different insights that can be derived from these cyber-relevant tweets, such as trending topics of tweets and the counts of Twitter mentions for Common Vulnerabilities and Exposures (CVEs), that can be used in an alert or report to augment current NVD-based risk assessment tools.
http://w3id.org/mlsea/pwc/scientificWork/A%20Framework%20of%20Explanation%20Generation%20toward%20Reliable%20Autonomous%20Robots                                                                                  A Framework of Explanation Generation toward Reliable Autonomous Robots                                                                                  To realize autonomous collaborative robots, it is important to increase the trust that users have in them. Toward this goal, this paper proposes an algorithm which endows an autonomous agent with the ability to explain the transition from the current state to the target state in a Markov decision process (MDP). According to cognitive science, to generate an explanation that is acceptable to humans, it is important to present the minimum information necessary to sufficiently understand an event. To meet this requirement, this study proposes a framework for identifying important elements in the decision-making process using a prediction model for the world and generating explanations based on these elements. To verify the ability of the proposed method to generate explanations, we conducted an experiment using a grid environment. It was inferred from the result of a simulation experiment that the explanation generated using the proposed method was composed of the minimum elements important for understanding the transition from the current state to the target state. Furthermore, subject experiments showed that the generated explanation was a good summary of the process of state transition, and that a high evaluation was obtained for the explanation of the reason for an action.
http://w3id.org/mlsea/pwc/scientificWork/A%20Framework%20to%20Counteract%20Suboptimal%20User-Behaviors%20in%20Exploratory%20Learning%20Environments%3A%20an%20Application%20to%20MOOCs                                                                                  A Framework to Counteract Suboptimal User-Behaviors in Exploratory Learning Environments: an Application to MOOCs                                                                                  While there is evidence that user-adaptive support can greatly enhance the effectiveness of educational systems, designing such support for exploratory learning environments (e.g., simulations) is still challenging due to the open-ended nature of their interaction. In particular, there is little a priori knowledge of which student's behaviors can be detrimental to learning in such environments. To address this problem, we focus on a data-driven user-modeling framework that uses logged interaction data to learn which behavioral or activity patterns should trigger help during interaction with a specific learning environment. This framework has been successfully used to provide adaptive support in interactive learning simulations. Here we present a novel application of this framework we are working on, namely to Massive Open Online Courses (MOOCs), a form of exploratory environment that could greatly benefit from adaptive support due to the large diversity of their users, but typically lack of such adaptation. We describe an experiment aimed at investigating the value of our framework to identify student's behaviors that can justify adapting to, and report some preliminary results.
http://w3id.org/mlsea/pwc/scientificWork/A%20Framework%20to%20Enhance%20Generalization%20of%20Deep%20Metric%20Learning%20methods%20using%20General%20Discriminative%20Feature%20Learning%20and%20Class%20Adversarial%20Neural%20Networks                                                                                  A Framework to Enhance Generalization of Deep Metric Learning methods using General Discriminative Feature Learning and Class Adversarial Neural Networks                                                                                  Metric learning algorithms aim to learn a distance function that brings the semantically similar data items together and keeps dissimilar ones at a distance. The traditional Mahalanobis distance learning is equivalent to find a linear projection. In contrast, Deep Metric Learning (DML) methods are proposed that automatically extract features from data and learn a non-linear transformation from input space to a semantically embedding space. Recently, many DML methods are proposed focused to enhance the discrimination power of the learned metric by providing novel sampling strategies or loss functions. This approach is very helpful when both the training and test examples are coming from the same set of categories. However, it is less effective in many applications of DML such as image retrieval and person-reidentification. Here, the DML should learn general semantic concepts from observed classes and employ them to rank or identify objects from unseen categories. Neglecting the generalization ability of the learned representation and just emphasizing to learn a more discriminative embedding on the observed classes may lead to the overfitting problem. To address this limitation, we propose a framework to enhance the generalization power of existing DML methods in a Zero-Shot Learning (ZSL) setting by general yet discriminative representation learning and employing a class adversarial neural network. To learn a more general representation, we propose to employ feature maps of intermediate layers in a deep neural network and enhance their discrimination power through an attention mechanism. Besides, a class adversarial network is utilized to enforce the deep model to seek class invariant features for the DML task. We evaluate our work on widely used machine vision datasets in a ZSL setting.
http://w3id.org/mlsea/pwc/scientificWork/A%20Framework%20using%20Contrastive%20Learning%20for%20Classification%20with%20Noisy%20Labels                                                                                  A Framework using Contrastive Learning for Classification with Noisy Labels                                                                                  We propose a framework using contrastive learning as a pre-training task to perform image classification in the presence of noisy labels. Recent strategies such as pseudo-labeling, sample selection with Gaussian Mixture models, weighted supervised contrastive learning have been combined into a fine-tuning phase following the pre-training. This paper provides an extensive empirical study showing that a preliminary contrastive learning step brings a significant gain in performance when using different loss functions: non-robust, robust, and early-learning regularized. Our experiments performed on standard benchmarks and real-world datasets demonstrate that: i) the contrastive pre-training increases the robustness of any loss function to noisy labels and ii) the additional fine-tuning phase can further improve accuracy but at the cost of additional complexity.
http://w3id.org/mlsea/pwc/scientificWork/A%20Free%20Lunch%20From%20ANN%3A%20Towards%20Efficient%2C%20Accurate%20Spiking%20Neural%20Networks%20Calibration                                                                                  A Free Lunch From ANN: Towards Efficient, Accurate Spiking Neural Networks Calibration                                                                                  Spiking Neural Network (SNN) has been recognized as one of the next generation of neural networks. Conventionally, SNN can be converted from a pre-trained ANN by only replacing the ReLU activation to spike activation while keeping the parameters intact. Perhaps surprisingly, in this work we show that a proper way to calibrate the parameters during the conversion of ANN to SNN can bring significant improvements. We introduce SNN Calibration, a cheap but extraordinarily effective method by leveraging the knowledge within a pre-trained Artificial Neural Network (ANN). Starting by analyzing the conversion error and its propagation through layers theoretically, we propose the calibration algorithm that can correct the error layer-by-layer. The calibration only takes a handful number of training data and several minutes to finish. Moreover, our calibration algorithm can produce SNN with state-of-the-art architecture on the large-scale ImageNet dataset, including MobileNet and RegNet. Extensive experiments demonstrate the effectiveness and efficiency of our algorithm. For example, our advanced pipeline can increase up to 69% top-1 accuracy when converting MobileNet on ImageNet compared to baselines. Codes are released at https://github.com/yhhhli/SNN_Calibration.
http://w3id.org/mlsea/pwc/scientificWork/A%20Frequency%20Domain%20Constraint%20for%20Synthetic%20and%20Real%20X-ray%20Image%20Super%20Resolution                                                                                  A Frequency Domain Constraint for Synthetic and Real X-ray Image Super Resolution                                                                                  Synthetic X-ray images are simulated X-ray images projected from CT data. High-quality synthetic X-ray images can facilitate various applications such as surgical image guidance systems and VR training simulations. However, it is difficult to produce high-quality arbitrary view synthetic X-ray images in real-time due to different CT slice thickness, high computational cost, and the complexity of algorithms. Our goal is to generate high-resolution synthetic X-ray images in real-time by upsampling low-resolution images with deep learning-based super-resolution methods. Reference-based Super Resolution (RefSR) has been well studied in recent years and has shown higher performance than traditional Single Image Super-Resolution (SISR). It can produce fine details by utilizing the reference image but still inevitably generates some artifacts and noise. In this paper, we introduce frequency domain loss as a constraint to further improve the quality of the RefSR results with fine details and without obvious artifacts. To the best of our knowledge, this is the first paper utilizing the frequency domain for the loss functions in the field of super-resolution. We achieved good results in evaluating our method on both synthetic and real X-ray image datasets.
http://w3id.org/mlsea/pwc/scientificWork/A%20Fresh%20Approach%20to%20Evaluate%20Performance%20in%20Distributed%20Parallel%20Genetic%20Algorithms                                                                                  A Fresh Approach to Evaluate Performance in Distributed Parallel Genetic Algorithms                                                                                  This work proposes a novel approach to evaluate and analyze the behavior of multi-population parallel genetic algorithms (PGAs) when running on a cluster of multi-core processors. In particular, we deeply study their numerical and computational behavior by proposing a mathematical model representing the observed performance curves. In them, we discuss the emerging mathematical descriptions of PGA performance instead of, e.g., individual isolated results subject to visual inspection, for a better understanding of the effects of the number of cores used (scalability), their migration policy (the migration gap, in this paper), and the features of the solved problem (type of encoding and problem size). The conclusions based on the real figures and the numerical models fitting them represent a fresh way of understanding their speed-up, running time, and numerical effort, allowing a comparison based on a few meaningful numeric parameters. This represents a set of conclusions beyond the usual textual lessons found in past works on PGAs. It can be used as an estimation tool for the future performance of the algorithms and a way of finding out their limitations.
http://w3id.org/mlsea/pwc/scientificWork/A%20Front-End%20for%20Dense%20Monocular%20SLAM%20using%20a%20Learned%20Outlier%20Mask%20Prior                                                                                  A Front-End for Dense Monocular SLAM using a Learned Outlier Mask Prior                                                                                  Recent achievements in depth prediction from a single RGB image have powered the new research area of combining convolutional neural networks (CNNs) with classical simultaneous localization and mapping (SLAM) algorithms. The depth prediction from a CNN provides a reasonable initial point in the optimization process in the traditional SLAM algorithms, while the SLAM algorithms further improve the CNN prediction online. However, most of the current CNN-SLAM approaches have only taken advantage of the depth prediction but not yet other products from a CNN. In this work, we explore the use of the outlier mask, a by-product from unsupervised learning of depth from video, as a prior in a classical probability model for depth estimate fusion to step up the outlier-resistant tracking performance of a SLAM front-end. On the other hand, some of the previous CNN-SLAM work builds on feature-based sparse SLAM methods, wasting the per-pixel dense prediction from a CNN. In contrast to these sparse methods, we devise a dense CNN-assisted SLAM front-end that is implementable with TensorFlow and evaluate it on both indoor and outdoor datasets.
http://w3id.org/mlsea/pwc/scientificWork/A%20Full%20Text-Dependent%20End%20to%20End%20Mispronunciation%20Detection%20and%20Diagnosis%20with%20Easy%20Data%20Augmentation%20Techniques                                                                                  A Full Text-Dependent End to End Mispronunciation Detection and Diagnosis with Easy Data Augmentation Techniques                                                                                  Recently, end-to-end mispronunciation detection and diagnosis (MD&D) systems has become a popular alternative to greatly simplify the model-building process of conventional hybrid DNN-HMM systems by representing complicated modules with a single deep network architecture. In this paper, in order to utilize the prior text in the end-to-end structure, we present a novel text-dependent model which is difference with sed-mdd, the model achieves a fully end-to-end system by aligning the audio with the phoneme sequences of the prior text inside the model through the attention mechanism. Moreover, the prior text as input will be a problem of imbalance between positive and negative samples in the phoneme sequence. To alleviate this problem, we propose three simple data augmentation methods, which effectively improve the ability of model to capture mispronounced phonemes. We conduct experiments on L2-ARCTIC, and our best performance improved from 49.29% to 56.08% in F-measure metric compared to the CNN-RNN-CTC model.
http://w3id.org/mlsea/pwc/scientificWork/A%20Full-Stack%20Search%20Technique%20for%20Domain%20Optimized%20Deep%20Learning%20Accelerators                                                                                  A Full-Stack Search Technique for Domain Optimized Deep Learning Accelerators                                                                                  The rapidly-changing deep learning landscape presents a unique opportunity for building inference accelerators optimized for specific datacenter-scale workloads. We propose Full-stack Accelerator Search Technique (FAST), a hardware accelerator search framework that defines a broad optimization environment covering key design decisions within the hardware-software stack, including hardware datapath, software scheduling, and compiler passes such as operation fusion and tensor padding. In this paper, we analyze bottlenecks in state-of-the-art vision and natural language processing (NLP) models, including EfficientNet and BERT, and use FAST to design accelerators capable of addressing these bottlenecks. FAST-generated accelerators optimized for single workloads improve Perf/TDP by 3.7x on average across all benchmarks compared to TPU-v3. A FAST-generated accelerator optimized for serving a suite of workloads improves Perf/TDP by 2.4x on average compared to TPU-v3. Our return on investment analysis shows that FAST-generated accelerators can potentially be practical for moderate-sized datacenter deployments.
http://w3id.org/mlsea/pwc/scientificWork/A%20Fully%20Problem-Dependent%20Regret%20Lower%20Bound%20for%20Finite-Horizon%20MDPs                                                                                  A Fully Problem-Dependent Regret Lower Bound for Finite-Horizon MDPs                                                                                  We derive a novel asymptotic problem-dependent lower-bound for regret minimization in finite-horizon tabular Markov Decision Processes (MDPs). While, similar to prior work (e.g., for ergodic MDPs), the lower-bound is the solution to an optimization problem, our derivation reveals the need for an additional constraint on the visitation distribution over state-action pairs that explicitly accounts for the dynamics of the MDP. We provide a characterization of our lower-bound through a series of examples illustrating how different MDPs may have significantly different complexity. 1) We first consider a 'difficult' MDP instance, where the novel constraint based on the dynamics leads to a larger lower-bound (i.e., a larger regret) compared to the classical analysis. 2) We then show that our lower-bound recovers results previously derived for specific MDP instances. 3) Finally, we show that, in certain 'simple' MDPs, the lower bound is considerably smaller than in the general case and it does not scale with the minimum action gap at all. We show that this last result is attainable (up to $poly(H)$ terms, where $H$ is the horizon) by providing a regret upper-bound based on policy gaps for an optimistic algorithm.
http://w3id.org/mlsea/pwc/scientificWork/A%20Fully%20Spiking%20Hybrid%20Neural%20Network%20for%20Energy-Efficient%20Object%20Detection                                                                                  A Fully Spiking Hybrid Neural Network for Energy-Efficient Object Detection                                                                                  This paper proposes a Fully Spiking Hybrid Neural Network (FSHNN) for energy-efficient and robust object detection in resource-constrained platforms. The network architecture is based on Convolutional SNN using leaky-integrate-fire neuron models. The model combines unsupervised Spike Time-Dependent Plasticity (STDP) learning with back-propagation (STBP) learning methods and also uses Monte Carlo Dropout to get an estimate of the uncertainty error. FSHNN provides better accuracy compared to DNN based object detectors while being 150X energy-efficient. It also outperforms these object detectors, when subjected to noisy input data and less labeled training data with a lower uncertainty error.
http://w3id.org/mlsea/pwc/scientificWork/A%20Function%20Based%20on%20Chebyshev%20Polynomials%20as%20an%20Alternative%20to%20the%20Sinc%20Function%20in%20FIR%20Filter%20Design                                                                                  A Function Based on Chebyshev Polynomials as an Alternative to the Sinc Function in FIR Filter Design                                                                                  The sinc function is often used as the basis for the design of discrete linear-phase FIR filters. However the Fourier transform of the truncated sinc function exhibits ripple in the pass band due to the Gibbs phenomenon. This paper introduces an alternative function based on Chebyshev polynomials whose Fourier transform decreases monotonically in the pass band. Furthermore this function features an intrinsic window function with an adjustable parameter influencing the Fourier transform in the transition and stop bands.
http://w3id.org/mlsea/pwc/scientificWork/A%20Fusion-Denoising%20Attack%20on%20InstaHide%20with%20Data%20Augmentation                                                                                  A Fusion-Denoising Attack on InstaHide with Data Augmentation                                                                                  InstaHide is a state-of-the-art mechanism for protecting private training images, by mixing multiple private images and modifying them such that their visual features are indistinguishable to the naked eye. In recent work, however, Carlini et al. show that it is possible to reconstruct private images from the encrypted dataset generated by InstaHide. Nevertheless, we demonstrate that Carlini et al.'s attack can be easily defeated by incorporating data augmentation into InstaHide. This leads to a natural question: is InstaHide with data augmentation secure? In this paper, we provide a negative answer to this question, by devising an attack for recovering private images from the outputs of InstaHide even when data augmentation is present. The basic idea is to use a comparative network to identify encrypted images that are likely to correspond to the same private image, and then employ a fusion-denoising network for restoring the private image from the encrypted ones, taking into account the effects of data augmentation. Extensive experiments demonstrate the effectiveness of the proposed attack in comparison to Carlini et al.'s attack.
http://w3id.org/mlsea/pwc/scientificWork/A%20GAN-Like%20Approach%20for%20Physics-Based%20Imitation%20Learning%20and%20Interactive%20Character%20Control                                                                                  A GAN-Like Approach for Physics-Based Imitation Learning and Interactive Character Control                                                                                  We present a simple and intuitive approach for interactive control of physically simulated characters. Our work builds upon generative adversarial networks (GAN) and reinforcement learning, and introduces an imitation learning framework where an ensemble of classifiers and an imitation policy are trained in tandem given pre-processed reference clips. The classifiers are trained to discriminate the reference motion from the motion generated by the imitation policy, while the policy is rewarded for fooling the discriminators. Using our GAN-based approach, multiple motor control policies can be trained separately to imitate different behaviors. In runtime, our system can respond to external control signal provided by the user and interactively switch between different policies. Compared to existing methods, our proposed approach has the following attractive properties: 1) achieves state-of-the-art imitation performance without manually designing and fine tuning a reward function; 2) directly controls the character without having to track any target reference pose explicitly or implicitly through a phase state; and 3) supports interactive policy switching without requiring any motion generation or motion matching mechanism. We highlight the applicability of our approach in a range of imitation and interactive control tasks, while also demonstrating its ability to withstand external perturbations as well as to recover balance. Overall, our approach generates high-fidelity motion, has low runtime cost, and can be easily integrated into interactive applications and games.
http://w3id.org/mlsea/pwc/scientificWork/A%20GMM%20approach%20to%20estimate%20the%20roughness%20of%20stochastic%20volatility                                                                                  A GMM approach to estimate the roughness of stochastic volatility                                                                                  We develop a GMM approach for estimation of log-normal stochastic volatility models driven by a fractional Brownian motion with unrestricted Hurst exponent. We show that a parameter estimator based on the integrated variance is consistent and, under stronger conditions, asymptotically normally distributed. We inspect the behavior of our procedure when integrated variance is replaced with a noisy measure of volatility calculated from discrete high-frequency data. The realized estimator contains sampling error, which skews the fractal coefficient toward 'illusive roughness.' We construct an analytical approach to control the impact of measurement error without introducing nuisance parameters. In a simulation study, we demonstrate convincing small sample properties of our approach based both on integrated and realized variance over the entire memory spectrum. We show the bias correction attenuates any systematic deviance in the parameter estimates. Our procedure is applied to empirical high-frequency data from numerous leading equity indexes. With our robust approach the Hurst index is estimated around 0.05, confirming roughness in stochastic volatility.
http://w3id.org/mlsea/pwc/scientificWork/A%20GPU%20Implementation%20of%20a%20Look-Ahead%20Optimal%20Controller%20for%20Eco-Driving%20Based%20on%20Dynamic%20Programming                                                                                  A GPU Implementation of a Look-Ahead Optimal Controller for Eco-Driving Based on Dynamic Programming                                                                                  Predictive energy management of Connected and Automated Vehicles (CAVs), in particular those with multiple power sources, has the potential to significantly improve energy savings in real-world driving conditions. In particular, the eco-driving problem seeks to design optimal speed and power usage profiles based upon available information from connectivity and advanced mapping features to minimize the fuel consumption between two designated locations. In this work, the eco-driving problem is formulated as a three-state receding horizon optimal control problem and solved via Dynamic Programming (DP). The optimal solution, in terms of vehicle speed and battery State of Charge (SoC) trajectories, allows a connected and automated hybrid electric vehicle to intelligently pass the signalized intersections and minimize fuel consumption over a prescribed route. To enable real-time implementation, a parallel architecture of DP is proposed for an NVIDIA GPU with CUDA programming. Simulation results indicate that the proposed optimal controller delivers more than 15% fuel economy benefits compared to a baseline control strategy and that the solver time can be reduced by more than 90% by the parallel implementation when compared to a serial implementation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Game-Theoretic%20Approach%20to%20Multi-Agent%20Trust%20Region%20Optimization                                                                                  A Game-Theoretic Approach to Multi-Agent Trust Region Optimization                                                                                  Trust region methods are widely applied in single-agent reinforcement learning problems due to their monotonic performance-improvement guarantee at every iteration. Nonetheless, when applied in multi-agent settings, the guarantee of trust region methods no longer holds because an agent's payoff is also affected by other agents' adaptive behaviors. To tackle this problem, we conduct a game-theoretical analysis in the policy space, and propose a multi-agent trust region learning method (MATRL), which enables trust region optimization for multi-agent learning. Specifically, MATRL finds a stable improvement direction that is guided by the solution concept of Nash equilibrium at the meta-game level. We derive the monotonic improvement guarantee in multi-agent settings and empirically show the local convergence of MATRL to stable fixed points in the two-player rotational differential game. To test our method, we evaluate MATRL in both discrete and continuous multiplayer general-sum games including checker and switch grid worlds, multi-agent MuJoCo, and Atari games. Results suggest that MATRL significantly outperforms strong multi-agent reinforcement learning baselines.
http://w3id.org/mlsea/pwc/scientificWork/A%20Game-Theoretic%20Framework%20for%20Coexistence%20of%20WiFi%20and%20Cellular%20Networks%20in%20the%206-GHz%20Unlicensed%20Spectrum                                                                                  A Game-Theoretic Framework for Coexistence of WiFi and Cellular Networks in the 6-GHz Unlicensed Spectrum                                                                                  We study the interaction of WiFi and 5G cellular networks as they exploit the recently unlocked 6-GHz spectrum for unlicensed access while conforming to the constraints imposed by the incumbent users. We derive the theoretical performance metrics for users of each radio access technology using stochastic geometry, thereby capturing the aggregate behaviour of the network. We propose a framework where the portions of cellular and WiFi networks are grouped to form entities that interact to satisfy their QoS demands by playing a non-cooperative game. The action of an entity corresponds to the fraction of its network elements operating in the 6-GHz band. Due to the decentralized nature of the game, we find the solution using distributed Best Response Algorithm, which improves the average datarate by 11.37% and 18.59% for cellular and WiFi networks, respectively. The results demonstrate how the system parameters affect the performance of a network at equilibrium and highlight the throughput gains as a result of using the 6-GHz bands. We tested our framework on a real-world setup with actual network locations, showing that practical implementation of multi-entity spectrum sharing is feasible even when the spatial distribution of the network and users are non-homogeneous.
http://w3id.org/mlsea/pwc/scientificWork/A%20Game-Theoretic%20Taxonomy%20of%20Visual%20Concepts%20in%20DNNs                                                                                  A Game-Theoretic Taxonomy of Visual Concepts in DNNs                                                                                  In this paper, we rethink how a DNN encodes visual concepts of different complexities from a new perspective, i.e. the game-theoretic multi-order interactions between pixels in an image. Beyond the categorical taxonomy of objects and the cognitive taxonomy of textures and shapes, we provide a new taxonomy of visual concepts, which helps us interpret the encoding of shapes and textures, in terms of concept complexities. In this way, based on multi-order interactions, we find three distinctive signal-processing behaviors of DNNs encoding textures. Besides, we also discover the flexibility for a DNN to encode shapes is lower than the flexibility of encoding textures. Furthermore, we analyze how DNNs encode outlier samples, and explore the impacts of network architectures on interactions. Additionally, we clarify the crucial role of the multi-order interactions in real-world applications. The code will be released when the paper is accepted.
http://w3id.org/mlsea/pwc/scientificWork/A%20Gaussian%20Process%20Model%20of%20Cross-Category%20Dynamics%20in%20Brand%20Choice                                                                                  A Gaussian Process Model of Cross-Category Dynamics in Brand Choice                                                                                  Understanding individual customers' sensitivities to prices, promotions, brand, and other aspects of the marketing mix is fundamental to a wide swath of marketing problems, including targeting and pricing. Companies that operate across many product categories have a unique opportunity, insofar as they can use purchasing data from one category to augment their insights in another. Such cross-category insights are especially crucial in situations where purchasing data may be rich in one category, and scarce in another. An important aspect of how consumers behave across categories is dynamics: preferences are not stable over time, and changes in individual-level preference parameters in one category may be indicative of changes in other categories, especially if those changes are driven by external factors. Yet, despite the rich history of modeling cross-category preferences, the marketing literature lacks a framework that flexibly accounts for textit{correlated dynamics}, or the cross-category interlinkages of individual-level sensitivity dynamics. In this work, we propose such a framework, leveraging individual-level, latent, multi-output Gaussian processes to build a nonparametric Bayesian choice model that allows information sharing of preference parameters across customers, time, and categories. We apply our model to grocery purchase data, and show that our model detects interesting dynamics of customers' price sensitivities across multiple categories. Managerially, we show that capturing correlated dynamics yields substantial predictive gains, relative to benchmarks. Moreover, we find that capturing correlated dynamics can have implications for understanding changes in consumers preferences over time, and developing targeted marketing strategies based on those dynamics.
http://w3id.org/mlsea/pwc/scientificWork/A%20General%203D%20Non-Stationary%20Massive%20MIMO%20GBSM%20for%206G%20Communication%20Systems                                                                                  A General 3D Non-Stationary Massive MIMO GBSM for 6G Communication Systems                                                                                  A general three-dimensional (3D) non-stationary massive multiple-input multiple-output (MIMO) geometry-based stochastic model (GBSM) for the sixth generation (6G) communication systems is proposed in the paper. The novelty of the model is that the model is designed to cover a variety of channel characteristics, including space-time-frequency (STF) non-stationarity, spherical wavefront, spatial consistency, channel hardening, etc. Firstly, the introduction of the twin-cluster channel model is given in detail. Secondly, the key statistical properties such as space-time-frequency correlation function (STFCF), space cross-correlation function (CCF), temporal autocorrelation function (ACF), frequency correlation function (FCF), and performance indicators, e.g., singular value spread (SVS), and channel capacity are derived. Finally, the simulation results are given and consistent with some measurements in relevant literatures, which validate that the proposed channel model has a certain value as a reference to model massive MIMO channel characteristics.
http://w3id.org/mlsea/pwc/scientificWork/A%20General%203D%20Non-Stationary%20Wireless%20Channel%20Model%20for%205G%20and%20Beyond                                                                                  A General 3D Non-Stationary Wireless Channel Model for 5G and Beyond                                                                                  In this paper, a novel three-dimensional (3D) non-stationary geometry-based stochastic model (GBSM) for the fifth generation (5G) and beyond 5G (B5G) systems is proposed. The proposed B5G channel model (B5GCM) is designed to capture various channel characteristics in (B)5G systems such as space-time-frequency (STF) non-stationarity, spherical wavefront (SWF), high delay resolution, time-variant velocities and directions of motion of the transmitter, receiver, and scatterers, spatial consistency, etc. By combining different channel properties into a general channel model framework, the proposed B5GCM is able to be applied to multiple frequency bands and multiple scenarios, including massive multiple-input multiple-output (MIMO), vehicle-to-vehicle (V2V), high-speed train (HST), and millimeter wave-terahertz (mmWave-THz) communication scenarios. Key statistics of the proposed B5GCM are obtained and compared with those of standard 5G channel models and corresponding measurement data, showing the generalization and usefulness of the proposed model.
http://w3id.org/mlsea/pwc/scientificWork/A%20General%203D%20Space-Time-Frequency%20Non-Stationary%20THz%20Channel%20Model%20for%206G%20Ultra-Massive%20MIMO%20Wireless%20Communication%20Systems                                                                                  A General 3D Space-Time-Frequency Non-Stationary THz Channel Model for 6G Ultra-Massive MIMO Wireless Communication Systems                                                                                  In this paper, a novel three-dimensional (3D) space-time-frequency (STF) non-stationary geometry-based stochastic model (GBSM) is proposed for the sixth generation (6G) terahertz (THz) wireless communication systems. The proposed THz channel model is very general having the capability to capture different channel characteristics in multiple THz application scenarios such as indoor scenarios, device-to-device (D2D) communications, ultra-massive multiple-input multiple-output (MIMO) communications, and long traveling paths of users. Also, the generality of the proposed channel model is demonstrated by the fact that it can easily be reduced to different simplified channel models to fit specific scenarios by properly adjusting model parameters. The proposed general channel model takes into consideration the non-stationarities in space, time, and frequency domains caused by ultra-massive MIMO, long traveling paths, and large bandwidths of THz communications, respectively. Statistical properties of the proposed general THz channel model are investigated. The accuracy and generality of the proposed channel model are verified by comparing the simulation results of the relative angle spread and root mean square (RMS) delay spread with corresponding channel measurements.
http://w3id.org/mlsea/pwc/scientificWork/A%20General%20Architecture%20for%20Behavior%20Modeling%20of%20Nonlinear%20Power%20Amplifier%20using%20Deep%20Convolutional%20Neural%20Network                                                                                  A General Architecture for Behavior Modeling of Nonlinear Power Amplifier using Deep Convolutional Neural Network                                                                                  Nonlinearity of power amplifier is one of the major limitations to the achievable capacity in wireless transmission systems. Nonlinear impairments are determined by the nonlinear distortions of the power amplifier and modulator imperfections. The Volterra model, several compact Volterra models and neural network models to establish a nonlinear model of power amplifier have all been demonstrated. However, the computational cost of these models increases and their implementation demands more signal processing resources as the signal bandwidth gets wider or the number of carrier aggregation. A completely different approach uses deep convolutional neural network to learn from the training data to figure out the nonlinear distortion. In this work, a low complexity, general architecture based on the deep real-valued convolutional neural network (DRVCNN) is proposed to build the nonlinear behavior of the power amplifier. With each of the multiple inputs equivalent to an input vector, the DRVCNN tensor weights are constructed from training data thanks to the current and historical envelope-dependent terms, I, and Q, which are components of the input. The effectiveness of the general framework in modeling single-carrier and multi-carrier power amplifiers is verified.
http://w3id.org/mlsea/pwc/scientificWork/A%20General%20Derivative%20Identity%20for%20the%20Conditional%20Mean%20Estimator%20in%20Gaussian%20Noise%20and%20Some%20Applications                                                                                  A General Derivative Identity for the Conditional Mean Estimator in Gaussian Noise and Some Applications                                                                                  Consider a channel ${ bf Y}={ bf X}+ { bf N}$ where ${ bf X}$ is an $n$-dimensional random vector, and ${ bf N}$ is a Gaussian vector with a covariance matrix ${ bf mathsf{K}}_{ bf N}$. The object under consideration in this paper is the conditional mean of ${ bf X}$ given ${ bf Y}={ bf y}$, that is ${ bf y} to E[{ bf X}|{ bf Y}={ bf y}]$. Several identities in the literature connect $E[{ bf X}|{ bf Y}={ bf y}]$ to other quantities such as the conditional variance, score functions, and higher-order conditional moments. The objective of this paper is to provide a unifying view of these identities. In the first part of the paper, a general derivative identity for the conditional mean is derived. Specifically, for the Markov chain ${ bf U} leftrightarrow { bf X} leftrightarrow { bf Y}$, it is shown that the Jacobian of $E[{ bf U}|{ bf Y}={ bf y}]$ is given by ${ bf mathsf{K}}_{{ bf N}}^{-1} { bf Cov} ( { bf X}, { bf U} | { bf Y}={ bf y})$. In the second part of the paper, via various choices of ${ bf U}$, the new identity is used to generalize many of the known identities and derive some new ones. First, a simple proof of the Hatsel and Nolte identity for the conditional variance is shown. Second, a simple proof of the recursive identity due to Jaffer is provided. Third, a new connection between the conditional cumulants and the conditional expectation is shown. In particular, it is shown that the $k$-th derivative of $E[X|Y=y]$ is the $(k+1)$-th conditional cumulant. The third part of the paper considers some applications. In a first application, the power series and the compositional inverse of $E[X|Y=y]$ are derived. In a second application, the distribution of the estimator error $(X-E[X|Y])$ is derived. In a third application, we construct consistent estimators (empirical Bayes estimators) of the conditional cumulants from an i.i.d. sequence $Y_1,...,Y_n$.
http://w3id.org/mlsea/pwc/scientificWork/A%20General%20Destriping%20Framework%20for%20Remote%20Sensing%20Images%20Using%20Flatness%20Constraint                                                                                  A General Destriping Framework for Remote Sensing Images Using Flatness Constraint                                                                                  Removing stripe noise, i.e., destriping, from remote sensing images is an essential task in terms of visual quality and subsequent processing. Most existing destriping methods are designed by combining a particular image regularization with a stripe noise characterization that cooperates with the regularization, which precludes us to examine and activate different regularizations to adapt to various target images. To resolve this, two requirements need to be considered: a general framework that can handle a variety of image regularizations in destriping, and a strong stripe noise characterization that can consistently capture the nature of stripe noise, regardless of the choice of image regularization. To this end, this paper proposes a general destriping framework using a newly-introduced stripe noise characterization, named flatness constraint, where we can handle various regularization functions in a unified manner. Specifically, we formulate the destriping problem as a nonsmooth convex optimization problem involving a general form of image regularization and the flatness constraint. The constraint mathematically models that the intensity of each stripe is constant along one direction, resulting in a strong characterization of stripe noise. For solving the optimization problem, we also develop an efficient algorithm based on a diagonally preconditioned primal-dual splitting algorithm (DP-PDS), which can automatically adjust the stepsizes. The effectiveness of our framework is demonstrated through destriping experiments, where we comprehensively compare combinations of a variety of image regularizations and stripe noise characterizations using hyperspectral images (HSI) and infrared (IR) videos.
http://w3id.org/mlsea/pwc/scientificWork/A%20General%20Framework%20for%20Airplane%20Air-to-Ground%20Communications%20in%20mmWave%20and%20Microwave%20Bands                                                                                  A General Framework for Airplane Air-to-Ground Communications in mmWave and Microwave Bands                                                                                  Airplane sensors and on-board equipment collect an increasingly large amount of maintenance data during flights that are used for airplane maintenance. We propose to download part of the data during airplane's descent via a cellular base station (BS) located at the airport. We formulate and solve an offline optimization problem to quantify how much data can be offloaded in a non-dedicated band while ensuring that the interference at the terrestrial BSs in the vicinity of the airport remains below a maximum allowable threshold. Our problem allows for adaptive tuning of transmit power, number of frequency channels to be used, and beamforming according to the position of the plane on the descent path. Our results show that during the last 5 minutes of descent, in the microwave band the plane can offload up to 5 GB of maintenance data in a 20~MHz band, while in the mmWave band the plane can offload up to 24 times more data in a 1~GHz band. Beamforming, power and bandwidth tuning are all crucial in maintaining a good performance in the mmWave band while in the microwave band, dynamic tuning of bandwidth does not improve the performance much.
http://w3id.org/mlsea/pwc/scientificWork/A%20General%20Framework%20for%20RIS-Aided%20mmWave%20Communication%20Networks%3A%20Channel%20Estimation%20and%20Mobile%20User%20Tracking                                                                                  A General Framework for RIS-Aided mmWave Communication Networks: Channel Estimation and Mobile User Tracking                                                                                  Reconfigurable intelligent surface (RIS) has been widely discussed as new technology to improve wireless communication performance. Based on the unique design of RIS, its elements can reflect, refract, absorb, or focus the incoming waves toward any desired direction. These functionalities turned out to be a major solution to overcome millimeter-wave (mmWave)'s high propagation conditions including path attenuation and blockage. However, channel estimation in RIS-aided communication is still a major concern due to the passive nature of RIS elements, and estimation overhead that arises with multiple-input multiple-output (MIMO) system. As a consequence, user tracking has not been analyzed yet. This paper is the first work that addresses channel estimation, beamforming, and user tracking under practical mmWave RIS-MIMO systems. By providing the mathematical relation of RIS design with a MIMO system, a three-stage framework is presented. Starting with estimating the channel between a base station (BS) and RIS using hierarchical beam searching, followed by estimating the channel between RIS and user using an iterative resolution algorithm. Lastly, a popular tracking algorithm is employed to track channel parameters between the RIS and the user. System analysis demonstrates the robustness and the effectiveness of the proposed framework in real-time scenarios.
http://w3id.org/mlsea/pwc/scientificWork/A%20General%20Katsuno-Mendelzon-Style%20Characterization%20of%20AGM%20Belief%20Base%20Revision%20for%20Arbitrary%20Monotonic%20Logics                                                                                  A General Katsuno-Mendelzon-Style Characterization of AGM Belief Base Revision for Arbitrary Monotonic Logics                                                                                  The AGM postulates by Alchourr '{o}n, G '{a}rdenfors, and Makinson continue to represent a cornerstone in research related to belief change. We generalize the approach of Katsuno and Mendelzon (KM) for characterizing AGM base revision from propositional logic to the setting of (multiple) base revision in arbitrary monotonic logics. Our core result is a representation theorem using the assignment of total - yet not transitive - 'preference' relations to belief bases. We also provide a characterization of all logics for which our result can be strengthened to preorder assignments (as in KM's original work).
http://w3id.org/mlsea/pwc/scientificWork/A%20General%20Method%20For%20Automatic%20Discovery%20of%20Powerful%20Interactions%20In%20Click-Through%20Rate%20Prediction                                                                                  A General Method For Automatic Discovery of Powerful Interactions In Click-Through Rate Prediction                                                                                  Modeling powerful interactions is a critical challenge in Click-through rate (CTR) prediction, which is one of the most typical machine learning tasks in personalized advertising and recommender systems. Although developing hand-crafted interactions is effective for a small number of datasets, it generally requires laborious and tedious architecture engineering for extensive scenarios. In recent years, several neural architecture search (NAS) methods have been proposed for designing interactions automatically. However, existing methods only explore limited types and connections of operators for interaction generation, leading to low generalization ability. To address these problems, we propose a more general automated method for building powerful interactions named AutoPI. The main contributions of this paper are as follows: AutoPI adopts a more general search space in which the computational graph is generalized from existing network connections, and the interactive operators in the edges of the graph are extracted from representative hand-crafted works. It allows searching for various powerful feature interactions to produce higher AUC and lower Logloss in a wide variety of applications. Besides, AutoPI utilizes a gradient-based search strategy for exploration with a significantly low computational cost. Experimentally, we evaluate AutoPI on a diverse suite of benchmark datasets, demonstrating the generalizability and efficiency of AutoPI over hand-crafted architectures and state-of-the-art NAS algorithms.
http://w3id.org/mlsea/pwc/scientificWork/A%20General%20Method%20for%20Event%20Detection%20on%20Social%20Media                                                                                  A General Method for Event Detection on Social Media                                                                                  Event detection on social media has attracted a number of researches, given the recent availability of large volumes of social media discussions. Previous works on social media event detection either assume a specific type of event, or assume certain behavior of observed variables. In this paper, we propose a general method for event detection on social media that makes few assumptions. The main assumption we make is that when an event occurs, affected semantic aspects will behave differently from its usual behavior. We generalize the representation of time units based on word embeddings of social media text, and propose an algorithm to detect events in time series in a general sense. In the experimental evaluation, we use a novel setting to test if our method and baseline methods can exhaustively catch all real-world news in the test period. The evaluation results show that when the event is quite unusual with regard to the base social media discussion, it can be captured more effectively with our method. Our method can be easily implemented and can be treated as a starting point for more specific applications.
http://w3id.org/mlsea/pwc/scientificWork/A%20General%20Model%20of%20Structured%20Cell%20Kinetics                                                                                  A General Model of Structured Cell Kinetics                                                                                  We present a modelling framework for the dynamics of cells structured by the concentration of a micromolecule they contain. We derive general equations for the evolution of the cell population and of the extra-cellular concentration of the molecule and apply this approach to models of silicosis and quorum sensing in Gram-negative bacteria
http://w3id.org/mlsea/pwc/scientificWork/A%20General%20Taylor%20Framework%20for%20Unifying%20and%20Revisiting%20Attribution%20Methods                                                                                  A General Taylor Framework for Unifying and Revisiting Attribution Methods                                                                                  Attribution methods provide an insight into the decision-making process of machine learning models, especially deep neural networks, by assigning contribution scores to each individual feature. However, the attribution problem has not been well-defined, which lacks a unified guideline to the contribution assignment process. Furthermore, existing attribution methods often built upon various empirical intuitions and heuristics. There still lacks a general theoretical framework that not only can offer a good description of the attribution problem, but also can be applied to unifying and revisiting existing attribution methods. To bridge the gap, in this paper, we propose a Taylor attribution framework, which models the attribution problem as how to decide individual payoffs in a coalition. Then, we reformulate fourteen mainstream attribution methods into the Taylor framework and analyze these attribution methods in terms of rationale, fidelity, and limitation in the framework. Moreover, we establish three principles for a good attribution in the Taylor attribution framework, i.e., low approximation error, correct Taylor contribution assignment, and unbiased baseline selection. Finally, we empirically validate the Taylor reformulations and reveal a positive correlation between the attribution performance and the number of principles followed by the attribution method via benchmarking on real-world datasets.
http://w3id.org/mlsea/pwc/scientificWork/A%20General%20Theory%20for%20the%20Evolution%20of%20Application%20Models%20--%20Full%20version                                                                                  A General Theory for the Evolution of Application Models -- Full version                                                                                  In this article we focus on evolving information systems. First a delimitation of the concept of evolution is provided, resulting in a first attempt to a general theory for such evolutions. The theory makes a distinction between the underlying information structure at the conceptual level, its evolution on the one hand, and the description and semantics of operations on the information structure and its population on the other hand. Main issues within this theory are object typing, type relatedness and identification of objects. In terms of these concepts, we propose some axioms on the well-formedness of evolution. In this general theory, the underlying data model is a parameter, making the theory applicable for a wide range of modelling techniques, including object-role modelling and object oriented techniques.
http://w3id.org/mlsea/pwc/scientificWork/A%20Generalised%20Inverse%20Reinforcement%20Learning%20Framework                                                                                  A Generalised Inverse Reinforcement Learning Framework                                                                                  The gloabal objective of inverse Reinforcement Learning (IRL) is to estimate the unknown cost function of some MDP base on observed trajectories generated by (approximate) optimal policies. The classical approach consists in tuning this cost function so that associated optimal trajectories (that minimise the cumulative discounted cost, i.e. the classical RL loss) are 'similar' to the observed ones. Prior contributions focused on penalising degenerate solutions and improving algorithmic scalability. Quite orthogonally to them, we question the pertinence of characterising optimality with respect to the cumulative discounted cost as it induces an implicit bias against policies with longer mixing times. State of the art value based RL algorithms circumvent this issue by solving for the fixed point of the Bellman optimality operator, a stronger criterion that is not well defined for the inverse problem. To alleviate this bias in IRL, we introduce an alternative training loss that puts more weights on future states which yields a reformulation of the (maximum entropy) IRL problem. The algorithms we devised exhibit enhanced performances (and similar tractability) than off-the-shelf ones in multiple OpenAI gym environments.
http://w3id.org/mlsea/pwc/scientificWork/A%20Generalizable%20Approach%20to%20Learning%20Optimizers                                                                                  A Generalizable Approach to Learning Optimizers                                                                                  A core issue with learning to optimize neural networks has been the lack of generalization to real world problems. To address this, we describe a system designed from a generalization-first perspective, learning to update optimizer hyperparameters instead of model parameters directly using novel features, actions, and a reward function. This system outperforms Adam at all neural network tasks including on modalities not seen during training. We achieve 2x speedups on ImageNet, and a 2.5x speedup on a language modeling task using over 5 orders of magnitude more compute than the training tasks.
http://w3id.org/mlsea/pwc/scientificWork/A%20Generalized%20LinDistFlow%20Model%20for%20Power%20Flow%20Analysis                                                                                  A Generalized LinDistFlow Model for Power Flow Analysis                                                                                  This paper proposes a new linear power flow model for distribution system with accurate voltage magnitude estimates. The new model can be seen as a generalization of LinDistFlow model to multiphase distribution system with generic network topology (radial or meshed) around arbitrary linearization point. We have shown that the approximation quality of the proposed model strictly dominates that of the fixed-point linearization (FPL) method, a popular linear power flow model for distribution system analysis, when both are linearized around zero injection point. Numerical examples using standard IEEE test feeders are provided to illustrate the effectiveness of the proposed model as well as the improvement in accuracy over existing methods when linearized around non-zero injection points.
http://w3id.org/mlsea/pwc/scientificWork/A%20Generalized%20Loss%20Function%20for%20Crowd%20Counting%20and%20Localization                                                                                  A Generalized Loss Function for Crowd Counting and Localization                                                                                   Previous work shows that a better density map representation can improve the performance of crowd counting. In this paper, we investigate learning the density map representation through an unbalanced optimal transport problem, and propose a generalized loss function to learn density maps for crowd counting and localization. We prove that pixel-wise L2 loss and Bayesian loss are special cases and suboptimal solutions to our proposed loss function. A perspective-guided transport cost function is further proposed to better handle the perspective transformation in crowd images. Since the predicted density will be pushed toward annotation positions, the density map prediction will be sparse and can naturally be used for localization. Finally, the proposed loss outperforms other losses on four large-scale datasets for counting, and achieves the best localization performance on NWPU-Crowd and UCF-QNRF. 
http://w3id.org/mlsea/pwc/scientificWork/A%20Generalized%20Projected%20Bellman%20Error%20for%20Off-policy%20Value%20Estimation%20in%20Reinforcement%20Learning                                                                                  A Generalized Projected Bellman Error for Off-policy Value Estimation in Reinforcement Learning                                                                                  Many reinforcement learning algorithms rely on value estimation, however, the most widely used algorithms -- namely temporal difference algorithms -- can diverge under both off-policy sampling and nonlinear function approximation. Many algorithms have been developed for off-policy value estimation based on the linear mean squared projected Bellman error (MSPBE) and are sound under linear function approximation. Extending these methods to the nonlinear case has been largely unsuccessful. Recently, several methods have been introduced that approximate a different objective -- the mean-squared Bellman error (MSBE) -- which naturally facilitate nonlinear approximation. In this work, we build on these insights and introduce a new generalized MSPBE that extends the linear MSPBE to the nonlinear setting. We show how this generalized objective unifies previous work and obtain new bounds for the value error of the solutions of the generalized objective. We derive an easy-to-use, but sound, algorithm to minimize the generalized objective, and show that it is more stable across runs, is less sensitive to hyperparameters, and performs favorably across four control domains with neural network function approximation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Generative%20Model%20for%20Raw%20Audio%20Using%20Transformer%20Architectures                                                                                  A Generative Model for Raw Audio Using Transformer Architectures                                                                                  This paper proposes a novel way of doing audio synthesis at the waveform level using Transformer architectures. We propose a deep neural network for generating waveforms, similar to wavenet. This is fully probabilistic, auto-regressive, and causal, i.e. each sample generated depends only on the previously observed samples. Our approach outperforms a widely used wavenet architecture by up to 9% on a similar dataset for predicting the next step. Using the attention mechanism, we enable the architecture to learn which audio samples are important for the prediction of the future sample. We show how causal transformer generative models can be used for raw waveform synthesis. We also show that this performance can be improved by another 2% by conditioning samples over a wider context. The flexibility of the current model to synthesize audio from latent representations suggests a large number of potential applications. The novel approach of using generative transformer architectures for raw audio synthesis is, however, still far away from generating any meaningful music, without using latent codes/meta-data to aid the generation process.
http://w3id.org/mlsea/pwc/scientificWork/A%20Generative%20Node-attribute%20Network%20Model%20for%20Detecting%20Generalized%20Structure                                                                                  A Generative Node-attribute Network Model for Detecting Generalized Structure                                                                                  Exploring meaningful structural regularities embedded in networks is a key to understanding and analyzing the structure and function of a network. The node-attribute information can help improve such understanding and analysis. However, most of the existing methods focus on detecting traditional communities, i.e., groupings of nodes with dense internal connections and sparse external ones. In this paper, based on the connectivity behavior of nodes and homogeneity of attributes, we propose a principle model (named GNAN), which can generate both topology information and attribute information. The new model can detect not only community structure, but also a range of other types of structure in networks, such as bipartite structure, core-periphery structure, and their mixture structure, which are collectively referred to as generalized structure. The proposed model that combines topological information and node-attribute information can detect communities more accurately than the model that only uses topology information. The dependency between attributes and communities can be automatically learned by our model and thus we can ignore the attributes that do not contain useful information. The model parameters are inferred by using the expectation-maximization algorithm. And a case study is provided to show the ability of our model in the semantic interpretability of communities. Experiments on both synthetic and real-world networks show that the new model is competitive with other state-of-the-art models.
http://w3id.org/mlsea/pwc/scientificWork/A%20Gentle%20Introduction%20to%20Scaling%20Laws%20in%20Biological%20Systems                                                                                  A Gentle Introduction to Scaling Laws in Biological Systems                                                                                  This paper investigates the role of size in biological organisms. More specifically, how the energy demand, expressed by the metabolic rate, changes according to the mass of an organism. Empirical evidence suggests a power-law relation between mass and metabolic rate, namely allometric law. For vascular organisms, the exponent $ beta$ of this power-law is smaller than one, which implies scaling economy; that is, the greater the organism is, the lesser energy per cell it demands. However, the numerical value of this exponent is a theme of an extensive debate and a central issue in comparative physiology. It is presented in this work some empirical data and a detailed discussion about the most successful theories to explain these issues. A historical perspective is also shown, beginning with the first empirical insights in the sec. 19 about scaling properties in biology, passing through the two more important theories that explain the scaling properties quantitatively. Firstly, the Rubner model, that consider organism surface area and heat dissipation to derive $ beta = 2/3$. Secondly, the West-Brown-Enquist theory, that explains such scaling properties as a consequence of the hierarchical and fractal nutrient distribution network, deriving $ beta = 3/4$.
http://w3id.org/mlsea/pwc/scientificWork/A%20Geometric%20Analysis%20of%20Neural%20Collapse%20with%20Unconstrained%20Features                                                                                  A Geometric Analysis of Neural Collapse with Unconstrained Features                                                                                  We provide the first global optimization landscape analysis of $Neural ;Collapse$ -- an intriguing empirical phenomenon that arises in the last-layer classifiers and features of neural networks during the terminal phase of training. As recently reported by Papyan et al., this phenomenon implies that ($i$) the class means and the last-layer classifiers all collapse to the vertices of a Simplex Equiangular Tight Frame (ETF) up to scaling, and ($ii$) cross-example within-class variability of last-layer activations collapses to zero. We study the problem based on a simplified $unconstrained ;feature ;model$, which isolates the topmost layers from the classifier of the neural network. In this context, we show that the classical cross-entropy loss with weight decay has a benign global landscape, in the sense that the only global minimizers are the Simplex ETFs while all other critical points are strict saddles whose Hessian exhibit negative curvature directions. In contrast to existing landscape analysis for deep neural networks which is often disconnected from practice, our analysis of the simplified model not only does it explain what kind of features are learned in the last layer, but it also shows why they can be efficiently optimized in the simplified settings, matching the empirical observations in practical deep network architectures. These findings could have profound implications for optimization, generalization, and robustness of broad interests. For example, our experiments demonstrate that one may set the feature dimension equal to the number of classes and fix the last-layer classifier to be a Simplex ETF for network training, which reduces memory cost by over $20 %$ on ResNet18 without sacrificing the generalization performance.
http://w3id.org/mlsea/pwc/scientificWork/A%20Geometry-Informed%20Deep%20Learning%20Framework%20for%20Ultra-Sparse%203D%20Tomographic%20Image%20Reconstruction                                                                                  A Geometry-Informed Deep Learning Framework for Ultra-Sparse 3D Tomographic Image Reconstruction                                                                                  Deep learning affords enormous opportunities to augment the armamentarium of biomedical imaging, albeit its design and implementation have potential flaws. Fundamentally, most deep learning models are driven entirely by data without consideration of any prior knowledge, which dramatically increases the complexity of neural networks and limits the application scope and model generalizability. Here we establish a geometry-informed deep learning framework for ultra-sparse 3D tomographic image reconstruction. We introduce a novel mechanism for integrating geometric priors of the imaging system. We demonstrate that the seamless inclusion of known priors is essential to enhance the performance of 3D volumetric computed tomography imaging with ultra-sparse sampling. The study opens new avenues for data-driven biomedical imaging and promises to provide substantially improved imaging tools for various clinical imaging and image-guided interventions.
http://w3id.org/mlsea/pwc/scientificWork/A%20Global%20Appearance%20and%20Local%20Coding%20Distortion%20based%20Fusion%20Framework%20for%20CNN%20based%20Filtering%20in%20Video%20Coding                                                                                  A Global Appearance and Local Coding Distortion based Fusion Framework for CNN based Filtering in Video Coding                                                                                  In-loop filtering is used in video coding to process the reconstructed frame in order to remove blocking artifacts. With the development of convolutional neural networks (CNNs), CNNs have been explored for in-loop filtering considering it can be treated as an image de-noising task. However, in addition to being a distorted image, the reconstructed frame is also obtained by a fixed line of block based encoding operations in video coding. It carries coding-unit based coding distortion of some similar characteristics. Therefore, in this paper, we address the filtering problem from two aspects, global appearance restoration for disrupted texture and local coding distortion restoration caused by fixed pipeline of coding. Accordingly, a three-stream global appearance and local coding distortion based fusion network is developed with a high-level global feature stream, a high-level local feature stream and a low-level local feature stream. Ablation study is conducted to validate the necessity of different features, demonstrating that the global features and local features can complement each other in filtering and achieve better performance when combined. To the best of our knowledge, we are the first one that clearly characterizes the video filtering process from the above global appearance and local coding distortion restoration aspects with experimental verification, providing a clear pathway to developing filter techniques. Experimental results demonstrate that the proposed method significantly outperforms the existing single-frame based methods and achieves 13.5%, 11.3%, 11.7% BD-Rate saving on average for AI, LDP and RA configurations, respectively, compared with the HEVC reference software.
http://w3id.org/mlsea/pwc/scientificWork/A%20Global%20Past-Future%20Early%20Exit%20Method%20for%20Accelerating%20Inference%20of%20Pre-trained%20Language%20Models                                                                                  A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models                                                                                  Early exit mechanism aims to accelerate the inference speed of large-scale pre-trained language models. The essential idea is to exit early without passing through all the inference layers at the inference stage. To make accurate predictions for downstream tasks, the hierarchical linguistic information embedded in all layers should be jointly considered. However, much of the research up to now has been limited to use local representations of the exit layer. Such treatment inevitably loses information of the unused past layers as well as the high-level features embedded in future layers, leading to sub-optimal performance. To address this issue, we propose a novel Past-Future method to make comprehensive predictions from a global perspective. We first take into consideration all the linguistic information embedded in the past layers and then take a further step to engage the future information which is originally inaccessible for predictions. Extensive experiments demonstrate that our method outperforms previous early exit methods by a large margin, yielding better and robust performance.
http://w3id.org/mlsea/pwc/scientificWork/A%20Globally%20Normalized%20Neural%20Model%20for%20Semantic%20Parsing                                                                                  A Globally Normalized Neural Model for Semantic Parsing                                                                                  In this paper, we propose a globally normalized model for context-free grammar (CFG)-based semantic parsing. Instead of predicting a probability, our model predicts a real-valued score at each step and does not suffer from the label bias problem. Experiments show that our approach outperforms locally normalized models on small datasets, but it does not yield improvement on a large dataset.
http://w3id.org/mlsea/pwc/scientificWork/A%20Good%20Image%20Generator%20Is%20What%20You%20Need%20for%20High-Resolution%20Video%20Synthesis                                                                                  A Good Image Generator Is What You Need for High-Resolution Video Synthesis                                                                                  Image and video synthesis are closely related areas aiming at generating content from noise. While rapid progress has been demonstrated in improving image-based models to handle large resolutions, high-quality renderings, and wide variations in image content, achieving comparable video generation results remains problematic. We present a framework that leverages contemporary image generators to render high-resolution videos. We frame the video synthesis problem as discovering a trajectory in the latent space of a pre-trained and fixed image generator. Not only does such a framework render high-resolution videos, but it also is an order of magnitude more computationally efficient. We introduce a motion generator that discovers the desired trajectory, in which content and motion are disentangled. With such a representation, our framework allows for a broad range of applications, including content and motion manipulation. Furthermore, we introduce a new task, which we call cross-domain video synthesis, in which the image and motion generators are trained on disjoint datasets belonging to different domains. This allows for generating moving objects for which the desired video data is not available. Extensive experiments on various datasets demonstrate the advantages of our methods over existing video generation techniques. Code will be released at https://github.com/snap-research/MoCoGAN-HD.
http://w3id.org/mlsea/pwc/scientificWork/A%20Gradient%20Estimator%20for%20Time-Varying%20Electrical%20Networks%20with%20Non-Linear%20Dissipation                                                                                  A Gradient Estimator for Time-Varying Electrical Networks with Non-Linear Dissipation                                                                                  We propose a method for extending the technique of equilibrium propagation for estimating gradients in fixed-point neural networks to the more general setting of directed, time-varying neural networks by modeling them as electrical circuits. We use electrical circuit theory to construct a Lagrangian capable of describing deep, directed neural networks modeled using nonlinear capacitors and inductors, linear resistors and sources, and a special class of nonlinear dissipative elements called fractional memristors. We then derive an estimator for the gradient of the physical parameters of the network, such as synapse conductances, with respect to an arbitrary loss function. This estimator is entirely local, in that it only depends on information locally available to each synapse. We conclude by suggesting methods for extending these results to networks of biologically plausible neurons, e.g. Hodgkin-Huxley neurons.
http://w3id.org/mlsea/pwc/scientificWork/A%20Gradient%20Method%20for%20Multilevel%20Optimization                                                                                  A Gradient Method for Multilevel Optimization                                                                                  Although application examples of multilevel optimization have already been discussed since the 1990s, the development of solution methods was almost limited to bilevel cases due to the difficulty of the problem. In recent years, in machine learning, Franceschi et al. have proposed a method for solving bilevel optimization problems by replacing their lower-level problems with the $T$ steepest descent update equations with some prechosen iteration number $T$. In this paper, we have developed a gradient-based algorithm for multilevel optimization with $n$ levels based on their idea and proved that our reformulation asymptotically converges to the original multilevel problem. As far as we know, this is one of the first algorithms with some theoretical guarantee for multilevel optimization. Numerical experiments show that a trilevel hyperparameter learning model considering data poisoning produces more stable prediction results than an existing bilevel hyperparameter learning model in noisy data settings.
http://w3id.org/mlsea/pwc/scientificWork/A%20Gradient-based%20Deep%20Neural%20Network%20Model%20for%20Simulating%20Multiphase%20Flow%20in%20Porous%20Media                                                                                  A Gradient-based Deep Neural Network Model for Simulating Multiphase Flow in Porous Media                                                                                  Simulation of multiphase flow in porous media is crucial for the effective management of subsurface energy and environment related activities. The numerical simulators used for modeling such processes rely on spatial and temporal discretization of the governing partial-differential equations (PDEs) into algebraic systems via numerical methods. These simulators usually require dedicated software development and maintenance, and suffer low efficiency from a runtime and memory standpoint. Therefore, developing cost-effective, data-driven models can become a practical choice since deep learning approaches are considered to be universal approximations. In this paper, we describe a gradient-based deep neural network (GDNN) constrained by the physics related to multiphase flow in porous media. We tackle the nonlinearity of flow in porous media induced by rock heterogeneity, fluid properties and fluid-rock interactions by decomposing the nonlinear PDEs into a dictionary of elementary differential operators. We use a combination of operators to handle rock spatial heterogeneity and fluid flow by advection. Since the augmented differential operators are inherently related to the physics of fluid flow, we treat them as first principles prior knowledge to regularize the GDNN training. We use the example of pressure management at geologic CO2 storage sites, where CO2 is injected in saline aquifers and brine is produced, and apply GDNN to construct a predictive model that is trained from physics-based simulation data and emulates the physics process. We demonstrate that GDNN can effectively predict the nonlinear patterns of subsurface responses including the temporal-spatial evolution of the pressure and saturation plumes. GDNN has great potential to tackle challenging problems that are governed by highly nonlinear physics and enables development of data-driven models with higher fidelity.
http://w3id.org/mlsea/pwc/scientificWork/A%20Grant-based%20Random%20Access%20Protocol%20in%20Extra-Large%20Massive%20MIMO%20System                                                                                  A Grant-based Random Access Protocol in Extra-Large Massive MIMO System                                                                                  Extra-large massive multiple-input multiple-output (XL-MIMO) systems is a new concept, where spatial non-stationarities allow activate a high number of user equipments (UEs). This paper focuses on a grant-based random access (RA) approach in the novel XL-MIMO channel scenarios. Modifications in the classical Strongest User Collision Resolution (SUCRe) protocol have been aggregated to explore the visibility regions (VRs) overlapping in XL-MIMO. The proposed grant-based RA protocol takes advantage of this new degree of freedom for improving the number of access attempts and accepted UEs. As a result, the proposed grant-based protocol for XL-MIMO systems is capable of reducing latency in the pilot allocation step.
http://w3id.org/mlsea/pwc/scientificWork/A%20Graph%20Convolutional%20Neural%20Network%20based%20Framework%20for%20Estimating%20Future%20Citations%20Count%20of%20Research%20Articles                                                                                  A Graph Convolutional Neural Network based Framework for Estimating Future Citations Count of Research Articles                                                                                  Scientific publications play a vital role in the career of a researcher. However, some articles become more popular than others among the research community and subsequently drive future research directions. One of the indicative signs of popular articles is the number of citations an article receives. The citation count, which is also the basis with various other metrics, such as the journal impact factor score, the $h$-index, is an essential measure for assessing a scientific paper's quality. In this work, we proposed a Graph Convolutional Network (GCN) based framework for estimating future research publication citations for both the short-term (1-year) and long-term (for 5-years and 10-years) duration. We have tested our proposed approach over the AMiner dataset, specifically on research articles from the computer science domain, consisting of more than 0.8 million articles.
http://w3id.org/mlsea/pwc/scientificWork/A%20Graph%20Federated%20Architecture%20with%20Privacy%20Preserving%20Learning                                                                                  A Graph Federated Architecture with Privacy Preserving Learning                                                                                  Federated learning involves a central processor that works with multiple agents to find a global model. The process consists of repeatedly exchanging estimates, which results in the diffusion of information pertaining to the local private data. Such a scheme can be inconvenient when dealing with sensitive data, and therefore, there is a need for the privatization of the algorithms. Furthermore, the current architecture of a server connected to multiple clients is highly sensitive to communication failures and computational overloads at the server. Thus in this work, we develop a private multi-server federated learning scheme, which we call graph federated learning. We use cryptographic and differential privacy concepts to privatize the federated learning algorithm that we extend to the graph structure. We study the effect of privatization on the performance of the learning algorithm for general private schemes that can be modeled as additive noise. We show under convexity and Lipschitz conditions, that the privatized process matches the performance of the non-private algorithm, even when we increase the noise variance.
http://w3id.org/mlsea/pwc/scientificWork/A%20Graph%20Neural%20Network%20Approach%20for%20Product%20Relationship%20Prediction                                                                                  A Graph Neural Network Approach for Product Relationship Prediction                                                                                  Graph Neural Networks have revolutionized many machine learning tasks in recent years, ranging from drug discovery, recommendation systems, image classification, social network analysis to natural language understanding. This paper shows their efficacy in modeling relationships between products and making predictions for unseen product networks. By representing products as nodes and their relationships as edges of a graph, we show how an inductive graph neural network approach, named GraphSAGE, can efficiently learn continuous representations for nodes and edges. These representations also capture product feature information such as price, brand, or engineering attributes. They are combined with a classification model for predicting the existence of the relationship between products. Using a case study of the Chinese car market, we find that our method yields double the prediction performance compared to an Exponential Random Graph Model-based method for predicting the co-consideration relationship between cars. While a vanilla GraphSAGE requires a partial network to make predictions, we introduce an `adjacency prediction model' to circumvent this limitation. This enables us to predict product relationships when no neighborhood information is known. Finally, we demonstrate how a permutation-based interpretability analysis can provide insights on how design attributes impact the predictions of relationships between products. This work provides a systematic method to predict the relationships between products in many different markets.
http://w3id.org/mlsea/pwc/scientificWork/A%20Graph%20VAE%20and%20Graph%20Transformer%20Approach%20to%20Generating%20Molecular%20Graphs                                                                                  A Graph VAE and Graph Transformer Approach to Generating Molecular Graphs                                                                                  We propose a combination of a variational autoencoder and a transformer based model which fully utilises graph convolutional and graph pooling layers to operate directly on graphs. The transformer model implements a novel node encoding layer, replacing the position encoding typically used in transformers, to create a transformer with no position information that operates on graphs, encoding adjacent node properties into the edge generation process. The proposed model builds on graph generative work operating on graphs with edge features, creating a model that offers improved scalability with the number of nodes in a graph. In addition, our model is capable of learning a disentangled, interpretable latent space that represents graph properties through a mapping between latent variables and graph properties. In experiments we chose a benchmark task of molecular generation, given the importance of both generated node and edge features. Using the QM9 dataset we demonstrate that our model performs strongly across the task of generating valid, unique and novel molecules. Finally, we demonstrate that the model is interpretable by generating molecules controlled by molecular properties, and we then analyse and visualise the learned latent representation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Graph-based%20Method%20for%20Session-based%20Recommendations                                                                                  A Graph-based Method for Session-based Recommendations                                                                                  We present a graph-based approach for the data management tasks and the efficient operation of a system for session-based next-item recommendations. The proposed method can collect data continuously and incrementally from an ecommerce web site, thus seemingly prepare the necessary data infrastructure for the recommendation algorithm to operate without any excessive training phase. Our work aims at developing a recommender method that represents a balance between data processing and management efficiency requirements and the effectiveness of the recommendations produced. We use the Neo4j graph database to implement a prototype of such a system. Furthermore, we use an industry dataset corresponding to a typical e-commerce session-based scenario, and we report on experiments using our graph-based approach and other state-of-the-art machine learning and deep learning methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20Graph-based%20Similarity%20Function%20for%20CBDT%3A%20Acquiring%20and%20Using%20New%20Information                                                                                  A Graph-based Similarity Function for CBDT: Acquiring and Using New Information                                                                                  One of the consequences of persistent technological change is that it force individuals to make decisions under extreme uncertainty. This means that traditional decision-making frameworks cannot be applied. To address this issue we introduce a variant of Case-Based Decision Theory, in which the solution to a problem obtains in terms of the distance to previous problems. We formalize this by defining a space based on an orthogonal basis of features of problems. We show how this framework evolves upon the acquisition of new information, namely features or values of them arising in new problems. We discuss how this can be useful to evaluate decisions based on not yet existing data.
http://w3id.org/mlsea/pwc/scientificWork/A%20Graph-based%20approach%20to%20derive%20the%20geodesic%20distance%20on%20Statistical%20manifolds%3A%20Application%20to%20Multimedia%20Information%20Retrieval                                                                                  A Graph-based approach to derive the geodesic distance on Statistical manifolds: Application to Multimedia Information Retrieval                                                                                  In this paper, we leverage the properties of non-Euclidean Geometry to define the Geodesic distance (GD) on the space of statistical manifolds. The Geodesic distance is a real and intuitive similarity measure that is a good alternative to the purely statistical and extensively used Kullback-Leibler divergence (KLD). Despite the effectiveness of the GD, a closed-form does not exist for many manifolds, since the geodesic equations are hard to solve. This explains that the major studies have been content to use numerical approximations. Nevertheless, most of those do not take account of the manifold properties, which leads to a loss of information and thus to low performances. We propose an approximation of the Geodesic distance through a graph-based method. This latter permits to well represent the structure of the statistical manifold, and respects its geometrical properties. Our main aim is to compare the graph-based approximation to the state of the art approximations. Thus, the proposed approach is evaluated for two statistical manifolds, namely the Weibull manifold and the Gamma manifold, considering the Content-Based Texture Retrieval application on different databases.
http://w3id.org/mlsea/pwc/scientificWork/A%20Graph-guided%20Multi-round%20Retrieval%20Method%20for%20Conversational%20Open-domain%20Question%20Answering                                                                                  A Graph-guided Multi-round Retrieval Method for Conversational Open-domain Question Answering                                                                                  In recent years, conversational agents have provided a natural and convenient access to useful information in people's daily life, along with a broad and new research topic, conversational question answering (QA). Among the popular conversational QA tasks, conversational open-domain QA, which requires to retrieve relevant passages from the Web to extract exact answers, is more practical but less studied. The main challenge is how to well capture and fully explore the historical context in conversation to facilitate effective large-scale retrieval. The current work mainly utilizes history questions to refine the current question or to enhance its representation, yet the relations between history answers and the current answer in a conversation, which is also critical to the task, are totally neglected. To address this problem, we propose a novel graph-guided retrieval method to model the relations among answers across conversation turns. In particular, it utilizes a passage graph derived from the hyperlink-connected passages that contains history answers and potential current answers, to retrieve more relevant passages for subsequent answer extraction. Moreover, in order to collect more complementary information in the historical context, we also propose to incorporate the multi-round relevance feedback technique to explore the impact of the retrieval context on current question understanding. Experimental results on the public dataset verify the effectiveness of our proposed method. Notably, the F1 score is improved by 5% and 11% with predicted history answers and true history answers, respectively.
http://w3id.org/mlsea/pwc/scientificWork/A%20Greedy%20Bit-flip%20Training%20Algorithm%20for%20Binarized%20Knowledge%20Graph%20Embeddings                                                                                  A Greedy Bit-flip Training Algorithm for Binarized Knowledge Graph Embeddings                                                                                  This paper presents a simple and effective discrete optimization method for training binarized knowledge graph embedding model B-CP. Unlike the prior work using a SGD-based method and quantization of real-valued vectors, the proposed method directly optimizes binary embedding vectors by a series of bit flipping operations. On the standard knowledge graph completion tasks, the B-CP model trained with the proposed method achieved comparable performance with that trained with SGD as well as state-of-the-art real-valued models with similar embedding dimensions.
http://w3id.org/mlsea/pwc/scientificWork/A%20Grid-Based%20Evolutionary%20Algorithm%20for%20Many-Objective%20Optimization                                                                                  A Grid-Based Evolutionary Algorithm for Many-Objective Optimization                                                                                  Balancing convergence and diversity plays a key role in evolutionary multiobjective optimization (EMO). Most current EMO algorithms perform well on problems with two or three objectives, but encounter difficulties in their scalability to many-objective optimization. This paper proposes a grid- based evolutionary algorithm (GrEA) to solve many-objective optimization problems. Our aim is to exploit the potential of the grid-based approach to strengthen the selection pressure towards the optimal direction while maintaining an extensive and uniform distribution among solutions. To this end, two concepts—grid dominance and grid difference—are introduced to determine the mutual relationship of individuals in a grid environment. Three grid-based criteria, i.e., grid ranking, grid crowding distance, and grid coordinate point distance, are incorporated into the fitness of individuals to distinguish them in both the mating and environmental selection processes. Moreover, a fitness adjustment strategy is developed by adaptively punishing individuals based on the neighborhood and grid dominance relations in order to avoid partial overcrowding as well as guide the search towards different directions in the archive. Six state-of-the-art EMO algorithms are selected as the peer algorithms to validate GrEA. A series of extensive experiments is conducted on 52 instances of nine test problems taken from three test suites. The experimental results show the effectiveness and competitiveness of the proposed GrEA in balancing convergence and diversity. The solution set obtained by GrEA can achieve a better coverage of the Pareto front than that obtained by other algorithms on most of the tested problems. Additionally, a parametric study reveals interesting insights of the division parameter in a grid and also indicates useful values for problems with different characteristics.
http://w3id.org/mlsea/pwc/scientificWork/A%20Grounded%20Approach%20to%20Modeling%20Generic%20Knowledge%20Acquisition                                                                                  A Grounded Approach to Modeling Generic Knowledge Acquisition                                                                                  We introduce and implement a cognitively plausible model for learning from generic language, statements that express generalizations about members of a category and are an important aspect of concept development in language acquisition (Carlson & Pelletier, 1995; Gelman, 2009). We extend a computational framework designed to model grounded language acquisition by introducing the concept network. This new layer of abstraction enables the system to encode knowledge learned from generic statements and represent the associations between concepts learned by the system. Through three tasks that utilize the concept network, we demonstrate that our extensions to ADAM can acquire generic information and provide an example of how ADAM can be used to model language acquisition.
http://w3id.org/mlsea/pwc/scientificWork/A%20Guidance%20and%20Maneuvering%20Control%20System%20Design%20with%20Anti-collision%20Using%20Stream%20Functions%20with%20Vortex%20Flows%20for%20Autonomous%20Marine%20Vessels                                                                                  A Guidance and Maneuvering Control System Design with Anti-collision Using Stream Functions with Vortex Flows for Autonomous Marine Vessels                                                                                  Autonomous marine vessels are expected to avoid inter-vessel collisions and comply with the international regulations for safe voyages. This paper presents a stepwise path planning method using stream functions. The dynamic flow of fluids is used as a guidance model, where the collision avoidance in static environments is achieved by applying the circular theorem in the sink flow. We extend this method to dynamic environments by adding vortex flows in the flow field. The stream function is recursively updated to enable on the fly waypoint decisions. The vessel avoids collisions and also complies with several rules of the Convention on the International Regulations for Preventing Collisions at Sea. The method is conceptually and computationally simple and convenient to tune, and yet versatile to handle complex and dense marine traffic with multiple dynamic obstacles. The ship dynamics are taken into account, by using B '{e}zier curves to generate a sufficiently smooth path with feasible curvature. Numerical simulations are conducted to verify the proposed method.
http://w3id.org/mlsea/pwc/scientificWork/A%20Guide%20to%20Reducing%20Carbon%20Emissions%20through%20Data%20Center%20Geographical%20Load%20Shifting                                                                                  A Guide to Reducing Carbon Emissions through Data Center Geographical Load Shifting                                                                                  Recent computing needs have lead technology companies to develop large scale, highly optimized data centers. These data centers represent large loads on electric power networks which have the unique flexibility to shift load both geographically and temporally. This paper focuses on how data centers can use their geographic load flexibility to reduce carbon emissions through clever interactions with electricity markets. Because electricity market clearing accounts for congestion and power flow physics in the electric grid, the carbon emissions associated with electricity use varies between (potentially geographically close) locations. Using our knowledge about this process, we propose a new and improved metric to guide geographic load shifting, which we refer to as the locational marginal carbon emission $ lambda_{ text{CO}_2}$. We compare this and three other shifting metrics on their ability to reduce carbon emissions and generation costs throughout the course of a year. Our analysis demonstrates that $ lambda_{ text{CO}_2}$ is more effective in reducing carbon emissions than more commonly proposed metrics that do not account for the specifics of the power grid.
http://w3id.org/mlsea/pwc/scientificWork/A%20Hands-on%20Comparison%20of%20DNNs%20for%20Dialog%20Separation%20Using%20Transfer%20Learning%20from%20Music%20Source%20Separation                                                                                  A Hands-on Comparison of DNNs for Dialog Separation Using Transfer Learning from Music Source Separation                                                                                  This paper describes a hands-on comparison on using state-of-the-art music source separation deep neural networks (DNNs) before and after task-specific fine-tuning for separating speech content from non-speech content in broadcast audio (i.e., dialog separation). The music separation models are selected as they share the number of channels (2) and sampling rate (44.1 kHz or higher) with the considered broadcast content, and vocals separation in music is considered as a parallel for dialog separation in the target application domain. These similarities are assumed to enable transfer learning between the tasks. Three models pre-trained on music (Open-Unmix, Spleeter, and Conv-TasNet) are considered in the experiments, and fine-tuned with real broadcast data. The performance of the models is evaluated before and after fine-tuning with computational evaluation metrics (SI-SIRi, SI-SDRi, 2f-model), as well as with a listening test simulating an application where the non-speech signal is partially attenuated, e.g., for better speech intelligibility. The evaluations include two reference systems specifically developed for dialog separation. The results indicate that pre-trained music source separation models can be used for dialog separation to some degree, and that they benefit from the fine-tuning, reaching a performance close to task-specific solutions.
http://w3id.org/mlsea/pwc/scientificWork/A%20Heuristic-driven%20Uncertainty%20based%20Ensemble%20Framework%20for%20Fake%20News%20Detection%20in%20Tweets%20and%20News%20Articles                                                                                  A Heuristic-driven Uncertainty based Ensemble Framework for Fake News Detection in Tweets and News Articles                                                                                  The significance of social media has increased manifold in the past few decades as it helps people from even the most remote corners of the world to stay connected. With the advent of technology, digital media has become more relevant and widely used than ever before and along with this, there has been a resurgence in the circulation of fake news and tweets that demand immediate attention. In this paper, we describe a novel Fake News Detection system that automatically identifies whether a news item is 'real' or 'fake', as an extension of our work in the CONSTRAINT COVID-19 Fake News Detection in English challenge. We have used an ensemble model consisting of pre-trained models followed by a statistical feature fusion network , along with a novel heuristic algorithm by incorporating various attributes present in news items or tweets like source, username handles, URL domains and authors as statistical feature. Our proposed framework have also quantified reliable predictive uncertainty along with proper class output confidence level for the classification task. We have evaluated our results on the COVID-19 Fake News dataset and FakeNewsNet dataset to show the effectiveness of the proposed algorithm on detecting fake news in short news content as well as in news articles. We obtained a best F1-score of 0.9892 on the COVID-19 dataset, and an F1-score of 0.9073 on the FakeNewsNet dataset.
http://w3id.org/mlsea/pwc/scientificWork/A%20Heuristically%20Assisted%20Deep%20Reinforcement%20Learning%20Approach%20for%20Network%20Slice%20Placement                                                                                  A Heuristically Assisted Deep Reinforcement Learning Approach for Network Slice Placement                                                                                  Network Slice placement with the problem of allocation of resources from a virtualized substrate network is an optimization problem which can be formulated as a multiobjective Integer Linear Programming (ILP) problem. However, to cope with the complexity of such a continuous task and seeking for optimality and automation, the use of Machine Learning (ML) techniques appear as a promising approach. We introduce a hybrid placement solution based on Deep Reinforcement Learning (DRL) and a dedicated optimization heuristic based on the Power of Two Choices principle. The DRL algorithm uses the so-called Asynchronous Advantage Actor Critic (A3C) algorithm for fast learning, and Graph Convolutional Networks (GCN) to automate feature extraction from the physical substrate network. The proposed Heuristically-Assisted DRL (HA-DRL) allows to accelerate the learning process and gain in resource usage when compared against other state-of-the-art approaches as the evaluation results evidence.
http://w3id.org/mlsea/pwc/scientificWork/A%20Hierarchical%20Coding%20Scheme%20for%20Glasses-free%203D%20Displays%20Based%20on%20Scalable%20Hybrid%20Layered%20Representation%20of%20Real-World%20Light%20Fields                                                                                  A Hierarchical Coding Scheme for Glasses-free 3D Displays Based on Scalable Hybrid Layered Representation of Real-World Light Fields                                                                                  This paper presents a novel hierarchical coding scheme for light fields based on transmittance patterns of low-rank multiplicative layers and Fourier disparity layers. The proposed scheme learns stacked multiplicative layers from subsets of light field views determined from different scanning orders. The multiplicative layers are optimized using a fast data-driven convolutional neural network (CNN). The spatial correlation in layer patterns is exploited with varying low ranks in factorization derived from singular value decomposition on a Krylov subspace. Further, encoding with HEVC efficiently removes intra-view and inter-view correlation in low-rank approximated layers. The initial subset of approximated decoded views from multiplicative representation is used to construct Fourier disparity layer (FDL) representation. The FDL model synthesizes second subset of views which is identified by a pre-defined hierarchical prediction order. The correlations between the prediction residue of synthesized views is further eliminated by encoding the residual signal. The set of views obtained from decoding the residual is employed in order to refine the FDL model and predict the next subset of views with improved accuracy. This hierarchical procedure is repeated until all light field views are encoded. The critical advantage of proposed hybrid layered representation and coding scheme is that it utilizes not just spatial and temporal redundancies, but efficiently exploits the strong intrinsic similarities among neighboring sub-aperture images in both horizontal and vertical directions as specified by different predication orders. Besides, the scheme is flexible to realize a range of multiple bitrates at the decoder within a single integrated system. The compression performance analyzed with real light field shows substantial bitrate savings, maintaining good reconstruction quality.
http://w3id.org/mlsea/pwc/scientificWork/A%20Hierarchical%20Dual%20Model%20of%20Environment-%20and%20Place-Specific%20Utility%20for%20Visual%20Place%20Recognition                                                                                  A Hierarchical Dual Model of Environment- and Place-Specific Utility for Visual Place Recognition                                                                                  Visual Place Recognition (VPR) approaches have typically attempted to match places by identifying visual cues, image regions or landmarks that have high ``utility'' in identifying a specific place. But this concept of utility is not singular - rather it can take a range of forms. In this paper, we present a novel approach to deduce two key types of utility for VPR: the utility of visual cues `specific' to an environment, and to a particular place. We employ contrastive learning principles to estimate both the environment- and place-specific utility of Vector of Locally Aggregated Descriptors (VLAD) clusters in an unsupervised manner, which is then used to guide local feature matching through keypoint selection. By combining these two utility measures, our approach achieves state-of-the-art performance on three challenging benchmark datasets, while simultaneously reducing the required storage and compute time. We provide further analysis demonstrating that unsupervised cluster selection results in semantically meaningful results, that finer grained categorization often has higher utility for VPR than high level semantic categorization (e.g. building, road), and characterise how these two utility measures vary across different places and environments. Source code is made publicly available at https://github.com/Nik-V9/HEAPUtil.
http://w3id.org/mlsea/pwc/scientificWork/A%20Hierarchical%20State-Machine-Based%20Framework%20for%20Platoon%20Manoeuvre%20Descriptions                                                                                  A Hierarchical State-Machine-Based Framework for Platoon Manoeuvre Descriptions                                                                                  This paper introduces the SEAD framework that simplifies the process of designing and describing autonomous vehicle platooning manoeuvres. Although a large body of research has been formulating platooning manoeuvres, it is still challenging to design, describe, read, and understand them. This difficulty largely arises from missing formalisation. To fill this gap, we analysed existing ways of describing manoeuvres, derived the causes of difficulty, and designed a framework that simplifies the manoeuvre design process. Alongside, a Manoeuvre Design Language was developed to structurally describe manoeuvres in a machine-readable format. Unlike state-of-the-art manoeuvre descriptions that require one state machine for every participating vehicle, the SEAD framework allows describing any manoeuvre from the single perspective of the platoon leader. %As a proof of concept, the proposed framework was implemented in the mixed traffic simulation environment BEHAVE for an autonomous highway scenario. Using this framework, we implemented several manoeuvres as they were described in literature. To demonstrate the applicability of the framework, an experiment was performed to evaluate the execution time performance of multiple alternatives of the Join-Middle manoeuvre. This proof-of-concept experiment revealed that the manoeuvre execution time can be reduced by 28 % through parallelising various steps without considerable secondary effects. We hope that the SEAD framework will pave the way for further research in the area of new manoeuvre design and optimisation by largely simplifying and unifying platooning manoeuvre representation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Hierarchical%20Transformation-Discriminating%20Generative%20Model%20for%20Few%20Shot%20Anomaly%20Detection                                                                                  A Hierarchical Transformation-Discriminating Generative Model for Few Shot Anomaly Detection                                                                                  Anomaly detection, the task of identifying unusual samples in data, often relies on a large set of training samples. In this work, we consider the setting of few-shot anomaly detection in images, where only a few images are given at training. We devise a hierarchical generative model that captures the multi-scale patch distribution of each training image. We further enhance the representation of our model by using image transformations and optimize scale-specific patch-discriminators to distinguish between real and fake patches of the image, as well as between different transformations applied to those patches. The anomaly score is obtained by aggregating the patch-based votes of the correct transformation across scales and image regions. We demonstrate the superiority of our method on both the one-shot and few-shot settings, on the datasets of Paris, CIFAR10, MNIST and FashionMNIST as well as in the setting of defect detection on MVTec. In all cases, our method outperforms the recent baseline methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20High%20Accuracy%20Image%20Hashing%20and%20Random%20Forest%20Classifier%20for%20Crack%20Detection%20in%20Concrete%20Surface%20Images                                                                                  A High Accuracy Image Hashing and Random Forest Classifier for Crack Detection in Concrete Surface Images                                                                                  Automatic detection of cracks in concrete surfaces based on image processing is a clear trend in modern civil engineering applications. Most infrastructure is made of concrete and cracks reveal degradation of the structural integrity of the facilities, which can lead to extreme structural failures. There are many approaches to overcome the difficulties in image-based crack detection, ranging from the pre-processing of the input image to the proper adjustment of efficient classifiers, passing through the essential feature selection step. This paper is related to the process of constructing features from images to allow a classifier to find the boundaries between images with and without cracks. The most common approaches to feature extraction are the convolutional techniques to extract relevant positional information from images and the filters for edge detection or background removal. Here we apply hashing techniques for the first time used for features extraction in this problem. The study of the classification capacity of hashes is carried out by comparing 5 different hash algorithms, 2 of which are based on wavelets. The effect of applying the z-transform on the images before calculating the hashes was also studied, which totals the study of 10 new features for this problem. A comparative study of 17 different algorithms from the scikit-learn library was carried out. The results show that 9 of the 10 features are relevant to the problem, as well as that the accuracy of the classifiers varied between 0.697 for the Naive-Bayes Gaussian classifier and 0.99 for the Random Forest (RF) classifier. The feature extraction algorithm developed in this work and the RF classifier algorithm is suitable for embedded applications, for example in inspection drones, as long as they are highly accurate and computationally light, both in terms of memory and processing time.
http://w3id.org/mlsea/pwc/scientificWork/A%20High-Dynamic-Range%20Digital%20RF-Over-Fiber%20Link%20for%20MRI%20Receive%20Coils%20Using%20Delta-Sigma%20Modulation                                                                                  A High-Dynamic-Range Digital RF-Over-Fiber Link for MRI Receive Coils Using Delta-Sigma Modulation                                                                                  The coaxial cables commonly used to connect RF coil arrays with the control console of an MRI scanner are susceptible to electromagnetic coupling. As the number of RF channel increases, such coupling could result in severe heating and pose a safety concern. Non-conductive transmission solutions based on fiber-optic cables are considered to be one of the alternatives, but are limited by the high dynamic range ($>80$~dB) of typical MRI signals. A new digital fiber-optic transmission system based on delta-sigma modulation (DSM) is developed to address this problem. A DSM-based optical link is prototyped using off-the-shelf components and bench-tested at different signal oversampling rates (OSR). An end-to-end dynamic range (DR) of 81~dB, which is sufficient for typical MRI signals, is obtained over a bandwidth of 200~kHz, which corresponds to $OSR=50$. A fully-integrated custom fourth-order continuous-time DSM (CT-DSM) is designed in 180~nm CMOS technology to enable transmission of full-bandwidth MRI signals (up to 1~MHz) with adequate DR. Initial electrical test results from this custom chip are also presented.
http://w3id.org/mlsea/pwc/scientificWork/A%20High-Performance%2C%20Reconfigurable%2C%20Fully%20Integrated%20Time-Domain%20Reflectometry%20Architecture%20Using%20Digital%20I%2FOs                                                                                  A High-Performance, Reconfigurable, Fully Integrated Time-Domain Reflectometry Architecture Using Digital I/Os                                                                                  Time-domain reflectometry (TDR) is an established means of measuring impedance inhomogeneity of a variety of waveguides, providing critical data necessary to characterize and optimize the performance of high-bandwidth computational and communication systems. However, TDR systems with both the high spatial resolution (sub-cm) and voltage resolution (sub-$ muV$) required to evaluate high-performance waveguides are physically large and often cost-prohibitive, severely limiting their utility as testing platforms and greatly limiting their use in characterizing and trouble-shooting fielded hardware. Consequently, there exists a growing technical need for an electronically simple, portable, and low-cost TDR technology. The receiver of a TDR system plays a key role in recording reflection waveforms; thus, such a receiver must have high analog bandwidth, high sampling rate, and high-voltage resolution. However, these requirements are difficult to meet using low-cost analog-to-digital converters (ADCs). This article describes a new TDR architecture, namely, jitter-based APC (JAPC), which obviates the need for external components based on an alternative concept, analog-to-probability conversion (APC) that was recently proposed. These results demonstrate that a fully reconfigurable and highly integrated TDR (iTDR) can be implemented on a field-programmable gate array (FPGA) chip without using any external circuit components. Empirical evaluation of the system was conducted using an HDMI cable as the device under test (DUT), and the resulting impedance inhomogeneity pattern (IIP) of the DUT was extracted with spatial and voltage resolutions of 5 cm and 80 $ muV$, respectively. These results demonstrate the feasibility of using the prototypical JAPC-based iTDR for real-world waveguide characterization applications
http://w3id.org/mlsea/pwc/scientificWork/A%20High-fidelity%2C%20Machine-learning%20Enhanced%20Queueing%20Network%20Simulation%20Model%20for%20Hospital%20Ultrasound%20Operations                                                                                  A High-fidelity, Machine-learning Enhanced Queueing Network Simulation Model for Hospital Ultrasound Operations                                                                                  We collaborate with a large teaching hospital in Shenzhen, China and build a high-fidelity simulation model for its ultrasound center to predict key performance metrics, including the distributions of queue length, waiting time and sojourn time, with high accuracy. The key challenge to build an accurate simulation model is to understanding the complicated patient routing at the ultrasound center. To address the issue, we propose a novel two-level routing component to the queueing network model. We apply machine learning tools to calibrate the key components of the queueing model from data with enhanced accuracy.
http://w3id.org/mlsea/pwc/scientificWork/A%20High-order%20Tuner%20for%20Accelerated%20Learning%20and%20Control                                                                                  A High-order Tuner for Accelerated Learning and Control                                                                                  Gradient-descent based iterative algorithms pervade a variety of problems in estimation, prediction, learning, control, and optimization. Recently iterative algorithms based on higher-order information have been explored in an attempt to lead to accelerated learning. In this paper, we explore a specific a high-order tuner that has been shown to result in stability with time-varying regressors in linearly parametrized systems, and accelerated convergence with constant regressors. We show that this tuner continues to provide bounded parameter estimates even if the gradients are corrupted by noise. Additionally, we also show that the parameter estimates converge exponentially to a compact set whose size is dependent on noise statistics. As the HT algorithms can be applied to a wide range of problems in estimation, filtering, control, and machine learning, the result obtained in this paper represents an important extension to the topic of real-time and fast decision making.
http://w3id.org/mlsea/pwc/scientificWork/A%20Holistic%20Approach%20to%20Interpretability%20in%20Financial%20Lending%3A%20Models%2C%20Visualizations%2C%20and%20Summary-Explanations                                                                                  A Holistic Approach to Interpretability in Financial Lending: Models, Visualizations, and Summary-Explanations                                                                                  Lending decisions are usually made with proprietary models that provide minimally acceptable explanations to users. In a future world without such secrecy, what decision support tools would one want to use for justified lending decisions? This question is timely, since the economy has dramatically shifted due to a pandemic, and a massive number of new loans will be necessary in the short term. We propose a framework for such decisions, including a globally interpretable machine learning model, an interactive visualization of it, and several types of summaries and explanations for any given decision. The machine learning model is a two-layer additive risk model, which resembles a two-layer neural network, but is decomposable into subscales. In this model, each node in the first (hidden) layer represents a meaningful subscale model, and all of the nonlinearities are transparent. Our online visualization tool allows exploration of this model, showing precisely how it came to its conclusion. We provide three types of explanations that are simpler than, but consistent with, the global model: case-based reasoning explanations that use neighboring past cases, a set of features that were the most important for the model's prediction, and summary-explanations that provide a customized sparse explanation for any particular lending decision made by the model. Our framework earned the FICO recognition award for the Explainable Machine Learning Challenge, which was the first public challenge in the domain of explainable machine learning.
http://w3id.org/mlsea/pwc/scientificWork/A%20Horserace%20of%20Volatility%20Models%20for%20Cryptocurrency%3A%20Evidence%20from%20Bitcoin%20Spot%20and%20Option%20Markets                                                                                  A Horserace of Volatility Models for Cryptocurrency: Evidence from Bitcoin Spot and Option Markets                                                                                  We test various volatility models using the Bitcoin spot price series. Our models include HIST, EMA ARCH, GARCH, and EGARCH, models. Both of our in-sample-fit and out-of-sample-forecast results suggest that GARCH and EGARCH models perform much better than other models. Moreover, the EGARCH model's asymmetric term is positive and insignificant, which suggests that Bitcoin prices lack the asymmetric volatility response to past returns. Finally, we formulate an option trading strategy by exploiting the volatility spread between the GARCH volatility forecast and the option's implied volatility. We show that a simple volatility-spread trading strategy with delta-hedging can yield robust profits.
http://w3id.org/mlsea/pwc/scientificWork/A%20Hybrid%20APM-CPGSO%20Approach%20for%20Constraint%20Satisfaction%20Problem%20Solving%3A%20Application%20to%20Remote%20Sensing                                                                                  A Hybrid APM-CPGSO Approach for Constraint Satisfaction Problem Solving: Application to Remote Sensing                                                                                  Constraint satisfaction problem (CSP) has been actively used for modeling and solving a wide range of complex real-world problems. However, it has been proven that developing efficient methods for solving CSP, especially for large problems, is very difficult and challenging. Existing complete methods for problem-solving are in most cases unsuitable. Therefore, proposing hybrid CSP-based methods for problem-solving has been of increasing interest in the last decades. This paper aims at proposing a novel approach that combines incomplete and complete CSP methods for problem-solving. The proposed approach takes advantage of the group search algorithm (GSO) and the constraint propagation (CP) methods to solve problems related to the remote sensing field. To the best of our knowledge, this paper represents the first study that proposes a hybridization between an improved version of GSO and CP in the resolution of complex constraint-based problems. Experiments have been conducted for the resolution of object recognition problems in satellite images. Results show good performances in terms of convergence and running time of the proposed CSP-based method compared to existing state-of-the-art methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20Hybrid%20Approach%20to%20Scalable%20and%20Robust%20Spoken%20Language%20Understanding%20in%20Enterprise%20Virtual%20Agents                                                                                  A Hybrid Approach to Scalable and Robust Spoken Language Understanding in Enterprise Virtual Agents                                                                                  Spoken language understanding (SLU) extracts the intended mean- ing from a user utterance and is a critical component of conversational virtual agents. In enterprise virtual agents (EVAs), language understanding is substantially challenging. First, the users are infrequent callers who are unfamiliar with the expectations of a pre-designed conversation flow. Second, the users are paying customers of an enterprise who demand a reliable, consistent and efficient user experience when resolving their issues. In this work, we describe a general and robust framework for intent and entity extraction utilizing a hybrid of statistical and rule-based approaches. Our framework includes confidence modeling that incorporates information from all components in the SLU pipeline, a critical addition for EVAs to en- sure accuracy. Our focus is on creating accurate and scalable SLU that can be deployed rapidly for a large class of EVA applications with little need for human intervention.
http://w3id.org/mlsea/pwc/scientificWork/A%20Hybrid%20Architecture%20for%20Federated%20and%20Centralized%20Learning                                                                                  A Hybrid Architecture for Federated and Centralized Learning                                                                                  Many of the machine learning tasks rely on centralized learning (CL), which requires the transmission of local datasets from the clients to a parameter server (PS) entailing huge communication overhead. To overcome this, federated learning (FL) has been suggested as a promising tool, wherein the clients send only the model updates to the PS instead of the whole dataset. However, FL demands powerful computational resources from the clients. In practice, not all the clients have sufficient computational resources to participate in training. To address this common scenario, we propose a more efficient approach called hybrid federated and centralized learning (HFCL), wherein only the clients with sufficient resources employ FL, while the remaining ones send their datasets to the PS, which computes the model on behalf of them. Then, the model parameters are aggregated at the PS. To improve the efficiency of dataset transmission, we propose two different techniques: i) increased computation-per-client and ii) sequential data transmission. Notably, the HFCL frameworks outperform FL with up to 20 % improvement in the learning accuracy when only half of the clients perform FL while having 50 % less communication overhead than CL since all the clients collaborate on the learning process with their datasets.
http://w3id.org/mlsea/pwc/scientificWork/A%20Hybrid%20Decomposition-based%20Multi-objective%20Evolutionary%20Algorithm%20for%20the%20Multi-Point%20Dynamic%20Aggregation%20Problem                                                                                  A Hybrid Decomposition-based Multi-objective Evolutionary Algorithm for the Multi-Point Dynamic Aggregation Problem                                                                                  An emerging optimisation problem from the real-world applications, named the multi-point dynamic aggregation (MPDA) problem, has become one of the active research topics of the multi-robot system. This paper focuses on a multi-objective MPDA problem which is to design an execution plan of the robots to minimise the number of robots and the maximal completion time of all the tasks. The strongly-coupled relationships among robots and tasks, the redundancy of the MPDA encoding, and the variable-size decision space of the MO-MPDA problem posed extra challenges for addressing the problem effectively. To address the above issues, we develop a hybrid decomposition-based multi-objective evolutionary algorithm (HDMOEA) using $ varepsilon $-constraint method. It selects the maximal completion time of all tasks as the main objective, and converted the other objective into constraints. HDMOEA decomposes a MO-MPDA problem into a series of scalar constrained optimization subproblems by assigning each subproblem with an upper bound robot number. All the subproblems are optimized simultaneously with the transferring knowledge from other subproblems. Besides, we develop a hybrid population initialisation mechanism to enhance the quality of initial solutions, and a reproduction mechanism to transmit effective information and tackle the encoding redundancy. Experimental results show that the proposed HDMOEA method significantly outperforms the state-of-the-art methods in terms of several most-used metrics.
http://w3id.org/mlsea/pwc/scientificWork/A%20Hybrid%20Genetic-Fuzzy%20Controller%20for%20a%2014-inches%20Astronomical%20Telescope%20Tracking                                                                                  A Hybrid Genetic-Fuzzy Controller for a 14-inches Astronomical Telescope Tracking                                                                                  The performance of on telescope depend strongly on its operating conditions. During pointing the telescope can move at a relatively high velocity, and the system can tolerate trajectory position errors higher than during tracking. On the contrary, during tracking Alt-Az telescopes generally move slower but still in a large dynamic range. In this case, the position errors must be as close to zero as possible. Tracking is one of the essential factors that affect the quality of astronomical observations. In this paper, a hybrid Genetic-Fuzzy approach to control the movement of a two-link direct-drive Celestron telescope is introduced. The proposed controller uses the Genetic algorithm (GA) for optimizing a fuzzy logic controller (FLC) to improve the tracking of the 14-inches Celestron telescope of the Kottamia Astronomical Observatory (KAO). The fuzzy logic input is a vector of the position error and its rate of change, and the output is torque. The GA objective function used here is the Integral Time Absolute Error (ITAE). The proposed method is compared with a conventional Proportional-Differential (PD) controller, an optimized PD controller with a GA, and a Fuzzy controller. The results show the effectiveness of the proposed controller to improve the dynamic response of the overall system.
http://w3id.org/mlsea/pwc/scientificWork/A%20Hybrid%20Model%20for%20Combining%20Neural%20Image%20Caption%20and%20k-Nearest%20Neighbor%20Approach%20for%20Image%20Captioning                                                                                  A Hybrid Model for Combining Neural Image Caption and k-Nearest Neighbor Approach for Image Captioning                                                                                  A hybrid model is proposed that integrates two popular image captioning methods to generate a text-based summary describing the contents of the image. The two image captioning models are the Neural Image Caption (NIC) and the k-nearest neighbor approach. These are trained individually on the training set. We extract a set of five features, from the validation set, for evaluating the results of the two models that in turn is used to train a logistic regression classifier. The BLEU-4 scores of the two models are compared for generating the binary-value ground truth for the logistic regression classifier. For the test set, the input images are first passed separately through the two models to generate the individual captions. The five-dimensional feature set extracted from the two models is passed to the logistic regression classifier to take a decision regarding the final caption generated which is the best of two captions generated by the models. Our implementation of the k-nearest neighbor model achieves a BLEU-4 score of 15.95 and the NIC model achieves a BLEU-4 score of 16.01, on the benchmark Flickr8k dataset. The proposed hybrid model is able to achieve a BLEU-4 score of 18.20 proving the validity of our approach.
http://w3id.org/mlsea/pwc/scientificWork/A%20Hybrid%20Queuing%20Model%20for%20Coordinated%20Vehicle%20Platooning%20on%20Mixed-Autonomy%20Highways%3A%20Training%20and%20Validation                                                                                  A Hybrid Queuing Model for Coordinated Vehicle Platooning on Mixed-Autonomy Highways: Training and Validation                                                                                  Platooning of connected and autonomous vehicles (CAVs) is an emerging technology with a strong potential for throughput improvement and fuel reduction. Adequate macroscopic models are critical for system-level efficiency and reliability of platooning. In this paper, we consider a hybrid queuing model for a mixed-autonomy highway section and develop an easy-to-use training algorithm. The model predicts CAV and non-CAV counts according to the traffic demand as well as key parameters of the highway section. The training algorithm learns the highway parameters from observed data in real time. We test the model and the algorithm in Simulation of Urban Mobility (SUMO) and show that the prediction error is around 15% in a stationary setting and around 25% in a non-stationary setting. We also show that the trained model leads to a platoon headway regulation policy very close to the simulated optimum. The proposed model and algorithm can directly support model-predictive decision-making for platooning in mixed autonomy.
http://w3id.org/mlsea/pwc/scientificWork/A%20Hybrid%20Recommender%20System%20for%20Recommending%20Smartphones%20to%20Prospective%20Customers                                                                                  A Hybrid Recommender System for Recommending Smartphones to Prospective Customers                                                                                  Recommender Systems are a subclass of machine learning systems that employ sophisticated information filtering strategies to reduce the search time and suggest the most relevant items to any particular user. Hybrid recommender systems combine multiple recommendation strategies in different ways to benefit from their complementary advantages. Some hybrid recommender systems have combined collaborative filtering and content-based approaches to build systems that are more robust. In this paper, we propose a hybrid recommender system, which combines Alternating Least Squares (ALS) based collaborative filtering with deep learning to enhance recommendation performance as well as overcome the limitations associated with the collaborative filtering approach, especially concerning its cold start problem. In essence, we use the outputs from ALS (collaborative filtering) to influence the recommendations from a Deep Neural Network (DNN), which combines characteristic, contextual, structural and sequential information, in a big data processing framework. We have conducted several experiments in testing the efficacy of the proposed hybrid architecture in recommending smartphones to prospective customers and compared its performance with other open-source recommenders. The results have shown that the proposed system has outperformed several existing hybrid recommender systems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Hybrid%20mmWave%20and%20Camera%20System%20for%20Long-Range%20Depth%20Imaging                                                                                  A Hybrid mmWave and Camera System for Long-Range Depth Imaging                                                                                  mmWave radars offer excellent depth resolution even at very long ranges owing to their high bandwidth. But their angular resolution is at least an order-of-magnitude worse than camera and lidar systems. Hence, mmWave radar is not a capable 3-D imaging solution in isolation. We propose Metamoran, a system that combines the complimentary strengths of radar and camera to obtain accurate, high resolution depth images over long ranges even in high clutter environments, all from a single fixed vantage point. Metamoran enables rich long-range depth imaging with applications in security and surveillance, roadside safety infrastructure and wide-area mapping. Our approach leverages the high angular resolution from cameras using computer vision techniques, including image segmentation and monocular depth estimation, to obtain object shape. Our core contribution is a method to convert this object shape into an RF I/Q equivalent, which we use in a novel radar processing pipeline to help declutter the scene and capture extremely weak reflections from objects at long distances. We perform a detailed evaluation of Metamoran's depth imaging capabilities in 400 diverse scenes. Our evaluation shows that Metamoran estimates the depth of static objects up to 90 m and moving objects up to 305 m and with a median error of 28 cm, an improvement of 13$ times$ compared to a naive radar+camera baseline and 23$ times$ compared to monocular depth estimation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Hyperbolic-to-Hyperbolic%20Graph%20Convolutional%20Network                                                                                  A Hyperbolic-to-Hyperbolic Graph Convolutional Network                                                                                  Hyperbolic graph convolutional networks (GCNs) demonstrate powerful representation ability to model graphs with hierarchical structure. Existing hyperbolic GCNs resort to tangent spaces to realize graph convolution on hyperbolic manifolds, which is inferior because tangent space is only a local approximation of a manifold. In this paper, we propose a hyperbolic-to-hyperbolic graph convolutional network (H2H-GCN) that directly works on hyperbolic manifolds. Specifically, we developed a manifold-preserving graph convolution that consists of a hyperbolic feature transformation and a hyperbolic neighborhood aggregation. The hyperbolic feature transformation works as linear transformation on hyperbolic manifolds. It ensures the transformed node representations still lie on the hyperbolic manifold by imposing the orthogonal constraint on the transformation sub-matrix. The hyperbolic neighborhood aggregation updates each node representation via the Einstein midpoint. The H2H-GCN avoids the distortion caused by tangent space approximations and keeps the global hyperbolic structure. Extensive experiments show that the H2H-GCN achieves substantial improvements on the link prediction, node classification, and graph classification tasks.
http://w3id.org/mlsea/pwc/scientificWork/A%20Hypothesis%20Testing%20Approach%20to%20Nonstationary%20Source%20Separation                                                                                  A Hypothesis Testing Approach to Nonstationary Source Separation                                                                                  The extraction of nonstationary signals from blind and semi-blind multivariate observations is a recurrent problem. Numerous algorithms have been developed for this problem, which are based on the exact or approximate joint diagonalization of second or higher order cumulant matrices/tensors of multichannel data. While a great body of research has been dedicated to joint diagonalization algorithms, the selection of the diagonalized matrix/tensor set remains highly problem-specific. Herein, various methods for nonstationarity identification are reviewed and a new general framework based on hypothesis testing is proposed, which results in a classification/clustering perspective to semi-blind source separation of nonstationary components. The proposed method is applied to noninvasive fetal ECG extraction, as case study.
http://w3id.org/mlsea/pwc/scientificWork/A%20Joint%20Design%20of%20MIMO-OFDM%20Dual-Function%20Radar%20Communication%20System%20Using%20Generalized%20Spatial%20Modulation                                                                                  A Joint Design of MIMO-OFDM Dual-Function Radar Communication System Using Generalized Spatial Modulation                                                                                  A novel dual-function radar communication (DFRC) system is proposed, that achieves high target resolution and high communication rate. It consists of a multiple-input multiple-output (MIMO) radar, where only a small number of antennas are active in each channel use. The probing waveforms are orthogonal frequency division multiplexing (OFDM) type. The OFDM carriers are divided into two groups, one that is used by the active antennas in a shared fashion, and another one, where each subcarrier is assigned to an active antenna in an exclusive fashion (private subcarriers). Target estimation is carried out based on the received and transmitted symbols. The system communicates information via the transmitted OFDM data symbols and the pattern of active antennas in a generalized spatial modulation (GSM) fashion. A multi-antenna communication receiver can identify the indices of active antennas via sparse signal recovery methods. The use of shared subcarriers enables high communication rate. The private subcarriers are used to synthesize a virtual array for high angular resolution, and also for improved estimation on the active antenna indices. The OFDM waveforms allow the communication receiver to easily mitigate the effect of frequency selective fading, while the use of a sparse array at the transmitter reduces the hardware cost of the system. The radar performance of the proposed DFRC system is evaluated via simulations, and bit error rate (BER) results for the communication system are provided.
http://w3id.org/mlsea/pwc/scientificWork/A%20Joint%20Energy%20and%20Latency%20Framework%20for%20Transfer%20Learning%20over%205G%20Industrial%20Edge%20Networks                                                                                  A Joint Energy and Latency Framework for Transfer Learning over 5G Industrial Edge Networks                                                                                  In this paper, we propose a transfer learning (TL)-enabled edge-CNN framework for 5G industrial edge networks with privacy-preserving characteristic. In particular, the edge server can use the existing image dataset to train the CNN in advance, which is further fine-tuned based on the limited datasets uploaded from the devices. With the aid of TL, the devices that are not participating in the training only need to fine-tune the trained edge-CNN model without training from scratch. Due to the energy budget of the devices and the limited communication bandwidth, a joint energy and latency problem is formulated, which is solved by decomposing the original problem into an uploading decision subproblem and a wireless bandwidth allocation subproblem. Experiments using ImageNet demonstrate that the proposed TL-enabled edge-CNN framework can achieve almost 85% prediction accuracy of the baseline by uploading only about 1% model parameters, for a compression ratio of 32 of the autoencoder.
http://w3id.org/mlsea/pwc/scientificWork/A%20Joint%20Intensity-Neuromorphic%20Event%20Imaging%20System%20for%20Resource%20Constrained%20Devices                                                                                  A Joint Intensity-Neuromorphic Event Imaging System for Resource Constrained Devices                                                                                  We present a novel adaptive multi-modal intensity-event algorithm to optimize an overall objective of object tracking under bit rate constraints for a host-chip architecture. The chip is a computationally resource constrained device acquiring high resolution intensity frames and events, while the host is capable of performing computationally expensive tasks. We develop a joint intensity-neuromorphic event rate-distortion compression framework with a quadtree (QT) based compression of intensity and events scheme. The data acquisition on the chip is driven by the presence of objects of interest in the scene as detected by an object detector. The most informative intensity and event data are communicated to the host under rate constraints, so that the best possible tracking performance is obtained. The detection and tracking of objects in the scene are done on the distorted data at the host. Intensity and events are jointly used in a fusion framework to enhance the quality of the distorted images, so as to improve the object detection and tracking performance. The performance assessment of the overall system is done in terms of the multiple object tracking accuracy (MOTA) score. Compared to using intensity modality only, there is an improvement in MOTA using both these modalities in different scenarios.
http://w3id.org/mlsea/pwc/scientificWork/A%20Joint%20Model%20for%20Dropped%20Pronoun%20Recovery%20and%20Conversational%20Discourse%20Parsing%20in%20Chinese%20Conversational%20Speech                                                                                  A Joint Model for Dropped Pronoun Recovery and Conversational Discourse Parsing in Chinese Conversational Speech                                                                                  In this paper, we present a neural model for joint dropped pronoun recovery (DPR) and conversational discourse parsing (CDP) in Chinese conversational speech. We show that DPR and CDP are closely related, and a joint model benefits both tasks. We refer to our model as DiscProReco, and it first encodes the tokens in each utterance in a conversation with a directed Graph Convolutional Network (GCN). The token states for an utterance are then aggregated to produce a single state for each utterance. The utterance states are then fed into a biaffine classifier to construct a conversational discourse graph. A second (multi-relational) GCN is then applied to the utterance states to produce a discourse relation-augmented representation for the utterances, which are then fused together with token states in each utterance as input to a dropped pronoun recovery layer. The joint model is trained and evaluated on a new Structure Parsing-enhanced Dropped Pronoun Recovery (SPDPR) dataset that we annotated with both two types of information. Experimental results on the SPDPR dataset and other benchmarks show that DiscProReco significantly outperforms the state-of-the-art baselines of both tasks.
http://w3id.org/mlsea/pwc/scientificWork/A%20Joint%20Network%20for%20Grasp%20Detection%20Conditioned%20on%20Natural%20Language%20Commands                                                                                  A Joint Network for Grasp Detection Conditioned on Natural Language Commands                                                                                  We consider the task of grasping a target object based on a natural language command query. Previous work primarily focused on localizing the object given the query, which requires a separate grasp detection module to grasp it. The cascaded application of two pipelines incurs errors in overlapping multi-object cases due to ambiguity in the individual outputs. This work proposes a model named Command Grasping Network(CGNet) to directly output command satisficing grasps from RGB image and textual command inputs. A dataset with ground truth (image, command, grasps) tuple is generated based on the VMRD dataset to train the proposed network. Experimental results on the generated test set show that CGNet outperforms a cascaded object-retrieval and grasp detection baseline by a large margin. Three physical experiments demonstrate the functionality and performance of CGNet.
http://w3id.org/mlsea/pwc/scientificWork/A%20Joint%20Technique%20for%20Nonlinearity%20Compensation%20in%20CO-OFDM%20Superchannel%20Systems                                                                                  A Joint Technique for Nonlinearity Compensation in CO-OFDM Superchannel Systems                                                                                  We propose a technique combining the singlechannel digital-back-propagation (SC-DBP) with phaseconjugated-twin-wave (PCTW) to compensate nonlinearities in CO-OFDM superchannel systems. This exhibits a similar performance as multi-channel DBP while providing increased transmission reach compared to SC-DBP, PCTW, and linear dispersion compensation (LDC).
http://w3id.org/mlsea/pwc/scientificWork/A%20Joint%20Wireless-Optical%20Front-haul%20Solution%20for%20Multi-user%20Massive%20MIMO%205G%20RAN                                                                                  A Joint Wireless-Optical Front-haul Solution for Multi-user Massive MIMO 5G RAN                                                                                  We demonstrate a high capacity IF-over-fiber mobile fronthaul solution for multi-user massive MIMO 5G RAN. Using this scheme, a record aggregated radio bandwidth of 25.6 GHz was transmitted on a single optical wavelength over 40 km without fiber chromatic dispersion compensation
http://w3id.org/mlsea/pwc/scientificWork/A%20Knowledge%20Enhanced%20Learning%20and%20Semantic%20Composition%20Model%20for%20Multi-Claim%20Fact%20Checking                                                                                  A Knowledge Enhanced Learning and Semantic Composition Model for Multi-Claim Fact Checking                                                                                  To inhibit the spread of rumorous information and its severe consequences, traditional fact checking aims at retrieving relevant evidence to verify the veracity of a given claim. Fact checking methods typically use knowledge graphs (KGs) as external repositories and develop reasoning mechanism to retrieve evidence for verifying the triple claim. However, existing methods only focus on verifying a single claim. As real-world rumorous information is more complex and a textual statement is often composed of multiple clauses (i.e. represented as multiple claims instead of a single one), multiclaim fact checking is not only necessary but more important for practical applications. Although previous methods for verifying a single triple can be applied repeatedly to verify multiple triples one by one, they ignore the contextual information implied in a multi-claim statement and could not learn the rich semantic information in the statement as a whole. In this paper, we propose an end-to-end knowledge enhanced learning and verification method for multi-claim fact checking. Our method consists of two modules, KG-based learning enhancement and multi-claim semantic composition. To fully utilize the contextual information, the KG-based learning enhancement module learns the dynamic context-specific representations via selectively aggregating relevant attributes of entities. To capture the compositional semantics of multiple triples, the multi-claim semantic composition module constructs the graph structure to model claim-level interactions, and integrates global and salient local semantics with multi-head attention. Experimental results on a real-world dataset and two benchmark datasets show the effectiveness of our method for multi-claim fact checking over KG.
http://w3id.org/mlsea/pwc/scientificWork/A%20Knowledge%20Graph-Enhanced%20Tensor%20Factorisation%20Model%20for%20Discovering%20Drug%20Targets                                                                                  A Knowledge Graph-Enhanced Tensor Factorisation Model for Discovering Drug Targets                                                                                  The drug discovery and development process is a long and expensive one, costing over 1 billion USD on average per drug and taking 10-15 years. To reduce the high levels of attrition throughout the process, there has been a growing interest in applying machine learning methodologies to various stages of drug discovery and development in the recent decade, especially at the earliest stage identification of druggable disease genes. In this paper, we have developed a new tensor factorisation model to predict potential drug targets (genes or proteins) for treating diseases. We created a three dimensional data tensor consisting of 1,048 gene targets, 860 diseases and 230,011 evidence attributes and clinical outcomes connecting them, using data extracted from the Open Targets and PharmaProjects databases. We enriched the data with gene target representations learned from a drug discovery oriented knowledge graph and applied our proposed method to predict the clinical outcomes for unseen gene target and disease pairs. We designed three evaluation strategies to measure the prediction performance and benchmarked several commonly used machine learning classifiers together with Bayesian matrix and tensor factorisation methods. The result shows that incorporating knowledge graph embeddings significantly improves the prediction accuracy and that training tensor factorisation alongside a dense neural network outperforms all other baselines. In summary, our framework combines two actively studied machine learning approaches to disease target identification, namely tensor factorisation and knowledge graph representation learning, which could be a promising avenue for further exploration in data driven drug discovery.
http://w3id.org/mlsea/pwc/scientificWork/A%20Language%20for%20Modeling%20And%20Optimizing%20Experimental%20Biological%20Protocols                                                                                  A Language for Modeling And Optimizing Experimental Biological Protocols                                                                                  Automation is becoming ubiquitous in all laboratory activities, leading towards precisely defined and codified laboratory protocols. However, the integration between laboratory protocols and mathematical models is still lacking. Models describe physical processes, while protocols define the steps carried out during an experiment: neither cover the domain of the other, although they both attempt to characterize the same phenomena. We should ideally start from an integrated description of both the model and the steps carried out to test it, to concurrently analyze uncertainties in model parameters, equipment tolerances, and data collection. To this end, we present a language to model and optimize experimental biochemical protocols that facilitates such an integrated description, and that can be combined with experimental data. We provide a probabilistic semantics for our language based on a Bayesian interpretation that formally characterizes the uncertainties in both the data collection, the underlying model, and the protocol operations. On a set of case studies we illustrate how the resulting framework allows for automated analysis and optimization of experimental protocols, including Gibson assembly protocols.
http://w3id.org/mlsea/pwc/scientificWork/A%20Large%20Visual%2C%20Qualitative%20and%20Quantitative%20Dataset%20of%20Web%20Pages                                                                                  A Large Visual, Qualitative and Quantitative Dataset of Web Pages                                                                                  The World Wide Web is not only one of the most important platforms of communication and information at present, but also an area of growing interest for scientific research. This motivates a lot of work and projects that require large amounts of data. However, there is no dataset that integrates the parameters and visual appearance of Web pages, because its collection is a costly task in terms of time and effort. With the support of various computer tools and programming scripts, we have created a large dataset of 49,438 Web pages. It consists of visual, textual and numerical data types, includes all countries worldwide, and considers a broad range of topics such as art, entertainment, economy, business, education, government, news, media, science, and environment, covering different cultural characteristics and varied design preferences. In this paper, we describe the process of collecting, debugging and publishing the final product, which is freely available. To demonstrate the usefulness of our dataset, we expose a binary classification model for detecting error Web pages, and a multi-class Web subject-based categorization, both problems using convolutional neural networks.
http://w3id.org/mlsea/pwc/scientificWork/A%20Large-Scale%20Analysis%20of%20Mixed%20Initiative%20in%20Information-Seeking%20Dialogues%20for%20Conversational%20Search                                                                                  A Large-Scale Analysis of Mixed Initiative in Information-Seeking Dialogues for Conversational Search                                                                                  Conversational search is a relatively young area of research that aims at automating an information-seeking dialogue. In this paper we help to position it with respect to other research areas within conversational Artificial Intelligence (AI) by analysing the structural properties of an information-seeking dialogue. To this end, we perform a large-scale dialogue analysis of more than 150K transcripts from 16 publicly available dialogue datasets. These datasets were collected to inform different dialogue-based tasks including conversational search. We extract different patterns of mixed initiative from these dialogue transcripts and use them to compare dialogues of different types. Moreover, we contrast the patterns found in information-seeking dialogues that are being used for research purposes with the patterns found in virtual reference interviews that were conducted by professional librarians. The insights we provide (1) establish close relations between conversational search and other conversational AI tasks; and (2) uncover limitations of existing conversational datasets to inform future data collection tasks.
http://w3id.org/mlsea/pwc/scientificWork/A%20Large-Scale%20Benchmark%20for%20Food%20Image%20Segmentation                                                                                  A Large-Scale Benchmark for Food Image Segmentation                                                                                  Food image segmentation is a critical and indispensible task for developing health-related applications such as estimating food calories and nutrients. Existing food image segmentation models are underperforming due to two reasons: (1) there is a lack of high quality food image datasets with fine-grained ingredient labels and pixel-wise location masks -- the existing datasets either carry coarse ingredient labels or are small in size; and (2) the complex appearance of food makes it difficult to localize and recognize ingredients in food images, e.g., the ingredients may overlap one another in the same image, and the identical ingredient may appear distinctly in different food images. In this work, we build a new food image dataset FoodSeg103 (and its extension FoodSeg154) containing 9,490 images. We annotate these images with 154 ingredient classes and each image has an average of 6 ingredient labels and pixel-wise masks. In addition, we propose a multi-modality pre-training approach called ReLeM that explicitly equips a segmentation model with rich and semantic food knowledge. In experiments, we use three popular semantic segmentation methods (i.e., Dilated Convolution based, Feature Pyramid based, and Vision Transformer based) as baselines, and evaluate them as well as ReLeM on our new datasets. We believe that the FoodSeg103 (and its extension FoodSeg154) and the pre-trained models using ReLeM can serve as a benchmark to facilitate future works on fine-grained food image understanding. We make all these datasets and methods public at url{https://xiongweiwu.github.io/foodseg103.html}.
http://w3id.org/mlsea/pwc/scientificWork/A%20Large-Scale%20Dataset%20for%20Benchmarking%20Elevator%20Button%20Segmentation%20and%20Character%20Recognition                                                                                  A Large-Scale Dataset for Benchmarking Elevator Button Segmentation and Character Recognition                                                                                  Human activities are hugely restricted by COVID-19, recently. Robots that can conduct inter-floor navigation attract much public attention, since they can substitute human workers to conduct the service work. However, current robots either depend on human assistance or elevator retrofitting, and fully autonomous inter-floor navigation is still not available. As the very first step of inter-floor navigation, elevator button segmentation and recognition hold an important position. Therefore, we release the first large-scale publicly available elevator panel dataset in this work, containing 3,718 panel images with 35,100 button labels, to facilitate more powerful algorithms on autonomous elevator operation. Together with the dataset, a number of deep learning based implementations for button segmentation and recognition are also released to benchmark future methods in the community. The dataset will be available at url{https://github.com/zhudelong/elevator_button_recognition
http://w3id.org/mlsea/pwc/scientificWork/A%20Large-Scale%20Rich%20Context%20Query%20and%20Recommendation%20Dataset%20in%20Online%20Knowledge-Sharing                                                                                  A Large-Scale Rich Context Query and Recommendation Dataset in Online Knowledge-Sharing                                                                                  Data plays a vital role in machine learning studies. In the research of recommendation, both user behaviors and side information are helpful to model users. So, large-scale real scenario datasets with abundant user behaviors will contribute a lot. However, it is not easy to get such datasets as most of them are only hold and protected by companies. In this paper, a new large-scale dataset collected from a knowledge-sharing platform is presented, which is composed of around 100M interactions collected within 10 days, 798K users, 165K questions, 554K answers, 240K authors, 70K topics, and more than 501K user query keywords. There are also descriptions of users, answers, questions, authors, and topics, which are anonymous. Note that each user's latest query keywords have not been included in previous open datasets, which reveal users' explicit information needs. We characterize the dataset and demonstrate its potential applications for recommendation study. Multiple experiments show the dataset can be used to evaluate algorithms in general top-N recommendation, sequential recommendation, and context-aware recommendation. This dataset can also be used to integrate search and recommendation and recommendation with negative feedback. Besides, tasks beyond recommendation, such as user gender prediction, most valuable answerer identification, and high-quality answer recognition, can also use this dataset. To the best of our knowledge, this is the largest real-world interaction dataset for personalized recommendation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Large-Scale%20Study%20on%20Unsupervised%20Spatiotemporal%20Representation%20Learning                                                                                  A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning                                                                                  We present a large-scale study on unsupervised spatiotemporal representation learning from videos. With a unified perspective on four recent image-based frameworks, we study a simple objective that can easily generalize all these methods to space-time. Our objective encourages temporally-persistent features in the same video, and in spite of its simplicity, it works surprisingly well across: (i) different unsupervised frameworks, (ii) pre-training datasets, (iii) downstream datasets, and (iv) backbone architectures. We draw a series of intriguing observations from this study, e.g., we discover that encouraging long-spanned persistency can be effective even if the timespan is 60 seconds. In addition to state-of-the-art results in multiple benchmarks, we report a few promising cases in which unsupervised pre-training can outperform its supervised counterpart. Code is made available at https://github.com/facebookresearch/SlowFast
http://w3id.org/mlsea/pwc/scientificWork/A%20Large-scale%20Evaluation%20of%20Neural%20Machine%20Transliteration%20for%20Indic%20Languages                                                                                  A Large-scale Evaluation of Neural Machine Transliteration for Indic Languages                                                                                  We take up the task of large-scale evaluation of neural machine transliteration between English and Indic languages, with a focus on multilingual transliteration to utilize orthographic similarity between Indian languages. We create a corpus of 600K word pairs mined from parallel translation corpora and monolingual corpora, which is the largest transliteration corpora for Indian languages mined from public sources. We perform a detailed analysis of multilingual transliteration and propose an improved multilingual training recipe for Indic languages. We analyze various factors affecting transliteration quality like language family, transliteration direction and word origin.
http://w3id.org/mlsea/pwc/scientificWork/A%20Large-scale%20Study%20on%20Unsupervised%20Outlier%20Model%20Selection%3A%20Do%20Internal%20Strategies%20Suffice%3F                                                                                  A Large-scale Study on Unsupervised Outlier Model Selection: Do Internal Strategies Suffice?                                                                                  Given an unsupervised outlier detection task, how should one select a detection algorithm as well as its hyperparameters (jointly called a model)? Unsupervised model selection is notoriously difficult, in the absence of hold-out validation data with ground-truth labels. Therefore, the problem is vastly understudied. In this work, we study the feasibility of employing internal model evaluation strategies for selecting a model for outlier detection. These so-called internal strategies solely rely on the input data (without labels) and the output (outlier scores) of the candidate models. We setup (and open-source) a large testbed with 39 detection tasks and 297 candidate models comprised of 8 detectors and various hyperparameter configurations. We evaluate 7 different strategies on their ability to discriminate between models w.r.t. detection performance, without using any labels. Our study reveals room for progress -- we find that none would be practically useful, as they select models only comparable to a state-of-the-art detector (with random configuration).
http://w3id.org/mlsea/pwc/scientificWork/A%20Latent%20Transformer%20for%20Disentangled%20Face%20Editing%20in%20Images%20and%20Videos                                                                                  A Latent Transformer for Disentangled Face Editing in Images and Videos                                                                                  High quality facial image editing is a challenging problem in the movie post-production industry, requiring a high degree of control and identity preservation. Previous works that attempt to tackle this problem may suffer from the entanglement of facial attributes and the loss of the person's identity. Furthermore, many algorithms are limited to a certain task. To tackle these limitations, we propose to edit facial attributes via the latent space of a StyleGAN generator, by training a dedicated latent transformation network and incorporating explicit disentanglement and identity preservation terms in the loss function. We further introduce a pipeline to generalize our face editing to videos. Our model achieves a disentangled, controllable, and identity-preserving facial attribute editing, even in the challenging case of real (i.e., non-synthetic) images and videos. We conduct extensive experiments on image and video datasets and show that our model outperforms other state-of-the-art methods in visual quality and quantitative evaluation. Source codes are available at https://github.com/InterDigitalInc/latent-transformer.
http://w3id.org/mlsea/pwc/scientificWork/A%20Latent%20space%20solver%20for%20PDE%20generalization                                                                                  A Latent space solver for PDE generalization                                                                                  In this work we propose a hybrid solver to solve partial differential equation (PDE)s in the latent space. The solver uses an iterative inferencing strategy combined with solution initialization to improve generalization of PDE solutions. The solver is tested on an engineering case and the results show that it can generalize well to several PDE conditions.
http://w3id.org/mlsea/pwc/scientificWork/A%20Learnable%20Self-supervised%20Task%20for%20Unsupervised%20Domain%20Adaptation%20on%20Point%20Clouds                                                                                  A Learnable Self-supervised Task for Unsupervised Domain Adaptation on Point Clouds                                                                                  Deep neural networks have achieved promising performance in supervised point cloud applications, but manual annotation is extremely expensive and time-consuming in supervised learning schemes. Unsupervised domain adaptation (UDA) addresses this problem by training a model with only labeled data in the source domain but making the model generalize well in the target domain. Existing studies show that self-supervised learning using both source and target domain data can help improve the adaptability of trained models, but they all rely on hand-crafted designs of the self-supervised tasks. In this paper, we propose a learnable self-supervised task and integrate it into a self-supervision-based point cloud UDA architecture. Specifically, we propose a learnable nonlinear transformation that transforms a part of a point cloud to generate abundant and complicated point clouds while retaining the original semantic information, and the proposed self-supervised task is to reconstruct the original point cloud from the transformed ones. In the UDA architecture, an encoder is shared between the networks for the self-supervised task and the main task of point cloud classification or segmentation, so that the encoder can be trained to extract features suitable for both the source and the target domain data. Experiments on PointDA-10 and PointSegDA datasets show that the proposed method achieves new state-of-the-art performance on both classification and segmentation tasks of point cloud UDA. Code will be made publicly available.
http://w3id.org/mlsea/pwc/scientificWork/A%20Learning-based%20Optimal%20Market%20Bidding%20Strategy%20for%20Price-Maker%20Energy%20Storage                                                                                  A Learning-based Optimal Market Bidding Strategy for Price-Maker Energy Storage                                                                                  Load serving entities with storage units reach sizes and performances that can significantly impact clearing prices in electricity markets. Nevertheless, price endogeneity is rarely considered in storage bidding strategies and modeling the electricity market is a challenging task. Meanwhile, model-free reinforcement learning such as the Actor-Critic are becoming increasingly popular for designing energy system controllers. Yet implementation frequently requires lengthy, data-intense, and unsafe trial-and-error training. To fill these gaps, we implement an online Supervised Actor-Critic (SAC) algorithm, supervised with a model-based controller -- Model Predictive Control (MPC). The energy storage agent is trained with this algorithm to optimally bid while learning and adjusting to its impact on the market clearing prices. We compare the supervised Actor-Critic algorithm with the MPC algorithm as a supervisor, finding that the former reaps higher profits via learning. Our contribution, thus, is an online and safe SAC algorithm that outperforms the current model-based state-of-the-art.
http://w3id.org/mlsea/pwc/scientificWork/A%20LiDAR%20Assisted%20Control%20Module%20with%20High%20Precision%20in%20Parking%20Scenarios%20for%20Autonomous%20Driving%20Vehicle                                                                                  A LiDAR Assisted Control Module with High Precision in Parking Scenarios for Autonomous Driving Vehicle                                                                                  Autonomous driving has been quite promising in recent years. The public has seen Robotaxi delivered by Waymo, Baidu, Cruise, and so on. While autonomous driving vehicles certainly have a bright future, we have to admit that it is still a long way to go for products such as Robotaxi. On the other hand, in less complex scenarios autonomous driving may have the potentiality to reliably outperform humans. For example, humans are good at interactive tasks (while autonomous driving systems usually do not), but we are often incompetent for tasks with strict precision demands. In this paper, we introduce a real-world, industrial scenario of which human drivers are not capable. The task required the ego vehicle to keep a stationary lateral distance (i.e. 3? <= 5 centimeters) with respect to a reference. To address this challenge, we redesigned the control module from Baidu Apollo open-source autonomous driving system. A precise (3? <= 2 centimeters) Error Feedback System was first built to partly replace the localization module. Then we investigated the control module thoroughly and added a real-time calibration algorithm to gain extra precision. We also built a simulation to fine-tune the control parameters. After all those works, the results are encouraging, showing that an end-to-end lateral precision with 3? <= 5 centimeters has been achieved. Further, we show that the results not only outperformed original Apollo modules but also beat specially trained and highly experienced human test drivers.
http://w3id.org/mlsea/pwc/scientificWork/A%20Light%20Stage%20on%20Every%20Desk                                                                                  A Light Stage on Every Desk                                                                                  Every time you sit in front of a TV or monitor, your face is actively illuminated by time-varying patterns of light. This paper proposes to use this time-varying illumination for synthetic relighting of your face with any new illumination condition. In doing so, we take inspiration from the light stage work of Debevec et al., who first demonstrated the ability to relight people captured in a controlled lighting environment. Whereas existing light stages require expensive, room-scale spherical capture gantries and exist in only a few labs in the world, we demonstrate how to acquire useful data from a normal TV or desktop monitor. Instead of subjecting the user to uncomfortable rapidly flashing light patterns, we operate on images of the user watching a YouTube video or other standard content. We train a deep network on images plus monitor patterns of a given user and learn to predict images of that user under any target illumination (monitor pattern). Experimental evaluation shows that our method produces realistic relighting results. Video results are available at http://grail.cs.washington.edu/projects/Light_Stage_on_Every_Desk/.
http://w3id.org/mlsea/pwc/scientificWork/A%20LightGBM%20based%20Forecasting%20of%20Dominant%20Wave%20Periods%20in%20Oceanic%20Waters                                                                                  A LightGBM based Forecasting of Dominant Wave Periods in Oceanic Waters                                                                                  In this paper, we propose a Light Gradient Boosting (LightGBM) to forecast dominant wave periods in oceanic waters. First, we use the data collected from CDIP buoys and apply various data filtering methods. The data filtering methods allow us to obtain a high-quality dataset for training and validation purposes. We then extract various wave-based features like wave heights, periods, skewness, kurtosis, etc., and atmospheric features like humidity, pressure, and air temperature for the buoys. Afterward, we train algorithms that use LightGBM and Extra Trees through a hv-block cross-validation scheme to forecast dominant wave periods for up to 30 days ahead. LightGBM has the R2 score of 0.94, 0.94, and 0.94 for 1-day ahead, 15-day ahead, and 30-day ahead prediction. Similarly, Extra Trees (ET) has an R2 score of 0.88, 0.86, and 0.85 for 1-day ahead, 15-day ahead, and 30 day ahead prediction. In case of the test dataset, LightGBM has R2 score of 0.94, 0.94, and 0.94 for 1-day ahead, 15-day ahead and 30-day ahead prediction. ET has R2 score of 0.88, 0.86, and 0.85 for 1-day ahead, 15-day ahead, and 30-day ahead prediction. A similar R2 score for both training and the test dataset suggests that the machine learning models developed in this paper are robust. Since the LightGBM algorithm outperforms ET for all the windows tested, it is taken as the final algorithm. Note that the performance of both methods does not decrease significantly as the forecast horizon increases. Likewise, the proposed method outperforms the numerical approaches included in this paper in the test dataset. For 1 day ahead prediction, the proposed algorithm has SI, Bias, CC, and RMSE of 0.09, 0.00, 0.97, and 1.78 compared to 0.268, 0.40, 0.63, and 2.18 for the European Centre for Medium-range Weather Forecasts (ECMWF) model, which outperforms all the other methods in the test dataset.
http://w3id.org/mlsea/pwc/scientificWork/A%20Lightweight%20CNN%20Model%20for%20Detecting%20Respiratory%20Diseases%20from%20Lung%20Auscultation%20Sounds%20using%20EMD-CWT-based%20Hybrid%20Scalogram                                                                                  A Lightweight CNN Model for Detecting Respiratory Diseases from Lung Auscultation Sounds using EMD-CWT-based Hybrid Scalogram                                                                                  Listening to lung sounds through auscultation is vital in examining the respiratory system for abnormalities. Automated analysis of lung auscultation sounds can be beneficial to the health systems in low-resource settings where there is a lack of skilled physicians. In this work, we propose a lightweight convolutional neural network (CNN) architecture to classify respiratory diseases using hybrid scalogram-based features of lung sounds. The hybrid scalogram features utilize the empirical mode decomposition (EMD) and continuous wavelet transform (CWT). The proposed scheme's performance is studied using a patient independent train-validation set from the publicly available ICBHI 2017 lung sound dataset. Employing the proposed framework, weighted accuracy scores of 99.20% for ternary chronic classification and 99.05% for six-class pathological classification are achieved, which outperform well-known and much larger VGG16 in terms of accuracy by 0.52% and 1.77% respectively. The proposed CNN model also outperforms other contemporary lightweight models while being computationally comparable.
http://w3id.org/mlsea/pwc/scientificWork/A%20Lightweight%20Concept%20Drift%20Detection%20and%20Adaptation%20Framework%20for%20IoT%20Data%20Streams                                                                                  A Lightweight Concept Drift Detection and Adaptation Framework for IoT Data Streams                                                                                  In recent years, with the increasing popularity of 'Smart Technology', the number of Internet of Things (IoT) devices and systems have surged significantly. Various IoT services and functionalities are based on the analytics of IoT streaming data. However, IoT data analytics faces concept drift challenges due to the dynamic nature of IoT systems and the ever-changing patterns of IoT data streams. In this article, we propose an adaptive IoT streaming data analytics framework for anomaly detection use cases based on optimized LightGBM and concept drift adaptation. A novel drift adaptation method named Optimized Adaptive and Sliding Windowing (OASW) is proposed to adapt to the pattern changes of online IoT data streams. Experiments on two public datasets show the high accuracy and efficiency of our proposed adaptive LightGBM model compared against other state-of-the-art approaches. The proposed adaptive LightGBM model can perform continuous learning and drift adaptation on IoT data streams without human intervention.
http://w3id.org/mlsea/pwc/scientificWork/A%20Lightweight%20Privacy-Preserving%20Scheme%20Using%20Label-based%20Pixel%20Block%20Mixing%20for%20Image%20Classification%20in%20Deep%20Learning                                                                                  A Lightweight Privacy-Preserving Scheme Using Label-based Pixel Block Mixing for Image Classification in Deep Learning                                                                                  To ensure the privacy of sensitive data used in the training of deep learning models, a number of privacy-preserving methods have been designed by the research community. However, existing schemes are generally designed to work with textual data, or are not efficient when a large number of images is used for training. Hence, in this paper we propose a lightweight and efficient approach to preserve image privacy while maintaining the availability of the training set. Specifically, we design the pixel block mixing algorithm for image classification privacy preservation in deep learning. To evaluate its utility, we use the mixed training set to train the ResNet50, VGG16, InceptionV3 and DenseNet121 models on the WIKI dataset and the CNBC face dataset. Experimental findings on the testing set show that our scheme preserves image privacy while maintaining the availability of the training set in the deep learning models. Additionally, the experimental results demonstrate that we achieve good performance for the VGG16 model on the WIKI dataset and both ResNet50 and DenseNet121 on the CNBC dataset. The pixel block algorithm achieves fairly high efficiency in the mixing of the images, and it is computationally challenging for the attackers to restore the mixed training set to the original training set. Moreover, data augmentation can be applied to the mixed training set to improve the training's effectiveness.
http://w3id.org/mlsea/pwc/scientificWork/A%20Lightweight%20ReLU-Based%20Feature%20Fusion%20for%20Aerial%20Scene%20Classification                                                                                  A Lightweight ReLU-Based Feature Fusion for Aerial Scene Classification                                                                                  In this paper, we propose a transfer-learning based model construction technique for the aerial scene classification problem. The core of our technique is a layer selection strategy, named ReLU-Based Feature Fusion (RBFF), that extracts feature maps from a pretrained CNN-based single-object image classification model, namely MobileNetV2, and constructs a model for the aerial scene classification task. RBFF stacks features extracted from the batch normalization layer of a few selected blocks of MobileNetV2, where the candidate blocks are selected based on the characteristics of the ReLU activation layers present in those blocks. The feature vector is then compressed into a low-dimensional feature space using dimension reduction algorithms on which we train a low-cost SVM classifier for the classification of the aerial images. We validate our choice of selected features based on the significance of the extracted features with respect to our classification pipeline. RBFF remarkably does not involve any training of the base CNN model except for a few parameters for the classifier, which makes the technique very cost-effective for practical deployments. The constructed model despite being lightweight outperforms several recently proposed models in terms of accuracy for a number of aerial scene datasets.
http://w3id.org/mlsea/pwc/scientificWork/A%20Lightweight%20and%20Gradient-Stable%20Nerual%20Layer                                                                                  A Lightweight and Gradient-Stable Nerual Layer                                                                                  We propose a neural-layer architecture based on Householder weighting and absolute-value activating, hence called Householder-absolute neural layer or simply Han-layer. Compared to a fully-connected layer with $d$-neurons and $d$ outputs, a Han-layer reduces the number of parameters and the corresponding complexity from $O(d^2)$ to $O(d)$. The Han-layer structure guarantees two desirable properties: (1) gradient stability (free of vanishing or exploding gradient), and (2) 1-Lipschitz continuity. Extensive numerical experiments show that one can strategically use Han-layers to replace fully-connected (FC) layers, reducing the number of model parameters while maintaining or even improving the generalization performance. We will showcase the capabilities of the Han-layer architecture on a few small stylized models, and also discuss its current limitations.
http://w3id.org/mlsea/pwc/scientificWork/A%20Linkage-based%20Doubly%20Imbalanced%20Graph%20Learning%20Framework%20for%20Face%20Clustering                                                                                  A Linkage-based Doubly Imbalanced Graph Learning Framework for Face Clustering                                                                                  In recent years, benefiting from the expressive power of Graph Convolutional Networks (GCNs), significant breakthroughs have been made in face clustering area. However, rare attention has been paid to GCN-based clustering on imbalanced data. Although imbalance problem has been extensively studied, the impact of imbalanced data on GCN- based linkage prediction task is quite different, which would cause problems in two aspects: imbalanced linkage labels and biased graph representations. The former is similar to that in classic image classification task, but the latter is a particular problem in GCN-based clustering via linkage prediction. Significantly biased graph representations in training can cause catastrophic over-fitting of a GCN model. To tackle these challenges, we propose a linkage-based doubly imbalanced graph learning framework for face clustering. In this framework, we evaluate the feasibility of those existing methods for imbalanced image classification problem on GCNs, and present a new method to alleviate the imbalanced labels and also augment graph representations using a Reverse-Imbalance Weighted Sampling (RIWS) strategy. With the RIWS strategy, probability-based class balancing weights could ensure the overall distribution of positive and negative samples; in addition, weighted random sampling provides diverse subgraph structures, which effectively alleviates the over-fitting problem and improves the representation ability of GCNs. Extensive experiments on series of imbalanced benchmark datasets synthesized from MS-Celeb-1M and DeepFashion demonstrate the effectiveness and generality of our proposed method. Our implementation and the synthesized datasets will be openly available on https://github.com/espectre/GCNs_on_imbalanced_datasets.
http://w3id.org/mlsea/pwc/scientificWork/A%20Lite%20Distributed%20Semantic%20Communication%20System%20for%20Internet%20of%20Things                                                                                  A Lite Distributed Semantic Communication System for Internet of Things                                                                                  The rapid development of deep learning (DL) and widespread applications of Internet-of-Things (IoT) have made the devices smarter than before, and enabled them to perform more intelligent tasks. However, it is challenging for any IoT device to train and run DL models independently due to its limited computing capability. In this paper, we consider an IoT network where the cloud/edge platform performs the DL based semantic communication (DeepSC) model training and updating while IoT devices perform data collection and transmission based on the trained model. To make it affordable for IoT devices, we propose a lite distributed semantic communication system based on DL, named L-DeepSC, for text transmission with low complexity, where the data transmission from the IoT devices to the cloud/edge works at the semantic level to improve transmission efficiency. Particularly, by pruning the model redundancy and lowering the weight resolution, the L-DeepSC becomes affordable for IoT devices and the bandwidth required for model weight transmission between IoT devices and the cloud/edge is reduced significantly. Through analyzing the effects of fading channels in forward-propagation and back-propagation during the training of L-DeepSC, we develop a channel state information (CSI) aided training processing to decrease the effects of fading channels on transmission. Meanwhile, we tailor the semantic constellation to make it implementable on capacity-limited IoT devices. Simulation demonstrates that the proposed L-DeepSC achieves competitive performance compared with traditional methods, especially in the low signal-to-noise (SNR) region. In particular, while it can reach as large as 40x compression ratio without performance degradation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Little%20Robustness%20Goes%20a%20Long%20Way%3A%20Leveraging%20Robust%20Features%20for%20Targeted%20Transfer%20Attacks                                                                                  A Little Robustness Goes a Long Way: Leveraging Robust Features for Targeted Transfer Attacks                                                                                  Adversarial examples for neural network image classifiers are known to be transferable: examples optimized to be misclassified by a source classifier are often misclassified as well by classifiers with different architectures. However, targeted adversarial examples -- optimized to be classified as a chosen target class -- tend to be less transferable between architectures. While prior research on constructing transferable targeted attacks has focused on improving the optimization procedure, in this work we examine the role of the source classifier. Here, we show that training the source classifier to be 'slightly robust' -- that is, robust to small-magnitude adversarial examples -- substantially improves the transferability of class-targeted and representation-targeted adversarial attacks, even between architectures as different as convolutional neural networks and transformers. The results we present provide insight into the nature of adversarial examples as well as the mechanisms underlying so-called 'robust' classifiers.
http://w3id.org/mlsea/pwc/scientificWork/A%20Load%20Balanced%20Recommendation%20Approach                                                                                  A Load Balanced Recommendation Approach                                                                                  Recommender systems (RSs) are software tools and algorithms developed to alleviate the problem of information overload, which makes it difficult for a user to make right decisions. Two main paradigms toward the recommendation problem are collaborative filtering and content-based filtering, which try to recommend the best items using ratings and content available. These methods typically face infamous problems including cold-start, diversity, scalability, and great computational expense. We argue that the uptake of deep learning and reinforcement learning methods is also questionable due to their computational complexities and uninterpretability. In this paper, we approach the recommendation problem from a new prospective. We borrow ideas from cluster head selection algorithms in wireless sensor networks and adapt them to the recommendation problem. In particular, we propose Load Balanced Recommender System (LBRS), which uses a probabilistic scheme for item recommendation. Furthermore, we factor in the importance of items in the recommendation process, which significantly improves the recommendation accuracy. We also introduce a method that considers a heterogeneity among items, in order to balance the similarity and diversity trade-off. Finally, we propose a new metric for diversity, which emphasizes the importance of diversity not only from an intra-list perspective, but also from a between-list point of view. With experiments in a simulation study performed on RecSim, we show that LBRS is effective and can outperform baseline methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20Log-Rectilinear%20Transformation%20for%20Foveated%20360-degree%20Video%20Streaming                                                                                  A Log-Rectilinear Transformation for Foveated 360-degree Video Streaming                                                                                  With the rapidly increasing resolutions of 360° cameras, head-mounted displays, and live-streaming services, streaming high-resolution panoramic videos over limited-bandwidth networks is becoming a critical challenge. Foveated video streaming can address this rising challenge in the context of eye-tracking-equipped virtual reality head-mounted displays. However, conventional log-polar foveated rendering suffers from a number of visual artifacts such as aliasing and flickering. In this paper, we introduce a new log-rectilinear transformation that incorporates summed-area table filtering and off-the-shelf video codecs to enable foveated streaming of 360° videos suitable for VR headsets with built-in eye-tracking. To validate our approach, we build a client-server system prototype for streaming 360° videos which leverages parallel algorithms over real-time video transcoding. We conduct quantitative experiments on an existing 360° video dataset and observe that the log-rectilinear transformation paired with summed-area table filtering heavily reduces flickering compared to log-polar subsampling while also yielding an additional 11% reduction in bandwidth usage.
http://w3id.org/mlsea/pwc/scientificWork/A%20Logical%20Neural%20Network%20Structure%20With%20More%20Direct%20Mapping%20From%20Logical%20Relations                                                                                  A Logical Neural Network Structure With More Direct Mapping From Logical Relations                                                                                  Logical relations widely exist in human activities. Human use them for making judgement and decision according to various conditions, which are embodied in the form of emph{if-then} rules. As an important kind of cognitive intelligence, it is prerequisite of representing and storing logical relations rightly into computer systems so as to make automatic judgement and decision, especially for high-risk domains like medical diagnosis. However, current numeric ANN (Artificial Neural Network) models are good at perceptual intelligence such as image recognition while they are not good at cognitive intelligence such as logical representation, blocking the further application of ANN. To solve it, researchers have tried to design logical ANN models to represent and store logical relations. Although there are some advances in this research area, recent works still have disadvantages because the structures of these logical ANN models still don't map more directly with logical relations which will cause the corresponding logical relations cannot be read out from their network structures. Therefore, in order to represent logical relations more clearly by the neural network structure and to read out logical relations from it, this paper proposes a novel logical ANN model by designing the new logical neurons and links in demand of logical representation. Compared with the recent works on logical ANN models, this logical ANN model has more clear corresponding with logical relations using the more direct mapping method herein, thus logical relations can be read out following the connection patterns of the network structure. Additionally, less neurons are used.
http://w3id.org/mlsea/pwc/scientificWork/A%20Lossless%20Intra%20Reference%20Block%20Recompression%20Scheme%20for%20Bandwidth%20Reduction%20in%20HEVC-IBC                                                                                  A Lossless Intra Reference Block Recompression Scheme for Bandwidth Reduction in HEVC-IBC                                                                                  The reference frame memory accesses in inter prediction result in high DRAM bandwidth requirement and power consumption. This problem is more intensive by the adoption of intra block copy (IBC), a new coding tool in the screen content coding (SCC) extension to High Efficiency Video Coding (HEVC). In this paper, we propose a lossless recompression scheme that compresses the reference blocks in intra prediction, i.e., intra block copy, before storing them into DRAM to alleviate this problem. The proposal performs pixel-wise texture analysis with an edge-based adaptive prediction method yet no signaling for direction in bitstreams, thus achieves a high gain for compression. Experimental results demonstrate that the proposed scheme shows a 72% data reduction rate on average, which solves the memory bandwidth problem.
http://w3id.org/mlsea/pwc/scientificWork/A%20Lottery%20Ticket%20Hypothesis%20Framework%20for%20Low-Complexity%20Device-Robust%20Neural%20Acoustic%20Scene%20Classification                                                                                  A Lottery Ticket Hypothesis Framework for Low-Complexity Device-Robust Neural Acoustic Scene Classification                                                                                  We propose a novel neural model compression strategy combining data augmentation, knowledge transfer, pruning, and quantization for device-robust acoustic scene classification (ASC). Specifically, we tackle the ASC task in a low-resource environment leveraging a recently proposed advanced neural network pruning mechanism, namely Lottery Ticket Hypothesis (LTH), to find a sub-network neural model associated with a small amount non-zero model parameters. The effectiveness of LTH for low-complexity acoustic modeling is assessed by investigating various data augmentation and compression schemes, and we report an efficient joint framework for low-complexity multi-device ASC, called emph{Acoustic Lottery}. Acoustic Lottery could compress an ASC model up to $1/10^{4}$ and attain a superior performance (validation accuracy of 79.4% and Log loss of 0.64) compared to its not compressed seed model. All results reported in this work are based on a joint effort of four groups, namely GT-USTC-UKE-Tencent, aiming to address the 'Low-Complexity Acoustic Scene Classification (ASC) with Multiple Devices' in the DCASE 2021 Challenge Task 1a.
http://w3id.org/mlsea/pwc/scientificWork/A%20Low-Complexity%20ADMM-based%20Massive%20MIMO%20Detectors%20via%20Deep%20Neural%20Networks                                                                                  A Low-Complexity ADMM-based Massive MIMO Detectors via Deep Neural Networks                                                                                  An alternate direction method of multipliers (ADMM)-based detectors can achieve good performance in both small and large-scale multiple-input multiple-output (MIMO) systems. However, due to the difficulty of choosing the optimal penalty parameters, their performance is limited. This paper presents a deep neural network (DNN)-based massive MIMO detection method which can overcome the above limitation. It exploits the unfolding technique and learns to estimate the penalty parameters. Additionally, a computationally cheaper detector is also proposed. The proposed methods can handle the higher-order modulation signals. Numerical results are presented to demonstrate the performances of the proposed methods compared with the existing works.
http://w3id.org/mlsea/pwc/scientificWork/A%20Low-Complexity%20MIMO%20Channel%20Estimator%20with%20Implicit%20Structure%20of%20a%20Convolutional%20Neural%20Network                                                                                  A Low-Complexity MIMO Channel Estimator with Implicit Structure of a Convolutional Neural Network                                                                                  A low-complexity convolutional neural network estimator which learns the minimum mean squared error channel estimator for single-antenna users was recently proposed. We generalize the architecture to the estimation of MIMO channels with multiple-antenna users and incorporate complexity-reducing assumptions based on the channel model. Learning is used in this context to combat the mismatch between the assumptions and real scenarios where the assumptions may not hold. We derive a high-level description of the estimator for arbitrary choices of the pilot sequence. It turns out that the proposed estimator has the implicit structure of a two-layered convolutional neural network, where the derived quantities can be relaxed to learnable parameters. We show that by using discrete Fourier transform based pilots the number of learnable network parameters decreases significantly and the online run time of the estimator is reduced considerably, where we can achieve linearithmic order of complexity in the number of antennas. Numerical results demonstrate performance gains compared to state-of-the-art algorithms from the field of compressive sensing or covariance estimation of the same or even higher computational complexity. The simulation code is available online.
http://w3id.org/mlsea/pwc/scientificWork/A%20Low-Complexity%20Method%20for%20FFT-based%20OFDM%20Sensing                                                                                  A Low-Complexity Method for FFT-based OFDM Sensing                                                                                  OFDM sensing is gaining increasing popularity in wideband radar applications as well as in joint communication and radar/radio sensing (JCAS). As JCAS will potentially be integrated into future mobile networks where OFDM is crucial, OFDM sensing is envisioned to be ubiquitously deployed. A fast Fourier transform (FFT) based OFDM sensing (FOS) method was proposed a decade ago and has been regarded as a de facto standard given its simplicity. In this article, we introduce an easy trick -- a pre-processing on target echo -- to further reduce the computational complexity of FOS without degrading key sensing performance. Underlying the trick is a newly disclosed feature of the target echo in OFDM sensing which, to the best of our knowledge, has not been effectively exploited yet.
http://w3id.org/mlsea/pwc/scientificWork/A%20Low-Cost%20Algorithm%20for%20Adaptive%20Sampling%20and%20Censoring%20in%20Diffusion%20Networks                                                                                  A Low-Cost Algorithm for Adaptive Sampling and Censoring in Diffusion Networks                                                                                  Distributed signal processing has attracted widespread attention in the scientific community due to its several advantages over centralized approaches. Recently, graph signal processing has risen to prominence, and adaptive distributed solutions have also been proposed in the area. Both in the classical framework and in graph signal processing, sampling and censoring techniques have been topics of intense research, since the cost associated with measuring and/or transmitting data throughout the entire network may be prohibitive in certain applications. In this paper, we propose a low-cost adaptive mechanism for sampling and censoring over diffusion networks that uses information from more nodes when the error in the network is high and from less nodes otherwise. It presents fast convergence during transient and a significant reduction in computational cost and energy consumption in steady state. As a censoring technique, we show that it is able to noticeably outperform other solutions. We also present a theoretical analysis to give insights about its operation, and to help the choice of suitable values for its parameters.
http://w3id.org/mlsea/pwc/scientificWork/A%20Low-Delay%20MAC%20for%20IoT%20Applications%3A%20Decentralized%20Optimal%20Scheduling%20of%20Queues%20without%20Explicit%20State%20Information%20Sharing                                                                                  A Low-Delay MAC for IoT Applications: Decentralized Optimal Scheduling of Queues without Explicit State Information Sharing                                                                                  We consider a system of several collocated nodes sharing a time slotted wireless channel, and seek a MAC (medium access control) that (i) provides low mean delay, (ii) has distributed control (i.e., there is no central scheduler), and (iii) does not require explicit exchange of state information or control signals. The design of such MAC protocols must keep in mind the need for contention access at light traffic, and scheduled access in heavy traffic, leading to the long-standing interest in hybrid, adaptive MACs. Working in the discrete time setting, for the distributed MAC design, we consider a practical information structure where each node has local information and some common information obtained from overhearing. In this setting, 'ZMAC' is an existing protocol that is hybrid and adaptive. We approach the problem via two steps (1) We show that it is sufficient for the policy to be 'greedy' and 'exhaustive'. Limiting the policy to this class reduces the problem to obtaining a queue switching policy at queue emptiness instants. (2) Formulating the delay optimal scheduling as a POMDP (partially observed Markov decision process), we show that the optimal switching rule is Stochastic Largest Queue (SLQ). Using this theory as the basis, we then develop a practical distributed scheduler, QZMAC, which is also tunable. We implement QZMAC on standard off-the-shelf TelosB motes and also use simulations to compare QZMAC with the full-knowledge centralized scheduler, and with ZMAC. We use our implementation to study the impact of false detection while overhearing the common information, and the efficiency of QZMAC. Our simulation results show that the mean delay with QZMAC is close that of the full-knowledge centralized scheduler.
http://w3id.org/mlsea/pwc/scientificWork/A%20Low-Overhead%20Hierarchical%20Beam-tracking%20Algorithm%20for%20THz%20Wireless%20Systems                                                                                  A Low-Overhead Hierarchical Beam-tracking Algorithm for THz Wireless Systems                                                                                  In this paper, a novel hierarchical beamtracking approach, which is suitable for terahertz (THz) wireless systems, is presented. The main idea is to employ a prediction based algorithm with a multi-resolution codebook, in order to decrease the required overhead of tracking and increase its robustness. The efficiency of the algorithm is evaluated in terms of the average number of pilots and mean square error (MSE) and is compared with the corresponding performance of the fast channel tracking (FCT) algorithm. Our results highlight the superiority of the proposed approach in comparison with FCT, in terms of tracking efficiency with low overhead.
http://w3id.org/mlsea/pwc/scientificWork/A%20Lyapunov-Based%20Methodology%20for%20Constrained%20Optimization%20with%20Bandit%20Feedback                                                                                  A Lyapunov-Based Methodology for Constrained Optimization with Bandit Feedback                                                                                  In a wide variety of applications including online advertising, contractual hiring, and wireless scheduling, the controller is constrained by a stringent budget constraint on the available resources, which are consumed in a random amount by each action, and a stochastic feasibility constraint that may impose important operational limitations on decision-making. In this work, we consider a general model to address such problems, where each action returns a random reward, cost, and penalty from an unknown joint distribution, and the decision-maker aims to maximize the total reward under a budget constraint $B$ on the total cost and a stochastic constraint on the time-average penalty. We propose a novel low-complexity algorithm based on Lyapunov optimization methodology, named ${ tt LyOn}$, and prove that for $K$ arms it achieves $O( sqrt{K B log B})$ regret and zero constraint-violation when $B$ is sufficiently large. The low computational cost and sharp performance bounds of ${ tt LyOn}$ suggest that Lyapunov-based algorithm design methodology can be effective in solving constrained bandit optimization problems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Machine%20Learning%20Approach%20to%20Forecast%20Economic%20Recessions%E2%80%94An%20Italian%20Case%20Study                                                                                  A Machine Learning Approach to Forecast Economic Recessions—An Italian Case Study                                                                                  Abstract:In economic activity , recessions represent a period of failure in Gross Domestic Product (GDP) and usually are presented as episodic and non-linear. For this reason, they are difficultto predictandappearasoneofthemainproblemsinmacroeconomicsforecasts.Aclassicexample turnsouttobethegreatrecessionthatoccurredbetween2008and2009thatwasnotpredicted. Inthispaper,thegoalistogiveadifferent,althoughcomplementary,approachconcerningthe classical econometric techniques, and to show how Machine Learning (ML) techniques may improve short-term forecasting accuracy . As a case study , we use Italian data on GDP and a few related variables. In particular, we evaluate the goodness of fitoftheforecastingproposedmodelina case study of the Italian GDP . The algorithm is trained on Italian macroeconomic variables over the period 1995:Q1-2019:Q2. We also compare the results using the same dataset through Classic Linear Regression Model. As a result, both statistical and ML approaches are able to predict economic downturns but higher accuracy is obtained using Nonlinear Autoregressive with exogenous variables (NARX) model.
http://w3id.org/mlsea/pwc/scientificWork/A%20Machine%20Learning%20Framework%20for%20Real-time%20Inverse%20Modeling%20and%20Multi-objective%20Process%20Optimization%20of%20Composites%20for%20Active%20Manufacturing%20Control                                                                                  A Machine Learning Framework for Real-time Inverse Modeling and Multi-objective Process Optimization of Composites for Active Manufacturing Control                                                                                  For manufacturing of aerospace composites, several parts may be processed simultaneously using convective heating in an autoclave. Due to uncertainties including tool placement, convective Boundary Conditions (BCs) vary in each run. As a result, temperature histories in some of the parts may not conform to process specifications due to under-curing or over-heating. Thermochemical analysis using Finite Element (FE) simulations are typically conducted prior to fabrication based on assumed range of BCs. This, however, introduces unnecessary constraints on the design. To monitor the process, thermocouples (TCs) are placed under tools near critical locations. The TC data may be used to back-calculate BCs using trial-and-error FE analysis. However, since the inverse heat transfer problem is ill-posed, many solutions are obtained for given TC data. In this study, a novel machine learning (ML) framework is presented capable of optimizing air temperature cycle in real-time based on TC data from multiple parts, for active control of manufacturing. The framework consists of two recurrent Neural Networks (NN) for inverse modeling of the ill-posed curing problem at the speed of 300 simulations/second, and a classification NN for multi-objective optimization of the air temperature at the speed of 35,000 simulations/second. A virtual demonstration of the framework for process optimization of three composite parts with data from three TCs is presented.
http://w3id.org/mlsea/pwc/scientificWork/A%20Machine%20Learning%20Model%20for%20Early%20Detection%20of%20Diabetic%20Foot%20using%20Thermogram%20Images                                                                                  A Machine Learning Model for Early Detection of Diabetic Foot using Thermogram Images                                                                                  Diabetes foot ulceration (DFU) and amputation are a cause of significant morbidity. The prevention of DFU may be achieved by the identification of patients at risk of DFU and the institution of preventative measures through education and offloading. Several studies have reported that thermogram images may help to detect an increase in plantar temperature prior to DFU. However, the distribution of plantar temperature may be heterogeneous, making it difficult to quantify and utilize to predict outcomes. We have compared a machine learning-based scoring technique with feature selection and optimization techniques and learning classifiers to several state-of-the-art Convolutional Neural Networks (CNNs) on foot thermogram images and propose a robust solution to identify the diabetic foot. A comparatively shallow CNN model, MobilenetV2 achieved an F1 score of ~95% for a two-feet thermogram image-based classification and the AdaBoost Classifier used 10 features and achieved an F1 score of 97 %. A comparison of the inference time for the best-performing networks confirmed that the proposed algorithm can be deployed as a smartphone application to allow the user to monitor the progression of the DFU in a home setting.
http://w3id.org/mlsea/pwc/scientificWork/A%20Machine%20Learning%20Model%20for%20Nowcasting%20Epidemic%20Incidence                                                                                  A Machine Learning Model for Nowcasting Epidemic Incidence                                                                                  Due to delay in reporting, the daily national and statewide COVID-19 incidence counts are often unreliable and need to be estimated from recent data. This process is known in economics as nowcasting. We describe in this paper a simple random forest statistical model for nowcasting the COVID - 19 daily new infection counts based on historic data along with a set of simple covariates, such as the currently reported infection counts, day of the week, and time since first reporting. We apply the model to adjust the daily infection counts in Ohio, and show that the predictions from this simple data-driven method compare favorably both in quality and computational burden to those obtained from the state-of-the-art hierarchical Bayesian model employing a complex statistical algorithm.
http://w3id.org/mlsea/pwc/scientificWork/A%20Machine%20Learning%20and%20Computer%20Vision%20Approach%20to%20Rapidly%20Optimize%20Multiscale%20Droplet%20Generation                                                                                  A Machine Learning and Computer Vision Approach to Rapidly Optimize Multiscale Droplet Generation                                                                                  Generating droplets from a continuous stream of fluid requires precise tuning of a device to find optimized control parameter conditions. It is analytically intractable to compute the necessary control parameter values of a droplet-generating device that produces optimized droplets. Furthermore, as the length scale of the fluid flow changes, the formation physics and optimized conditions that induce flow decomposition into droplets also change. Hence, a single proportional integral derivative controller is too inflexible to optimize devices of different length scales or different control parameters, while classification machine learning techniques take days to train and require millions of droplet images. Therefore, the question is posed, can a single method be created that universally optimizes multiple length-scale droplets using only a few data points and is faster than previous approaches? In this paper, a Bayesian optimization and computer vision feedback loop is designed to quickly and reliably discover the control parameter values that generate optimized droplets within different length-scale devices. This method is demonstrated to converge on optimum parameter values using 60 images in only 2.3 hours, 30x faster than previous approaches. Model implementation is demonstrated for two different length-scale devices: a milliscale inkjet device and a microfluidics device.
http://w3id.org/mlsea/pwc/scientificWork/A%20Markov%20Decision%20Process%20Approach%20for%20Managing%20Medical%20Drone%20Deliveries                                                                                  A Markov Decision Process Approach for Managing Medical Drone Deliveries                                                                                  We consider the problem of optimizing the distribution operations at a drone hub that dispatches drones to different geographic locations generating stochastic demands for medical supplies. Drone delivery is an innovative method that introduces many benefits, such as low-contact delivery, thereby reducing the spread of pandemic and vaccine-preventable diseases. While we focus on medical supply delivery for this work, drone delivery is suitable for many other items, including food, postal parcels, and e-commerce. In this paper, our goal is to address drone delivery challenges related to the stochastic demands of different geographic locations. We consider different classes of demand related to geographic locations that require different flight ranges, which is directly related to the amount of charge held in a drone battery. We classify the stochastic demands based on their distance from the drone hub, use a Markov decision process to model the problem, and perform computational tests using realistic data representing a prominent drone delivery company. We solve the problem using a reinforcement learning method and show its high performance compared with the exact solution found using dynamic programming. Finally, we analyze the results and provide insights for managing the drone hub operations.
http://w3id.org/mlsea/pwc/scientificWork/A%20Markov%20Reward%20Process-Based%20Approach%20to%20Spatial%20Interpolation                                                                                  A Markov Reward Process-Based Approach to Spatial Interpolation                                                                                  The interpolation of spatial data can be of tremendous value in various applications, such as forecasting weather from only a few measurements of meteorological or remote sensing data. Existing methods for spatial interpolation, such as variants of kriging and spatial autoregressive models, tend to suffer from at least one of the following limitations: (a) the assumption of stationarity, (b) the assumption of isotropy, and (c) the trade-off between modelling local or global spatial interaction. Addressing these issues in this work, we propose the use of Markov reward processes (MRPs) as a spatial interpolation method, and we introduce three variants thereof: (i) a basic static discount MRP (SD-MRP), (ii) an accurate but mostly theoretical optimised MRP (O-MRP), and (iii) a transferable weight prediction MRP (WP-MRP). All variants of MRP interpolation operate locally, while also implicitly accounting for global spatial relationships in the entire system through recursion. Additionally, O-MRP and WP-MRP no longer assume stationarity and are robust to anisotropy. We evaluated our proposed methods by comparing the mean absolute errors of their interpolated grid cells to those of 7 common baselines, selected from models based on spatial autocorrelation, (spatial) regression, and deep learning. We performed detailed evaluations on two publicly available datasets (local GDP values, and COVID-19 patient trajectory data). The results from these experiments clearly show the competitive advantage of MRP interpolation, which achieved significantly lower errors than the existing methods in 23 out of 40 experimental conditions, or 35 out of 40 when including O-MRP.
http://w3id.org/mlsea/pwc/scientificWork/A%20Masked%20Segmental%20Language%20Model%20for%20Unsupervised%20Natural%20Language%20Segmentation                                                                                  A Masked Segmental Language Model for Unsupervised Natural Language Segmentation                                                                                  Segmentation remains an important preprocessing step both in languages where 'words' or other important syntactic/semantic units (like morphemes) are not clearly delineated by white space, as well as when dealing with continuous speech data, where there is often no meaningful pause between words. Near-perfect supervised methods have been developed for use in resource-rich languages such as Chinese, but many of the world's languages are both morphologically complex, and have no large dataset of 'gold' segmentations into meaningful units. To solve this problem, we propose a new type of Segmental Language Model (Sun and Deng, 2018; Kawakami et al., 2019; Wang et al., 2021) for use in both unsupervised and lightly supervised segmentation tasks. We introduce a Masked Segmental Language Model (MSLM) built on a span-masking transformer architecture, harnessing the power of a bi-directional masked modeling context and attention. In a series of experiments, our model consistently outperforms Recurrent SLMs on Chinese (PKU Corpus) in segmentation quality, and performs similarly to the Recurrent model on English (PTB). We conclude by discussing the different challenges posed in segmenting phonemic-type writing systems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Masked%20Segmental%20Language%20Model%20for%20Unsupervised%20Natural%20Language%20Segmentation                                                                                  A Masked Segmental Language Model for Unsupervised Natural Language Segmentation                                                                                  We introduce a Masked Segmental Language Model (MSLM) for joint language modeling and unsupervised segmentation. While near-perfect supervised methods have been developed for segmenting human-like linguistic units in resource-rich languages such as Chinese, many of the world's languages are both morphologically complex, and have no large dataset of ``gold'' segmentations for supervised training. Segmental Language Models offer a unique approach by conducting unsupervised segmentation as the byproduct of a neural language modeling objective. However, current SLMs are limited in their scalability due to their recurrent architecture. We propose a new type of SLM for use in both unsupervised and lightly supervised segmentation tasks. The MSLM is built on a span-masking transformer architecture, harnessing a masked bidirectional modeling context and attention, as well as adding the potential for model scalability. In a series of experiments, our model outperforms the segmentation quality of recurrent SLMs on Chinese, and performs similarly to the recurrent model on English.
http://w3id.org/mlsea/pwc/scientificWork/A%20Master%20Key%20Backdoor%20for%20Universal%20Impersonation%20Attack%20against%20DNN-based%20Face%20Verification                                                                                  A Master Key Backdoor for Universal Impersonation Attack against DNN-based Face Verification                                                                                  We introduce a new attack against face verification systems based on Deep Neural Networks (DNN). The attack relies on the introduction into the network of a hidden backdoor, whose activation at test time induces a verification error allowing the attacker to impersonate any user. The new attack, named Master Key backdoor attack, operates by interfering with the training phase, so to instruct the DNN to always output a positive verification answer when the face of the attacker is presented at its input. With respect to existing attacks, the new backdoor attack offers much more flexibility, since the attacker does not need to know the identity of the victim beforehand. In this way, he can deploy a Universal Impersonation attack in an open-set framework, allowing him to impersonate any enrolled users, even those that were not yet enrolled in the system when the attack was conceived. We present a practical implementation of the attack targeting a Siamese-DNN face verification system, and show its effectiveness when the system is trained on VGGFace2 dataset and tested on LFW and YTF datasets. According to our experiments, the Master Key backdoor attack provides a high attack success rate even when the ratio of poisoned training data is as small as 0.01, thus raising a new alarm regarding the use of DNN-based face verification systems in security-critical applications.
http://w3id.org/mlsea/pwc/scientificWork/A%20Mathematical%20Analysis%20of%20Learning%20Loss%20for%20Active%20Learning%20in%20Regression                                                                                  A Mathematical Analysis of Learning Loss for Active Learning in Regression                                                                                  Active learning continues to remain significant in the industry since it is data efficient. Not only is it cost effective on a constrained budget, continuous refinement of the model allows for early detection and resolution of failure scenarios during the model development stage. Identifying and fixing failures with the model is crucial as industrial applications demand that the underlying model performs accurately in all foreseeable use cases. One popular state-of-the-art technique that specializes in continuously refining the model via failure identification is Learning Loss. Although simple and elegant, this approach is empirically motivated. Our paper develops a foundation for Learning Loss which enables us to propose a novel modification we call LearningLoss++. We show that gradients are crucial in interpreting how Learning Loss works, with rigorous analysis and comparison of the gradients between Learning Loss and LearningLoss++. We also propose a convolutional architecture that combines features at different scales to predict the loss. We validate LearningLoss++ for regression on the task of human pose estimation (using MPII and LSP datasets), as done in Learning Loss. We show that LearningLoss++ outperforms in identifying scenarios where the model is likely to perform poorly, which on model refinement translates into reliable performance in the open world.
http://w3id.org/mlsea/pwc/scientificWork/A%20Mathematical%20Foundation%20for%20Robust%20Machine%20Learning%20based%20on%20Bias-Variance%20Trade-off                                                                                  A Mathematical Foundation for Robust Machine Learning based on Bias-Variance Trade-off                                                                                  A common assumption in machine learning is that samples are independently and identically distributed (i.i.d). However, the contributions of different samples are not identical in training. Some samples are difficult to learn and some samples are noisy. The unequal contributions of samples has a considerable effect on training performances. Studies focusing on unequal sample contributions (e.g., easy, hard, noisy) in learning usually refer to these contributions as robust machine learning (RML). Weighing and regularization are two common techniques in RML. Numerous learning algorithms have been proposed but the strategies for dealing with easy/hard/noisy samples differ or even contradict with different learning algorithms. For example, some strategies take the hard samples first, whereas some strategies take easy first. Conducting a clear comparison for existing RML algorithms in dealing with different samples is difficult due to lack of a unified theoretical framework for RML. This study attempts to construct a mathematical foundation for RML based on the bias-variance trade-off theory. A series of definitions and properties are presented and proved. Several classical learning algorithms are also explained and compared. Improvements of existing methods are obtained based on the comparison. A unified method that combines two classical learning strategies is proposed.
http://w3id.org/mlsea/pwc/scientificWork/A%20Mathematical%20Model%20For%20the%20Spread%20of%20a%20Virus                                                                                  A Mathematical Model For the Spread of a Virus                                                                                  This paper describes a mathematical model for the spread of a virus through an isolated population of a given size. The model uses three, color-coded components, called molecules (red for infected and still contagious; green for infected, but no longer contagious; and blue for uninfected). In retrospect, the model turns out to be a digital analogue for the well-known SIR model of Kermac and McKendrick (1927). In our RGB model, the number of accumulated infections goes through three phases, beginning at a very low level, then changing to a transition ramp of rapid growth, and ending in a plateau of final values. Consequently, the differential change or growth rate begins at 0, rises to a peak corresponding to the maximum slope of the transition ramp, and then falls back to 0. The properties of these time variations, including the slope, duration, and height of the transition ramp, and the width and height of the infection rate, depend on a single parameter - the time that a red molecule is contagious divided by the average time between collisions of the molecules. Various temporal milestones, including the starting time of the transition ramp, the time that the accumulating number of infections obtains its maximum slope, and the location of the peak of the infection rate depend on the size of the population in addition to the contagious lifetime ratio. Explicit formulas for these quantities are derived and summarized. Finally, Appendix E has been added to describe the effect of vaccinations.
http://w3id.org/mlsea/pwc/scientificWork/A%20Mathematical%20Model%20of%20COVID-19%20Transmission                                                                                  A Mathematical Model of COVID-19 Transmission                                                                                  Disease transmission is studied through disciplines like epidemiology, applied mathematics, and statistics. Mathematical simulation models for transmission have implications in solving public and personal health challenges. The SIR model uses a compartmental approach including dynamic and nonlinear behavior of transmission through three factors: susceptible, infected, and removed (recovered and deceased) individuals. Using the Lambert W Function, we propose a framework to study solutions of the SIR model. This demonstrates the applications of COVID-19 transmission data to model the spread of a real-world disease. Different models of disease including the SIR, SIRmp and SEIRpqr model are compared with respect to their ability to predict disease spread. Physical distancing impacts and personal protection equipment use are discussed with relevance to the COVID-19 spread.
http://w3id.org/mlsea/pwc/scientificWork/A%20Matrix%20Autoencoder%20Framework%20to%20Align%20the%20Functional%20and%20Structural%20Connectivity%20Manifolds%20as%20Guided%20by%20Behavioral%20Phenotypes                                                                                  A Matrix Autoencoder Framework to Align the Functional and Structural Connectivity Manifolds as Guided by Behavioral Phenotypes                                                                                  We propose a novel matrix autoencoder to map functional connectomes from resting state fMRI (rs-fMRI) to structural connectomes from Diffusion Tensor Imaging (DTI), as guided by subject-level phenotypic measures. Our specialized autoencoder infers a low dimensional manifold embedding for the rs-fMRI correlation matrices that mimics a canonical outer-product decomposition. The embedding is simultaneously used to reconstruct DTI tractography matrices via a second manifold alignment decoder and to predict inter-subject phenotypic variability via an artificial neural network. We validate our framework on a dataset of 275 healthy individuals from the Human Connectome Project database and on a second clinical dataset consisting of 57 subjects with Autism Spectrum Disorder. We demonstrate that the model reliably recovers structural connectivity patterns across individuals, while robustly extracting predictive and interpretable brain biomarkers in a cross-validated setting. Finally, our framework outperforms several baselines at predicting behavioral phenotypes in both real-world datasets.
http://w3id.org/mlsea/pwc/scientificWork/A%20Max-Min%20Entropy%20Framework%20for%20Reinforcement%20Learning                                                                                  A Max-Min Entropy Framework for Reinforcement Learning                                                                                  In this paper, we propose a max-min entropy framework for reinforcement learning (RL) to overcome the limitation of the soft actor-critic (SAC) algorithm implementing the maximum entropy RL in model-free sample-based learning. Whereas the maximum entropy RL guides learning for policies to reach states with high entropy in the future, the proposed max-min entropy framework aims to learn to visit states with low entropy and maximize the entropy of these low-entropy states to promote better exploration. For general Markov decision processes (MDPs), an efficient algorithm is constructed under the proposed max-min entropy framework based on disentanglement of exploration and exploitation. Numerical results show that the proposed algorithm yields drastic performance improvement over the current state-of-the-art RL algorithms.
http://w3id.org/mlsea/pwc/scientificWork/A%20Maximum%20Principle%20approach%20to%20deterministic%20Mean%20Field%20Games%20of%20Control%20with%20Absorption                                                                                  A Maximum Principle approach to deterministic Mean Field Games of Control with Absorption                                                                                  We study a class of deterministic mean field games on finite and infinite time horizons arising in models of optimal exploitation of exhaustible resources. The main characteristic of our game is an absorption constraint on the players' state process. As a result of the state constraint the optimal time of absorption becomes part of the equilibrium. This requires a novel approach when applying Pontyagin's maximum principle. We prove the existence and uniqueness of equilibria and solve the infinite horizon models in closed form. As players may drop out of the game over time, equilibrium production rates need not be monotone nor smooth.
http://w3id.org/mlsea/pwc/scientificWork/A%20Maximum-Likelihood-based%20Multi-User%20LoRa%20Receiver%20Implemented%20in%20GNU%20Radio                                                                                  A Maximum-Likelihood-based Multi-User LoRa Receiver Implemented in GNU Radio                                                                                  LoRa is a popular low-power wide-area network (LPWAN) technology that uses spread-spectrum to achieve long-range connectivity and resilience to noise and interference. For energy efficiency reasons, LoRa adopts a pure ALOHA access scheme, which leads to reduced network throughput due to packet collisions at the gateways. To alleviate this issue, in this paper we analyze and implement a LoRa receiver that is able to decode LoRa packets from two interfering users. Our main contribution is a two-user detector derived in a maximum-likelihood fashion using a detailed interference model. As the complexity of the maximum-likelihood sequence estimation is prohibitive, a complexity-reduction technique is introduced to enable a practical implementation of the proposed two-user detector. This detector has been implemented along with an interference-robust synchronization algorithm on the GNU Radio Software-Defined-Radio (SDR) platform. The SDR implementation shows the effectiveness of the proposed method and also allows its experimental evaluation. Measurements indicate that our detector inherently leverages the time offset between the two colliding users to separate and demodulate their contributions.
http://w3id.org/mlsea/pwc/scientificWork/A%20Mean%20Field%20Game%20Approach%20to%20Equilibrium%20Pricing%20with%20Market%20Clearing%20Condition                                                                                  A Mean Field Game Approach to Equilibrium Pricing with Market Clearing Condition                                                                                  In this work, we study an equilibrium-based continuous asset pricing problem which seeks to form a price process endogenously by requiring it to balance the flow of sales-and-purchase orders in the exchange market, where a large number of agents are interacting through the market price. Adopting a mean field game (MFG) approach, we find a special form of forward-backward stochastic differential equations of McKean-Vlasov type with common noise whose solution provides a good approximate of the market price. We show the convergence of the net order flow to zero in the large N-limit and get the order of convergence in N under some conditions. We also extend the model to a setup with multiple populations where the agents within each population share the same cost and coefficient functions but they can be different population by population.
http://w3id.org/mlsea/pwc/scientificWork/A%20Mean-Field%20Game%20Approach%20to%20Equilibrium%20Pricing%20in%20Solar%20Renewable%20Energy%20Certificate%20Markets                                                                                  A Mean-Field Game Approach to Equilibrium Pricing in Solar Renewable Energy Certificate Markets                                                                                  Solar Renewable Energy Certificate (SREC) markets are a market-based system that incentivizes solar energy generation. A regulatory body imposes a lower bound on the amount of energy each regulated firm must generate via solar means, providing them with a tradeable certificate for each MWh generated. Firms seek to navigate the market optimally by modulating their SREC generation and trading rates. As such, the SREC market can be viewed as a stochastic game, where agents interact through the SREC price. We study this stochastic game by solving the mean-field game (MFG) limit with sub-populations of heterogeneous agents. Market participants optimize costs accounting for trading frictions, cost of generation, non-linear non-compliance costs, and generation uncertainty. Moreover, we endogenize SREC price through market clearing. We characterize firms' optimal controls as the solution of McKean-Vlasov (MV) FBSDEs and determine the equilibrium SREC price. We establish the existence and uniqueness of a solution to this MV-FBSDE, and prove that the MFG strategies form an $ epsilon$-Nash equilibrium for the finite player game. Finally, we develop a numerical scheme for solving the MV-FBSDEs and conduct a simulation study.
http://w3id.org/mlsea/pwc/scientificWork/A%20Measure%20of%20Research%20Taste                                                                                  A Measure of Research Taste                                                                                  Researchers are often evaluated by citation-based metrics. Such metrics can inform hiring, promotion, and funding decisions. Concerns have been expressed that popular citation-based metrics incentivize researchers to maximize the production of publications. Such incentives may not be optimal for scientific progress. Here we present a citation-based measure that rewards both productivity and taste: the researcher's ability to focus on impactful contributions. The presented measure, CAP, balances the impact of publications and their quantity, thus incentivizing researchers to consider whether a publication is a useful addition to the literature. CAP is simple, interpretable, and parameter-free. We analyze the characteristics of CAP for highly-cited researchers in biology, computer science, economics, and physics, using a corpus of millions of publications and hundreds of millions of citations with yearly temporal granularity. CAP produces qualitatively plausible outcomes and has a number of advantages over prior metrics. Results can be explored at https://cap-measure.org/
http://w3id.org/mlsea/pwc/scientificWork/A%20Mechanism%20for%20Producing%20Aligned%20Latent%20Spaces%20with%20Autoencoders                                                                                  A Mechanism for Producing Aligned Latent Spaces with Autoencoders                                                                                  Aligned latent spaces, where meaningful semantic shifts in the input space correspond to a translation in the embedding space, play an important role in the success of downstream tasks such as unsupervised clustering and data imputation. In this work, we prove that linear and nonlinear autoencoders produce aligned latent spaces by stretching along the left singular vectors of the data. We fully characterize the amount of stretching in linear autoencoders and provide an initialization scheme to arbitrarily stretch along the top directions using these networks. We also quantify the amount of stretching in nonlinear autoencoders in a simplified setting. We use our theoretical results to align drug signatures across cell types in gene expression space and semantic shifts in word embedding spaces.
http://w3id.org/mlsea/pwc/scientificWork/A%20Memory%20Optimized%20Data%20Structure%20for%20Binary%20Chromosomes%20in%20Genetic%20Algorithm                                                                                  A Memory Optimized Data Structure for Binary Chromosomes in Genetic Algorithm                                                                                  This paper presents a memory-optimized metadata-based data structure for implementation of binary chromosome in Genetic Algorithm. In GA different types of genotypes are used depending on the problem domain. Among these, binary genotype is the most popular one for non-enumerated encoding owing to its representational and computational simplicity. This paper proposes a memory-optimized implementation approach of binary genotype. The approach improves the memory utilization as well as capacity of retaining alleles. Mathematical proof has been provided to establish the same.
http://w3id.org/mlsea/pwc/scientificWork/A%20Meta%20Learning%20Approach%20to%20Discerning%20Causal%20Graph%20Structure                                                                                  A Meta Learning Approach to Discerning Causal Graph Structure                                                                                  We explore the usage of meta-learning to derive the causal direction between variables by optimizing over a measure of distribution simplicity. We incorporate a stochastic graph representation which includes latent variables and allows for more generalizability and graph structure expression. Our model is able to learn causal direction indicators for complex graph structures despite effects of latent confounders. Further, we explore robustness of our method with respect to violations of our distributional assumptions and data scarcity. Our model is particularly robust to modest data scarcity, but is less robust to distributional changes. By interpreting the model predictions as stochastic events, we propose a simple ensemble method classifier to reduce the outcome variability as an average of biased events. This methodology demonstrates ability to infer the existence as well as the direction of a causal relationship between data distributions.
http://w3id.org/mlsea/pwc/scientificWork/A%20Meta-Learning%20Approach%20for%20Medical%20Image%20Registration                                                                                  A Meta-Learning Approach for Medical Image Registration                                                                                  Non-rigid registration is a necessary but challenging task in medical imaging studies. Recently, unsupervised registration models have shown good performance, but they often require a large-scale training dataset and long training times. Therefore, in real world application where only dozens to hundreds of image pairs are available, existing models cannot be practically used. To address these limitations, we propose a novel unsupervised registration model which is integrated with a gradient-based meta learning framework. In particular, we train a meta learner which finds an initialization point of parameters by utilizing a variety of existing registration datasets. To quickly adapt to various tasks, the meta learner was updated to get close to the center of parameters which are fine-tuned for each registration task. Thereby, our model can adapt to unseen domain tasks via a short fine-tuning process and perform accurate registration. To verify the superiority of our model, we train the model for various 2D medical image registration tasks such as retinal choroid Optical Coherence Tomography Angiography (OCTA), CT organs, and brain MRI scans and test on registration of retinal OCTA Superficial Capillary Plexus (SCP). In our experiments, the proposed model obtained significantly improved performance in terms of accuracy and training time compared to other registration models.
http://w3id.org/mlsea/pwc/scientificWork/A%20Metamodel%20Structure%20For%20Regression%20Analysis%3A%20Application%20To%20Prediction%20Of%20Autism%20Spectrum%20Disorder%20Severity                                                                                  A Metamodel Structure For Regression Analysis: Application To Prediction Of Autism Spectrum Disorder Severity                                                                                  Traditional regression models do not generalize well when learning from small and noisy datasets. Here we propose a novel metamodel structure to improve the regression result. The metamodel is composed of multiple classification base models and a regression model built upon the base models. We test this structure on the prediction of autism spectrum disorder (ASD) severity as measured by the ADOS communication (ADOS COMM) score from resting-state fMRI data, using a variety of base models. The metamodel outperforms traditional regression models as measured by the Pearson correlation coefficient between true and predicted scores and stability. In addition, we found that the metamodel is more flexible and more generalizable.
http://w3id.org/mlsea/pwc/scientificWork/A%20Method%20of%20Moments%20for%20Mixture%20Models%20and%20Hidden%20Markov%20Models                                                                                  A Method of Moments for Mixture Models and Hidden Markov Models                                                                                  Mixture models are a fundamental tool in applied statistics and machine learning for treating data taken from multiple subpopulations. The current practice for estimating the parameters of such models relies on local search heuristics (e.g., the EM algorithm) which are prone to failure, and existing consistent methods are unfavorable due to their high computational and sample complexity which typically scale exponentially with the number of mixture components. This work develops an efficient method of moments approach to parameter estimation for a broad class of high-dimensional mixture models with many components, including multi-view mixtures of Gaussians (such as mixtures of axis-aligned Gaussians) and hidden Markov models. The new method leads to rigorous unsupervised learning results for mixture models that were not achieved by previous works; and, because of its simplicity, it offers a viable alternative to EM for practical deployment.
http://w3id.org/mlsea/pwc/scientificWork/A%20Method%20to%20Reveal%20Speaker%20Identity%20in%20Distributed%20ASR%20Training%2C%20and%20How%20to%20Counter%20It                                                                                  A Method to Reveal Speaker Identity in Distributed ASR Training, and How to Counter It                                                                                  End-to-end Automatic Speech Recognition (ASR) models are commonly trained over spoken utterances using optimization methods like Stochastic Gradient Descent (SGD). In distributed settings like Federated Learning, model training requires transmission of gradients over a network. In this work, we design the first method for revealing the identity of the speaker of a training utterance with access only to a gradient. We propose Hessian-Free Gradients Matching, an input reconstruction technique that operates without second derivatives of the loss function (required in prior works), which can be expensive to compute. We show the effectiveness of our method using the DeepSpeech model architecture, demonstrating that it is possible to reveal the speaker's identity with 34% top-1 accuracy (51% top-5 accuracy) on the LibriSpeech dataset. Further, we study the effect of two well-known techniques, Differentially Private SGD and Dropout, on the success of our method. We show that a dropout rate of 0.2 can reduce the speaker identity accuracy to 0% top-1 (0.5% top-5).
http://w3id.org/mlsea/pwc/scientificWork/A%20Methodology%20for%20Bi-Directional%20Knowledge-Based%20Assessment%20of%20Compliance%20to%20Continuous%20Application%20of%20Clinical%20Guidelines                                                                                  A Methodology for Bi-Directional Knowledge-Based Assessment of Compliance to Continuous Application of Clinical Guidelines                                                                                  Clinicians often do not sufficiently adhere to evidence-based clinical guidelines in a manner sensitive to the context of each patient. It is important to detect such deviations, typically including redundant or missing actions, even when the detection is performed retrospectively, so as to inform both the attending clinician and policy makers. Furthermore, it would be beneficial to detect such deviations in a manner proportional to the level of the deviation, and not to simply use arbitrary cut-off values. In this study, we introduce a new approach for automated guideline-based quality assessment of the care process, the bidirectional knowledge-based assessment of compliance (BiKBAC) method. Our BiKBAC methodology assesses the degree of compliance when applying clinical guidelines, with respect to multiple different aspects of the guideline (e.g., the guideline's process and outcome objectives). The assessment is performed through a highly detailed, automated quality-assessment retrospective analysis, which compares a formal representation of the guideline and of its process and outcome intentions (we use the Asbru language for that purpose) with the longitudinal electronic medical record of its continuous application over a significant time period, using both a top-down and a bottom-up approach, which we explain in detail. Partial matches of the data to the process and to the outcome objectives are resolved using fuzzy temporal logic. We also introduce the DiscovErr system, which implements the BiKBAC approach, and present its detailed architecture. The DiscovErr system was evaluated in a separate study in the type 2 diabetes management domain, by comparing its performance to a panel of three clinicians, with highly encouraging results with respect to the completeness and correctness of its comments.
http://w3id.org/mlsea/pwc/scientificWork/A%20Methodology%20for%20Exploring%20Deep%20Convolutional%20Features%20in%20Relation%20to%20Hand-Crafted%20Features%20with%20an%20Application%20to%20Music%20Audio%20Modeling                                                                                  A Methodology for Exploring Deep Convolutional Features in Relation to Hand-Crafted Features with an Application to Music Audio Modeling                                                                                  Understanding the features learned by deep models is important from a model trust perspective, especially as deep systems are deployed in the real world. Most recent approaches for deep feature understanding or model explanation focus on highlighting input data features that are relevant for classification decisions. In this work, we instead take the perspective of relating deep features to well-studied, hand-crafted features that are meaningful for the application of interest. We propose a methodology and set of systematic experiments for exploring deep features in this setting, where input feature importance approaches for deep feature understanding do not apply. Our experiments focus on understanding which hand-crafted and deep features are useful for the classification task of interest, how robust these features are for related tasks and how similar the deep features are to the meaningful hand-crafted features. Our proposed method is general to many application areas and we demonstrate its utility on orchestral music audio data.
http://w3id.org/mlsea/pwc/scientificWork/A%20Methodology%20for%20the%20Offline%20Evaluation%20of%20Recommender%20Systems%20in%20a%20User%20Interface%20with%20Multiple%20Carousels                                                                                  A Methodology for the Offline Evaluation of Recommender Systems in a User Interface with Multiple Carousels                                                                                  Many video-on-demand and music streaming services provide the user with a page consisting of several recommendation lists, i.e. widgets or swipeable carousels, each built with a specific criterion (e.g. most recent, TV series, etc.). Finding efficient strategies to select which carousels to display is an active research topic of great industrial interest. In this setting, the overall quality of the recommendations of a new algorithm cannot be assessed by measuring solely its individual recommendation quality. Rather, it should be evaluated in a context where other recommendation lists are already available, to account for how they complement each other. This is not considered by traditional offline evaluation protocols. Hence, we propose an offline evaluation protocol for a carousel setting in which the recommendation quality of a model is measured by how much it improves upon that of an already available set of carousels. We report experiments on publicly available datasets on the movie domain and notice that under a carousel setting the ranking of the algorithms change. In particular, when a SLIM carousel is available, matrix factorization models tend to be preferred, while item-based models are penalized. We also propose to extend ranking metrics to the two-dimensional carousel layout in order to account for a known position bias, i.e. users will not explore the lists sequentially, but rather concentrate on the top-left corner of the screen.
http://w3id.org/mlsea/pwc/scientificWork/A%20Microarchitecture%20Implementation%20Framework%20for%20Online%20Learning%20with%20Temporal%20Neural%20Networks                                                                                  A Microarchitecture Implementation Framework for Online Learning with Temporal Neural Networks                                                                                  Temporal Neural Networks (TNNs) are spiking neural networks that use time as a resource to represent and process information, similar to the mammalian neocortex. In contrast to compute-intensive deep neural networks that employ separate training and inference phases, TNNs are capable of extremely efficient online incremental/continual learning and are excellent candidates for building edge-native sensory processing units. This work proposes a microarchitecture framework for implementing TNNs using standard CMOS. Gate-level implementations of three key building blocks are presented: 1) multi-synapse neurons, 2) multi-neuron columns, and 3) unsupervised and supervised online learning algorithms based on Spike Timing Dependent Plasticity (STDP). The proposed microarchitecture is embodied in a set of characteristic scaling equations for assessing the gate count, area, delay and power for any TNN design. Post-synthesis results (in 45nm CMOS) for the proposed designs are presented, and their online incremental learning capability is demonstrated.
http://w3id.org/mlsea/pwc/scientificWork/A%20Microsimulation%20Analysis%20of%20the%20Distributional%20Impact%20over%20the%20Three%20Waves%20of%20the%20COVID-19%20Crisis%20in%20Ireland                                                                                  A Microsimulation Analysis of the Distributional Impact over the Three Waves of the COVID-19 Crisis in Ireland                                                                                  This paper relies on a microsimulation framework to undertake an analysis of the distributional implications of the COVID-19 crisis over three waves. Given the lack of real-time survey data during the fast moving crisis, it applies a nowcasting methodology and real-time aggregate administrative data to calibrate an income survey and to simulate changes in the tax benefit system that attempted to mitigate the impacts of the crisis. Our analysis shows how crisis-induced income-support policy innovations combined with existing progressive elements of the tax-benefit system were effective in avoiding an increase in income inequality at all stages of waves 1-3 of the COVID-19 emergency in Ireland. There was, however, a decline in generosity over time as benefits became more targeted. On a methodological level, our paper makes a specific contribution in relation to the choice of welfare measure in assessing the impact of the COVID-19 crisis on inequality.
http://w3id.org/mlsea/pwc/scientificWork/A%20Millimeter-Wave%20Self-Mixing%20Array%20with%20Large%20Gain%20and%20Wide%20Angular%20Receiving%20Range                                                                                  A Millimeter-Wave Self-Mixing Array with Large Gain and Wide Angular Receiving Range                                                                                  The concept of self-mixing antenna arrays is presented and analyzed with respect to its beneficial behavior of large gain over a wide angular range. The large gain is attained by an antenna array with large element spacing, where all array element signals are combined approximately coherently over the entire angular receiving range. This functionality is achieved by the self-mixing principle, where an exact description via an intermediate frequency (IF) array factor is derived. For verification purposes, a 4 x 2 self-mixing array is fabricated and measured in the frequency range from 34 GHz to 39 GHz. A multiple-resonances millimeter-wave microstrip patch antenna has been especially developed to achieve large bandwidth and a wide angular receiving range. The broad beamwidth is achieved by two parasitic patches and suitable radiation characteristics of the resonant modes. The self-mixing of the receive signal is realized at each antenna element by a Schottky diode with an optimized operating point. The down-converted array element signals are then combined and measured at the IF. The receive power is increased significantly over a large angular range as compared to conventional array feeding techniques. The simulation results are verified by measurements, which show very good agreement.
http://w3id.org/mlsea/pwc/scientificWork/A%20Million%20Tweets%20Are%20Worth%20a%20Few%20Points%3A%20Tuning%20Transformers%20for%20Customer%20Service%20Tasks                                                                                  A Million Tweets Are Worth a Few Points: Tuning Transformers for Customer Service Tasks                                                                                  In online domain-specific customer service applications, many companies struggle to deploy advanced NLP models successfully, due to the limited availability of and noise in their datasets. While prior research demonstrated the potential of migrating large open-domain pretrained models for domain-specific tasks, the appropriate (pre)training strategies have not yet been rigorously evaluated in such social media customer service settings, especially under multilingual conditions. We address this gap by collecting a multilingual social media corpus containing customer service conversations (865k tweets), comparing various pipelines of pretraining and finetuning approaches, applying them on 5 different end tasks. We show that pretraining a generic multilingual transformer model on our in-domain dataset, before finetuning on specific end tasks, consistently boosts performance, especially in non-English settings.
http://w3id.org/mlsea/pwc/scientificWork/A%20Minimalist%20Approach%20to%20Offline%20Reinforcement%20Learning                                                                                  A Minimalist Approach to Offline Reinforcement Learning                                                                                  Offline reinforcement learning (RL) defines the task of learning from a fixed batch of data. Due to errors in value estimation from out-of-distribution actions, most offline RL algorithms take the approach of constraining or regularizing the policy with the actions contained in the dataset. Built on pre-existing RL algorithms, modifications to make an RL algorithm work offline comes at the cost of additional complexity. Offline RL algorithms introduce new hyperparameters and often leverage secondary components such as generative models, while adjusting the underlying RL algorithm. In this paper we aim to make a deep RL algorithm work while making minimal changes. We find that we can match the performance of state-of-the-art offline RL algorithms by simply adding a behavior cloning term to the policy update of an online RL algorithm and normalizing the data. The resulting algorithm is a simple to implement and tune baseline, while more than halving the overall run time by removing the additional computational overhead of previous methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20Minimax%20Lower%20Bound%20for%20Low-Rank%20Matrix-Variate%20Logistic%20Regression                                                                                  A Minimax Lower Bound for Low-Rank Matrix-Variate Logistic Regression                                                                                  This paper considers the problem of matrix-variate logistic regression. It derives the fundamental error threshold on estimating low-rank coefficient matrices in the logistic regression problem by obtaining a lower bound on the minimax risk. The bound depends explicitly on the dimension and distribution of the covariates, the rank and energy of the coefficient matrix, and the number of samples. The resulting bound is proportional to the intrinsic degrees of freedom in the problem, which suggests the sample complexity of the low-rank matrix logistic regression problem can be lower than that for vectorized logistic regression. The proof techniques utilized in this work also set the stage for development of minimax lower bounds for tensor-variate logistic regression problems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Minimum-Footprint%20Implementation%20of%20Discrete-Time%20ADRC                                                                                  A Minimum-Footprint Implementation of Discrete-Time ADRC                                                                                  To foster the adoption of active disturbance rejection control (ADRC) and support its deployment even on low-cost embedded systems, this article introduces the most efficient implementation of linear discrete-time ADRC to date. While maintaining all features and the exact performance characteristics of the state-space form, computational efforts and storage requirements are reduced to a minimum compared to all existing implementations. This opens up new possibilities to use ADRC in applications with high sample rates, tight timing constraints, or low computational power.
http://w3id.org/mlsea/pwc/scientificWork/A%20Mitigation%20Score%20for%20COVID-19                                                                                  A Mitigation Score for COVID-19                                                                                  This note describes a simple score to indicate the effectiveness of mitigation against infections of COVID-19 as observed by new case counts. The score includes normalization, making comparisons across jurisdictions possible. The smoothing employed provides robustness in the face of reporting vagaries while retaining salient features of evolution, enabling a clearer picture for decision makers and the public.
http://w3id.org/mlsea/pwc/scientificWork/A%20Model%20Randomization%20Approach%20to%20Statistical%20Parameter%20Privacy                                                                                  A Model Randomization Approach to Statistical Parameter Privacy                                                                                  In this paper, we study a privacy filter design problem for a sequence of sensor measurements whose joint probability density function (p.d.f.) depends on a private parameter. To ensure parameter privacy, we propose a filter design framework which consists of two components: a randomizer and a nonlinear transformation. The randomizer takes the private parameter as input and randomly generates a pseudo parameter. The nonlinear mapping transforms the measurements such that the joint p.d.f. of the filter's output depends on the pseudo parameter rather than the private parameter. It also ensures that the joint p.d.f. of the filter's output belongs to the same family of distributions as that of the measurements. The nonlinear transformation has a feedforward-feedback structure that allows real-time and causal generation of the disguised measurements with low complexity using a recursive structure. The design of the randomizer is formulated as an optimization problem subject to a privacy constraint, in terms of mutual information, and it is shown that the optimal randomizer is the solution of a convex optimization problem. Using information-theoretic inequalities, we show that the performance of any estimator of the private parameter, based on the output of the privacy filter, is limited by the privacy constraint. The structure of the nonlinear transformation is studied in the special cases of independent and identically distributed, Markovian, and Gauss-Markov measurements. Our results show that the privacy filter in the Gauss-Markov case can be implemented as two one-step ahead Kalman predictors and a set of minimum mean square error predictors. The Kalman predictors significantly reduce the complexity of computing the disguised measurements. A numerical example on occupancy privacy in a building automation system illustrates the approach.
http://w3id.org/mlsea/pwc/scientificWork/A%20Model%20of%20Market%20Making%20and%20Price%20Impact                                                                                  A Model of Market Making and Price Impact                                                                                  Traders constantly consider the price impact associated with changing their positions. This paper seeks to understand how price impact emerges from the quoting strategies of market makers. To this end, market making is modeled as a dynamic auction using the mathematical framework of Stochastic Differential Games. In Nash Equilibrium, the market makers' quoting strategies generate a price impact function that is of the same form as the celebrated Almgren-Chriss model. The key insight is that price impact is the mechanism through which market makers earn profits while matching their books. As such, price impact is an essential feature of markets where flow is intermediated by market makers.
http://w3id.org/mlsea/pwc/scientificWork/A%20Model-Driven%20Approach%20to%20Machine%20Learning%20and%20Software%20Modeling%20for%20the%20IoT                                                                                  A Model-Driven Approach to Machine Learning and Software Modeling for the IoT                                                                                  Models are used in both Software Engineering (SE) and Artificial Intelligence (AI). SE models may specify the architecture at different levels of abstraction and for addressing different concerns at various stages of the software development life-cycle, from early conceptualization and design, to verification, implementation, testing and evolution. However, AI models may provide smart capabilities, such as prediction and decision-making support. For instance, in Machine Learning (ML), which is currently the most popular sub-discipline of AI, mathematical models may learn useful patterns in the observed data and can become capable of making predictions. The goal of this work is to create synergy by bringing models in the said communities together and proposing a holistic approach to model-driven software development for intelligent systems that require ML. We illustrate how software models can become capable of creating and dealing with ML models in a seamless manner. The main focus is on the domain of the Internet of Things (IoT), where both ML and model-driven SE play a key role. In the context of the need to take a Cyber-Physical System-of-Systems perspective of the targeted architecture, an integrated design environment for both SE and ML sub-systems would best support the optimization and overall efficiency of the implementation of the resulting system. In particular, we implement the proposed approach, called ML-Quadrat, based on ThingML, and validate it using a case study from the IoT domain, as well as through an empirical user evaluation. It transpires that the proposed approach is not only feasible, but may also contribute to the performance leap of software development for smart Cyber-Physical Systems (CPS) which are connected to the IoT, as well as an enhanced user experience of the practitioners who use the proposed modeling solution.
http://w3id.org/mlsea/pwc/scientificWork/A%20Model-Theoretic%20Approach%20to%20Modular%20Implementation%2C%20with%20Application%20to%20Biological%20Systems                                                                                  A Model-Theoretic Approach to Modular Implementation, with Application to Biological Systems                                                                                  We present a method for logical specification of the behavior of a system based on the specification of its components and for representing and specifying the effect of a class of communication 'signaling pathways' among them. Variations in the actions and effects (semantics) of the pathways can be seen in the resulting system. We show how to 'enrich' the behavior of the original components by exactly what is provided by the signaling pathways. This has the potential to allow a formal proof of a property of the system as a whole, or a formal diagnosis why a system of communicating modules may not satisfy a desired specification, and if and how it can be 'fixed.' There are five distinct stages in our view: 1. A description of the connections between potential components at the level of which potential modules are connected. 2. Specific choices for those modules 3. An 'elaboration' of the connections among different modules to connections among members of those different modules 4. A description of the semantics of those connections (what they actually do) 5. A 'padding' of the modules chosen in step 2 to allow for the action of the connections (step 3), as described in step 4. The formalism is completely hierarchical -- in other words, the formalism works equally for the system as a whole, for each of the modules, and for any larger system in which the original system is a component. As it turns out, for whatever reason -- perhaps evolutionary advantage optimization at the right level of abstraction -- many biological systems are hierarchically modular, with the modules communicating via various pathways. Thus, a formalism such as the one presented here could have application to questions in biology.
http://w3id.org/mlsea/pwc/scientificWork/A%20Modern%20Perspective%20on%20Query%20Likelihood%20with%20Deep%20Generative%20Retrieval%20Models                                                                                  A Modern Perspective on Query Likelihood with Deep Generative Retrieval Models                                                                                  Existing neural ranking models follow the text matching paradigm, where document-to-query relevance is estimated through predicting the matching score. Drawing from the rich literature of classical generative retrieval models, we introduce and formalize the paradigm of deep generative retrieval models defined via the cumulative probabilities of generating query terms. This paradigm offers a grounded probabilistic view on relevance estimation while still enabling the use of modern neural architectures. In contrast to the matching paradigm, the probabilistic nature of generative rankers readily offers a fine-grained measure of uncertainty. We adopt several current neural generative models in our framework and introduce a novel generative ranker (T-PGN), which combines the encoding capacity of Transformers with the Pointer Generator Network model. We conduct an extensive set of evaluation experiments on passage retrieval, leveraging the MS MARCO Passage Re-ranking and TREC Deep Learning 2019 Passage Re-ranking collections. Our results show the significantly higher performance of the T-PGN model when compared with other generative models. Lastly, we demonstrate that exploiting the uncertainty information of deep generative rankers opens new perspectives to query/collection understanding, and significantly improves the cut-off prediction task.
http://w3id.org/mlsea/pwc/scientificWork/A%20Modest%20Pareto%20Optimisation%20Analysis%20of%20Dependency%20Parsers%20in%202021                                                                                  A Modest Pareto Optimisation Analysis of Dependency Parsers in 2021                                                                                  We evaluate three leading dependency parser systems from different paradigms on a small yet diverse subset of languages in terms of their accuracy-efficiency Pareto front. As we are interested in efficiency, we evaluate core parsers without pretrained language models (as these are typically huge networks and would constitute most of the compute time) or other augmentations that can be transversally applied to any of them. Biaffine parsing emerges as a well-balanced default choice, with sequence-labelling parsing being preferable if inference speed (but not training energy cost) is the priority.
http://w3id.org/mlsea/pwc/scientificWork/A%20Modified%20Batch%20Intrinsic%20Plasticity%20Method%20for%20Pre-training%20the%20Random%20Coefficients%20of%20Extreme%20Learning%20Machines                                                                                  A Modified Batch Intrinsic Plasticity Method for Pre-training the Random Coefficients of Extreme Learning Machines                                                                                  In extreme learning machines (ELM) the hidden-layer coefficients are randomly set and fixed, while the output-layer coefficients of the neural network are computed by a least squares method. The randomly-assigned coefficients in ELM are known to influence its performance and accuracy significantly. In this paper we present a modified batch intrinsic plasticity (modBIP) method for pre-training the random coefficients in the ELM neural networks. The current method is devised based on the same principle as the batch intrinsic plasticity (BIP) method, namely, by enhancing the information transmission in every node of the neural network. It differs from BIP in two prominent aspects. First, modBIP does not involve the activation function in its algorithm, and it can be applied with any activation function in the neural network. In contrast, BIP employs the inverse of the activation function in its construction, and requires the activation function to be invertible (or monotonic). The modBIP method can work with the often-used non-monotonic activation functions (e.g. Gaussian, swish, Gaussian error linear unit, and radial-basis type functions), with which BIP breaks down. Second, modBIP generates target samples on random intervals with a minimum size, which leads to highly accurate computation results when combined with ELM. The combined ELM/modBIP method is markedly more accurate than ELM/BIP in numerical simulations. Ample numerical experiments are presented with shallow and deep neural networks for function approximation and boundary/initial value problems with partial differential equations. They demonstrate that the combined ELM/modBIP method produces highly accurate simulation results, and that its accuracy is insensitive to the random-coefficient initializations in the neural network. This is in sharp contrast with the ELM results without pre-training of the random coefficients.
http://w3id.org/mlsea/pwc/scientificWork/A%20Modified%20Convolutional%20Network%20for%20Auto-encoding%20based%20on%20Pattern%20Theory%20Growth%20Function                                                                                  A Modified Convolutional Network for Auto-encoding based on Pattern Theory Growth Function                                                                                  This brief paper reports the shortcoming of a variant of convolutional neural network whose components are developed based on the pattern theory framework.
http://w3id.org/mlsea/pwc/scientificWork/A%20Modified%20Randomization%20Test%20for%20the%20Level%20of%20Clustering                                                                                  A Modified Randomization Test for the Level of Clustering                                                                                  Suppose a researcher observes individuals within a county within a state. Given concerns about correlation across individuals, it is common to group observations into clusters and conduct inference treating observations across clusters as roughly independent. However, a researcher that has chosen to cluster at the county level may be unsure of their decision, given knowledge that observations are independent across states. This paper proposes a modified randomization test as a robustness check for the chosen level of clustering in a linear regression setting. Existing tests require either the number of states or number of counties to be large. Our method is designed for settings with few states and few counties. While the method is conservative, it has competitive power in settings that may be relevant to empirical work.
http://w3id.org/mlsea/pwc/scientificWork/A%20Modified%20SEIR%20Model%20for%20the%20Spread%20of%20COVID-19%20Considering%20Different%20Vaccine%20Types                                                                                  A Modified SEIR Model for the Spread of COVID-19 Considering Different Vaccine Types                                                                                  The COVID-19 pandemic has influenced the lives of people globally. In the past year many researchers have proposed different models and approaches to explore in what ways the spread of the disease could be mitigated. One of the models that have been used a great deal is the Susceptible-Exposed-Infectious-Recovered (SEIR) model. Some researchers have modified the traditional SEIR model, and proposed new versions of it. However, to the best of our knowledge, the state-of-the-art papers have not considered the effect of different vaccine types, meaning single shot and double shot vaccines, in their SEIR model. In this paper, we propose a modified version of the SEIR model which takes into account the effect of different vaccine types. We compare how different policies for the administration of the vaccine can influence the rate at which people are exposed to the disease, get infected, recover, and pass away. Our results suggest that taking the double shot vaccine such as Pfizer-BioNTech and Moderna does a better job at mitigating the spread and fatality rate of the disease compared to the single shot vaccine, due to its higher efficacy.
http://w3id.org/mlsea/pwc/scientificWork/A%20Modified%20Schmidl-Cox%20OFDM%20Timing%20Detector                                                                                  A Modified Schmidl-Cox OFDM Timing Detector                                                                                  We describe a simple modification of the Schmidl-Cox detector for establishing timing in OFDM transmissions that stabilizes performance in transitions from no-signal to signal, or vice-versa. Moreover, the proposed modification scales the detector's metric between 0 and 1 for all scenarios, simplifying threshold setting, and improves timing detector SNR.
http://w3id.org/mlsea/pwc/scientificWork/A%20Modular%20Approach%20for%20Synchronized%20Wireless%20Multimodal%20Multisensor%20Data%20Acquisition%20in%20Highly%20Dynamic%20Social%20Settings                                                                                  A Modular Approach for Synchronized Wireless Multimodal Multisensor Data Acquisition in Highly Dynamic Social Settings                                                                                  Existing data acquisition literature for human behavior research provides wired solutions, mainly for controlled laboratory setups. In uncontrolled free-standing conversation settings, where participants are free to walk around, these solutions are unsuitable. While wireless solutions are employed in the broadcasting industry, they can be prohibitively expensive. In this work, we propose a modular and cost-effective wireless approach for synchronized multisensor data acquisition of social human behavior. Our core idea involves a cost-accuracy trade-off by using Network Time Protocol (NTP) as a source reference for all sensors. While commonly used as a reference in ubiquitous computing, NTP is widely considered to be insufficiently accurate as a reference for video applications, where Precision Time Protocol (PTP) or Global Positioning System (GPS) based references are preferred. We argue and show, however, that the latency introduced by using NTP as a source reference is adequate for human behavior research, and the subsequent cost and modularity benefits are a desirable trade-off for applications in this domain. We also describe one instantiation of the approach deployed in a real-world experiment to demonstrate the practicality of our setup in-the-wild.
http://w3id.org/mlsea/pwc/scientificWork/A%20Modular%20and%20Transferable%20Reinforcement%20Learning%20Framework%20for%20the%20Fleet%20Rebalancing%20Problem                                                                                  A Modular and Transferable Reinforcement Learning Framework for the Fleet Rebalancing Problem                                                                                  Mobility on demand (MoD) systems show great promise in realizing flexible and efficient urban transportation. However, significant technical challenges arise from operational decision making associated with MoD vehicle dispatch and fleet rebalancing. For this reason, operators tend to employ simplified algorithms that have been demonstrated to work well in a particular setting. To help bridge the gap between novel and existing methods, we propose a modular framework for fleet rebalancing based on model-free reinforcement learning (RL) that can leverage an existing dispatch method to minimize system cost. In particular, by treating dispatch as part of the environment dynamics, a centralized agent can learn to intermittently direct the dispatcher to reposition free vehicles and mitigate against fleet imbalance. We formulate RL state and action spaces as distributions over a grid partitioning of the operating area, making the framework scalable and avoiding the complexities associated with multiagent RL. Numerical experiments, using real-world trip and network data, demonstrate that this approach has several distinct advantages over baseline methods including: improved system cost; high degree of adaptability to the selected dispatch method; and the ability to perform scale-invariant transfer learning between problem instances with similar vehicle and request distributions.
http://w3id.org/mlsea/pwc/scientificWork/A%20Modulation%20Front-End%20for%20Music%20Audio%20Tagging                                                                                  A Modulation Front-End for Music Audio Tagging                                                                                  Convolutional Neural Networks have been extensively explored in the task of automatic music tagging. The problem can be approached by using either engineered time-frequency features or raw audio as input. Modulation filter bank representations that have been actively researched as a basis for timbre perception have the potential to facilitate the extraction of perceptually salient features. We explore end-to-end learned front-ends for audio representation learning, ModNet and SincModNet, that incorporate a temporal modulation processing block. The structure is effectively analogous to a modulation filter bank, where the FIR filter center frequencies are learned in a data-driven manner. The expectation is that a perceptually motivated filter bank can provide a useful representation for identifying music features. Our experimental results provide a fully visualisable and interpretable front-end temporal modulation decomposition of raw audio. We evaluate the performance of our model against the state-of-the-art of music tagging on the MagnaTagATune dataset. We analyse the impact on performance for particular tags when time-frequency bands are subsampled by the modulation filters at a progressively reduced rate. We demonstrate that modulation filtering provides promising results for music tagging and feature representation, without using extensive musical domain knowledge in the design of this front-end.
http://w3id.org/mlsea/pwc/scientificWork/A%20Monotone%20Approximate%20Dynamic%20Programming%20Approach%20for%20the%20Stochastic%20Scheduling%2C%20Allocation%2C%20and%20Inventory%20Replenishment%20Problem%3A%20Applications%20to%20Drone%20and%20Electric%20Vehicle%20Battery%20Swap%20Stations                                                                                  A Monotone Approximate Dynamic Programming Approach for the Stochastic Scheduling, Allocation, and Inventory Replenishment Problem: Applications to Drone and Electric Vehicle Battery Swap Stations                                                                                  There is a growing interest in using electric vehicles (EVs) and drones for many applications. However, battery-oriented issues, including range anxiety and battery degradation, impede adoption. Battery swap stations are one alternative to reduce these concerns that allow the swap of depleted for full batteries in minutes. We consider the problem of deriving actions at a battery swap station when explicitly considering the uncertain arrival of swap demand, battery degradation, and replacement. We model the operations at a battery swap station using a finite horizon Markov Decision Process model for the stochastic scheduling, allocation, and inventory replenishment problem (SAIRP), which determines when and how many batteries are charged, discharged, and replaced over time. We present theoretical proofs for the monotonicity of the value function and monotone structure of an optimal policy for special SAIRP cases. Due to the curses of dimensionality, we develop a new monotone approximate dynamic programming (ADP) method, which intelligently initializes a value function approximation using regression. In computational tests, we demonstrate the superior performance of the new regression-based monotone ADP method as compared to exact methods and other monotone ADP methods. Further, with the tests, we deduce policy insights for drone swap stations.
http://w3id.org/mlsea/pwc/scientificWork/A%20More%20Compact%20Object%20Detector%20Head%20Network%20with%20Feature%20Enhancement%20and%20Relational%20Reasoning                                                                                  A More Compact Object Detector Head Network with Feature Enhancement and Relational Reasoning                                                                                  Modeling implicit feature interaction patterns is of significant importance to object detection tasks. However, in the two-stage detectors, due to the excessive use of hand-crafted components, it is very difficult to reason about the implicit relationship of the instance features. To tackle this problem, we analyze three different levels of feature interaction relationships, namely, the dependency relationship between the cropped local features and global features, the feature autocorrelation within the instance, and the cross-correlation relationship between the instances. To this end, we propose a more compact object detector head network (CODH), which can not only preserve global context information and condense the information density, but also allows instance-wise feature enhancement and relational reasoning in a larger matrix space. Without bells and whistles, our method can effectively improve the detection performance while significantly reducing the parameters of the model, e.g., with our method, the parameters of the head network is 0.6 times smaller than the state-of-the-art Cascade R-CNN, yet the performance boost is 1.3% on COCO test-dev. Without losing generality, we can also build a more lighter head network for other multi-stage detectors by assembling our method.
http://w3id.org/mlsea/pwc/scientificWork/A%20Multi%20Polarization%20Square%20Patch%20Antenna%20with%20a%20Reconfigurable%20Feeding%20Network                                                                                  A Multi Polarization Square Patch Antenna with a Reconfigurable Feeding Network                                                                                  A multi-polarization square patch antenna with a reconfigurable feeding network is presented in this paper. The reconfigurable feeding network of this antenna is implemented on an FR-4 substrate by a Wilkinson power divider and a branch line coupler which perform amplitude distribution in the feeding network. Besides, two switching circuits which consist of one PIN diode (BAR63-02w) and its DC biasing circuit manage the RF signal flow on this feeding network. These switching circuits control the phase of the RF signal applied to the square patch, so it can provide linear polarization, left-hand and right-hand circular polarization at 2.45 GHz which has many applications in wireless networks. The simulated and measured results are presented which illuminate acceptable axial ratio bandwidth (ARBW) for both right-hand and left-hand circular polarization in (2.38-2.48 GHz) and minimum -10 dB return loss at 2.45 GHz.
http://w3id.org/mlsea/pwc/scientificWork/A%20Multi-Branch%20Hybrid%20Transformer%20Networkfor%20Corneal%20Endothelial%20Cell%20Segmentation                                                                                  A Multi-Branch Hybrid Transformer Networkfor Corneal Endothelial Cell Segmentation                                                                                  Corneal endothelial cell segmentation plays a vital role inquantifying clinical indicators such as cell density, coefficient of variation,and hexagonality. However, the corneal endothelium's uneven reflectionand the subject's tremor and movement cause blurred cell edges in theimage, which is difficult to segment, and need more details and contextinformation to release this problem. Due to the limited receptive field oflocal convolution and continuous downsampling, the existing deep learn-ing segmentation methods cannot make full use of global context andmiss many details. This paper proposes a Multi-Branch hybrid Trans-former Network (MBT-Net) based on the transformer and body-edgebranch. Firstly, We use the convolutional block to focus on local tex-ture feature extraction and establish long-range dependencies over space,channel, and layer by the transformer and residual connection. Besides,We use the body-edge branch to promote local consistency and to provideedge position information. On the self-collected dataset TM-EM3000 andpublic Alisarine dataset, compared with other State-Of-The-Art (SOTA)methods, the proposed method achieves an improvement.
http://w3id.org/mlsea/pwc/scientificWork/A%20Multi-Implicit%20Neural%20Representation%20for%20Fonts                                                                                  A Multi-Implicit Neural Representation for Fonts                                                                                  Fonts are ubiquitous across documents and come in a variety of styles. They are either represented in a native vector format or rasterized to produce fixed resolution images. In the first case, the non-standard representation prevents benefiting from latest network architectures for neural representations; while, in the latter case, the rasterized representation, when encoded via networks, results in loss of data fidelity, as font-specific discontinuities like edges and corners are difficult to represent using neural networks. Based on the observation that complex fonts can be represented by a superposition of a set of simpler occupancy functions, we introduce textit{multi-implicits} to represent fonts as a permutation-invariant set of learned implict functions, without losing features (e.g., edges and corners). However, while multi-implicits locally preserve font features, obtaining supervision in the form of ground truth multi-channel signals is a problem in itself. Instead, we propose how to train such a representation with only local supervision, while the proposed neural architecture directly finds globally consistent multi-implicits for font families. We extensively evaluate the proposed representation for various tasks including reconstruction, interpolation, and synthesis to demonstrate clear advantages with existing alternatives. Additionally, the representation naturally enables glyph completion, wherein a single characteristic font is used to synthesize a whole font family in the target style.
http://w3id.org/mlsea/pwc/scientificWork/A%20Multi-Layered%20Approach%20for%20Measuring%20the%20Simulation-to-Reality%20Gap%20of%20Radar%20Perception%20for%20Autonomous%20Driving                                                                                  A Multi-Layered Approach for Measuring the Simulation-to-Reality Gap of Radar Perception for Autonomous Driving                                                                                  With the increasing safety validation requirements for the release of a self-driving car, alternative approaches, such as simulation-based testing, are emerging in addition to conventional real-world testing. In order to rely on virtual tests the employed sensor models have to be validated. For this reason, it is necessary to quantify the discrepancy between simulation and reality in order to determine whether a certain fidelity is sufficient for a desired intended use. There exists no sound method to measure this simulation-to-reality gap of radar perception for autonomous driving. We address this problem by introducing a multi-layered evaluation approach, which consists of a combination of an explicit and an implicit sensor model evaluation. The former directly evaluates the realism of the synthetically generated sensor data, while the latter refers to an evaluation of a downstream target application. In order to demonstrate the method, we evaluated the fidelity of three typical radar model types (ideal, data-driven, ray tracing-based) and their applicability for virtually testing radar-based multi-object tracking. We have shown the effectiveness of the proposed approach in terms of providing an in-depth sensor model assessment that renders existing disparities visible and enables a realistic estimation of the overall model fidelity across different scenarios.
http://w3id.org/mlsea/pwc/scientificWork/A%20Multi-Level%20Attention%20Model%20for%20Evidence-Based%20Fact%20Checking                                                                                  A Multi-Level Attention Model for Evidence-Based Fact Checking                                                                                  Evidence-based fact checking aims to verify the truthfulness of a claim against evidence extracted from textual sources. Learning a representation that effectively captures relations between a claim and evidence can be challenging. Recent state-of-the-art approaches have developed increasingly sophisticated models based on graph structures. We present a simple model that can be trained on sequence structures. Our model enables inter-sentence attentions at different levels and can benefit from joint training. Results on a large-scale dataset for Fact Extraction and VERification (FEVER) show that our model outperforms the graph-based approaches and yields 1.09% and 1.42% improvements in label accuracy and FEVER score, respectively, over the best published model.
http://w3id.org/mlsea/pwc/scientificWork/A%20Multi-Persona%20Chatbot%20for%20Hotline%20Counselor%20Training                                                                                  A Multi-Persona Chatbot for Hotline Counselor Training                                                                                  Suicide prevention hotline counselors aid individuals during difficult times through millions of calls and chats. A chatbot cannot safely replace a counselor, but we explore whether a chatbot can be developed to help train human counselors. Such a system needs to simulate intimate situations across multiple practice sessions. Open-domain dialogue systems frequently suffer from generic responses that do not characterize personal stories, so we look to infuse conversations with persona information by mimicking prototype conversations. Towards building a {``}Crisisbot{''} hotline visitor simulation, we propose a counseling strategy annotation scheme and a multi-task framework that leverages these counselor strategies to retrieve similar examples, generate diverse sub-utterances, and interleave prototype and generated sub-utterances into complex responses. We evaluate this framework with crowdworkers and experienced hotline counselors. The framework considerably increases response diversity and specificity, with limited impact to coherence. Our results also show a considerable discrepancy between crowdworker and counselor judgements, which emphasizes the importance of including target populations in system development and evaluation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Multi-Size%20Neural%20Network%20with%20Attention%20Mechanism%20for%20Answer%20Selection                                                                                  A Multi-Size Neural Network with Attention Mechanism for Answer Selection                                                                                  Semantic matching is of central significance to the answer selection task which aims to select correct answers for a given question from a candidate answer pool. A useful method is to employ neural networks with attention to generate sentences representations in a way that information from pair sentences can mutually influence the computation of representations. In this work, an effective architecture,multi-size neural network with attention mechanism (AM-MSNN),is introduced into the answer selection task. This architecture captures more levels of language granularities in parallel, because of the various sizes of filters comparing with single-layer CNN and multi-layer CNNs. Meanwhile it extends the sentence representations by attention mechanism, thus containing more information for different types of questions. The empirical study on three various benchmark tasks of answer selection demonstrates the efficacy of the proposed model in all the benchmarks and its superiority over competitors. The experimental results show that (1) multi-size neural network (MSNN) is a more useful method to capture abstract features on different levels of granularities than single/multi-layer CNNs; (2) the attention mechanism (AM) is a better strategy to derive more informative representations; (3) AM-MSNN is a better architecture for the answer selection task for the moment.
http://w3id.org/mlsea/pwc/scientificWork/A%20Multi-Task%20Deep%20Learning%20Framework%20for%20Building%20Footprint%20Segmentation                                                                                  A Multi-Task Deep Learning Framework for Building Footprint Segmentation                                                                                  The task of building footprint segmentation has been well-studied in the context of remote sensing (RS) as it provides valuable information in many aspects, however, difficulties brought by the nature of RS images such as variations in the spatial arrangements and in-consistent constructional patterns require studying further, since it often causes poorly classified segmentation maps. We address this need by designing a joint optimization scheme for the task of building footprint delineation and introducing two auxiliary tasks; image reconstruction and building footprint boundary segmentation with the intent to reveal the common underlying structure to advance the classification accuracy of a single task model under the favor of auxiliary tasks. In particular, we propose a deep multi-task learning (MTL) based unified fully convolutional framework which operates in an end-to-end manner by making use of joint loss function with learnable loss weights considering the homoscedastic uncertainty of each task loss. Experimental results conducted on the SpaceNet6 dataset demonstrate the potential of the proposed MTL framework as it improves the classification accuracy considerably compared to single-task and lesser compounded tasks.
http://w3id.org/mlsea/pwc/scientificWork/A%20Multi-View%20Deep%20Learning%20Approach%20for%20Cross%20Domain%20User%20Modeling%20in%20Recommendation%20Systems                                                                                  A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems                                                                                  Recent online services rely heavily on automatic personalization to recommend relevant content to a large number of users. This requires systems to scale promptly to accommodate the stream of new users visiting the online services for the first time. In this work, we propose a content-based recommendation system to address both the recommendation quality and the system scalability. We propose to use a rich feature set to represent users, according to their web browsing history and search queries. We use a Deep Learning approach to map users and items to a latent space where the similarity between users and their preferred items is maximized. We extend the model to jointly learn from features of items from different domains and user features by introducing a multi-view Deep Learning model. We show how to make this rich-feature based user representation scalable by reducing the dimension of the inputs and the amount of training data. The rich user feature representation allows the model to learn relevant user behavior patterns and give useful recommendations for users who do not have any interaction with the service, given that they have adequate search and browsing history. The combination of different domains into a single model for learning helps improve the recommendation quality across all the domains, as well as having a more compact and a semantically richer user latent feature vector. We experiment with our approach on three real-world recommendation systems acquired from different sources of Microsoft products: Windows Apps recommendation, News recommendation, and Movie/TV recommendation. Results indicate that our approach is significantly better than the state-of-the-art algorithms (up to 49% enhancement on existing users and 115% enhancement on new users). In addition, experiments on a publicly open data set also indicate the superiority of our method in comparison with transitional generative topic models, for modeling cross-domain recommender systems. Scalability analysis show that our multi-view DNN model can easily scale to encompass millions of users and billions of item entries. Experimental results also confirm that combining features from all domains produces much better performance than building separate models for each domain.
http://w3id.org/mlsea/pwc/scientificWork/A%20Multi-View%20Framework%20to%20Detect%20Redundant%20Activity%20Labels%20for%20More%20Representative%20Event%20Logs%20in%20Process%20Mining                                                                                  A Multi-View Framework to Detect Redundant Activity Labels for More Representative Event Logs in Process Mining                                                                                  Process mining aims to gain knowledge of business processes via the discovery of process models from event logs generated by information systems. The insights revealed from process mining heavily rely on the quality of the event logs. Activities extracted from different data sources or the free-text nature within the same system may lead to inconsistent labels. Such inconsistency would then lead to redundancy in activity labels, which refer to labels that have different syntax but share the same behaviours. Redundant activity labels could introduce unnecessary complexities to the event logs. The identifications of these labels from data-driven process discovery are difficult and rely heavily on human intervention. Neither existing process discovery algorithms nor event data preprocessing techniques can solve such redundancy efficiently. In this paper, we propose a multi-view approach to automatically detect redundant activity labels using not only context-aware features such as control--flow relations and attribute values but also semantic features from the event logs. Our evaluation of several publicly available datasets and a real-life case study demonstrate that our approach can efficiently detect redundant activity labels even with low-occurrence frequencies. The proposed approach can add value to the preprocessing step to generate more representative event logs.
http://w3id.org/mlsea/pwc/scientificWork/A%20Multi-task%20Deep%20Feature%20Selection%20Method%20for%20Brain%20Imaging%20Genetics                                                                                  A Multi-task Deep Feature Selection Method for Brain Imaging Genetics                                                                                  Using brain imaging quantitative traits (QTs) to identify the genetic risk factors is an important research topic in imaging genetics. Many efforts have been made via building linear models, e.g. linear regression (LR), to extract the association between imaging QTs and genetic factors such as single nucleotide polymorphisms (SNPs). However, to the best of our knowledge, these linear models could not fully uncover the complicated relationship due to the loci's elusive and diverse impacts on imaging QTs. Though deep learning models can extract the nonlinear relationship, they could not select relevant genetic factors. In this paper, we proposed a novel multi-task deep feature selection (MTDFS) method for brain imaging genetics. MTDFS first adds a multi-task one-to-one layer and imposes a hybrid sparsity-inducing penalty to select relevant SNPs making significant contributions to abnormal imaging QTs. It then builds a multi-task deep neural network to model the complicated associations between imaging QTs and SNPs. MTDFS can not only extract the nonlinear relationship but also arms the deep neural network with the feature selection capability. We compared MTDFS to both LR and single-task DFS (DFS) methods on the real neuroimaging genetic data. The experimental results showed that MTDFS performed better than both LR and DFS in terms of the QT-SNP relationship identification and feature selection. In a word, MTDFS is powerful for identifying risk loci and could be a great supplement to the method library for brain imaging genetics.
http://w3id.org/mlsea/pwc/scientificWork/A%20Multi-task%20convolutional%20neural%20network%20for%20blind%20stereoscopic%20image%20quality%20assessment%20using%20naturalness%20analysis                                                                                  A Multi-task convolutional neural network for blind stereoscopic image quality assessment using naturalness analysis                                                                                  This paper addresses the problem of blind stereoscopic image quality assessment (NR-SIQA) using a new multi-task deep learning based-method. In the field of stereoscopic vision, the information is fairly distributed between the left and right views as well as the binocular phenomenon. In this work, we propose to integrate these characteristics to estimate the quality of stereoscopic images without reference through a convolutional neural network. Our method is based on two main tasks: the first task predicts naturalness analysis based features adapted to stereo images, while the second task predicts the quality of such images. The former, so-called auxiliary task, aims to find more robust and relevant features to improve the quality prediction. To do this, we compute naturalness-based features using a Natural Scene Statistics (NSS) model in the complex wavelet domain. It allows to capture the statistical dependency between pairs of the stereoscopic images. Experiments are conducted on the well known LIVE PHASE I and LIVE PHASE II databases. The results obtained show the relevance of our method when comparing with those of the state-of-the-art. Our code is available online on https://github.com/Bourbia-Salima/multitask-cnn-nrsiqa_2021.
http://w3id.org/mlsea/pwc/scientificWork/A%20Multilingual%20Modeling%20Method%20for%20Span-Extraction%20Reading%20Comprehension                                                                                  A Multilingual Modeling Method for Span-Extraction Reading Comprehension                                                                                  Span-extraction reading comprehension models have made tremendous advances enabled by the availability of large-scale, high-quality training datasets. Despite such rapid progress and widespread application, extractive reading comprehension datasets in languages other than English remain scarce, and creating such a sufficient amount of training data for each language is costly and even impossible. An alternative to creating large-scale high-quality monolingual span-extraction training datasets is to develop multilingual modeling approaches and systems which can transfer to the target language without requiring training data in that language. In this paper, in order to solve the scarce availability of extractive reading comprehension training data in the target language, we propose a multilingual extractive reading comprehension approach called XLRC by simultaneously modeling the existing extractive reading comprehension training data in a multilingual environment using self-adaptive attention and multilingual attention. Specifically, we firstly construct multilingual parallel corpora by translating the existing extractive reading comprehension datasets (i.e., CMRC 2018) from the target language (i.e., Chinese) into different language families (i.e., English). Secondly, to enhance the final target representation, we adopt self-adaptive attention (SAA) to combine self-attention and inter-attention to extract the semantic relations from each pair of the target and source languages. Furthermore, we propose multilingual attention (MLA) to learn the rich knowledge from various language families. Experimental results show that our model outperforms the state-of-the-art baseline (i.e., RoBERTa_Large) on the CMRC 2018 task, which demonstrate the effectiveness of our proposed multi-lingual modeling approach and show the potentials in multilingual NLP tasks.
http://w3id.org/mlsea/pwc/scientificWork/A%20Multiplexed%20Network%20for%20End-to-End%2C%20Multilingual%20OCR                                                                                  A Multiplexed Network for End-to-End, Multilingual OCR                                                                                  Recent advances in OCR have shown that an end-to-end (E2E) training pipeline that includes both detection and recognition leads to the best results. However, many existing methods focus primarily on Latin-alphabet languages, often even only case-insensitive English characters. In this paper, we propose an E2E approach, Multiplexed Multilingual Mask TextSpotter, that performs script identification at the word level and handles different scripts with different recognition heads, all while maintaining a unified loss that simultaneously optimizes script identification and multiple recognition heads. Experiments show that our method outperforms the single-head model with similar number of parameters in end-to-end recognition tasks, and achieves state-of-the-art results on MLT17 and MLT19 joint text detection and script identification benchmarks. We believe that our work is a step towards the end-to-end trainable and scalable multilingual multi-purpose OCR system. Our code and model will be released.
http://w3id.org/mlsea/pwc/scientificWork/A%20Multivariate%20Density%20Forecast%20Approach%20for%20Online%20Power%20System%20Security%20Assessment                                                                                  A Multivariate Density Forecast Approach for Online Power System Security Assessment                                                                                  A multivariate density forecast model based on deep learning is designed in this paper to forecast the joint cumulative distribution functions (JCDFs) of multiple security margins in power systems. Differing from existing multivariate density forecast models, the proposed method requires no a priori hypotheses on the distribution of forecasting targets. In addition, based on the universal approximation capability of neural networks, the value domain of the proposed approach has been proven to include all continuous JCDFs. The forecasted JCDF is further employed to calculate the deterministic security assessment index evaluating the security level of future power system operations. Numerical tests verify the superiority of the proposed method over current multivariate density forecast models. The deterministic security assessment index is demonstrated to be more informative for operators than security margins as well.
http://w3id.org/mlsea/pwc/scientificWork/A%20Mutual%20Information%20Maximization%20Approach%20for%20the%20Spurious%20Solution%20Problem%20in%20Weakly%20Supervised%20Question%20Answering                                                                                  A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering                                                                                  Weakly supervised question answering usually has only the final answers as supervision signals while the correct solutions to derive the answers are not provided. This setting gives rise to the spurious solution problem: there may exist many spurious solutions that coincidentally derive the correct answer, but training on such solutions can hurt model performance (e.g., producing wrong solutions or answers). For example, for discrete reasoning tasks as on DROP, there may exist many equations to derive a numeric answer, and typically only one of them is correct. Previous learning methods mostly filter out spurious solutions with heuristics or using model confidence, but do not explicitly exploit the semantic correlations between a question and its solution. In this paper, to alleviate the spurious solution problem, we propose to explicitly exploit such semantic correlations by maximizing the mutual information between question-answer pairs and predicted solutions. Extensive experiments on four question answering datasets show that our method significantly outperforms previous learning methods in terms of task performance and is more effective in training models to produce correct solutions.
http://w3id.org/mlsea/pwc/scientificWork/A%20N-Path%20Receiver%20With%20Harmonic%20Response%20Suppression                                                                                  A N-Path Receiver With Harmonic Response Suppression                                                                                  A downconversion receiver employing a switch-based N-path filter with reduced harmonic response around the third- and fifth- LO harmonics is presented. The N-path filter is placed in a frequency-translation feedback loop that is effective at the 3rd and the 5th LO harmonics to mitigate harmonic downconversion. A pulse-width-modulated LO (PWM-LO) clocking scheme is used in the feedback upconverter to reduce the noise injected around the LO harmonic at the input of N-path downconverter. The compression resulting from blockers around the 3rd and the 5th LO harmonics is also suppressed as a result of reduced harmonic response. Compensation of peak frequency shift of the N-path response due to parasitic input capacitance is also described.
http://w3id.org/mlsea/pwc/scientificWork/A%20Natural%20Adaptive%20Process%20for%20Collective%20Decision-Making                                                                                  A Natural Adaptive Process for Collective Decision-Making                                                                                  Consider an urn filled with balls, each labeled with one of several possible collective decisions. Now, let a random voter draw two balls from the urn and pick her more preferred as the collective decision. Relabel the losing ball with the collective decision, put both balls back into the urn, and repeat. Once in a while, relabel a randomly drawn ball with a random collective decision. We prove that the empirical distribution of collective decisions produced by this process approximates a maximal lottery, a celebrated probabilistic voting rule proposed by Peter C. Fishburn (Rev. Econ. Stud., 51(4), 1984). In fact, the probability that the collective decision in round $n$ is made according to a maximal lottery increases exponentially in $n$. The proposed procedure is more flexible than traditional voting rules and bears strong similarities to natural processes studied in biology, physics, and chemistry as well as algorithms proposed in machine learning.
http://w3id.org/mlsea/pwc/scientificWork/A%20Natural%20Disasters%20Index                                                                                  A Natural Disasters Index                                                                                  Natural disasters, such as tornadoes, floods, and wildfire pose risks to life and property, requiring the intervention of insurance corporations. One of the most visible consequences of changing climate is an increase in the intensity and frequency of extreme weather events. The relative strengths of these disasters are far beyond the habitual seasonal maxima, often resulting in subsequent increases in property losses. Thus, insurance policies should be modified to endure increasingly volatile catastrophic weather events. We propose a Natural Disasters Index (NDI) for the property losses caused by natural disasters in the United States based on the 'Storm Data' published by the National Oceanic and Atmospheric Administration. The proposed NDI is an attempt to construct a financial instrument for hedging the intrinsic risk. The NDI is intended to forecast the degree of future risk that could forewarn the insurers and corporations allowing them to transfer insurance risk to capital market investors. This index could also be modified to other regions and countries.
http://w3id.org/mlsea/pwc/scientificWork/A%20Near-Optimal%20Algorithm%20for%20Debiasing%20Trained%20Machine%20Learning%20Models                                                                                  A Near-Optimal Algorithm for Debiasing Trained Machine Learning Models                                                                                  We present a scalable post-processing algorithm for debiasing trained models, including deep neural networks (DNNs), which we prove to be near-optimal by bounding its excess Bayes risk. We empirically validate its advantages on standard benchmark datasets across both classical algorithms as well as modern DNN architectures and demonstrate that it outperforms previous post-processing methods while performing on par with in-processing. In addition, we show that the proposed algorithm is particularly effective for models trained at scale where post-processing is a natural and practical choice.
http://w3id.org/mlsea/pwc/scientificWork/A%20Negation%20Quantum%20Decision%20Model%20to%20Predict%20the%20Interference%20Effect%20in%20Categorization                                                                                  A Negation Quantum Decision Model to Predict the Interference Effect in Categorization                                                                                  Categorization is a significant task in decision-making, which is a key part of human behavior. An interference effect is caused by categorization in some cases, which breaks the total probability principle. A negation quantum model (NQ model) is developed in this article to predict the interference. Taking the advantage of negation to bring more information in the distribution from a different perspective, the proposed model is a combination of the negation of a probability distribution and the quantum decision model. Information of the phase contained in quantum probability and the special calculation method to it can easily represented the interference effect. The results of the proposed NQ model is closely to the real experiment data and has less error than the existed models.
http://w3id.org/mlsea/pwc/scientificWork/A%20Neighbourhood%20Framework%20for%20Resource-Lean%20Content%20Flagging                                                                                  A Neighbourhood Framework for Resource-Lean Content Flagging                                                                                  We propose a novel framework for cross-lingual content flagging with limited target-language data, which significantly outperforms prior work in terms of predictive performance. The framework is based on a nearest-neighbour architecture. It is a modern instantiation of the vanilla k-nearest neighbour model, as we use Transformer representations in all its components. Our framework can adapt to new source-language instances, without the need to be retrained from scratch. Unlike prior work on neighbourhood-based approaches, we encode the neighbourhood information based on query--neighbour interactions. We propose two encoding schemes and we show their effectiveness using both qualitative and quantitative analysis. Our evaluation results on eight languages from two different datasets for abusive language detection show sizable improvements of up to 9.5 F1 points absolute (for Italian) over strong baselines. On average, we achieve 3.6 absolute F1 points of improvement for the three languages in the Jigsaw Multilingual dataset and 2.14 points for the WUL dataset.
http://w3id.org/mlsea/pwc/scientificWork/A%20Neural%20Acoustic%20Echo%20Canceller%20Optimized%20Using%20An%20Automatic%20Speech%20Recognizer%20And%20Large%20Scale%20Synthetic%20Data                                                                                  A Neural Acoustic Echo Canceller Optimized Using An Automatic Speech Recognizer And Large Scale Synthetic Data                                                                                  We consider the problem of recognizing speech utterances spoken to a device which is generating a known sound waveform; for example, recognizing queries issued to a digital assistant which is generating responses to previous user inputs. Previous work has proposed building acoustic echo cancellation (AEC) models for this task that optimize speech enhancement metrics using both neural network as well as signal processing approaches. Since our goal is to recognize the input speech, we consider enhancements which improve word error rates (WERs) when the predicted speech signal is passed to an automatic speech recognition (ASR) model. First, we augment the loss function with a term that produces outputs useful to a pre-trained ASR model and show that this augmented loss function improves WER metrics. Second, we demonstrate that augmenting our training dataset of real world examples with a large synthetic dataset improves performance. Crucially, applying SpecAugment style masks to the reference channel during training aids the model in adapting from synthetic to real domains. In experimental evaluations, we find the proposed approaches improve performance, on average, by 57% over a signal processing baseline and 45% over the neural AEC model without the proposed changes.
http://w3id.org/mlsea/pwc/scientificWork/A%20Neural%20Edge-Editing%20Approach%20for%20Document-Level%20Relation%20Graph%20Extraction                                                                                  A Neural Edge-Editing Approach for Document-Level Relation Graph Extraction                                                                                  In this paper, we propose a novel edge-editing approach to extract relation information from a document. We treat the relations in a document as a relation graph among entities in this approach. The relation graph is iteratively constructed by editing edges of an initial graph, which might be a graph extracted by another system or an empty graph. The way to edit edges is to classify them in a close-first manner using the document and temporally-constructed graph information; each edge is represented with a document context information by a pretrained transformer model and a graph context information by a graph convolutional neural network model. We evaluate our approach on the task to extract material synthesis procedures from materials science texts. The experimental results show the effectiveness of our approach in editing the graphs initialized by our in-house rule-based system and empty graphs.
http://w3id.org/mlsea/pwc/scientificWork/A%20Neural%20Network%20for%20Semigroups                                                                                  A Neural Network for Semigroups                                                                                  Tasks like image reconstruction in computer vision, matrix completion in recommender systems and link prediction in graph theory, are well studied in machine learning literature. In this work, we apply a denoising autoencoder-based neural network architecture to the task of completing partial multiplication (Cayley) tables of finite semigroups. We suggest a novel loss function for that task based on the algebraic nature of the semigroup data. We also provide a software package for conducting experiments similar to those carried out in this work. Our experiments showed that with only about 10% of the available data, it is possible to build a model capable of reconstructing a full Cayley from only half of it in about 80% of cases.
http://w3id.org/mlsea/pwc/scientificWork/A%20Neural%20Pre-Conditioning%20Active%20Learning%20Algorithm%20to%20Reduce%20Label%20Complexity                                                                                  A Neural Pre-Conditioning Active Learning Algorithm to Reduce Label Complexity                                                                                  Deep learning (DL) algorithms rely on massive amounts of labeled data. Semi-supervised learning (SSL) and active learning (AL) aim to reduce this label complexity by leveraging unlabeled data or carefully acquiring labels, respectively. In this work, we primarily focus on designing an AL algorithm but first argue for a change in how AL algorithms should be evaluated. Although unlabeled data is readily available in pool-based AL, AL algorithms are usually evaluated by measuring the increase in supervised learning (SL) performance at consecutive acquisition steps. Because this measures performance gains from both newly acquired instances and newly acquired labels, we propose to instead evaluate the label efficiency of AL algorithms by measuring the increase in SSL performance at consecutive acquisition steps. After surveying tools that can be used to this end, we propose our neural pre-conditioning (NPC) algorithm inspired by a Neural Tangent Kernel (NTK) analysis. Our algorithm incorporates the classifier's uncertainty on unlabeled data and penalizes redundant samples within candidate batches to efficiently acquire a diverse set of informative labels. Furthermore, we prove that NPC improves downstream training in the large-width regime in a manner previously observed to correlate with generalization. Comparisons with other AL algorithms show that a state-of-the-art SSL algorithm coupled with NPC can achieve high performance using very few labeled data.
http://w3id.org/mlsea/pwc/scientificWork/A%20Neural%20Tangent%20Kernel%20Perspective%20of%20GANs                                                                                  A Neural Tangent Kernel Perspective of GANs                                                                                  We propose a novel theoretical framework of analysis for Generative Adversarial Networks (GANs). We reveal a fundamental flaw of previous analyses which, by incorrectly modeling GANs' training scheme, are subject to ill-defined discriminator gradients. We overcome this issue which impedes a principled study of GAN training, solving it within our framework by taking into account the discriminator's architecture. To this end, we leverage the theory of infinite-width neural networks for the discriminator via its Neural Tangent Kernel. We characterize the trained discriminator for a wide range of losses and establish general differentiability properties of the network. From this, we derive new insights about the convergence of the generated distribution, advancing our understanding of GANs' training dynamics. We empirically corroborate these results via an analysis toolkit based on our framework, unveiling intuitions that are consistent with GAN practice.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Approach%20To%20Text%20Rating%20Classification%20Using%20Sentiment%20Analysis                                                                                  A New Approach To Text Rating Classification Using Sentiment Analysis                                                                                  Typical use cases of sentiment analysis usually revolve around assessing the probability of a text belonging to a certain sentiment and deriving insight concerning it; little work has been done to explore further use cases derived using those probabilities in the context of rating. In this paper, we redefine the sentiment proportion values as building blocks for a triangle structure, allowing us to derive variables for a new formula for classifying text given in the form of product reviews into a group of higher and a group of lower ratings and prove a dependence exists between the sentiments and the ratings.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Approach%20of%20Data%20Pre-processing%20for%20Data%20Compression%20in%20Smart%20Grids                                                                                  A New Approach of Data Pre-processing for Data Compression in Smart Grids                                                                                  The conventional approach to pre-process data for compression is to apply transforms such as the Fourier, the Karhunen-Lo `{e}ve, or wavelet transforms. One drawback from adopting such an approach is that it is independent of the use of the compressed data, which may induce significant optimality losses when measured in terms of final utility (instead of being measured in terms of distortion). We therefore revisit this paradigm by tayloring the data pre-processing operation to the utility function of the decision-making entity using the compressed (and therefore noisy) data. More specifically, the utility function consists of an Lp-norm, which is very relevant in the area of smart grids. Both a linear and a non-linear use-oriented transforms are designed and compared with conventional data pre-processing techniques, showing that the impact of compression noise can be significantly reduced.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Approach%20to%20Estimating%20Loss-Given-Default%20Distribution                                                                                  A New Approach to Estimating Loss-Given-Default Distribution                                                                                  Based on an idea of using a last passage time, we propose a new method for estimating loss-given-default distribution implied in current credit market and company-specific financial conditions. Since the market standard is the predetermined constant loss rate (60%) for all firms, it is hard to estimate loss-given-default distribution by just observing the market information such as default time distribution, default probability, and CDS spreads. To overcome this difficulty, we construct a hybrid model with the last passage time of the leverage ratio (defined as the ratio of a firm's assets over its debt) to a certain level. The last passage time, which is not a stopping time, allows us to appropriately model the timing of severe firm-value deterioration which is not apparent to credit market participants. Under minimal and standard assumptions, our model captures asset value dynamics when close to default, while treating default as an unexpected event. We explicitly obtain the distribution of the leverage ratio at default time. As we base the model solely on the leverage ratio and do not introduce additional risk factors, both the model and the procedure for calibrating model parameters to the credit market are simple. We illustrate this procedure in detail by calculating the loss-given-default distribution implied in the credit market.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Approach%20to%20Overgenerating%20and%20Scoring%20Abstractive%20Summaries                                                                                  A New Approach to Overgenerating and Scoring Abstractive Summaries                                                                                  We propose a new approach to generate multiple variants of the target summary with diverse content and varying lengths, then score and select admissible ones according to users' needs. Abstractive summarizers trained on single reference summaries may struggle to produce outputs that achieve multiple desirable properties, i.e., capturing the most important information, being faithful to the original, grammatical and fluent. In this paper, we propose a two-staged strategy to generate a diverse set of candidate summaries from the source text in stage one, then score and select admissible ones in stage two. Importantly, our generator gives a precise control over the length of the summary, which is especially well-suited when space is limited. Our selectors are designed to predict the optimal summary length and put special emphasis on faithfulness to the original text. Both stages can be effectively trained, optimized and evaluated. Our experiments on benchmark summarization datasets suggest that this paradigm can achieve state-of-the-art performance.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Channel%20Estimation%20Strategy%20in%20Intelligent%20Reflecting%20Surface%20Assisted%20Networks                                                                                  A New Channel Estimation Strategy in Intelligent Reflecting Surface Assisted Networks                                                                                  Channel estimation is the main hurdle to reaping the benefits promised by the intelligent reflecting surface (IRS), due to its absence of ability to transmit/receive pilot signals as well as the huge number of channel coefficients associated with its reflecting elements. Recently, a breakthrough was made in reducing the channel estimation overhead by revealing that the IRS-BS (base station) channels are common in the cascaded user-IRS-BS channels of all the users, and if the cascaded channel of one typical user is estimated, the other users' cascaded channels can be estimated very quickly based on their correlation with the typical user's channel cite{b5}. One limitation of this strategy, however, is the waste of user energy, because many users need to keep silent when the typical user's channel is estimated. In this paper, we reveal another correlation hidden in the cascaded user-IRS-BS channels by observing that the user-IRS channel is common in all the cascaded channels from users to each BS antenna as well. Building upon this finding, we propose a novel two-phase channel estimation protocol in the uplink communication. Specifically, in Phase I, the correlation coefficients between the channels of a typical BS antenna and those of the other antennas are estimated; while in Phase II, the cascaded channel of the typical antenna is estimated. In particular, all the users can transmit throughput Phase I and Phase II. Under this strategy, it is theoretically shown that the minimum number of time instants required for perfect channel estimation is the same as that of the aforementioned strategy in the ideal case without BS noise. Then, in the case with BS noise, we show by simulation that the channel estimation error of our proposed scheme is significantly reduced thanks to the full exploitation of the user energy.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Class%20of%20Efficient%20Adaptive%20Filters%20for%20Online%20Nonlinear%20Modeling                                                                                  A New Class of Efficient Adaptive Filters for Online Nonlinear Modeling                                                                                  Nonlinear models are known to provide excellent performance in real-world applications that often operate in non-ideal conditions. However, such applications often require online processing to be performed with limited computational resources. To address this problem, we propose a new class of efficient nonlinear models for online applications. The proposed algorithms are based on linear-in-the-parameters (LIP) nonlinear filters using functional link expansions. In order to make this class of functional link adaptive filters (FLAFs) efficient, we propose low-complexity expansions and frequency-domain adaptation of the parameters. Among this family of algorithms, we also define the partitioned-block frequency-domain FLAF, whose implementation is particularly suitable for online nonlinear modeling problems. We assess and compare frequency-domain FLAFs with different expansions providing the best possible tradeoff between performance and computational complexity. Experimental results prove that the proposed algorithms can be considered as an efficient and effective solution for online applications, such as the acoustic echo cancellation, even in the presence of adverse nonlinear conditions and with limited availability of computational resources.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Dimension%20in%20Testimony%3A%20Relighting%20Video%20with%20Reflectance%20Field%20Exemplars                                                                                  A New Dimension in Testimony: Relighting Video with Reflectance Field Exemplars                                                                                  We present a learning-based method for estimating 4D reflectance field of a person given video footage illuminated under a flat-lit environment of the same subject. For training data, we use one light at a time to illuminate the subject and capture the reflectance field data in a variety of poses and viewpoints. We estimate the lighting environment of the input video footage and use the subject's reflectance field to create synthetic images of the subject illuminated by the input lighting environment. We then train a deep convolutional neural network to regress the reflectance field from the synthetic images. We also use a differentiable renderer to provide feedback for the network by matching the relit images with the input video frames. This semi-supervised training scheme allows the neural network to handle unseen poses in the dataset as well as compensate for the lighting estimation error. We evaluate our method on the video footage of the real Holocaust survivors and show that our method outperforms the state-of-the-art methods in both realism and speed.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Formalism%2C%20Method%20and%20Open%20Issues%20for%20Zero-Shot%20Coordination                                                                                  A New Formalism, Method and Open Issues for Zero-Shot Coordination                                                                                  In many coordination problems, independently reasoning humans are able to discover mutually compatible policies. In contrast, independently trained self-play policies are often mutually incompatible. Zero-shot coordination (ZSC) has recently been proposed as a new frontier in multi-agent reinforcement learning to address this fundamental issue. Prior work approaches the ZSC problem by assuming players can agree on a shared learning algorithm but not on labels for actions and observations, and proposes other-play as an optimal solution. However, until now, this 'label-free' problem has only been informally defined. We formalize this setting as the label-free coordination (LFC) problem by defining the label-free coordination game. We show that other-play is not an optimal solution to the LFC problem as it fails to consistently break ties between incompatible maximizers of the other-play objective. We introduce an extension of the algorithm, other-play with tie-breaking, and prove that it is optimal in the LFC problem and an equilibrium in the LFC game. Since arbitrary tie-breaking is precisely what the ZSC setting aims to prevent, we conclude that the LFC problem does not reflect the aims of ZSC. To address this, we introduce an alternative informal operationalization of ZSC as a starting point for future work.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Look%20to%20Three-Factor%20Fama-French%20Regression%20Model%20using%20Sample%20Innovations                                                                                  A New Look to Three-Factor Fama-French Regression Model using Sample Innovations                                                                                  The Fama-French model is widely used in assessing the portfolio's performance compared to market returns. In Fama-French models, all factors are time-series data. The cross-sectional data are slightly different from the time series data. A distinct problem with time-series regressions is that R-squared in time series regressions is usually very high, especially compared with typical R-squared for cross-sectional data. The high value of R-squared may cause misinterpretation that the regression model fits the observed data well, and the variance in the dependent variable is explained well by the independent variables. Thus, to do regression analysis, and overcome with the serial dependence and volatility clustering, we use standard econometrics time series models to derive sample innovations. In this study, we revisit and validate the Fama-French models in two different ways: using the factors and asset returns in the Fama-French model and considering the sample innovations in the Fama-French model instead of studying the factors. Comparing the two methods considered in this study, we suggest the Fama-French model should be considered with heavy tail distributions as the tail behavior is relevant in Fama-French models, including financial data, and the QQ plot does not validate that the choice of the normal distribution as the theoretical distribution for the noise in the model.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Notion%20of%20Individually%20Fair%20Clustering%3A%20%24%CE%B1%24-Equitable%20%24k%24-Center                                                                                  A New Notion of Individually Fair Clustering: $α$-Equitable $k$-Center                                                                                  Clustering is a fundamental problem in unsupervised machine learning, and fair variants of it have recently received significant attention due to its societal implications. In this work we introduce a novel definition of individual fairness for clustering problems. Specifically, in our model, each point $j$ has a set of other points $ mathcal{S}_j$ that it perceives as similar to itself, and it feels that it is fairly treated if the quality of service it receives in the solution is $ alpha$-close (in a multiplicative sense, for a given $ alpha geq 1$) to that of the points in $ mathcal{S}_j$. We begin our study by answering questions regarding the structure of the problem, namely for what values of $ alpha$ the problem is well-defined, and what the behavior of the emph{Price of Fairness (PoF)} for it is. For the well-defined region of $ alpha$, we provide efficient and easily-implementable approximation algorithms for the $k$-center objective, which in certain cases enjoy bounded-PoF guarantees. We finally complement our analysis by an extensive suite of experiments that validates the effectiveness of our theoretical results.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Parallel%20Adaptive%20Clustering%20and%20its%20Application%20to%20Streaming%20Data                                                                                  A New Parallel Adaptive Clustering and its Application to Streaming Data                                                                                  This paper presents a parallel adaptive clustering (PAC) algorithm to automatically classify data while simultaneously choosing a suitable number of classes. Clustering is an important tool for data analysis and understanding in a broad set of areas including data reduction, pattern analysis, and classification. However, the requirement to specify the number of clusters in advance and the computational burden associated with clustering large sets of data persist as challenges in clustering. We propose a new parallel adaptive clustering (PAC) algorithm that addresses these challenges by adaptively computing the number of clusters and leveraging the power of parallel computing. The algorithm clusters disjoint subsets of the data on parallel computation threads. We develop regularized set mi{k}-means to efficiently cluster the results from the parallel threads. A refinement step further improves the clusters. The PAC algorithm offers the capability to adaptively cluster data sets which change over time by reusing the information from previous time steps to decrease computation. We provide theoretical analysis and numerical experiments to characterize the performance of the method, validate its properties, and demonstrate the computational efficiency of the method.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Parametrization%20of%20Correlation%20Matrices                                                                                  A New Parametrization of Correlation Matrices                                                                                  We introduce a novel parametrization of the correlation matrix. The reparametrization facilitates modeling of correlation and covariance matrices by an unrestricted vector, where positive definiteness is an innate property. This parametrization can be viewed as a generalization of Fisther's Z-transformation to higher dimensions and has a wide range of potential applications. An algorithm for reconstructing the unique n x n correlation matrix from any d-dimensional vector (with d = n(n-1)/2) is provided, and we derive its numerical complexity.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Pathway%20to%20Approximate%20Energy%20Expenditure%20and%20Recovery%20of%20an%20Athlete                                                                                  A New Pathway to Approximate Energy Expenditure and Recovery of an Athlete                                                                                  This work proposes to use evolutionary computation as a pathway to allow a new perspective on the modeling of energy expenditure and recovery of an individual athlete during exercise. We revisit a theoretical concept called the 'three component hydraulic model' which is designed to simulate metabolic systems during exercise and which is able to address recently highlighted shortcomings of currently applied performance models. This hydraulic model has not been entirely validated on individual athletes because it depends on physiological measures that cannot be acquired in the required precision or quantity. This paper introduces a generalized interpretation and formalization of the three component hydraulic model that removes its ties to concrete metabolic measures and allows to use evolutionary computation to fit its parameters to an athlete.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Pricing%20Theory%20That%20Solves%20the%20St.%20Petersburg%20Paradox                                                                                  A New Pricing Theory That Solves the St. Petersburg Paradox                                                                                  The St. Petersburg Paradox, an important topic in probability theory, has not been solved in the last 280 years. Since Nicolaus Bernoulli proposed the St. Petersburg Paradox in 1738, many people had tried to solve it and had proposed various explanations, but all were not satisfactory. In this paper we propose a new pricing theory with several rules, which incidentally resolves this paradox. The new pricing theory states that so-called fair (reasonable) pricing should be judged by the seller and the buyer independently. Reasonable pricing for the seller may not be appropriate for the buyer. The seller cares about costs, while the buyer is concerned about the realistic prospect of returns.The pricing theory we proposed can be applied to financial markets to solve the confusion that financial asset return with fat tails distribution will cause the option pricing formula to fail, thus making up the theoretical defects of quantitative financial pricing theory.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Score%20for%20Adaptive%20Tests%20in%20Bayesian%20and%20Credal%20Networks                                                                                  A New Score for Adaptive Tests in Bayesian and Credal Networks                                                                                  A test is adaptive when its sequence and number of questions is dynamically tuned on the basis of the estimated skills of the taker. Graphical models, such as Bayesian networks, are used for adaptive tests as they allow to model the uncertainty about the questions and the skills in an explainable fashion, especially when coping with multiple skills. A better elicitation of the uncertainty in the question/skills relations can be achieved by interval probabilities. This turns the model into a credal network, thus making more challenging the inferential complexity of the queries required to select questions. This is especially the case for the information theoretic quantities used as scores to drive the adaptive mechanism. We present an alternative family of scores, based on the mode of the posterior probabilities, and hence easier to explain. This makes considerably simpler the evaluation in the credal case, without significantly affecting the quality of the adaptive process. Numerical tests on synthetic and real-world data are used to support this claim.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Set%20of%20Financial%20Instruments                                                                                  A New Set of Financial Instruments                                                                                  In complete markets, there are risky assets and a riskless asset. It is assumed that the riskless asset and the risky asset are traded continuously in time and that the market is frictionless. In this paper, we propose a new method for hedging derivatives assuming that a hedger should not always rely on trading existing assets that are used to form a linear portfolio comprised of the risky asset, the riskless asset, and standard derivatives, but rather should design a set of specific, most-suited financial instruments for the hedging problem. We introduce a sequence of new financial instruments best suited for hedging jump-diffusion and stochastic volatility market models. The new instruments we introduce are perpetual derivatives. More specifically, they are options with perpetual maturities. In a financial market where perpetual derivatives are introduced, there is a new set of partial and partial-integro differential equations for pricing derivatives. Our analysis demonstrates that the set of new financial instruments together with a risk measure called the tail-loss ratio measure defined by the new instrument's return series can be potentially used as an early warning system for a market crash.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Signal%20Representation%20Using%20Complex%20Conjugate%20Pair%20Sums                                                                                  A New Signal Representation Using Complex Conjugate Pair Sums                                                                                  This letter introduces a real valued summation known as Complex Conjugate Pair Sum (CCPS). The space spanned by CCPS and its one circular downshift is called { em Complex Conjugate Subspace (CCS)}. For a given positive integer $N geq3$, there exists $ frac{ varphi(N)}{2}$ CCPSs forming $ frac{ varphi(N)}{2}$ CCSs, where $ varphi(N)$ is the Euler's totient function. We prove that these CCSs are mutually orthogonal and their direct sum form a $ varphi(N)$ dimensional subspace $s_N$ of $ mathbb{C}^N$. We propose that any signal of finite length $N$ is represented as a linear combination of elements from a special basis of $s_d$, for each divisor $d$ of $N$. This defines a new transform named as Complex Conjugate Periodic Transform (CCPT). Later, we compared CCPT with DFT (Discrete Fourier Transform) and RPT (Ramanujan Periodic Transform). It is shown that, using CCPT we can estimate the period, hidden periods and frequency information of a signal. Whereas, RPT does not provide the frequency information. For a complex valued input signal, CCPT offers computational benefit over DFT. A CCPT dictionary based method is proposed to extract non-divisor period information.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20Surprise%20Measure%20for%20Extracting%20Interesting%20Relationships%20between%20Persons                                                                                  A New Surprise Measure for Extracting Interesting Relationships between Persons                                                                                  One way to enhance user engagement in search engines is to suggest interesting facts to the user. Although relationships between persons are important as a target for text mining, there are few effective approaches for extracting the interesting relationships between persons. We therefore propose a method for extracting interesting relationships between persons from natural language texts by focusing on their surprisingness. Our method first extracts all personal relationships from dependency trees for the texts and then calculates surprise scores for distributed representations of the extracted relationships in an unsupervised manner. The unique point of our method is that it does not require any labeled dataset with annotation for the surprising personal relationships. The results of the human evaluation show that the proposed method could extract more interesting relationships between persons from Japanese Wikipedia articles than a popularity-based baseline method. We demonstrate our proposed method as a chrome plugin on google search.
http://w3id.org/mlsea/pwc/scientificWork/A%20New%20View%20of%20Multi-modal%20Language%20Analysis%3A%20Audio%20and%20Video%20Features%20as%20Text%20%60%60Styles%27%27                                                                                  A New View of Multi-modal Language Analysis: Audio and Video Features as Text ``Styles''                                                                                  Imposing the style of one image onto another is called style transfer. For example, the style of a Van Gogh painting might be imposed on a photograph to yield an interesting hybrid. This paper applies the adaptive normalization used for image style transfer to language semantics, i.e., the style is the way the words are said (tone of voice and facial expressions) and these are style-transferred onto the text. The goal is to learn richer representations for multi-modal utterances using style-transferred multi-modal features. The proposed Style-Transfer Transformer (STT) grafts a stepped styled adaptive layer-normalization onto a transformer network, the output from which is used in sentiment analysis and emotion recognition problems. In addition to achieving performance on par with the state-of-the art (but using less than a third of the model parameters), we examine the relative contributions of each mode when used in the downstream applications.
http://w3id.org/mlsea/pwc/scientificWork/A%20News%20Recommender%20System%20Considering%20Temporal%20Dynamics%20and%20Diversity                                                                                  A News Recommender System Considering Temporal Dynamics and Diversity                                                                                  In a news recommender system, a reader's preferences change over time. Some preferences drift quite abruptly (short-term preferences), while others change over a longer period of time (long-term preferences). Although the existing news recommender systems consider the reader's full history, they often ignore the dynamics in the reader's behavior. Thus, they cannot meet the demand of the news readers for their time-varying preferences. In addition, the state-of-the-art news recommendation models are often focused on providing accurate predictions, which can work well in traditional recommendation scenarios. However, in a news recommender system, diversity is essential, not only to keep news readers engaged, but also to play a key role in a democratic society. In this PhD dissertation, our goal is to build a news recommender system to address these two challenges. Our system should be able to: (i) accommodate the dynamics in reader behavior; and (ii) consider both accuracy and diversity in the design of the recommendation model. Our news recommender system can also work for unprofiled, anonymous and short-term readers, by leveraging the rich side information of the news items and by including the implicit feedback in our model. We evaluate our model with multiple evaluation measures (both accuracy and diversity-oriented metrics) to demonstrate the effectiveness of our methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20News-based%20Machine%20Learning%20Model%20for%20Adaptive%20Asset%20Pricing                                                                                  A News-based Machine Learning Model for Adaptive Asset Pricing                                                                                  The paper proposes a new asset pricing model -- the News Embedding UMAP Selection (NEUS) model, to explain and predict the stock returns based on the financial news. Using a combination of various machine learning algorithms, we first derive a company embedding vector for each basis asset from the financial news. Then we obtain a collection of the basis assets based on their company embedding. After that for each stock, we select the basis assets to explain and predict the stock return with high-dimensional statistical methods. The new model is shown to have a significantly better fitting and prediction power than the Fama-French 5-factor model.
http://w3id.org/mlsea/pwc/scientificWork/A%20Noise-Aware%20Memory-Attention%20Network%20Architecture%20for%20Regression-Based%20Speech%20Enhancement                                                                                  A Noise-Aware Memory-Attention Network Architecture for Regression-Based Speech Enhancement                                                                                  We propose a novel noise-aware memory-attention network (NAMAN) for regression-based speech enhancement, aiming at improving quality of enhanced speech in unseen noise conditions. The NAMAN architecture consists of three parts, a main regression network, a memory block and an attention block. First, a long short-term memory recurrent neural network (LSTM-RNN) is adopted as the main network to well model the acoustic context of neighboring frames. Next, the memory block is built with an extensive set of noise feature vectors as the prior noise bases. Finally, the attention block serves as an auxiliary network to improve the noise awareness of the main network by encoding the dynamic noise information at frame level through additional features obtained by weighing the existing noise basis vectors in the memory block. Our experiments show that the proposed NAMAN framework is compact and outperforms the state-of-the-art dynamic noise-aware training approaches in low SNR conditions.
http://w3id.org/mlsea/pwc/scientificWork/A%20Non-Conservative%20Stability%20Criterion%20for%20Networked%20Control%20Systems%20with%20time-varying%20Packet%20Delays                                                                                  A Non-Conservative Stability Criterion for Networked Control Systems with time-varying Packet Delays                                                                                  A networked output feedback loop subject to packetized transmissions of the output signal is considered. Based on the small gain theorem, an easy-to-use stability criterion covering two important cases is presented. In the first case a packet numbering mechanism is employed whereas in the second case neither packet numbering nor synchronization between sender and receiver is assumed. The analysis makes use of acausal subsystems and deduces the optimal constant time delay that should be used in a nominal controller design such that additional packet delay variations introduced by the network are maximized. A simulation example of a networked control system with a filtered Smith predictor illustrates the application of the proposed criterion and compares the results to different approaches from literature.
http://w3id.org/mlsea/pwc/scientificWork/A%20Non-Linear%20Structural%20Probe                                                                                  A Non-Linear Structural Probe                                                                                  Probes are models devised to investigate the encoding of knowledge -- e.g. syntactic structure -- in contextual representations. Probes are often designed for simplicity, which has led to restrictions on probe design that may not allow for the full exploitation of the structure of encoded information; one such restriction is linearity. We examine the case of a structural probe (Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic structure in contextual representations through learning only linear transformations. By observing that the structural probe learns a metric, we are able to kernelize it and develop a novel non-linear variant with an identical number of parameters. We test on 6 languages and find that the radial-basis function (RBF) kernel, in conjunction with regularization, achieves a statistically significant improvement over the baseline in all languages -- implying that at least part of the syntactic knowledge is encoded non-linearly. We conclude by discussing how the RBF kernel resembles BERT's self-attention layers and speculate that this resemblance leads to the RBF-based probe's stronger performance.
http://w3id.org/mlsea/pwc/scientificWork/A%20Non-Negative%20Matrix%20Factorization%20Game                                                                                  A Non-Negative Matrix Factorization Game                                                                                  We present a novel game-theoretic formulation of Non-Negative Matrix Factorization (NNMF), a popular data-analysis method with many scientific and engineering applications. The game-theoretic formulation is shown to have favorable scaling and parallelization properties, while retaining reconstruction and convergence performance comparable to the traditional Multiplicative Updates algorithm.
http://w3id.org/mlsea/pwc/scientificWork/A%20Non-asymptotic%20Approach%20to%20Best-Arm%20Identification%20for%20Gaussian%20Bandits                                                                                  A Non-asymptotic Approach to Best-Arm Identification for Gaussian Bandits                                                                                  We propose a new strategy for best-arm identification with fixed confidence of Gaussian variables with bounded means and unit variance. This strategy, called Exploration-Biased Sampling, is not only asymptotically optimal: it is to the best of our knowledge the first strategy with non-asymptotic bounds that asymptotically matches the sample complexity.But the main advantage over other algorithms like Track-and-Stop is an improved behavior regarding exploration: Exploration-Biased Sampling is biased towards exploration in a subtle but natural way that makes it more stable and interpretable. These improvements are allowed by a new analysis of the sample complexity optimization problem, which yields a faster numerical resolution scheme and several quantitative regularity results that we believe of high independent interest.
http://w3id.org/mlsea/pwc/scientificWork/A%20Non-commutative%20Extension%20of%20Lee-Seung%27s%20Algorithm%20for%20Positive%20Semidefinite%20Factorizations                                                                                  A Non-commutative Extension of Lee-Seung's Algorithm for Positive Semidefinite Factorizations                                                                                  Given a matrix $X in mathbb{R}_+^{m times n}$ with nonnegative entries, a Positive Semidefinite (PSD) factorization of $X$ is a collection of $r times r$-dimensional PSD matrices $ {A_i }$ and $ {B_j }$ satisfying $X_{ij}= mathrm{tr}(A_i B_j)$ for all $ i in [m], j in [n]$. PSD factorizations are fundamentally linked to understanding the expressiveness of semidefinite programs as well as the power and limitations of quantum resources in information theory. The PSD factorization task generalizes the Non-negative Matrix Factorization (NMF) problem where we seek a collection of $r$-dimensional nonnegative vectors $ {a_i }$ and $ {b_j }$ satisfying $X_{ij}= a_i^ top b_j$, for all $i in [m], j in [n]$ -- one can recover the latter problem by choosing matrices in the PSD factorization to be diagonal. The most widely used algorithm for computing NMFs of a matrix is the Multiplicative Update algorithm developed by Lee and Seung, in which nonnegativity of the updates is preserved by scaling with positive diagonal matrices. In this paper, we describe a non-commutative extension of Lee-Seung's algorithm, which we call the Matrix Multiplicative Update (MMU) algorithm, for computing PSD factorizations. The MMU algorithm ensures that updates remain PSD by congruence scaling with the matrix geometric mean of appropriate PSD matrices, and it retains the simplicity of implementation that Lee-Seung's algorithm enjoys. Building on the Majorization-Minimization framework, we show that under our update scheme the squared loss objective is non-increasing and fixed points correspond to critical points. The analysis relies on Lieb's Concavity Theorem. Beyond PSD factorizations, we use the MMU algorithm as a primitive to calculate block-diagonal PSD factorizations and tensor PSD factorizations. We demonstrate the utility of our method with experiments on real and synthetic data.
http://w3id.org/mlsea/pwc/scientificWork/A%20Non-parametric%20View%20of%20FedAvg%20and%20FedProx%3A%20Beyond%20Stationary%20Points                                                                                  A Non-parametric View of FedAvg and FedProx: Beyond Stationary Points                                                                                  Federated Learning (FL) is a promising decentralized learning framework and has great potentials in privacy preservation and in lowering the computation load at the cloud. Recent work showed that FedAvg and FedProx - the two widely-adopted FL algorithms - fail to reach the stationary points of the global optimization objective even for homogeneous linear regression problems. Further, it is concerned that the common model learned might not generalize well locally at all in the presence of heterogeneity. In this paper, we analyze the convergence and statistical efficiency of FedAvg and FedProx, addressing the above two concerns. Our analysis is based on the standard non-parametric regression in a reproducing kernel Hilbert space (RKHS), and allows for heterogeneous local data distributions and unbalanced local datasets. We prove that the estimation errors, measured in either the empirical norm or the RKHS norm, decay with a rate of 1/t in general and exponentially for finite-rank kernels. In certain heterogeneous settings, these upper bounds also imply that both FedAvg and FedProx achieve the optimal error rate. To further analytically quantify the impact of the heterogeneity at each client, we propose and characterize a novel notion-federation gain, defined as the reduction of the estimation error for a client to join the FL. We discover that when the data heterogeneity is moderate, a client with limited local data can benefit from a common model with a large federation gain. Numerical experiments further corroborate our theoretical findings.
http://w3id.org/mlsea/pwc/scientificWork/A%20Non-sequential%20Approach%20to%20Deep%20User%20Interest%20Model%20for%20CTR%20Prediction                                                                                  A Non-sequential Approach to Deep User Interest Model for CTR Prediction                                                                                  Click-Through Rate (CTR) prediction plays an important role in many industrial applications, and recently a lot of attention is paid to the deep interest models which use attention mechanism to capture user interests from historical behaviors. However, most current models are based on sequential models which truncate the behavior sequences by a fixed length, thus have difficulties in handling very long behavior sequences. Another big problem is that sequences with the same length can be quite different in terms of time, carrying completely different meanings. In this paper, we propose a non-sequential approach to tackle the above problems. Specifically, we first represent the behavior data in a sparse key-vector format, where the vector contains rich behavior info such as time, count and category. Next, we enhance the Deep Interest Network to take such rich information into account by a novel attention network. The sparse representation makes it practical to handle large scale long behavior sequences. Finally, we introduce a multidimensional partition framework to mine behavior interactions. The framework can partition data into custom designed time buckets to capture the interactions among information aggregated in different time buckets. Similarly, it can also partition the data into different categories and capture the interactions among them. Experiments are conducted on two public datasets: one is an advertising dataset and the other is a production recommender dataset. Our models outperform other state-of-the-art models on both datasets.
http://w3id.org/mlsea/pwc/scientificWork/A%20Nonlinear%20Observability%20Analysis%20of%20Ambient%20Wind%20Estimation%20with%20Uncalibrated%20Sensors%2C%20Inspired%20by%20Insect%20Neural%20Encoding                                                                                  A Nonlinear Observability Analysis of Ambient Wind Estimation with Uncalibrated Sensors, Inspired by Insect Neural Encoding                                                                                  Estimating the direction of ambient fluid flow is key for many flying or swimming animals and robots, but can only be accomplished through indirect measurements and active control. Recent work with tethered flying insects indicates that their sensory representation of orientation, apparent flow, direction of movement, and control is represented by a 2-dimensional angular encoding in the central brain. This representation simplifies sensory integration by projecting the direction (but not scale) of measurements with different units onto a universal polar coordinate frame. To align these angular measurements with one another and the motor system does, however, require a calibration of angular gain and offset for each sensor. This calibration could change with time due to changes in the environment or physical structure. The circumstances under which small robots and animals with angular sensors and changing calibrations could self-calibrate and estimate the direction of ambient fluid flow while moving remains an open question. Here, a methodical nonlinear observability analysis is presented to address this. The analysis shows that it is mathematically feasible to continuously estimate flow direction and perform regular self-calibrations by adopting frequent changes in course (or active prevention thereof) and orientation, and requires fusion and temporal differentiation of three sensory measurements: apparent flow, orientation (or its derivative), and direction of motion (or its derivative). These conclusions are consistent with the zigzagging trajectories exhibited by many plume tracking organisms, suggesting that perhaps flow estimation is a secondary driver of their trajectory structure.
http://w3id.org/mlsea/pwc/scientificWork/A%20Nonmyopic%20Approach%20to%20Cost-Constrained%20Bayesian%20Optimization                                                                                  A Nonmyopic Approach to Cost-Constrained Bayesian Optimization                                                                                  Bayesian optimization (BO) is a popular method for optimizing expensive-to-evaluate black-box functions. BO budgets are typically given in iterations, which implicitly assumes each evaluation has the same cost. In fact, in many BO applications, evaluation costs vary significantly in different regions of the search space. In hyperparameter optimization, the time spent on neural network training increases with layer size; in clinical trials, the monetary cost of drug compounds vary; and in optimal control, control actions have differing complexities. Cost-constrained BO measures convergence with alternative cost metrics such as time, money, or energy, for which the sample efficiency of standard BO methods is ill-suited. For cost-constrained BO, cost efficiency is far more important than sample efficiency. In this paper, we formulate cost-constrained BO as a constrained Markov decision process (CMDP), and develop an efficient rollout approximation to the optimal CMDP policy that takes both the cost and future iterations into account. We validate our method on a collection of hyperparameter optimization problems as well as a sensor set selection application.
http://w3id.org/mlsea/pwc/scientificWork/A%20Normal%20Form%20Characterization%20for%20Efficient%20Boolean%20Skolem%20Function%20Synthesis                                                                                  A Normal Form Characterization for Efficient Boolean Skolem Function Synthesis                                                                                  Boolean Skolem function synthesis concerns synthesizing outputs as Boolean functions of inputs such that a relational specification between inputs and outputs is satisfied. This problem, also known as Boolean functional synthesis, has several applications, including design of safe controllers for autonomous systems, certified QBF solving, cryptanalysis etc. Recently, complexity theoretic hardness results have been shown for the problem, although several algorithms proposed in the literature are known to work well in practice. This dichotomy between theoretical hardness and practical efficacy has motivated the research into normal forms or representations of input specifications that permit efficient synthesis, thus explaining perhaps the efficacy of these algorithms. In this paper we go one step beyond this and ask if there exists a normal form representation that can in fact precisely characterize 'efficient' synthesis. We present a normal form called SAUNF that precisely characterizes tractable synthesis in the following sense: a specification is polynomial time synthesizable iff it can be compiled to SAUNF in polynomial time. Additionally, a specification admits a polynomial-sized functional solution iff there exists a semantically equivalent polynomial-sized SAUNF representation. SAUNF is exponentially more succinct than well-established normal forms like BDDs and DNNFs, used in the context of AI problems, and strictly subsumes other more recently proposed forms like SynNNF. It enjoys compositional properties that are similar to those of DNNF. Thus, SAUNF provides the right trade-off in knowledge representation for Boolean functional synthesis.
http://w3id.org/mlsea/pwc/scientificWork/A%20Note%20on%20Connecting%20Barlow%20Twins%20with%20Negative-Sample-Free%20Contrastive%20Learning                                                                                  A Note on Connecting Barlow Twins with Negative-Sample-Free Contrastive Learning                                                                                  In this report, we relate the algorithmic design of Barlow Twins' method to the Hilbert-Schmidt Independence Criterion (HSIC), thus establishing it as a contrastive learning approach that is free of negative samples. Through this perspective, we argue that Barlow Twins (and thus the class of negative-sample-free contrastive learning methods) suggests a possibility to bridge the two major families of self-supervised learning philosophies: non-contrastive and contrastive approaches. In particular, Barlow twins exemplified how we could combine the best practices of both worlds: avoiding the need of large training batch size and negative sample pairing (like non-contrastive methods) and avoiding symmetry-breaking network designs (like contrastive methods).
http://w3id.org/mlsea/pwc/scientificWork/A%20Note%20on%20Data%20Simulations%20for%20Voting%20by%20Evaluation                                                                                  A Note on Data Simulations for Voting by Evaluation                                                                                  Voting rules based on evaluation inputs rather than preference orders have been recently proposed, like majority judgement, range voting or approval voting. Traditionally, probabilistic analysis of voting rules supposes the use of simulation models to generate preferences data, like the Impartial Culture (IC) or Impartial and Anonymous Culture (IAC) models. But these simulation models are not suitable for the analysis of evaluation-based voting rules as they generate preference orders instead of the needed evaluations. We propose in this paper several simulation models for generating evaluation-based voting inputs. These models, inspired by classical ones, are defined, tested and compared for recommendation purpose.
http://w3id.org/mlsea/pwc/scientificWork/A%20Note%20on%20Relative%20Consciousness                                                                                  A Note on Relative Consciousness                                                                                  This paper describes a mathematical formulation for measuring how one system can estimate the consciousness of another. This consciousness estimate is always relative to the observer. The paper shows how this formulation leads to simple resolutions of some key problems of consciousness.
http://w3id.org/mlsea/pwc/scientificWork/A%20Note%20on%20Utility%20Maximization%20with%20Proportional%20Transaction%20Costs%20and%20Stability%20of%20Optimal%20Portfolios                                                                                  A Note on Utility Maximization with Proportional Transaction Costs and Stability of Optimal Portfolios                                                                                  The aim of this short note is to establish a limit theorem for the optimal trading strategies in the setup of the utility maximization problem with proportional transaction costs. This limit theorem resolves the open question from [4]. The main idea of our proof is to establish a uniqueness result for the optimal strategy. The proof of the uniqueness is heavily based on the dual approach which was developed recently in [6,7,8].
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%203D-UNet%20Deep%20Learning%20Framework%20Based%20on%20High-Dimensional%20Bilateral%20Grid%20for%20Edge%20Consistent%20Single%20Image%20Depth%20Estimation                                                                                  A Novel 3D-UNet Deep Learning Framework Based on High-Dimensional Bilateral Grid for Edge Consistent Single Image Depth Estimation                                                                                  The task of predicting smooth and edge-consistent depth maps is notoriously difficult for single image depth estimation. This paper proposes a novel Bilateral Grid based 3D convolutional neural network, dubbed as 3DBG-UNet, that parameterizes high dimensional feature space by encoding compact 3D bilateral grids with UNets and infers sharp geometric layout of the scene. Further, another novel 3DBGES-UNet model is introduced that integrate 3DBG-UNet for inferring an accurate depth map given a single color view. The 3DBGES-UNet concatenates 3DBG-UNet geometry map with the inception network edge accentuation map and a spatial object's boundary map obtained by leveraging semantic segmentation and train the UNet model with ResNet backbone. Both models are designed with a particular attention to explicitly account for edges or minute details. Preserving sharp discontinuities at depth edges is critical for many applications such as realistic integration of virtual objects in AR video or occlusion-aware view synthesis for 3D display applications.The proposed depth prediction network achieves state-of-the-art performance in both qualitative and quantitative evaluations on the challenging NYUv2-Depth data. The code and corresponding pre-trained weights will be made publicly available.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Algorithm%20to%20Report%20CSI%20in%20MIMO-Based%20Wireless%20Networks                                                                                  A Novel Algorithm to Report CSI in MIMO-Based Wireless Networks                                                                                  In wireless communication, accurate channel state information (CSI) is of pivotal importance. In practice, due to processing and feedback delays, estimated CSI can be outdated, which can severely deteriorate the performance of the communication system. Besides, to feedback estimated CSI, a strong compression of the CSI, evaluated at the user equipment (UE), is performed to reduce the over-the-air (OTA) overhead. Such compression strongly reduces the precision of the estimated CSI, which ultimately impacts the performance of multiple-input multiple-output (MIMO) precoding. Motivated by such issues, we present a novel scalable idea of reporting CSI in wireless networks, which is applicable to both time-division duplex (TDD) and frequency-division duplex (FDD) systems. In particular, the novel approach introduces the use of a channel predictor function, e.g., Kalman filter (KF), at both ends of the communication system to predict CSI. Simulation-based results demonstrate that the novel approach reduces not only the channel mean-squared-error (MSE) but also the OTA overhead to feedback the estimated CSI when there is immense variation in the mobile radio channel. Besides, in the immobile radio channel, feedback can be eliminated, which brings the benefit of further reducing the OTA overhead. Additionally, the proposed method provides a significant signal-to-noise ratio (SNR) gain in both the channel conditions, i.e., highly mobile and immobile.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Approach%20for%20Semiconductor%20Etching%20Process%20with%20Inductive%20Biases                                                                                  A Novel Approach for Semiconductor Etching Process with Inductive Biases                                                                                  The etching process is one of the most important processes in semiconductor manufacturing. We have introduced the state-of-the-art deep learning model to predict the etching profiles. However, the significant problems violating physics have been found through various techniques such as explainable artificial intelligence and representation of prediction uncertainty. To address this problem, this paper presents a novel approach to apply the inductive biases for etching process. We demonstrate that our approach fits the measurement faster than physical simulator while following the physical behavior. Our approach would bring a new opportunity for better etching process with higher accuracy and lower cost.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Approach%20to%20Curiosity%20and%20Explainable%20Reinforcement%20Learning%20via%20Interpretable%20Sub-Goals                                                                                  A Novel Approach to Curiosity and Explainable Reinforcement Learning via Interpretable Sub-Goals                                                                                  Two key challenges within Reinforcement Learning involve improving (a) agent learning within environments with sparse extrinsic rewards and (b) the explainability of agent actions. We describe a curious subgoal focused agent to address both these challenges. We use a novel method for curiosity produced from a Generative Adversarial Network (GAN) based model of environment transitions that is robust to stochastic environment transitions. Additionally, we use a subgoal generating network to guide navigation. The explainability of the agent's behavior is increased by decomposing complex tasks into a sequence of interpretable subgoals that do not require any manual design. We show that this method also enables the agent to solve challenging procedurally-generated tasks that contain stochastic transitions above other state-of-the-art methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Approach%20to%20Discover%20Switch%20Behaviours%20in%20Process%20Mining                                                                                  A Novel Approach to Discover Switch Behaviours in Process Mining                                                                                  Process mining is a relatively new subject which builds a bridge between process modelling and data mining. An exclusive choice in a process model usually splits the process into different branches. However, in some processes, it is possible to switch from one branch to another. The inductive miner guarantees to return sound process models, but fails to return a precise model when there are switch behaviours between different exclusive choice branches due to the limitation of process trees. In this paper, we present a novel extension to the process tree model to support switch behaviours between different branches of the exclusive choice operator and propose a novel extension to the inductive miner to discover sound process models with switch behaviours. The proposed discovery technique utilizes the theory of a previous study to detect possible switch behaviours. We apply both artificial and publicly-available datasets to evaluate our approach. Our results show that our approach can improve the precision of discovered models by 36% while maintaining high fitness values compared to the original inductive miner.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Approach%20to%20Lifelong%20Learning%3A%20The%20Plastic%20Support%20Structure                                                                                  A Novel Approach to Lifelong Learning: The Plastic Support Structure                                                                                  We propose a novel approach to lifelong learning, introducing a compact encapsulated support structure which endows a network with the capability to expand its capacity as needed to learn new tasks while preventing the loss of learned tasks. This is achieved by splitting neurons with high semantic drift and constructing an adjacent network to encode the new tasks at hand. We call this the Plastic Support Structure (PSS), it is a compact structure to learn new tasks that cannot be efficiently encoded in the existing structure of the network. We validate the PSS on public datasets against existing lifelong learning architectures, showing it performs similarly to them but without prior knowledge of the task and in some cases with fewer parameters and in a more understandable fashion where the PSS is an encapsulated container for specific features related to specific tasks, thus making it an ideal 'add-on' solution for endowing a network to learn more tasks.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Approximate%20Hamming%20Weight%20Computing%20for%20Spiking%20Neural%20Networks%3A%20an%20FPGA%20Friendly%20Architecture                                                                                  A Novel Approximate Hamming Weight Computing for Spiking Neural Networks: an FPGA Friendly Architecture                                                                                  Hamming weights of sparse and long binary vectors are important modules in many scientific applications, particularly in spiking neural networks that are of our interest. To improve both area and latency of their FPGA implementations, we propose a method inspired from synaptic transmission failure for exploiting FPGA lookup tables to compress long input vectors. To evaluate the effectiveness of this approach, we count the number of `1's of the compressed vector using a simple linear adder. We classify the compressors into shallow ones with up to two levels of lookup tables and deep ones with more than two levels. The architecture generated by this approach shows up to 82% and 35% reductions for different configurations of shallow compressors in area and latency respectively. Moreover, our simulation results show that calculating the Hamming weight of a 1024-bit vector of a spiking neural network by the use of only deep compressors preserves the chaotic behavior of the network while slightly impacts on the learning performance.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Automatic%20Modulation%20Classification%20Scheme%20Based%20on%20Multi-Scale%20Networks                                                                                  A Novel Automatic Modulation Classification Scheme Based on Multi-Scale Networks                                                                                  Automatic modulation classification enables intelligent communications and it is of crucial importance in today's and future wireless communication networks. Although many automatic modulation classification schemes have been proposed, they cannot tackle the intra-class diversity problem caused by the dynamic changes of the wireless communication environment. In order to overcome this problem, inspired by face recognition, a novel automatic modulation classification scheme is proposed by using the multi-scale network in this paper. Moreover, a novel loss function that combines the center loss and the cross entropy loss is exploited to learn both discriminative and separable features in order to further improve the classification performance. Extensive simulation results demonstrate that our proposed automatic modulation classification scheme can achieve better performance than the benchmark schemes in terms of the classification accuracy. The influence of the network parameters and the loss function with the two-stage training strategy on the classification accuracy of our proposed scheme are investigated.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Bayesian%20Approach%20for%20the%20Two-Dimensional%20Harmonic%20Retrieval%20Problem                                                                                  A Novel Bayesian Approach for the Two-Dimensional Harmonic Retrieval Problem                                                                                  Sparse signal recovery algorithms like sparse Bayesian learning work well but the complexity quickly grows when tackling higher dimensional parametric dictionaries. In this work we propose a novel Bayesian strategy to address the two dimensional harmonic retrieval problem, through remodeling and reparameterization of the standard data model. This new model allows us to introduce a block sparsity structure in a manner that enables a natural pairing of the parameters in the two dimensions. The numerical simulations demonstrate that the inference algorithm developed (H-MSBL) does not suffer from source identifiability issues and is capable of estimating the harmonic components in challenging scenarios, while maintaining a low computational complexity.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Binocular%20Eye-Tracking%20SystemWith%20Stereo%20Stimuli%20for%203D%20Gaze%20Estimation                                                                                  A Novel Binocular Eye-Tracking SystemWith Stereo Stimuli for 3D Gaze Estimation                                                                                  Eye-tracking technologies have been widely used in applications like psychological studies and human computer interactions (HCI). However, most current eye trackers focus on 2D point of gaze (PoG) estimation and cannot provide accurate gaze depth.Concerning future applications such as HCI with 3D displays, we propose a novel binocular eye tracking device with stereo stimuli to provide highly accurate 3D PoG estimation. In our device, the 3D stereo imaging system can provide users with a friendly and immersive 3D visual experience without wearing any accessories. The eye capturing system can directly record the users eye movements under 3D stimuli without disturbance. A regression based 3D eye tracking model is built based on collected eye movement data under stereo stimuli. Our model estimates users 2D gaze with features defined by eye region landmarks and further estimates 3D PoG with a multi source feature set constructed by comprehensive eye movement features and disparity features from stereo stimuli. Two test stereo scenes with different depths of field are designed to verify the model effectiveness. Experimental results show that the average error for 2D gaze estimation was 0.66 degree and for 3D PoG estimation, the average errors are 1.85~cm/0.15~m over the workspace volume 50~cm $ times$ 30~cm $ times$ 75~cm/2.4~m $ times$ 4.0~m $ times$ 7.9~m separately.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Convergence%20Analysis%20for%20Algorithms%20of%20the%20Adam%20Family%20and%20Beyond                                                                                  A Novel Convergence Analysis for Algorithms of the Adam Family and Beyond                                                                                  Why does the original analysis of Adam fail, but it still converges very well in practice on a broad range of problems? There are still some mysteries about Adam that have not been unraveled. This paper provides a novel non-convex analysis of Adam and its many variants to uncover some of these mysteries. Our analysis exhibits that an increasing or large enough 'momentum' parameter for the first-order moment used in practice is sufficient to ensure Adam and its many variants converge under a mild boundness condition on the adaptive scaling factor of the step size. In contrast, the original problematic analysis of Adam uses a momentum parameter that decreases to zero, which is the key reason that makes it diverge on some problems. To the best of our knowledge, this is the first time the gap between analysis and practice is bridged. Our analysis also exhibits more insights for practical implementations of Adam, e.g., increasing the momentum parameter in a stage-wise manner in accordance with stagewise decreasing step size would help improve the convergence. Our analysis of the Adam family is modular such that it can be (has been) extended to solving other optimization problems, e.g., compositional, min-max and bi-level problems. As an interesting yet non-trivial use case, we present an extension for solving non-convex min-max optimization in order to address a gap in the literature that either requires a large batch or has double loops. Our empirical studies corroborate the theory and also demonstrate the effectiveness in solving min-max problems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Deep%20ML%20Architecture%20by%20Integrating%20Visual%20Simultaneous%20Localization%20and%20Mapping%20%28vSLAM%29%20into%20Mask%20R-CNN%20for%20Real-time%20Surgical%20Video%20Analysis                                                                                  A Novel Deep ML Architecture by Integrating Visual Simultaneous Localization and Mapping (vSLAM) into Mask R-CNN for Real-time Surgical Video Analysis                                                                                  Seven million people suffer surgical complications each year, but with sufficient surgical training and review, 50 % of these complications could be prevented. To improve surgical performance, existing research uses various deep learning (DL) technologies including convolutional neural networks (CNN) and recurrent neural networks (RNN) to automate surgical tool and workflow detection. However, there is room to improve accuracy; real-time analysis is also minimal due to the complexity of CNN. In this research, a novel DL architecture is proposed to integrate visual simultaneous localization and mapping (vSLAM) into Mask R-CNN. This architecture, vSLAM-CNN (vCNN), for the first time, integrates the best of both worlds, inclusive of (1) vSLAM for object detection, by focusing on geometric information for region proposals, and (2) CNN for object recognition, by focusing on semantic information for image classification, combining them into one joint end-to-end training process. This method, using spatio-temporal information in addition to visual features, is evaluated on M2CAI 2016 challenge datasets, achieving the state-of-the-art results with 96.8 mAP for tool detection and 97.5 mean Jaccard score for workflow detection, surpassing all previous works, and reaching a 50 FPS performance, 10x faster than the region-based CNN. A region proposal module (RPM) replaces the region proposal network (RPN) in Mask R-CNN, accurately placing bounding boxes and lessening the annotation requirement. Furthermore, a Microsoft HoloLens 2 application is developed to provide an augmented reality (AR)-based solution for surgical training and assistance.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Deep%20Reinforcement%20Learning%20Based%20Stock%20Direction%20Prediction%20using%20Knowledge%20Graph%20and%20Community%20Aware%20Sentiments                                                                                  A Novel Deep Reinforcement Learning Based Stock Direction Prediction using Knowledge Graph and Community Aware Sentiments                                                                                  Stock market prediction has been an important topic for investors, researchers, and analysts. Because it is affected by too many factors, stock market prediction is a difficult task to handle. In this study, we propose a novel method that is based on deep reinforcement learning methodologies for the direction prediction of stocks using sentiments of community and knowledge graph. For this purpose, we firstly construct a social knowledge graph of users by analyzing relations between connections. After that, time series analysis of related stock and sentiment analysis is blended with deep reinforcement methodology. Turkish version of Bidirectional Encoder Representations from Transformers (BerTurk) is employed to analyze the sentiments of the users while deep Q-learning methodology is used for the deep reinforcement learning side of the proposed model to construct the deep Q network. In order to demonstrate the effectiveness of the proposed model, Garanti Bank (GARAN), Akbank (AKBNK), T 'urkiye .I c{s} Bankas{ i} (ISCTR) stocks in Istanbul Stock Exchange are used as a case study. Experiment results show that the proposed novel model achieves remarkable results for stock market prediction task.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Disaster%20Image%20Dataset%20and%20Characteristics%20Analysis%20using%20Attention%20Model                                                                                  A Novel Disaster Image Dataset and Characteristics Analysis using Attention Model                                                                                  The advancement of deep learning technology has enabled us to develop systems that outperform any other classification technique. However, success of any empirical system depends on the quality and diversity of the data available to train the proposed system. In this research, we have carefully accumulated a relatively challenging dataset that contains images collected from various sources for three different disasters: fire, water and land. Besides this, we have also collected images for various damaged infrastructure due to natural or man made calamities and damaged human due to war or accidents. We have also accumulated image data for a class named non-damage that contains images with no such disaster or sign of damage in them. There are 13,720 manually annotated images in this dataset, each image is annotated by three individuals. We are also providing discriminating image class information annotated manually with bounding box for a set of 200 test images. Images are collected from different news portals, social media, and standard datasets made available by other researchers. A three layer attention model (TLAM) is trained and average five fold validation accuracy of 95.88% is achieved. Moreover, on the 200 unseen test images this accuracy is 96.48%. We also generate and compare attention maps for these test images to determine the characteristics of the trained attention model. Our dataset is available at https://niloy193.github.io/Disaster-Dataset
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Edge%20Detection%20Operator%20for%20Identifying%20Buildings%20in%20Augmented%20Reality%20Applications                                                                                  A Novel Edge Detection Operator for Identifying Buildings in Augmented Reality Applications                                                                                  Augmented Reality is an environment-enhancing technology, widely applied in many domains, such as tourism and culture. One of the major challenges in this field is precise detection and extraction of building information through Computer Vision techniques. Edge detection is one of the building blocks operations for many feature extraction solutions in Computer Vision. AR systems use edge detection for building extraction or for extraction of facade details from buildings. In this paper, we propose a novel filter operator for edge detection that aims to extract building contours or facade features better. The proposed filter gives more weight for finding vertical and horizontal edges that is an important feature for our aim.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Ensemble%20Learning%20Approach%20to%20Unsupervised%20Record%20Linkage                                                                                  A Novel Ensemble Learning Approach to Unsupervised Record Linkage                                                                                  Record linkage is a process of identifying records that refer to the same real-world entity. Many existing approaches to record linkage apply supervised machine learning techniques to generate a classification model that classifies a pair of records as either match or non-match. The main requirement of such an approach is a labelled training dataset. In many real-world applications no labelled dataset is available hence manual labelling is required to create a sufficiently sized training dataset for a supervised ma- chine learning algorithm. Semi-supervised machine learning techniques, such as self-learning or active learning, which require only a small manually labelled training dataset have been applied to record link- age. These techniques reduce the requirement on the manual labelling of the training dataset. However, they have yet to achieve a level of accuracy similar to that of supervised learning techniques. In this paper we propose a new approach to unsupervised record linkage based on a combination of ensemble learning and enhanced automatic self-learning. In the proposed approach an ensemble of automatic self-learning models is generated with different similarity measure schemes. In order to further improve the automatic self-learning process we incorporate field weighting into the automatic seed selection for each of the self-learning models. We propose an unsupervised diversity measure to ensure that there is high diversity among the selected self-learning models. Finally, we propose to use the contribution ratios of self-learning models to remove those with poor accuracy from the ensemble. We have evaluated our approach on 4 publicly available datasets which are commonly used in the record linkage community. Our experimental results show that our proposed approach has advantages over the state-of-the-art semi-supervised and unsupervised record linkage techniques. In 3 out of 4 datasets it also achieves com- parable results to those of the supervised approaches.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Estimator%20of%20Mutual%20Information%20for%20Learning%20to%20Disentangle%20Textual%20Representations                                                                                  A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations                                                                                  Learning disentangled representations of textual data is essential for many natural language tasks such as fair classification, style transfer and sentence generation, among others. The existent dominant approaches in the context of text data {either rely} on training an adversary (discriminator) that aims at making attribute values difficult to be inferred from the latent code {or rely on minimising variational bounds of the mutual information between latent code and the value attribute}. {However, the available methods suffer of the impossibility to provide a fine-grained control of the degree (or force) of disentanglement.} {In contrast to} {adversarial methods}, which are remarkably simple, although the adversary seems to be performing perfectly well during the training phase, after it is completed a fair amount of information about the undesired attribute still remains. This paper introduces a novel variational upper bound to the mutual information between an attribute and the latent code of an encoder. Our bound aims at controlling the approximation error via the Renyi's divergence, leading to both better disentangled representations and in particular, a precise control of the desirable degree of disentanglement {than state-of-the-art methods proposed for textual data}. Furthermore, it does not suffer from the degeneracy of other losses in multi-class scenarios. We show the superiority of this method on fair classification and on textual style transfer tasks. Additionally, we provide new insights illustrating various trade-offs in style transfer when attempting to learn disentangled representations and quality of the generated sentence.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Falling-Ball%20Algorithm%20for%20Image%20Segmentation                                                                                  A Novel Falling-Ball Algorithm for Image Segmentation                                                                                  Image segmentation refers to the separation of objects from the background, and has been one of the most challenging aspects of digital image processing. Practically it is impossible to design a segmentation algorithm which has 100% accuracy, and therefore numerous segmentation techniques have been proposed in the literature, each with certain limitations. In this paper, a novel Falling-Ball algorithm is presented, which is a region-based segmentation algorithm, and an alternative to watershed transform (based on waterfall model). The proposed algorithm detects the catchment basins by assuming that a ball falling from hilly terrains will stop in a catchment basin. Once catchment basins are identified, the association of each pixel with one of the catchment basin is obtained using multi-criterion fuzzy logic. Edges are constructed by dividing image into different catchment basins with the help of a membership function. Finally closed contour algorithm is applied to find closed regions and objects within closed regions are segmented using intensity information. The performance of the proposed algorithm is evaluated both objectively as well as subjectively. Simulation results show that the proposed algorithms gives superior performance over conventional Sobel edge detection methods and the watershed segmentation algorithm. For comparative analysis, various comparison methods are used for demonstrating the superiority of proposed methods over existing segmentation methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Framework%20Integrating%20AI%20Model%20and%20Enzymological%20Experiments%20Promotes%20Identification%20of%20SARS-CoV-2%203CL%20Protease%20Inhibitors%20and%20Activity-based%20Probe                                                                                  A Novel Framework Integrating AI Model and Enzymological Experiments Promotes Identification of SARS-CoV-2 3CL Protease Inhibitors and Activity-based Probe                                                                                  The identification of protein-ligand interaction plays a key role in biochemical research and drug discovery. Although deep learning has recently shown great promise in discovering new drugs, there remains a gap between deep learning-based and experimental approaches. Here we propose a novel framework, named AIMEE, integrating AI Model and Enzymology Experiments, to identify inhibitors against 3CL protease of SARS-CoV-2, which has taken a significant toll on people across the globe. From a bioactive chemical library, we have conducted two rounds of experiments and identified six novel inhibitors with a hit rate of 29.41%, and four of them showed an IC50 value less than 3 { mu}M. Moreover, we explored the interpretability of the central model in AIMEE, mapping the deep learning extracted features to domain knowledge of chemical properties. Based on this knowledge, a commercially available compound was selected and proven to be an activity-based probe of 3CLpro. This work highlights the great potential of combining deep learning models and biochemical experiments for intelligent iteration and expanding the boundaries of drug discovery.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20GCN%20based%20Indoor%20Localization%20System%20with%20Multiple%20Access%20Points                                                                                  A Novel GCN based Indoor Localization System with Multiple Access Points                                                                                  With the rapid development of indoor location-based services (LBSs), the demand for accurate localization keeps growing as well. To meet this demand, we propose an indoor localization algorithm based on graph convolutional network (GCN). We first model access points (APs) and the relationships between them as a graph, and utilize received signal strength indication (RSSI) to make up fingerprints. Then the graph and the fingerprint will be put into GCN for feature extraction, and get classification by multilayer perceptron (MLP).In the end, experiments are performed under a 2D scenario and 3D scenario with floor prediction. In the 2D scenario, the mean distance error of GCN-based method is 11m, which improves by 7m and 13m compare with DNN-based and CNN-based schemes respectively. In the 3D scenario, the accuracy of predicting buildings and floors are up to 99.73% and 93.43% respectively. Moreover, in the case of predicting floors and buildings correctly, the mean distance error is 13m, which outperforms DNN-based and CNN-based schemes, whose mean distance errors are 34m and 26m respectively.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Generalised%20Meta-Heuristic%20Framework%20for%20Dynamic%20Capacitated%20Arc%20Routing%20Problems                                                                                  A Novel Generalised Meta-Heuristic Framework for Dynamic Capacitated Arc Routing Problems                                                                                  The capacitated arc routing problem (CARP) is a challenging combinatorial optimisation problem abstracted from many real-world applications, such as waste collection, road gritting and mail delivery. However, few studies considered dynamic changes during the vehicles' service, which can cause the original schedule infeasible or obsolete. The few existing studies are limited by the dynamic scenarios considered, and by overly complicated algorithms that are unable to benefit from the wealth of contributions provided by the existing CARP literature. In this paper, we first provide a mathematical formulation of dynamic CARP (DCARP) and design a simulation system that is able to consider dynamic events while a routing solution is already partially executed. We then propose a novel framework which can benefit from existing static CARP optimisation algorithms so that they could be used to handle DCARP instances. The framework is very flexible. In response to a dynamic event, it can use either a simple restart strategy or a sequence transfer strategy that benefits from past optimisation experience. Empirical studies have been conducted on a wide range of DCARP instances to evaluate our proposed framework. The results show that the proposed framework significantly improves over state-of-the-art dynamic optimisation algorithms.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Graph-Theoretic%20Deep%20Representation%20Learning%20Method%20for%20Multi-Label%20Remote%20Sensing%20Image%20Retrieval                                                                                  A Novel Graph-Theoretic Deep Representation Learning Method for Multi-Label Remote Sensing Image Retrieval                                                                                  This paper presents a novel graph-theoretic deep representation learning method in the framework of multi-label remote sensing (RS) image retrieval problems. The proposed method aims to extract and exploit multi-label co-occurrence relationships associated to each RS image in the archive. To this end, each training image is initially represented with a graph structure that provides region-based image representation combining both local information and the related spatial organization. Unlike the other graph-based methods, the proposed method contains a novel learning strategy to train a deep neural network for automatically predicting a graph structure of each RS image in the archive. This strategy employs a region representation learning loss function to characterize the image content based on its multi-label co-occurrence relationship. Experimental results show the effectiveness of the proposed method for retrieval problems in RS compared to state-of-the-art deep representation learning methods. The code of the proposed method is publicly available at https://git.tu-berlin.de/rsim/GT-DRL-CBIR .
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Heap-based%20Pilot%20Assignment%20for%20Full%20Duplex%20Cell-Free%20Massive%20MIMO%20with%20Zero-Forcing                                                                                  A Novel Heap-based Pilot Assignment for Full Duplex Cell-Free Massive MIMO with Zero-Forcing                                                                                  This paper investigates the combined benefits of full-duplex (FD) and cell-free massive multiple-input multipleoutput (CF-mMIMO), where a large number of distributed access points (APs) having FD capability simultaneously serve numerous uplink and downlink user equipments (UEs) on the same time-frequency resources. To enable the incorporation of FD technology in CF-mMIMO systems, we propose a novel heapbased pilot assignment algorithm, which not only can mitigate the effects of pilot contamination but also reduce the involved computational complexity. Then, we formulate a robust design problem for spectral efficiency (SE) maximization in which the power control and AP-UE association are jointly optimized, resulting in a difficult mixed-integer nonconvex programming. To solve this problem, we derive a more tractable problem before developing a very simple iterative algorithm based on inner approximation method with polynomial computational complexity. Numerical results show that our proposed methods with realistic parameters significantly outperform the existing approaches in terms of the quality of channel estimate and SE.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Hybrid%20Deep%20Learning%20Approach%20for%20Non-Intrusive%20Load%20Monitoring%20of%20Residential%20Appliance%20Based%20on%20Long%20Short%20Term%20Memory%20and%20Convolutional%20Neural%20Networks                                                                                  A Novel Hybrid Deep Learning Approach for Non-Intrusive Load Monitoring of Residential Appliance Based on Long Short Term Memory and Convolutional Neural Networks                                                                                  Energy disaggregation or nonintrusive load monitoring (NILM), is a single-input blind source discrimination problem, aims to interpret the mains user electricity consumption into appliance level measurement. This article presents a new approach for power disaggregation by using a deep recurrent long short term memory (LSTM) network combined with convolutional neural networks (CNN). Deep neural networks have been shown to be a significant way for these types of problems because of their complexity and huge number of trainable paramters. Hybrid method that proposed in the article could significantly increase the overall accuracy of NILM because it benefits from both network advantages. The proposed method used sequence-to-sequence learning, where the input is a window of the mains and the output is a window of the target appliance. The proposed deep neural network approach has been applied to real-world household energy dataset 'REFIT'. The REFIT electrical load measurements dataset described in this paper includes whole house aggregate loads and nine individual appliance measurements at 8-second intervals per house, collected continuously over a period of two years from 20 houses around the UK. The proposed method achieve significant performance, improving accuracy and F1-score measures by 95.93% and 80.93% ,respectively which demonstrates the effectiveness and superiority of the proposed approach for home energy monitoring. Comparison of proposed method and other recently published method has been presented and discussed based on accuracy, number of considered appliances and size of the deep neural network trainable parameters. The proposed method shows remarkable performance compare to other previous methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Interaction-based%20Methodology%20Towards%20Explainable%20AI%20with%20Better%20Understanding%20of%20Pneumonia%20Chest%20X-ray%20Images                                                                                  A Novel Interaction-based Methodology Towards Explainable AI with Better Understanding of Pneumonia Chest X-ray Images                                                                                  In the field of eXplainable AI (XAI), robust 'blackbox' algorithms such as Convolutional Neural Networks (CNNs) are known for making high prediction performance. However, the ability to explain and interpret these algorithms still require innovation in the understanding of influential and, more importantly, explainable features that directly or indirectly impact the performance of predictivity. A number of methods existing in literature focus on visualization techniques but the concepts of explainability and interpretability still require rigorous definition. In view of the above needs, this paper proposes an interaction-based methodology -- Influence Score (I-score) -- to screen out the noisy and non-informative variables in the images hence it nourishes an environment with explainable and interpretable features that are directly associated to feature predictivity. We apply the proposed method on a real world application in Pneumonia Chest X-ray Image data set and produced state-of-the-art results. We demonstrate how to apply the proposed approach for more general big data problems by improving the explainability and interpretability without sacrificing the prediction performance. The contribution of this paper opens a novel angle that moves the community closer to the future pipelines of XAI problems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Malware%20Detection%20Mechanism%20based%20on%20Features%20Extracted%20from%20Converted%20Malware%20Binary%20Images                                                                                  A Novel Malware Detection Mechanism based on Features Extracted from Converted Malware Binary Images                                                                                  Our computer systems for decades have been threatened by various types of hardware and software attacks of which Malwares have been one of them. This malware has the ability to steal, destroy, contaminate, gain unintended access, or even disrupt the entire system. There have been techniques to detect malware by performing static and dynamic analysis of malware files, but, stealthy malware has circumvented the static analysis method and for dynamic analysis, there have been previous works that propose different methods to detect malware but, in this work we propose a novel technique to detect malware. We use malware binary images and then extract different features from the same and then employ different ML-classifiers on the dataset thus obtained. We show that this technique is successful in differentiating classes of malware based on the features extracted.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Multi-Period%20and%20Multilateral%20Price%20Index                                                                                  A Novel Multi-Period and Multilateral Price Index                                                                                  A novel approach to price indices, leading to an innovative solution in both a multi-period or a multilateral framework, is presented. The index turns out to be the generalized least squares solution of a regression model linking values and quantities of the commodities. The index reference basket, which is the union of the intersections of the baskets of all country/period taken in pair, has a coverage broader than extant indices. The properties of the index are investigated and updating formulas established. Applications to both real and simulated data provide evidence of the better index performance in comparison with extant alternatives.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Multi-scale%20Dilated%203D%20CNN%20for%20Epileptic%20Seizure%20Prediction                                                                                  A Novel Multi-scale Dilated 3D CNN for Epileptic Seizure Prediction                                                                                  Accurate prediction of epileptic seizures allows patients to take preventive measures in advance to avoid possible injuries. In this work, a novel convolutional neural network (CNN) is proposed to analyze time, frequency, and channel information of electroencephalography (EEG) signals. The model uses three-dimensional (3D) kernels to facilitate the feature extraction over the three dimensions. The application of multiscale dilated convolution enables the 3D kernel to have more flexible receptive fields. The proposed CNN model is evaluated with the CHB-MIT EEG database, the experimental results indicate that our model outperforms the existing state-of-the-art, achieves 80.5% accuracy, 85.8% sensitivity and 75.1% specificity.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20NOMA%20Solution%20with%20RIS%20Partitioning                                                                                  A Novel NOMA Solution with RIS Partitioning                                                                                  Reconfigurable intelligent surface (RIS) empowered communications with non-orthogonal multiple access (NOMA) has recently become as an appealing research direction for the next-generation wireless communications. In this paper, we propose a novel NOMA solution with RIS partitioning, where we aim to enhance the spectrum efficiency by improving the ergodic rate of all users, and to maximize the user fairness. In the proposed system, we distribute the physical resources among users such that the base station (BS) and RIS are dedicated to serve different clusters of users. Furthermore, we formulate an RIS partitioning optimization problem to slice the RIS elements between the users such that the user fairness is maximized. The formulated problem is a non-convex and non-linear integer programming (NLIP) problem with a combinatorial feasible set, which is very challenging to solve. Therefore, we exploit the structure of the problem to bound its feasible set and obtain a sub-optimal solution by sequentially applying three efficient search algorithms. Furthermore, we derive exact and asymptotic expressions for the outage probability. Simulation results clearly indicate the superiority of the proposed system over the considered benchmark systems in terms of ergodic sum-rate, outage probability, and user fairness performance.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Non-population-based%20Meta-heuristic%20Optimizer%20Inspired%20by%20the%20Philosophy%20of%20Yi%20Jing                                                                                  A Novel Non-population-based Meta-heuristic Optimizer Inspired by the Philosophy of Yi Jing                                                                                  Drawing inspiration from the philosophy of Yi Jing, Yin-Yang pair optimization (YYPO) has been shown to achieve competitive performance in single objective optimizations. Besides, it has the advantage of low time complexity when comparing to other population-based optimization. As a conceptual extension of YYPO, we proposed the novel Yi optimization (YI) algorithm as one of the best non-population-based optimizer. Incorporating both the harmony and reversal concept of Yi Jing, we replace the Yin-Yang pair with a Yi-point, in which we utilize the Levy flight to update the solution and balance both the effort of the exploration and the exploitation in the optimization process. As a conceptual prototype, we examine YI with IEEE CEC 2017 benchmark and compare its performance with a Levy flight-based optimizer CV1.0, the state-of-the-art dynamical Yin-Yang pair optimization in YYPO family and a few classical optimizers. According to the experimental results, YI shows highly competitive performance while keeping the low time complexity. Hence, the results of this work have implications for enhancing meta-heuristic optimizer using the philosophy of Yi Jing, which deserves research attention.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Physics-based%20Channel%20Model%20for%20Reconfigurable%20Intelligent%20Surface-assisted%20Multi-user%20Communication%20Systems                                                                                  A Novel Physics-based Channel Model for Reconfigurable Intelligent Surface-assisted Multi-user Communication Systems                                                                                  The reconfigurable intelligent surface (RIS) is one of the promising technologies contributing to the next generation smart radio environment. A novel physics-based RIS channel model is proposed. Particularly, we consider the RIS and the scattering environment as a whole by studying the signal's multipath propagation, as well as the radiation pattern of the RIS. The model suggests that the RIS-assisted wireless channel can be approximated by a Rician distribution. Analytical expressions are derived for the shape factor and the scale factor of the distribution. For the case of continuous phase shifts, the distribution depends on the number of elements of the RIS and the observing direction of the receiver. For the case of continuous phase shifts, the distribution further depends on the quantization level of the RIS phase error. The scaling law of the average received power is obtained from the scale factor of the distribution. For the application scenarios where RIS functions as an anomalous reflector, we investigate the performance of single RIS-assisted multiple access networks for time-division multiple access (TDMA), frequency-division multiple access (FDMA) and non-orthogonal multiple access (NOMA). Closed-form expressions for the outage probability of the proposed channel model are derived. It is proved that a constant diversity order exists, which is independent of the number of RIS elements. Simulation results are presented to confirm that the proposed model applies effectively to the phased-array implemented RISs.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Real-Time%20Energy%20Management%20Strategy%20for%20Grid-Supporting%20Microgrid%3A%20Enabling%20Flexible%20Trading%20Power                                                                                  A Novel Real-Time Energy Management Strategy for Grid-Supporting Microgrid: Enabling Flexible Trading Power                                                                                  In recent years, there has been significant growth of distributed energy resources (DERs) penetration in the power grid. The stochastic and intermittent features of variable DERs such as roof top photovoltaic (PV) bring substantial uncertainties to the grid on the consumer end and weaken the grid reliability. In addition, the fact that numerous DERs are widespread in the grid makes it hard to monitor and manage DERs. To address this challenge, this paper proposes a novel real-time grid-supporting energy management (GSEM) strategy for grid-supporting microgrid (MG). This strategy can not only properly manage DERs in a MG but also enable DERs to provide grid services, which enables a MG to be grid-supporting via flexible trading power. The proposed GSEM strategy is based on a 2-step optimization which includes a routine economic dispatch (ED) step and an acceptable trading power range determination step. Numerical simulations demonstrate the performance of the proposed GSEM strategy which enables the grid operator to have a dispatch choice of trading power with MG and enhance the reliability and resilience of the main grid.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20SEPIC-%C4%86uk%20Based%20High%20Gain%20Solar%20Micro-Inverter%20for%20Grid%20Integration                                                                                  A Novel SEPIC-Ćuk Based High Gain Solar Micro-Inverter for Grid Integration                                                                                  Solar micro-inverters are becoming increasingly popular as they are modular, and they posses the capability of extracting maximum available power from the individual photovoltaic (PV) modules of a solar array. For realizing micro-inverters single stage transformer-less topologies are preferred as they offer better power evacuation efficacy. A SEPIC- 'Cuk based transformer-less micro-inverter, having only one high frequency switch and four line frequency switches, is proposed in this paper. The proposed converter can be employed to interface a 35 V PV module to a 220 V single phase ac grid. As a very high gain is required to be achieved for the converter, it is made to operate in discontinuous conduction mode (DCM) for all possible operating conditions. Since the ground of the each PV modules is connected to the ground of the utility, there is no possibility of leakage current flow between the module and the utility. Detailed simulation studies are carried out to ascertain the efficacy of the proposed micro-inverter. A laboratory prototype of the inverter is fabricated, and detailed experimental studies are carried out to confirm the viability of the proposed scheme.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Self-Learning%20Framework%20for%20Bladder%20Cancer%20Grading%20Using%20Histopathological%20Images                                                                                  A Novel Self-Learning Framework for Bladder Cancer Grading Using Histopathological Images                                                                                  Recently, bladder cancer has been significantly increased in terms of incidence and mortality. Currently, two subtypes are known based on tumour growth: non-muscle invasive (NMIBC) and muscle-invasive bladder cancer (MIBC). In this work, we focus on the MIBC subtype because it is of the worst prognosis and can spread to adjacent organs. We present a self-learning framework to grade bladder cancer from histological images stained via immunohistochemical techniques. Specifically, we propose a novel Deep Convolutional Embedded Attention Clustering (DCEAC) which allows classifying histological patches into different severity levels of the disease, according to the patterns established in the literature. The proposed DCEAC model follows a two-step fully unsupervised learning methodology to discern between non-tumour, mild and infiltrative patterns from high-resolution samples of 512x512 pixels. Our system outperforms previous clustering-based methods by including a convolutional attention module, which allows refining the features of the latent space before the classification stage. The proposed network exceeds state-of-the-art approaches by 2-3% across different metrics, achieving a final average accuracy of 0.9034 in a multi-class scenario. Furthermore, the reported class activation maps evidence that our model is able to learn by itself the same patterns that clinicians consider relevant, without incurring prior annotation steps. This fact supposes a breakthrough in muscle-invasive bladder cancer grading which bridges the gap with respect to train the model on labelled data.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Semi-supervised%20Framework%20for%20Call%20Center%20Agent%20Malpractice%20Detection%20via%20Neural%20Feature%20Learning                                                                                  A Novel Semi-supervised Framework for Call Center Agent Malpractice Detection via Neural Feature Learning                                                                                  This work presents a practical solution to the problem of call center agent malpractice. A semi-supervised framework comprising of non-linear power transformation, neural feature learning and k-means clustering is outlined. We put these building blocks together and tune the parameters so that the best performance was obtained. The data used in the experiments is obtained from our in-house call center. It is made up of recorded agent-customer conversations which have been annotated using a convolutional neural network based segmenter. The methods provided a means of tuning the parameters of the neural network to achieve a desirable result. We show that, using our proposed framework, it is possible to significantly reduce the malpractice classification error of a k-means-only clustering model which would serve the same purpose. Additionally, by presenting the amount of silence per call as a key performance indicator, we show that the proposed system has enhanced agents performance at our call center since deployment.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Solution%20of%20Using%20Mixed%20Reality%20in%20Bowel%20and%20Oral%20and%20Maxillofacial%20Surgical%20Telepresence%3A%203D%20Mean%20Value%20Cloning%20algorithm                                                                                  A Novel Solution of Using Mixed Reality in Bowel and Oral and Maxillofacial Surgical Telepresence: 3D Mean Value Cloning algorithm                                                                                  Background and aim: Most of the Mixed Reality models used in the surgical telepresence are suffering from discrepancies in the boundary area and spatial-temporal inconsistency due to the illumination variation in the video frames. The aim behind this work is to propose a new solution that helps produce the composite video by merging the augmented video of the surgery site and the virtual hand of the remote expertise surgeon. The purpose of the proposed solution is to decrease the processing time and enhance the accuracy of merged video by decreasing the overlay and visualization error and removing occlusion and artefacts. Methodology: The proposed system enhanced the mean value cloning algorithm that helps to maintain the spatial-temporal consistency of the final composite video. The enhanced algorithm includes the 3D mean value coordinates and improvised mean value interpolant in the image cloning process, which helps to reduce the sawtooth, smudging and discolouration artefacts around the blending region. Results: As compared to the state of the art solution, the accuracy in terms of overlay error of the proposed solution is improved from 1.01mm to 0.80mm whereas the accuracy in terms of visualization error is improved from 98.8% to 99.4%. The processing time is reduced to 0.173 seconds from 0.211 seconds. Conclusion: Our solution helps make the object of interest consistent with the light intensity of the target image by adding the space distance that helps maintain the spatial consistency in the final merged video.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Surrogate-assisted%20Evolutionary%20Algorithm%20Applied%20to%20Partition-based%20Ensemble%20Learning                                                                                  A Novel Surrogate-assisted Evolutionary Algorithm Applied to Partition-based Ensemble Learning                                                                                  We propose a novel surrogate-assisted Evolutionary Algorithm for solving expensive combinatorial optimization problems. We integrate a surrogate model, which is used for fitness value estimation, into a state-of-the-art P3-like variant of the Gene-Pool Optimal Mixing Algorithm (GOMEA) and adapt the resulting algorithm for solving non-binary combinatorial problems. We test the proposed algorithm on an ensemble learning problem. Ensembling several models is a common Machine Learning technique to achieve better performance. We consider ensembles of several models trained on disjoint subsets of a dataset. Finding the best dataset partitioning is naturally a combinatorial non-binary optimization problem. Fitness function evaluations can be extremely expensive if complex models, such as Deep Neural Networks, are used as learners in an ensemble. Therefore, the number of fitness function evaluations is typically limited, necessitating expensive optimization techniques. In our experiments we use five classification datasets from the OpenML-CC18 benchmark and Support-vector Machines as learners in an ensemble. The proposed algorithm demonstrates better performance than alternative approaches, including Bayesian optimization algorithms. It manages to find better solutions using just several thousand fitness function evaluations for an ensemble learning problem with up to 500 variables.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Transformer%20Based%20Semantic%20Segmentation%20Scheme%20for%20Fine-Resolution%20Remote%20Sensing%20Images                                                                                  A Novel Transformer Based Semantic Segmentation Scheme for Fine-Resolution Remote Sensing Images                                                                                  The fully convolutional network (FCN) with an encoder-decoder architecture has been the standard paradigm for semantic segmentation. The encoder-decoder architecture utilizes an encoder to capture multilevel feature maps, which are incorporated into the final prediction by a decoder. As the context is crucial for precise segmentation, tremendous effort has been made to extract such information in an intelligent fashion, including employing dilated/atrous convolutions or inserting attention modules. However, these endeavors are all based on the FCN architecture with ResNet or other backbones, which cannot fully exploit the context from the theoretical concept. By contrast, we introduce the Swin Transformer as the backbone to extract the context information and design a novel decoder of densely connected feature aggregation module (DCFAM) to restore the resolution and produce the segmentation map. The experimental results on two remotely sensed semantic segmentation datasets demonstrate the effectiveness of the proposed scheme.Code is available at https://github.com/WangLibo1995/GeoSeg
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Transmission%20Policy%20for%20Intelligent%20Reflecting%20Surface%20Assisted%20Wireless%20Powered%20Sensor%20Networks                                                                                  A Novel Transmission Policy for Intelligent Reflecting Surface Assisted Wireless Powered Sensor Networks                                                                                  This paper proposes a novel transmission policy for an intelligent reflecting surface (IRS) assisted wireless powered sensor network (WPSN). An IRS is deployed to enhance the performance of wireless energy transfer (WET) and wireless information transfer (WIT) by intelligently adjusting phase shifts of each reflecting elements. To achieve its self-sustainability, the IRS needs to collect energy from energy station to support its control circuit operation. Our proposed policy for the considered WPSN is called IRS assisted harvest-then-transmit time switching, which is able to schedule the transmission time slots by switching between energy collection and energy reflection modes. We study the achievable sum throughput of the proposed transmission policy and investigate a joint design of the transmission time slots, the power allocation, as well as the discrete phase shifts of the WET and WIT. This formulates the problem as a mixed-integer non-linear program, which is NP-hard and non-convex. We first relax it to one with continuous phase shifts, and then propose a two-step approach and decompose the original problem into two sub-problems. We solve the first sub-problem with respect to the phase shifts of the WIT in terms of closed-form expression. For the second sub-problem, we consider a special case without the circuit power of each sensor node, the Lagrange dual method and the KKT conditions are applied to derive the optimal closed-form transmission time slots, power allocation, and phase shift of the WET. Then we generalise the case with the circuit power of each sensor node, which can be solved via employing a semi-definite programming relaxation. The optimal discrete phase shifts can be obtained by quantizing the continuous values. Numerical results demonstrate the effectiveness of the proposed policy and validate the beneficial role of the IRS in comparison to the benchmark schemes.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Unified%20Framework%20for%20Solving%20Reachability%2C%20Viability%20and%20Invariance%20Problems                                                                                  A Novel Unified Framework for Solving Reachability, Viability and Invariance Problems                                                                                  The level set method is a widely used tool for solving reachability and invariance problems. However, some shortcomings, such as the difficulties of handling dissipation function and constructing terminal conditions for solving the Hamilton-Jacobi partial differential equation, limit the application of the level set method in some problems with non-affine nonlinear systems and irregular target sets. This paper proposes a method that can effectively avoid the above tricky issues and thus has better generality. In the proposed method, the reachable or invariant sets with different time horizons are characterized by some non-zero sublevel sets of a value function. This value function is not obtained by solving a viscosity solution of the partial differential equation but by recursion and interpolation approximation. At the end of this paper, some examples are taken to illustrate the accuracy and generality of the proposed method.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Unified%20Model%20for%20Multi-exposure%20Stereo%20Coding%20Based%20on%20Low%20Rank%20Tucker-ALS%20and%203D-HEVC                                                                                  A Novel Unified Model for Multi-exposure Stereo Coding Based on Low Rank Tucker-ALS and 3D-HEVC                                                                                  Display technology must offer high dynamic range (HDR) contrast-based depth induction and 3D personalization simultaneously. Efficient algorithms to compress HDR stereo data is critical. Direct capturing of HDR content is complicated due to the high expense and scarcity of HDR cameras. The HDR 3D images could be generated in low-cost by fusing low-dynamic-range (LDR) images acquired using a stereo camera with various exposure settings. In this paper, an efficient scheme for coding multi-exposure stereo images is proposed based on a tensor low-rank approximation scheme. The multi-exposure fusion can be realized to generate HDR stereo output at the decoder for increased realism and exaggerated binocular 3D depth cues. For exploiting spatial redundancy in LDR stereo images, the stack of multi-exposure stereo images is decomposed into a set of projection matrices and a core tensor following an alternating least squares Tucker decomposition model. The compact, low-rank representation of the scene, thus, generated is further processed by 3D extension of High Efficiency Video Coding standard. The encoding with 3D-HEVC enhance the proposed scheme efficiency by exploiting intra-frame, inter-view and the inter-component redundancies in low-rank approximated representation. We consider constant luminance property of IPT and Y'CbCr color space to precisely approximate intensity prediction and perceptually minimize the encoding distortion. Besides, the proposed scheme gives flexibility to adjust the bitrate of tensor latent components by changing the rank of core tensor and its quantization. Extensive experiments on natural scenes demonstrate that the proposed scheme outperforms state-of-the-art JPEG-XT and 3D-HEVC range coding standards.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Upsampling%20and%20Context%20Convolution%20for%20Image%20Semantic%20Segmentation                                                                                  A Novel Upsampling and Context Convolution for Image Semantic Segmentation                                                                                  Semantic segmentation, which refers to pixel-wise classification of an image, is a fundamental topic in computer vision owing to its growing importance in robot vision and autonomous driving industries. It provides rich information about objects in the scene such as object boundary, category, and location. Recent methods for semantic segmentation often employ an encoder-decoder structure using deep convolutional neural networks. The encoder part extracts feature of the image using several filters and pooling operations, whereas the decoder part gradually recovers the low-resolution feature maps of the encoder into a full input resolution feature map for pixel-wise prediction. However, the encoder-decoder variants for semantic segmentation suffer from severe spatial information loss, caused by pooling operations or convolutions with stride, and does not consider the context in the scene. In this paper, we propose a dense upsampling convolution method based on guided filtering to effectively preserve the spatial information of the image in the network. We further propose a novel local context convolution method that not only covers larger-scale objects in the scene but covers them densely for precise object boundary delineation. Theoretical analyses and experimental results on several benchmark datasets verify the effectiveness of our method. Qualitatively, our approach delineates object boundaries at a level of accuracy that is beyond the current excellent methods. Quantitatively, we report a new record of 82.86% and 81.62% of pixel accuracy on ADE20K and Pascal-Context benchmark datasets, respectively. In comparison with the state-of-the-art methods, the proposed method offers promising improvements.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20Visualization%20System%20of%20Using%20Augmented%20Reality%20in%20Knee%20Replacement%20Surgery%3A%20Enhanced%20Bidirectional%20Maximum%20Correntropy%20Algorithm                                                                                  A Novel Visualization System of Using Augmented Reality in Knee Replacement Surgery: Enhanced Bidirectional Maximum Correntropy Algorithm                                                                                  Background and aim: Image registration and alignment are the main limitations of augmented reality-based knee replacement surgery. This research aims to decrease the registration error, eliminate outcomes that are trapped in local minima to improve the alignment problems, handle the occlusion, and maximize the overlapping parts. Methodology: markerless image registration method was used for Augmented reality-based knee replacement surgery to guide and visualize the surgical operation. While weight least square algorithm was used to enhance stereo camera-based tracking by filling border occlusion in right to left direction and non-border occlusion from left to right direction. Results: This study has improved video precision to 0.57 mm~0.61 mm alignment error. Furthermore, with the use of bidirectional points, for example, forwards and backwards directional cloud point, the iteration on image registration was decreased. This has led to improve the processing time as well. The processing time of video frames was improved to 7.4~11.74 fps. Conclusions: It seems clear that this proposed system has focused on overcoming the misalignment difficulty caused by movement of patient and enhancing the AR visualization during knee replacement surgery. The proposed system was reliable and favorable which helps in eliminating alignment error by ascertaining the optimal rigid transformation between two cloud points and removing the outliers and non-Gaussian noise. The proposed augmented reality system helps in accurate visualization and navigation of anatomy of knee such as femur, tibia, cartilage, blood vessels, etc.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20lightweight%20Convolutional%20Neural%20Network%2C%20ExquisiteNetV2                                                                                  A Novel lightweight Convolutional Neural Network, ExquisiteNetV2                                                                                  In the paper of ExquisiteNetV1, the ability of classification of ExquisiteNetV1 is worse than DenseNet. In this article, we propose a faster and better model ExquisiteNetV2. We conduct many experiments to evaluate its performance. We test ExquisiteNetV2, ExquisiteNetV1 and other 9 well-known models on 15 credible datasets under the same condition. According to the experimental results, ExquisiteNetV2 gets the highest classification accuracy over half of the datasets. Important of all, ExquisiteNetV2 has fewest amounts of parameters. Besides, in most instances, ExquisiteNetV2 has fastest computing speed.
http://w3id.org/mlsea/pwc/scientificWork/A%20Novel%20mapping%20for%20visual%20to%20auditory%20sensory%20substitution                                                                                  A Novel mapping for visual to auditory sensory substitution                                                                                  visual information can be converted into audio stream via sensory substitution devices in order to give visually impaired people the chance of perception of their surrounding easily and simultaneous to performing everyday tasks. In this study, visual environmental features namely, coordinate, type of objects and their size are assigned to audio features related to music tones such as frequency, time duration and note permutations. Results demonstrated that this new method has more training time efficiency in comparison with our previous method named VBTones which sinusoidal tones were applied. Moreover, results in blind object recognition for real objects was achieved 88.05 on average.
http://w3id.org/mlsea/pwc/scientificWork/A%20Numerical%20Approach%20to%20Pricing%20Exchange%20Options%20under%20Stochastic%20Volatility%20and%20Jump-Diffusion%20Dynamics                                                                                  A Numerical Approach to Pricing Exchange Options under Stochastic Volatility and Jump-Diffusion Dynamics                                                                                  We consider a method of lines (MOL) approach to determine prices of European and American exchange options when underlying asset prices are modelled with stochastic volatility and jump-diffusion dynamics. As the MOL, as with any other numerical scheme for PDEs, becomes increasingly complex when higher dimensions are involved, we first simplify the problem by transforming the exchange option into a call option written on the ratio of the yield processes of the two assets. This is achieved by taking the second asset yield process as the numeraire. We also characterize the near-maturity behavior of the early exercise boundary of the American exchange option and analyze how model parameters affect this behavior. Using the MOL scheme, we conduct a numerical comparative static analysis of exchange option prices with respect to the model parameters and investigate the impact of stochastic volatility and jumps to option prices. We also consider the effect of boundary conditions at far-but-finite limits of the computational domain on the overall efficiency of the MOL scheme. Toward these objectives, a brief exposition of the MOL and how it can be implemented on computing software are provided.
http://w3id.org/mlsea/pwc/scientificWork/A%20One-Shot%20Texture-Perceiving%20Generative%20Adversarial%20Network%20for%20Unsupervised%20Surface%20Inspection                                                                                  A One-Shot Texture-Perceiving Generative Adversarial Network for Unsupervised Surface Inspection                                                                                  Visual surface inspection is a challenging task owing to the highly diverse appearance of target surfaces and defective regions. Previous attempts heavily rely on vast quantities of training examples with manual annotation. However, in some practical cases, it is difficult to obtain a large number of samples for inspection. To combat it, we propose a hierarchical texture-perceiving generative adversarial network (HTP-GAN) that is learned from the one-shot normal image in an unsupervised scheme. Specifically, the HTP-GAN contains a pyramid of convolutional GANs that can capture the global structure and fine-grained representation of an image simultaneously. This innovation helps distinguishing defective surface regions from normal ones. In addition, in the discriminator, a texture-perceiving module is devised to capture the spatially invariant representation of normal image via directional convolutions, making it more sensitive to defective areas. Experiments on a variety of datasets consistently demonstrate the effectiveness of our method.
http://w3id.org/mlsea/pwc/scientificWork/A%20PMU-Based%20Machine%20Learning%20Application%20for%20Fast%20Detection%20of%20Forced%20Oscillations%20from%20Wind%20Farms                                                                                  A PMU-Based Machine Learning Application for Fast Detection of Forced Oscillations from Wind Farms                                                                                  Today's evolving power system contains an increasing amount of power electronic interfaced energy sources and loads that require a paradigm shift in utility operations. Sub-synchronous oscillations at frequencies around 13-15 Hz, for instance, have been reported by utilities due to wind farm controller interactions with the grid. Dynamics at such frequencies are unobservable by most SCADA tools due to low sampling frequencies and lack of synchronization. Real-time or off-line frequency domain analysis of phasor measurement unit (PMU) data has become a valuable method to identify such phenomena, at the expense of costly power system data and communication infrastructure. This article proposes an alternative machine learning (ML) based application for sub-synchronous oscillation detection in wind farm applications. The application is targeted for real-time implementation at the edge, resulting in significant savings in terms of data and communication requirements. Validation is performed using data from a North American wind farm operator.
http://w3id.org/mlsea/pwc/scientificWork/A%20PSO%20Strategy%20of%20Finding%20Relevant%20Web%20Documents%20using%20a%20New%20Similarity%20Measure                                                                                  A PSO Strategy of Finding Relevant Web Documents using a New Similarity Measure                                                                                  In the world of the Internet and World Wide Web, which offers a tremendous amount of information, an increasing emphasis is being given to searching services and functionality. Currently, a majority of web portals offer their searching utilities, be it better or worse. These can search for the content within the sites, mainly text the textual content of documents. In this paper a novel similarity measure called SMDR (Similarity Measure for Documents Retrieval) is proposed to help retrieve more similar documents from the repository thus contributing considerably to the effectiveness of Web Information Retrieval (WIR) process. Bio-inspired PSO methodology is used with the intent to reduce the response time of the system and optimizes WIR process, hence contributes to the efficiency of the system. This paper also demonstrates a comparative study of the proposed system with the existing method in terms of accuracy, sensitivity, F-measure and specificity. Finally, extensive experiments are conducted on CACM collections. Better precision-recall rates are achieved than the existing system. Experimental results demonstrate the effectiveness and efficiency of the proposed system.
http://w3id.org/mlsea/pwc/scientificWork/A%20Peek%20Into%20the%20Reasoning%20of%20Neural%20Networks%3A%20Interpreting%20with%20Structural%20Visual%20Concepts                                                                                  A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts                                                                                  Despite substantial progress in applying neural networks (NN) to a wide variety of areas, they still largely suffer from a lack of transparency and interpretability. While recent developments in explainable artificial intelligence attempt to bridge this gap (e.g., by visualizing the correlation between input pixels and final outputs), these approaches are limited to explaining low-level relationships, and crucially, do not provide insights on error correction. In this work, we propose a framework (VRX) to interpret classification NNs with intuitive structural visual concepts. Given a trained classification model, the proposed VRX extracts relevant class-specific visual concepts and organizes them using structural concept graphs (SCG) based on pairwise concept relationships. By means of knowledge distillation, we show VRX can take a step towards mimicking the reasoning process of NNs and provide logical, concept-level explanations for final model decisions. With extensive experiments, we empirically show VRX can meaningfully answer 'why' and 'why not' questions about the prediction, providing easy-to-understand insights about the reasoning process. We also show that these insights can potentially provide guidance on improving NN's performance.
http://w3id.org/mlsea/pwc/scientificWork/A%20Perceptual%20Distortion%20Reduction%20Framework%3A%20Towards%20Generating%20Adversarial%20Examples%20with%20High%20Perceptual%20Quality%20and%20Attack%20Success%20Rate                                                                                  A Perceptual Distortion Reduction Framework: Towards Generating Adversarial Examples with High Perceptual Quality and Attack Success Rate                                                                                  Most of the adversarial attack methods suffer from large perceptual distortions such as visible artifacts, when the attack strength is relatively high. These perceptual distortions contain a certain portion which contributes less to the attack success rate. This portion of distortions, which is induced by unnecessary modifications and lack of proper perceptual distortion constraint, is the target of the proposed framework. In this paper, we propose a perceptual distortion reduction framework to tackle this problem from two perspectives. Firstly, we propose a perceptual distortion constraint and add it into the objective function to jointly optimize the perceptual distortions and attack success rate. Secondly, we propose an adaptive penalty factor $ lambda$ to balance the discrepancies between different samples. Since SGD and Momentum-SGD cannot optimize our complex non-convex problem, we exploit Adam in optimization. Extensive experiments have verified the superiority of our proposed framework.
http://w3id.org/mlsea/pwc/scientificWork/A%20Perturbation%20Approach%20to%20Optimal%20Investment%2C%20Liability%20Ratio%2C%20and%20Dividend%20Strategies                                                                                  A Perturbation Approach to Optimal Investment, Liability Ratio, and Dividend Strategies                                                                                  We study an optimal dividend problem for an insurer who simultaneously controls investment weights in a financial market, liability ratio in the insurance business, and dividend payout rate. The insurer seeks an optimal strategy to maximize her expected utility of dividend payments over an infinite horizon. By applying a perturbation approach, we obtain the optimal strategy and the value function in closed form for log and power utility. We conduct an economic analysis to investigate the impact of various model parameters and risk aversion on the insurer's optimal strategy.
http://w3id.org/mlsea/pwc/scientificWork/A%20Phase%20Theory%20of%20MIMO%20LTI%20Systems                                                                                  A Phase Theory of MIMO LTI Systems                                                                                  In this paper, we define the phase response for a class of multi-input multi-output (MIMO) linear time-invariant (LTI) systems whose frequency responses are (semi-)sectorial at all frequencies. The newly defined phase concept subsumes the well-known notions of positive real systems and negative imaginary systems. We formulate a small phase theorem for feedback stability, which complements the celebrated small gain theorem. The small phase theorem lays the foundation of a phase theory of MIMO systems. We also discuss time-domain interpretations of phase-bounded systems via both energy signal analysis and power signal analysis. In addition, a sectored real lemma is derived for the computation of MIMO phases, which serves as a natural counterpart of the bounded real lemma.
http://w3id.org/mlsea/pwc/scientificWork/A%20Photonic-Circuits-Inspired%20Compact%20Network%3A%20Toward%20Real-Time%20Wireless%20Signal%20Classification%20at%20the%20Edge                                                                                  A Photonic-Circuits-Inspired Compact Network: Toward Real-Time Wireless Signal Classification at the Edge                                                                                  Machine learning (ML) methods are ubiquitous in wireless communication systems and have proven powerful for applications including radio-frequency (RF) fingerprinting, automatic modulation classification, and cognitive radio. However, the large size of ML models can make them difficult to implement on edge devices for latency-sensitive downstream tasks. In wireless communication systems, ML data processing at a sub-millisecond scale will enable real-time network monitoring to improve security and prevent infiltration. In addition, compact and integratable hardware platforms which can implement ML models at the chip scale will find much broader application to wireless communication networks. Toward real-time wireless signal classification at the edge, we propose a novel compact deep network that consists of a photonic-hardware-inspired recurrent neural network model in combination with a simplified convolutional classifier, and we demonstrate its application to the identification of RF emitters by their random transmissions. With the proposed model, we achieve 96.32% classification accuracy over a set of 30 identical ZigBee devices when using 50 times fewer training parameters than an existing state-of-the-art CNN classifier. Thanks to the large reduction in network size, we demonstrate real-time RF fingerprinting with 0.219 ms latency using a small-scale FPGA board, the PYNQ-Z1.
http://w3id.org/mlsea/pwc/scientificWork/A%20Physics-Constrained%20Deep%20Learning%20Model%20for%20Simulating%20Multiphase%20Flow%20in%203D%20Heterogeneous%20Porous%20Media                                                                                  A Physics-Constrained Deep Learning Model for Simulating Multiphase Flow in 3D Heterogeneous Porous Media                                                                                  In this work, an efficient physics-constrained deep learning model is developed for solving multiphase flow in 3D heterogeneous porous media. The model fully leverages the spatial topology predictive capability of convolutional neural networks, and is coupled with an efficient continuity-based smoother to predict flow responses that need spatial continuity. Furthermore, the transient regions are penalized to steer the training process such that the model can accurately capture flow in these regions. The model takes inputs including properties of porous media, fluid properties and well controls, and predicts the temporal-spatial evolution of the state variables (pressure and saturation). While maintaining the continuity of fluid flow, the 3D spatial domain is decomposed into 2D images for reducing training cost, and the decomposition results in an increased number of training data samples and better training efficiency. Additionally, a surrogate model is separately constructed as a postprocessor to calculate well flow rate based on the predictions of state variables from the deep learning model. We use the example of CO2 injection into saline aquifers, and apply the physics-constrained deep learning model that is trained from physics-based simulation data and emulates the physics process. The model performs prediction with a speedup of ~1400 times compared to physics-based simulations, and the average temporal errors of predicted pressure and saturation plumes are 0.27% and 0.099% respectively. Furthermore, water production rate is efficiently predicted by a surrogate model for well flow rate, with a mean error less than 5%. Therefore, with its unique scheme to cope with the fidelity in fluid flow in porous media, the physics-constrained deep learning model can become an efficient predictive model for computationally demanding inverse problems or other coupled processes.
http://w3id.org/mlsea/pwc/scientificWork/A%20Physics-Informed%20Deep%20Learning%20Paradigm%20for%20Traffic%20State%20and%20Fundamental%20Diagram%20Estimation                                                                                  A Physics-Informed Deep Learning Paradigm for Traffic State and Fundamental Diagram Estimation                                                                                  Traffic state estimation (TSE) bifurcates into two categories, model-driven and data-driven (e.g., machine learning, ML), while each suffers from either deficient physics or small data. To mitigate these limitations, recent studies introduced a hybrid paradigm, physics-informed deep learning (PIDL), which contains both model-driven and data-driven components. This paper contributes an improved version, called physics-informed deep learning with a fundamental diagram learner (PIDL+FDL), which integrates ML terms into the model-driven component to learn a functional form of a fundamental diagram (FD), i.e., a mapping from traffic density to flow or velocity. The proposed PIDL+FDL has the advantages of performing the TSE learning, model parameter identification, and FD estimation simultaneously. We demonstrate the use of PIDL+FDL to solve popular first-order and second-order traffic flow models and reconstruct the FD relation as well as model parameters that are outside the FD terms. We then evaluate the PIDL+FDL-based TSE using the Next Generation SIMulation (NGSIM) dataset. The experimental results show the superiority of the PIDL+FDL in terms of improved estimation accuracy and data efficiency over advanced baseline TSE methods, and additionally, the capacity to properly learn the unknown underlying FD relation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Physics-based%20and%20Data-driven%20Linear%20Three-Phase%20Power%20Flow%20Model%20for%20Distribution%20Power%20Systems                                                                                  A Physics-based and Data-driven Linear Three-Phase Power Flow Model for Distribution Power Systems                                                                                  Distribution power systems (DPSs) are mostly unbalanced, and their loads may have notable static voltage characteristics (ZIP loads). Hence, despite abundant papers on linear single-phase power flow models, it is still necessary to study linear three-phase distribution power flow models. To this end, this paper proposes a physics-based and data-driven linear three-phase power flow model for DPSs. We first formulate how to amalgamate data-driven techniques into a physics-based power flow model to obtain our linear model. This amalgamation makes our linear model independent of the assumptions commonly used in the literature (e.g., nodal voltages are nearly 1.0 p.u.) and thus have a relatively high accuracy generally - even when those assumptions become invalid. We then reveal how to apply our model to the DPSs with ZIP loads. We also show that with the Huber penalty function employed, the adverse impact of bad data on our model's accuracy is significantly reduced, rendering our model robust against poor data quality. Case studies have demonstrated that our model generally has 2 to over 10-fold smaller average errors than other linear power flow models, enjoys a satisfying accuracy against bad data, and facilitates a faster solution to DPS analysis and optimization problems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Picture%20May%20Be%20Worth%20a%20Hundred%20Words%20for%20Visual%20Question%20Answering                                                                                  A Picture May Be Worth a Hundred Words for Visual Question Answering                                                                                  How far can we go with textual representations for understanding pictures? In image understanding, it is essential to use concise but detailed image representations. Deep visual features extracted by vision models, such as Faster R-CNN, are prevailing used in multiple tasks, and especially in visual question answering (VQA). However, conventional deep visual features may struggle to convey all the details in an image as we humans do. Meanwhile, with recent language models' progress, descriptive text may be an alternative to this problem. This paper delves into the effectiveness of textual representations for image understanding in the specific context of VQA. We propose to take description-question pairs as input, instead of deep visual features, and fed them into a language-only Transformer model, simplifying the process and the computational cost. We also experiment with data augmentation techniques to increase the diversity in the training set and avoid learning statistical bias. Extensive evaluations have shown that textual representations require only about a hundred words to compete with deep visual features on both VQA 2.0 and VQA-CP v2.
http://w3id.org/mlsea/pwc/scientificWork/A%20Picture%20is%20Worth%20a%20Collaboration%3A%20Accumulating%20Design%20Knowledge%20for%20Computer-Vision-based%20Hybrid%20Intelligence%20Systems                                                                                  A Picture is Worth a Collaboration: Accumulating Design Knowledge for Computer-Vision-based Hybrid Intelligence Systems                                                                                  Computer vision (CV) techniques try to mimic human capabilities of visual perception to support labor-intensive and time-consuming tasks like the recognition and localization of critical objects. Nowadays, CV increasingly relies on artificial intelligence (AI) to automatically extract useful information from images that can be utilized for decision support and business process automation. However, the focus of extant research is often exclusively on technical aspects when designing AI-based CV systems while neglecting socio-technical facets, such as trust, control, and autonomy. For this purpose, we consider the design of such systems from a hybrid intelligence (HI) perspective and aim to derive prescriptive design knowledge for CV-based HI systems. We apply a reflective, practice-inspired design science approach and accumulate design knowledge from six comprehensive CV projects. As a result, we identify four design-related mechanisms (i.e., automation, signaling, modification, and collaboration) that inform our derived meta-requirements and design principles. This can serve as a basis for further socio-technical research on CV-based HI systems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Pilot%20Study%20of%20Smart%20Agricultural%20Irrigation%20using%20Unmanned%20Aerial%20Vehicles%20and%20IoT-Based%20Cloud%20System                                                                                  A Pilot Study of Smart Agricultural Irrigation using Unmanned Aerial Vehicles and IoT-Based Cloud System                                                                                  This article introduces a new mobile-based application of modern information and communication technology in agriculture based on Internet of Things (IoT), embedded systems and an unmanned aerial vehicle (UAV). The proposed agricultural monitoring system was designed and implemented using Arduino microcontroller boards, Wi-Fi modules, water pumps and electronic environmental sensors, namely temperature, humidity and soil moisture. The role of UAV in this study is to collect these environmental data from different regions of the farm. Then, the quantity of water irrigation is automatically computed for each region in the cloud. Moreover, the developed system can monitor the farm conditions including the water requirements remotely on Android mobile application to guide the farmers. The results of this study demonstrated that our proposed IoT-based embedded system can be effective to avoid unnecessary and wasted water irrigation within the framework of smart agriculture.
http://w3id.org/mlsea/pwc/scientificWork/A%20Pointer%20Network%20Architecture%20for%20Joint%20Morphological%20Segmentation%20and%20Tagging                                                                                  A Pointer Network Architecture for Joint Morphological Segmentation and Tagging                                                                                  Morphologically Rich Languages (MRLs) such as Arabic, Hebrew and Turkish often require Morphological Disambiguation (MD), i.e., the prediction of morphological decomposition of tokens into morphemes, early in the pipeline. Neural MD may be addressed as a simple pipeline, where segmentation is followed by sequence tagging, or as an end-to-end model, predicting morphemes from raw tokens. Both approaches are sub-optimal; the former is heavily prone to error propagation, and the latter does not enjoy explicit access to the basic processing units called morphemes. This paper offers MD architecture that combines the symbolic knowledge of morphemes with the learning capacity of neural end-to-end modeling. We propose a new, general and easy-to-implement Pointer Network model where the input is a morphological lattice and the output is a sequence of indices pointing at a single disambiguated path of morphemes. We demonstrate the efficacy of the model on segmentation and tagging, for Hebrew and Turkish texts, based on their respective Universal Dependencies (UD) treebanks. Our experiments show that with complete lattices, our model outperforms all shared-task results on segmenting and tagging these languages. On the SPMRL treebank, our model outperforms all previously reported results for Hebrew MD in realistic scenarios.
http://w3id.org/mlsea/pwc/scientificWork/A%20Portfolio%20Choice%20Problem%20Under%20Risk%20Capacity%20Constraint                                                                                  A Portfolio Choice Problem Under Risk Capacity Constraint                                                                                  This paper studies an optimal investing problem for a retiree facing longevity risk and living standard risk. We formulate the investing problem as a portfolio choice problem under a time-varying risk capacity constraint. We derive the optimal investment strategy under the specific condition on model parameters in terms of second-order ordinary differential equations. We demonstrate an endogenous number that measures the expected value to sustain the spending post-retirement. The optimal portfolio is nearly neutral to the stock market movement if the portfolio's value is higher than this number; but, if the portfolio is not worth enough to sustain the retirement spending, the retiree actively invests in the stock market for the higher expected return. Besides, we solve an optimal portfolio choice problem under a leverage constraint and show that the optimal portfolio would lose significantly in stressed markets. This paper shows that the time-varying risk capacity constraint has important implications for asset allocation in retirement.
http://w3id.org/mlsea/pwc/scientificWork/A%20Practical%20Approach%20for%20Rate-Distortion-Perception%20Analysis%20in%20Learned%20Image%20Compression                                                                                  A Practical Approach for Rate-Distortion-Perception Analysis in Learned Image Compression                                                                                  Rate-distortion optimization (RDO) of codecs, where distortion is quantified by the mean-square error, has been a standard practice in image/video compression over the years. RDO serves well for optimization of codec performance for evaluation of the results in terms of PSNR. However, it is well known that the PSNR does not correlate well with perceptual evaluation of images; hence, RDO is not well suited for perceptual optimization of codecs. Recently, rate-distortion-perception trade-off has been formalized by taking the Kullback-Leibner (KL) divergence between the distributions of the original and reconstructed images as a perception measure. Learned image compression methods that simultaneously optimize rate, mean-square loss, VGG loss, and an adversarial loss were proposed. Yet, there exists no easy approach to fix the rate, distortion or perception at a desired level in a practical learned image compression solution to perform an analysis of the trade-off between rate, distortion and perception measures. In this paper, we propose a practical approach to fix the rate to carry out perception-distortion analysis at a fixed rate in order to perform perceptual evaluation of image compression results in a principled manner. Experimental results provide several insights for practical rate-distortion-perception analysis in learned image compression.
http://w3id.org/mlsea/pwc/scientificWork/A%20Practical%20Assessment%20of%20the%20Power%20Grid%20Inertia%20Constant%20Using%20PMUs                                                                                  A Practical Assessment of the Power Grid Inertia Constant Using PMUs                                                                                  Installation of phasor measurement units (PMUs) in a number of substations in the power grid can help assess a set of its values and parameters, in particular those related to the dynamics when disturbances occur in the system. Inertia constant of the power grid is one of the system stability related parameters that is essential for planning of the system spinning reserve. Estimates of the grid inertia constant require precise information of the system frequency at the time of disturbance. In this paper, an improved method for such estimate is presented, which is based on the derived data of frequency variations obtained from PMUs. In addition, a way to obtain the appropriate interval for the lowest error estimate is discussed. The method has been applied to assess the inertia constant of the Iranian power grid, by obtaining the rate of change of frequency after a disturbance in the defined interval, and the rate of specified power imbalance between electrical power input and output.
http://w3id.org/mlsea/pwc/scientificWork/A%20Practical%20Method%20for%20Constructing%20Equivariant%20Multilayer%20Perceptrons%20for%20Arbitrary%20Matrix%20Groups                                                                                  A Practical Method for Constructing Equivariant Multilayer Perceptrons for Arbitrary Matrix Groups                                                                                  Symmetries and equivariance are fundamental to the generalization of neural networks on domains such as images, graphs, and point clouds. Existing work has primarily focused on a small number of groups, such as the translation, rotation, and permutation groups. In this work we provide a completely general algorithm for solving for the equivariant layers of matrix groups. In addition to recovering solutions from other works as special cases, we construct multilayer perceptrons equivariant to multiple groups that have never been tackled before, including $ mathrm{O}(1,3)$, $ mathrm{O}(5)$, $ mathrm{Sp}(n)$, and the Rubik's cube group. Our approach outperforms non-equivariant baselines, with applications to particle physics and dynamical systems. We release our software library to enable researchers to construct equivariant layers for arbitrary matrix groups.
http://w3id.org/mlsea/pwc/scientificWork/A%20Practical%20Proposal%20for%20State%20Estimation%20at%20Balanced%2C%20Radial%20Distribution%20Systems                                                                                  A Practical Proposal for State Estimation at Balanced, Radial Distribution Systems                                                                                  The ever-increasing deployment of distributed resources and the opportunities offered to loads for more active roles has changed the previously unidirectional and relatively straight-forward operating profile of distribution systems (DS). DS will be required to be monitored closely for robustness and sufficient power quality. State estimation of transmission systems has consistently served as a monitoring tool, which drives system-wide control actions and, thus, ensures the operational integrity of the electric grid. An update to the classic state estimation for the case of DS is offered in this work, based on a power flow formulation for radial networks that does not require measurements or estimate of the voltage angles.
http://w3id.org/mlsea/pwc/scientificWork/A%20Pre-training%20Oracle%20for%20Predicting%20Distances%20in%20Social%20Networks                                                                                  A Pre-training Oracle for Predicting Distances in Social Networks                                                                                  In this paper, we propose a novel method to make distance predictions in real-world social networks. As predicting missing distances is a difficult problem, we take a two-stage approach. Structural parameters for families of synthetic networks are first estimated from a small set of measurements of a real-world network and these synthetic networks are then used to pre-train the predictive neural networks. Since our model first searches for the most suitable synthetic graph parameters which can be used as an 'oracle' to create arbitrarily large training data sets, we call our approach 'Oracle Search Pre-training' (OSP). For example, many real-world networks exhibit a Power law structure in their node degree distribution, so a Power law model can provide a foundation for the desired oracle to generate synthetic pre-training networks, if the appropriate Power law graph parameters can be estimated. Accordingly, we conduct experiments on real-world Facebook, Email, and Train Bombing networks and show that OSP outperforms models without pre-training, models pre-trained with inaccurate parameters, and other distance prediction schemes such as Low-rank Matrix Completion. In particular, we achieve a prediction error of less than one hop with only 1% of sampled distances from the social network. OSP can be easily extended to other domains such as random networks by choosing an appropriate model to generate synthetic training data, and therefore promises to impact many different network learning problems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Precise%20Performance%20Analysis%20of%20Support%20Vector%20Regression                                                                                  A Precise Performance Analysis of Support Vector Regression                                                                                  In this paper, we study the hard and soft support vector regression techniques applied to a set of $n$ linear measurements of the form $y_i= boldsymbol{ beta}_ star^{T}{ bf x}_i +n_i$ where $ boldsymbol{ beta}_ star$ is an unknown vector, $ left {{ bf x}_i right }_{i=1}^n$ are the feature vectors and $ left {{n}_i right }_{i=1}^n$ model the noise. Particularly, under some plausible assumptions on the statistical distribution of the data, we characterize the feasibility condition for the hard support vector regression in the regime of high dimensions and, when feasible, derive an asymptotic approximation for its risk. Similarly, we study the test risk for the soft support vector regression as a function of its parameters. Our results are then used to optimally tune the parameters intervening in the design of hard and soft support vector regression algorithms. Based on our analysis, we illustrate that adding more samples may be harmful to the test performance of support vector regression, while it is always beneficial when the parameters are optimally selected. Such a result reminds a similar phenomenon observed in modern learning architectures according to which optimally tuned architectures present a decreasing test performance curve with respect to the number of samples.
http://w3id.org/mlsea/pwc/scientificWork/A%20Preference%20Random%20Walk%20Algorithm%20for%20Link%20Prediction%20through%20Mutual%20Influence%20Nodes%20in%20Complex%20Networks                                                                                  A Preference Random Walk Algorithm for Link Prediction through Mutual Influence Nodes in Complex Networks                                                                                  Predicting links in complex networks has been one of the essential topics within the realm of data mining and science discovery over the past few years. This problem remains an attempt to identify future, deleted, and redundant links using the existing links in a graph. Local random walk is considered to be one of the most well-known algorithms in the category of quasi-local methods. It traverses the network using the traditional random walk with a limited number of steps, randomly selecting one adjacent node in each step among the nodes which have equal importance. Then this method uses the transition probability between node pairs to calculate the similarity between them. However, in most datasets, this method is not able to perform accurately in scoring remarkably similar nodes. In the present article, an efficient method is proposed for improving local random walk by encouraging random walk to move, in every step, towards the node which has a stronger influence. Therefore, the next node is selected according to the influence of the source node. To do so, using mutual information, the concept of the asymmetric mutual influence of nodes is presented. A comparison between the proposed method and other similarity-based methods (local, quasi-local, and global) has been performed, and results have been reported for 11 real-world networks. It had a higher prediction accuracy compared with other link prediction approaches.
http://w3id.org/mlsea/pwc/scientificWork/A%20Preliminary%20Study%20of%20a%20Two-Stage%20Paradigm%20for%20Preserving%20Speaker%20Identity%20in%20Dysarthric%20Voice%20Conversion                                                                                  A Preliminary Study of a Two-Stage Paradigm for Preserving Speaker Identity in Dysarthric Voice Conversion                                                                                  We propose a new paradigm for maintaining speaker identity in dysarthric voice conversion (DVC). The poor quality of dysarthric speech can be greatly improved by statistical VC, but as the normal speech utterances of a dysarthria patient are nearly impossible to collect, previous work failed to recover the individuality of the patient. In light of this, we suggest a novel, two-stage approach for DVC, which is highly flexible in that no normal speech of the patient is required. First, a powerful parallel sequence-to-sequence model converts the input dysarthric speech into a normal speech of a reference speaker as an intermediate product, and a nonparallel, frame-wise VC model realized with a variational autoencoder then converts the speaker identity of the reference speech back to that of the patient while assumed to be capable of preserving the enhanced quality. We investigate several design options. Experimental evaluation results demonstrate the potential of our approach to improving the quality of the dysarthric speech while maintaining the speaker identity.
http://w3id.org/mlsea/pwc/scientificWork/A%20Pricing%20Mechanism%20to%20Jointly%20Mitigate%20Market%20Power%20and%20Environmental%20Externalities%20in%20Electricity%20Markets                                                                                  A Pricing Mechanism to Jointly Mitigate Market Power and Environmental Externalities in Electricity Markets                                                                                  The electricity industry has been one of the first to face technological changes motivated by sustainability concerns. Whilst efficiency aspects of market design have tended to focus upon market power concerns, the new policy challenges emphasise sustainability. We argue that market designs need to develop remedies for market conduct integrated with regard to environmental externalities. Accordingly, we develop an incentive-based market clearing mechanism using a power network representation with a distinctive feature of incomplete information regarding generation costs. The shortcomings of price caps to mitigate market power, in this context, are overcome with the proposed mechanism.
http://w3id.org/mlsea/pwc/scientificWork/A%20Primer%20on%20Multi-Neuron%20Relaxation-based%20Adversarial%20Robustness%20Certification                                                                                  A Primer on Multi-Neuron Relaxation-based Adversarial Robustness Certification                                                                                  The existence of adversarial examples poses a real danger when deep neural networks are deployed in the real world. The go-to strategy to quantify this vulnerability is to evaluate the model against specific attack algorithms. This approach is however inherently limited, as it says little about the robustness of the model against more powerful attacks not included in the evaluation. We develop a unified mathematical framework to describe relaxation-based robustness certification methods, which go beyond adversary-specific robustness evaluation and instead provide provable robustness guarantees against attacks by any adversary. We discuss the fundamental limitations posed by single-neuron relaxations and show how the recent ``k-ReLU'' multi-neuron relaxation framework of Singh et al. (2019) obtains tighter correlation-aware activation bounds by leveraging additional relational constraints among groups of neurons. Specifically, we show how additional pre-activation bounds can be mapped to corresponding post-activation bounds and how they can in turn be used to obtain tighter robustness certificates. We also present an intuitive way to visualize different relaxation-based certification methods. By approximating multiple non-linearities jointly instead of separately, the k-ReLU method is able to bypass the convex barrier imposed by single neuron relaxations.
http://w3id.org/mlsea/pwc/scientificWork/A%20Primer%20on%20Pretrained%20Multilingual%20Language%20Models                                                                                  A Primer on Pretrained Multilingual Language Models                                                                                  Multilingual Language Models ( MLLMs) such as mBERT, XLM, XLM-R, textit{etc.} have emerged as a viable option for bringing the power of pretraining to a large number of languages. Given their success in zero-shot transfer learning, there has emerged a large body of work in (i) building bigger MLLMs~covering a large number of languages (ii) creating exhaustive benchmarks covering a wider variety of tasks and languages for evaluating MLLMs~ (iii) analysing the performance of MLLMs~on monolingual, zero-shot cross-lingual and bilingual tasks (iv) understanding the universal language patterns (if any) learnt by MLLMs~ and (v) augmenting the (often) limited capacity of MLLMs~ to improve their performance on seen or even unseen languages. In this survey, we review the existing literature covering the above broad areas of research pertaining to MLLMs. Based on our survey, we recommend some promising directions of future research.
http://w3id.org/mlsea/pwc/scientificWork/A%20Primer%20on%20Techtile%3A%20An%20R%26D%20Testbed%20for%20Distributed%20Communication%2C%20Sensing%20and%20Positioning                                                                                  A Primer on Techtile: An R&D Testbed for Distributed Communication, Sensing and Positioning                                                                                  The Techtile measurement infrastructure is a multi-functional, versatile testbed for new communication and sensing technologies relying on fine-grained distributed resources. The facility enables experimental research on hyper-connected interactive environments and validation of new wireless connectivity, sensing and positioning solutions. It consists of a data acquisition and processing equipment backbone and a fabric of dispersed edge computing devices, Software-Defined Radios, sensors, and LED sources. These bring intelligence close to the applications and can also collectively function as a massive, distributed resource. Furthermore, the infrastructure allows exploring more degrees and new types of diversity, i.e., scaling up the number of elements, introducing `3D directional diversity' by deploying the distributed elements with different orientations, and `interface diversity' by exploiting multiple technologies and hybrid signals (RF, acoustic, and visible light).
http://w3id.org/mlsea/pwc/scientificWork/A%20Priori%20Generalization%20Error%20Analysis%20of%20Two-Layer%20Neural%20Networks%20for%20Solving%20High%20Dimensional%20Schr%C3%B6dinger%20Eigenvalue%20Problems                                                                                  A Priori Generalization Error Analysis of Two-Layer Neural Networks for Solving High Dimensional Schrödinger Eigenvalue Problems                                                                                  This paper analyzes the generalization error of two-layer neural networks for computing the ground state of the Schr 'odinger operator on a $d$-dimensional hypercube. We prove that the convergence rate of the generalization error is independent of the dimension $d$, under the a priori assumption that the ground state lies in a spectral Barron space. We verify such assumption by proving a new regularity estimate for the ground state in the spectral Barron space. The later is achieved by a fixed point argument based on the Krein-Rutman theorem.
http://w3id.org/mlsea/pwc/scientificWork/A%20Privacy-Preserving%20Approach%20to%20Extraction%20of%20Personal%20Information%20through%20Automatic%20Annotation%20and%20Federated%20Learning                                                                                  A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning                                                                                  We curated WikiPII, an automatically labeled dataset composed of Wikipedia biography pages, annotated for personal information extraction. Although automatic annotation can lead to a high degree of label noise, it is an inexpensive process and can generate large volumes of annotated documents. We trained a BERT-based NER model with WikiPII and showed that with an adequately large training dataset, the model can significantly decrease the cost of manual information extraction, despite the high level of label noise. In a similar approach, organizations can leverage text mining techniques to create customized annotated datasets from their historical data without sharing the raw data for human annotation. Also, we explore collaborative training of NER models through federated learning when the annotation is noisy. Our results suggest that depending on the level of trust to the ML operator and the volume of the available data, distributed training can be an effective way of training a personal information identifier in a privacy-preserved manner. Research material is available at https://github.com/ratmcu/wikipiifed.
http://w3id.org/mlsea/pwc/scientificWork/A%20Privacy-Preserving%20and%20Trustable%20Multi-agent%20Learning%20Framework                                                                                  A Privacy-Preserving and Trustable Multi-agent Learning Framework                                                                                  Distributed multi-agent learning enables agents to cooperatively train a model without requiring to share their datasets. While this setting ensures some level of privacy, it has been shown that, even when data is not directly shared, the training process is vulnerable to privacy attacks including data reconstruction and model inversion attacks. Additionally, malicious agents that train on inverted labels or random data, may arbitrarily weaken the accuracy of the global model. This paper addresses these challenges and presents Privacy-preserving and trustable Distributed Learning (PT-DL), a fully decentralized framework that relies on Differential Privacy to guarantee strong privacy protections of the agents' data, and Ethereum smart contracts to ensure trustability. The paper shows that PT-DL is resilient up to a 50% collusion attack, with high probability, in a malicious trust model and the experimental evaluation illustrates the benefits of the proposed model as a privacy-preserving and trustable distributed multi-agent learning system on several classification tasks.
http://w3id.org/mlsea/pwc/scientificWork/A%20Probabilistic%20Approach%20to%20Neural%20Network%20Pruning                                                                                  A Probabilistic Approach to Neural Network Pruning                                                                                  Neural network pruning techniques reduce the number of parameters without compromising predicting ability of a network. Many algorithms have been developed for pruning both over-parameterized fully-connected networks (FCNs) and convolutional neural networks (CNNs), but analytical studies of capabilities and compression ratios of such pruned sub-networks are lacking. We theoretically study the performance of two pruning techniques (random and magnitude-based) on FCNs and CNNs. Given a target network {whose weights are independently sampled from appropriate distributions}, we provide a universal approach to bound the gap between a pruned and the target network in a probabilistic sense. The results establish that there exist pruned networks with expressive power within any specified bound from the target network.
http://w3id.org/mlsea/pwc/scientificWork/A%20Probabilistic%20Approach%20to%20Personalize%20Type-based%20Facet%20Ranking%20for%20POI%20Suggestion                                                                                  A Probabilistic Approach to Personalize Type-based Facet Ranking for POI Suggestion                                                                                  Faceted Search Systems (FSS) have become one of the main search interfaces used in vertical search systems, offering users meaningful facets to refine their search query and narrow down the results quickly to find the intended search target. This work focuses on the problem of ranking type-based facets. In a structured information space, type-based facets (t-facets) indicate the category to which each object belongs. When they belong to a large multi-level taxonomy, it is desirable to rank them separately before ranking other facet groups. This helps the searcher in filtering the results according to their type first. This also makes it easier to rank the rest of the facets once the type of the intended search target is selected. Existing research employs the same ranking methods for different facet groups. In this research, we propose a two-step approach to personalize t-facet ranking. The first step assigns a relevance score to each individual leaf-node t-facet. The score is generated using probabilistic models and it reflects t-facet relevance to the query and the user profile. In the second step, this score is used to re-order and select the sub-tree to present to the user. We investigate the usefulness of the proposed method to a Point Of Interest (POI) suggestion task. Our evaluation aims at capturing the user effort required to fulfil her search needs by using the ranked facets. The proposed approach achieved better results than other existing personalized baselines.
http://w3id.org/mlsea/pwc/scientificWork/A%20Probabilistic%20Forecast-Driven%20Strategy%20for%20a%20Risk-Aware%20Participation%20in%20the%20Capacity%20Firming%20Market%3A%20extended%20version                                                                                  A Probabilistic Forecast-Driven Strategy for a Risk-Aware Participation in the Capacity Firming Market: extended version                                                                                  This paper addresses the energy management of a grid-connected renewable generation plant coupled with a battery energy storage device in the capacity firming market, designed to promote renewable power generation facilities in small non-interconnected grids. The core contribution is to propose a probabilistic forecast-driven strategy, modeled as a min-max-min robust optimization problem with recourse. It is solved using a Benders-dual cutting plane algorithm and a column and constraints generation algorithm in a tractable manner. A dynamic risk-averse parameters selection strategy based on the quantile forecasts distribution is proposed to improve the results. A secondary contribution is to use a recently developed deep learning model known as normalizing flows to generate quantile forecasts of renewable generation for the robust optimization problem. This technique provides a general mechanism for defining expressive probability distributions, only requiring the specification of a base distribution and a series of bijective transformations. Overall, the robust approach improves the results over a deterministic approach with nominal point forecasts by finding a trade-off between conservative and risk-seeking policies. The case study uses the photovoltaic generation monitored on-site at the University of Li `ege (ULi `ege), Belgium.
http://w3id.org/mlsea/pwc/scientificWork/A%20Probabilistic%20Framework%20for%20Lexicon-based%20Keyword%20Spotting%20in%20Handwritten%20Text%20Images                                                                                  A Probabilistic Framework for Lexicon-based Keyword Spotting in Handwritten Text Images                                                                                  Query by String Keyword Spotting (KWS) is here considered as a key technology for indexing large collections of handwritten text images to allow fast textual access to the contents of these collections. Under this perspective, a probabilistic framework for lexicon-based KWS in text images is presented. The presentation aims at providing a tutorial view that helps to understand the relations between classical statements of KWS and the relative challenges entailed by these statements. More specifically, the development of the proposed framework makes it self-evident that word recognition or classification implicitly or explicitly underlies any formulation of KWS. Moreover, it clearly suggests that the same statistical models and training methods successfully used for handwriting text recognition can advantageously be used also for KWS, even though KWS does not generally require or rely on any kind of previously produced image transcripts. These ideas are developed into a specific, probabilistically sound approach for segmentation-free, lexicon-based, query-by-string KWS. Experiments carried out using this approach are presented, which support the consistency and general interest of the proposed framework. Several datasets, traditionally used for KWS benchmarking are considered, with results significantly better than those previously published for these datasets. In addition, results on two new, larger handwritten text image datasets are reported, showing the great potential of the methods proposed in this paper for indexing and textual search in large collections of handwritten documents.
http://w3id.org/mlsea/pwc/scientificWork/A%20Probabilistic%20State%20Space%20Model%20for%20Joint%20Inference%20from%20Differential%20Equations%20and%20Data                                                                                  A Probabilistic State Space Model for Joint Inference from Differential Equations and Data                                                                                  Mechanistic models with differential equations are a key component of scientific applications of machine learning. Inference in such models is usually computationally demanding, because it involves repeatedly solving the differential equation. The main problem here is that the numerical solver is hard to combine with standard inference techniques. Recent work in probabilistic numerics has developed a new class of solvers for ordinary differential equations (ODEs) that phrase the solution process directly in terms of Bayesian filtering. We here show that this allows such methods to be combined very directly, with conceptual and numerical ease, with latent force models in the ODE itself. It then becomes possible to perform approximate Bayesian inference on the latent force as well as the ODE solution in a single, linear complexity pass of an extended Kalman filter / smoother - that is, at the cost of computing a single ODE solution. We demonstrate the expressiveness and performance of the algorithm by training, among others, a non-parametric SIRD model on data from the COVID-19 outbreak.
http://w3id.org/mlsea/pwc/scientificWork/A%20Procedural%20World%20Generation%20Framework%20for%20Systematic%20Evaluation%20of%20Continual%20Learning                                                                                  A Procedural World Generation Framework for Systematic Evaluation of Continual Learning                                                                                  Several families of continual learning techniques have been proposed to alleviate catastrophic interference in deep neural network training on non-stationary data. However, a comprehensive comparison and analysis of limitations remains largely open due to the inaccessibility to suitable datasets. Empirical examination not only varies immensely between individual works, it further currently relies on contrived composition of benchmarks through subdivision and concatenation of various prevalent static vision datasets. In this work, our goal is to bridge this gap by introducing a computer graphics simulation framework that repeatedly renders only upcoming urban scene fragments in an endless real-time procedural world generation process. At its core lies a modular parametric generative model with adaptable generative factors. The latter can be used to flexibly compose data streams, which significantly facilitates a detailed analysis and allows for effortless investigation of various continual learning schemes.
http://w3id.org/mlsea/pwc/scientificWork/A%20Proposal%20for%20an%20Interactive%20Shell%20Based%20on%20a%20Typed%20Lambda%20Calculus                                                                                  A Proposal for an Interactive Shell Based on a Typed Lambda Calculus                                                                                  This paper presents Favalon, a functional programming language built on the premise of a lambda calculus for use as an interactive shell replacement. Favalon seamlessly integrates with typed versions of existing libraries and commands using type inference, flexible runtime type metadata, and the same techniques employed by shells to link commands together. Much of Favalon's syntax is customizable via user-defined functions, allowing it to be extended by anyone who is familiar with a command-line shell. Furthermore, Favalon's type inference engine can be separated from its runtime library and easily repurposed for other applications.
http://w3id.org/mlsea/pwc/scientificWork/A%20Prospective%20Look%3A%20Key%20Enabling%20Technologies%2C%20Applications%20and%20Open%20Research%20Topics%20in%206G%20Networks                                                                                  A Prospective Look: Key Enabling Technologies, Applications and Open Research Topics in 6G Networks                                                                                  The fifth generation (5G) mobile networks are envisaged to enable a plethora of breakthrough advancements in wireless technologies, providing support of a diverse set of services over a single platform. While the deployment of 5G systems is scaling up globally, it is time to look ahead for beyond 5G systems. This is driven by the emerging societal trends, calling for fully automated systems and intelligent services supported by extended reality and haptics communications. To accommodate the stringent requirements of their prospective applications, which are data-driven and defined by extremely low-latency, ultra-reliable, fast and seamless wireless connectivity, research initiatives are currently focusing on a progressive roadmap towards the sixth generation (6G) networks. In this article, we shed light on some of the major enabling technologies for 6G, which are expected to revolutionize the fundamental architectures of cellular networks and provide multiple homogeneous artificial intelligence-empowered services, including distributed communications, control, computing, sensing, and energy, from its core to its end nodes. Particularly, this paper aims to answer several 6G framework related questions: What are the driving forces for the development of 6G? How will the enabling technologies of 6G differ from those in 5G? What kind of applications and interactions will they support which would not be supported by 5G? We address these questions by presenting a profound study of the 6G vision and outlining five of its disruptive technologies, i.e., terahertz communications, programmable metasurfaces, drone-based communications, backscatter communications and tactile internet, as well as their potential applications. Then, by leveraging the state-of-the-art literature surveyed for each technology, we discuss their requirements, key challenges, and open research problems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Prospective%20Observational%20Study%20to%20Investigate%20Performance%20of%20a%20Chest%20X-ray%20Artificial%20Intelligence%20Diagnostic%20Support%20Tool%20Across%2012%20U.S.%20Hospitals                                                                                  A Prospective Observational Study to Investigate Performance of a Chest X-ray Artificial Intelligence Diagnostic Support Tool Across 12 U.S. Hospitals                                                                                  Importance: An artificial intelligence (AI)-based model to predict COVID-19 likelihood from chest x-ray (CXR) findings can serve as an important adjunct to accelerate immediate clinical decision making and improve clinical decision making. Despite significant efforts, many limitations and biases exist in previously developed AI diagnostic models for COVID-19. Utilizing a large set of local and international CXR images, we developed an AI model with high performance on temporal and external validation. Conclusions and Relevance: AI-based diagnostic tools may serve as an adjunct, but not replacement, for clinical decision support of COVID-19 diagnosis, which largely hinges on exposure history, signs, and symptoms. While AI-based tools have not yet reached full diagnostic potential in COVID-19, they may still offer valuable information to clinicians taken into consideration along with clinical signs and symptoms.
http://w3id.org/mlsea/pwc/scientificWork/A%20Protection%20Method%20of%20Trained%20CNN%20Model%20with%20Secret%20Key%20from%20Unauthorized%20Access                                                                                  A Protection Method of Trained CNN Model with Secret Key from Unauthorized Access                                                                                  In this paper, we propose a novel method for protecting convolutional neural network (CNN) models with a secret key set so that unauthorized users without the correct key set cannot access trained models. The method enables us to protect not only from copyright infringement but also the functionality of a model from unauthorized access without any noticeable overhead. We introduce three block-wise transformations with a secret key set to generate learnable transformed images: pixel shuffling, negative/positive transformation, and FFX encryption. Protected models are trained by using transformed images. The results of experiments with the CIFAR and ImageNet datasets show that the performance of a protected model was close to that of non-protected models when the key set was correct, while the accuracy severely dropped when an incorrect key set was given. The protected model was also demonstrated to be robust against various attacks. Compared with the state-of-the-art model protection with passports, the proposed method does not have any additional layers in the network, and therefore, there is no overhead during training and inference processes.
http://w3id.org/mlsea/pwc/scientificWork/A%20Prototype%20of%20Reconfigurable%20Intelligent%20Surface%20with%20Continuous%20Control%20of%20the%20Reflection%20Phase                                                                                  A Prototype of Reconfigurable Intelligent Surface with Continuous Control of the Reflection Phase                                                                                  With the development of the next generation of mobile networks, new research challenges have emerged, and new technologies have been proposed to face them. On the one hand, the reconfigurable intelligent surface (RIS) technology is being investigated for partially controlling the wireless channels. The RIS is a promising technology for improving the signal quality by controlling the scattering of the electromagnetic waves in a nearly passive manner. On the other hand, ambient backscatter communications (AmBC) is another promising technology that is tailored for addressing the energy efficiency requirements for the Internet of Things (IoT). This technique enables low-power communications by backscattering ambient signals and, thus, reusing existing electromagnetic waves for communications. RIS technology can be utilized in the context of AmBC for improving the system performance. In this paper, we report a prototype of an RIS that offers the capability of controlling the phase shift of the reflected waves in a continuous manner, and we characterize its characteristics by using full-wave simulations and through experimental measurements. Specifically, we introduce a phase shift model for predicting the signal reflected by the RIS prototype. We apply the proposed model for optimizing an RISassisted AmBC system and we demonstrate that the use of an RIS can significantly improve the system performance.
http://w3id.org/mlsea/pwc/scientificWork/A%20Provably-Efficient%20Model-Free%20Algorithm%20for%20Constrained%20Markov%20Decision%20Processes                                                                                  A Provably-Efficient Model-Free Algorithm for Constrained Markov Decision Processes                                                                                  This paper presents the first model-free, simulator-free reinforcement learning algorithm for Constrained Markov Decision Processes (CMDPs) with sublinear regret and zero constraint violation. The algorithm is named Triple-Q because it includes three key components: a Q-function (also called action-value function) for the cumulative reward, a Q-function for the cumulative utility for the constraint, and a virtual-Queue that (over)-estimates the cumulative constraint violation. Under Triple-Q, at each step, an action is chosen based on the pseudo-Q-value that is a combination of the three 'Q' values. The algorithm updates the reward and utility Q-values with learning rates that depend on the visit counts to the corresponding (state, action) pairs and are periodically reset. In the episodic CMDP setting, Triple-Q achieves $ tilde{ cal O} left( frac{1 }{ delta}H^4 S^{ frac{1}{2}}A^{ frac{1}{2}}K^{ frac{4}{5}} right)$ regret, where $K$ is the total number of episodes, $H$ is the number of steps in each episode, $S$ is the number of states, $A$ is the number of actions, and $ delta$ is Slater's constant. Furthermore, Triple-Q guarantees zero constraint violation, both on expectation and with a high probability, when $K$ is sufficiently large. Finally, the computational complexity of Triple-Q is similar to SARSA for unconstrained MDPs and is computationally efficient.
http://w3id.org/mlsea/pwc/scientificWork/A%20Pseudo%20Label-wise%20Attention%20Network%20for%20Automatic%20ICD%20Coding                                                                                  A Pseudo Label-wise Attention Network for Automatic ICD Coding                                                                                  Automatic International Classification of Diseases (ICD) coding is defined as a kind of text multi-label classification problem, which is difficult because the number of labels is very large and the distribution of labels is unbalanced. The label-wise attention mechanism is widely used in automatic ICD coding because it can assign weights to every word in full Electronic Medical Records (EMR) for different ICD codes. However, the label-wise attention mechanism is computational redundant and costly. In this paper, we propose a pseudo label-wise attention mechanism to tackle the problem. Instead of computing different attention modes for different ICD codes, the pseudo label-wise attention mechanism automatically merges similar ICD codes and computes only one attention mode for the similar ICD codes, which greatly compresses the number of attention modes and improves the predicted accuracy. In addition, we apply a more convenient and effective way to obtain the ICD vectors, and thus our model can predict new ICD codes by calculating the similarities between EMR vectors and ICD vectors. Extensive experiments show the superior performance of our model. On the public MIMIC-III dataset and private Xiangya dataset, our model achieves micro f1 of 0.583 and 0.806, respectively, which outperforms other competing models. Furthermore, we verify the ability of our model in predicting new ICD codes. The case study shows how pseudo label-wise attention works, and demonstrates the effectiveness of pseudo label-wise attention mechanism.
http://w3id.org/mlsea/pwc/scientificWork/A%20Pursuit-Evasion%20Differential%20Game%20with%20Strategic%20Information%20Acquisition                                                                                  A Pursuit-Evasion Differential Game with Strategic Information Acquisition                                                                                  This paper studies a two-person linear-quadratic-Gaussian pursuit-evasion differential game with costly but controlled information. One player can decide when to observe the other player's state. However, one observation of another player's state comes with two costs: the direct cost of observing and the implicit cost of exposing his state. We call games of this type a Pursuit-Evasion-Exposure-Concealment (PEEC) game. The PEEC game constitutes two types of strategies: The control strategies and the observation strategies. We fully characterize the Nash control strategies of the PEEC game using techniques such as completing squares and the calculus of variations. We show that the derivation of the Nash observation strategies and the Nash control strategies can be decoupled. We develop a set of necessary conditions that facilitate the numerical computation of the Nash observation strategies. We show, in theory, that players with less maneuverability prefer concealment to exposure. We also show that when the game's horizon goes to infinity, the Nash observation strategy is to observe periodically, and the expected distance between the pursuer and the evader goes to zero with a bounded second moment. We conducted a series of numerical experiments to study the proposed PEEC game. We illustrate the numerical results using both figures and animation. Numerical results show that the pursuer can maintain high-grade performance even when the number of observations is limited. We also show that an evader with low maneuverability can still escape if the evader increases his stealthiness.
http://w3id.org/mlsea/pwc/scientificWork/A%20Quantum%20Hopfield%20Associative%20Memory%20Implemented%20on%20an%20Actual%20Quantum%20Processor                                                                                  A Quantum Hopfield Associative Memory Implemented on an Actual Quantum Processor                                                                                  In this work, we present a Quantum Hopfield Associative Memory (QHAM) and demonstrate its capabilities in simulation and hardware using IBM Quantum Experience. The QHAM is based on a quantum neuron design which can be utilized for many different machine learning applications and can be implemented on real quantum hardware without requiring mid-circuit measurement or reset operations. We analyze the accuracy of the neuron and the full QHAM considering hardware errors via simulation with hardware noise models as well as with implementation on the 15-qubit ibmq_16_melbourne device. The quantum neuron and the QHAM are shown to be resilient to noise and require low qubit overhead and gate complexity. We benchmark the QHAM by testing its effective memory capacity and demonstrate its capabilities in the NISQ-era of quantum hardware. This demonstration of the first functional QHAM to be implemented in NISQ-era quantum hardware is a significant step in machine learning at the leading edge of quantum computing.
http://w3id.org/mlsea/pwc/scientificWork/A%20Quantum%20algorithm%20for%20linear%20PDEs%20arising%20in%20Finance                                                                                  A Quantum algorithm for linear PDEs arising in Finance                                                                                  We propose a hybrid quantum-classical algorithm, originated from quantum chemistry, to price European and Asian options in the Black-Scholes model. Our approach is based on the equivalence between the pricing partial differential equation and the Schrodinger equation in imaginary time. We devise a strategy to build a shallow quantum circuit approximation to this equation, only requiring few qubits. This constitutes a promising candidate for the application of Quantum Computing techniques (with large number of qubits affected by noise) in Quantitative Finance.
http://w3id.org/mlsea/pwc/scientificWork/A%20Quasiconvex%20Formulation%20for%20Radial%20Cameras                                                                                  A Quasiconvex Formulation for Radial Cameras                                                                                   In this paper we study structure from motion problems for 1D radial cameras. Under this model the projection of a 3D point is a line in the image plane going through the principal point, which makes the model invariant to radial distortion and changes in focal length. It can therefore effectively be applied to uncalibrated image collections without the need for explicit estimation of camera intrinsics. We show that the reprojection errors of 1D radial cameras are examples of quasiconvex functions. This opens up the possibility to solve a general class of relevant reconstruction problems globally optimally using tools from convex optimization. In fact, our resulting algorithm is based on solving a series of LP problems. We perform an extensive experimental evaluation, on both synthetic and real data, showing that a whole class of multiview geometry problems across a range of different cameras models with varying and unknown intrinsic calibration can be reliably and accurately solved within the same framework. 
http://w3id.org/mlsea/pwc/scientificWork/A%20Query%20Language%20for%20Summarizing%20and%20Analyzing%20Business%20Process%20Data                                                                                  A Query Language for Summarizing and Analyzing Business Process Data                                                                                  In modern enterprises, Business Processes (BPs) are realized over a mix of workflows, IT systems, Web services and direct collaborations of people. Accordingly, process data (i.e., BP execution data such as logs containing events, interaction messages and other process artifacts) is scattered across several systems and data sources, and increasingly show all typical properties of the Big Data. Understanding the execution of process data is challenging as key business insights remain hidden in the interactions among process entities: most objects are interconnected, forming complex, heterogeneous but often semi-structured networks. In the context of business processes, we consider the Big Data problem as a massive number of interconnected data islands from personal, shared and business data. We present a framework to model process data as graphs, i.e., Process Graph, and present abstractions to summarize the process graph and to discover concept hierarchies for entities based on both data objects and their interactions in process graphs. We present a language, namely BP-SPARQL, for the explorative querying and understanding of process graphs from various user perspectives. We have implemented a scalable architecture for querying, exploration and analysis of process graphs. We report on experiments performed on both synthetic and real-world datasets that show the viability and efficiency of the approach.
http://w3id.org/mlsea/pwc/scientificWork/A%20Query-Driven%20Topic%20Model                                                                                  A Query-Driven Topic Model                                                                                  Topic modeling is an unsupervised method for revealing the hidden semantic structure of a corpus. It has been increasingly widely adopted as a tool in the social sciences, including political science, digital humanities and sociological research in general. One desirable property of topic models is to allow users to find topics describing a specific aspect of the corpus. A possible solution is to incorporate domain-specific knowledge into topic modeling, but this requires a specification from domain experts. We propose a novel query-driven topic model that allows users to specify a simple query in words or phrases and return query-related topics, thus avoiding tedious work from domain experts. Our proposed approach is particularly attractive when the user-specified query has a low occurrence in a text corpus, making it difficult for traditional topic models built on word cooccurrence patterns to identify relevant topics. Experimental results demonstrate the effectiveness of our model in comparison with both classical topic models and neural topic models.
http://w3id.org/mlsea/pwc/scientificWork/A%20Question%20of%20Time%3A%20Revisiting%20the%20Use%20of%20Recursive%20Filtering%20for%20Temporal%20Calibration%20of%20Multisensor%20Systems                                                                                  A Question of Time: Revisiting the Use of Recursive Filtering for Temporal Calibration of Multisensor Systems                                                                                  We examine the problem of time delay estimation, or temporal calibration, in the context of multisensor data fusion. Differences in processing intervals and other factors typically lead to a relative delay between measurement updates from disparate sensors. Correct (optimal) data fusion demands that the relative delay must either be known in advance or identified online. There have been several recent proposals in the literature to determine the delay using recursive, causal filters such as the extended Kalman filter (EKF). We carefully review this formulation and show that there are fundamental issues with the structure of the EKF (and related algorithms) when the delay is included in the filter state vector as a parameter to be estimated. These structural issues, in turn, leave recursive filters prone to bias and inconsistency. Our theoretical analysis is supported by simulation studies that demonstrate the implications in terms of filter performance; although tuning of the filter noise variances may reduce the chance of inconsistency or divergence, the underlying structural concerns remain. We offer brief suggestions for ways to maintain the computational efficiency of recursive filtering for temporal calibration while avoiding the drawbacks of the standard filtering algorithms.
http://w3id.org/mlsea/pwc/scientificWork/A%20Question-answering%20Based%20Framework%20for%20Relation%20Extraction%20Validation                                                                                  A Question-answering Based Framework for Relation Extraction Validation                                                                                  Relation extraction is an important task in knowledge acquisition and text understanding. Existing works mainly focus on improving relation extraction by extracting effective features or designing reasonable model structures. However, few works have focused on how to validate and correct the results generated by the existing relation extraction models. We argue that validation is an important and promising direction to further improve the performance of relation extraction. In this paper, we explore the possibility of using question answering as validation. Specifically, we propose a novel question-answering based framework to validate the results from relation extraction models. Our proposed framework can be easily applied to existing relation classifiers without any additional information. We conduct extensive experiments on the popular NYT dataset to evaluate the proposed framework, and observe consistent improvements over five strong baselines.
http://w3id.org/mlsea/pwc/scientificWork/A%20RBA%20model%20for%20the%20chemostat%20modeling                                                                                  A RBA model for the chemostat modeling                                                                                  The purpose of this paper is to show that it is possible to replace Monod's type model of a chemostat by a constraint based model of bacteria at the genome scale. This new model is an extension of the RBA model of bacteria developed in a batch mode to the chemostat. This new model, and the associated framework, leads to a dramatic improvement in the prediction capacities of the chemostat behaviour. Indeed, for example, the internal states of the bacteria are now part of the prediction outputs and the chemostat behaviour can now be predicted for any limiting source. Finally, the first interests of this new predictive method are illustrated on a set of classic situations where predictions are already close of the well-known biological observations about chemostat. This paper is an extended version of [8] that includes a discussion on the modeling assumptions.
http://w3id.org/mlsea/pwc/scientificWork/A%20Random%20Antenna%20Subset%20Selection%20Jamming%20Method%20against%20Multistatic%20Radar%20System                                                                                  A Random Antenna Subset Selection Jamming Method against Multistatic Radar System                                                                                  Multistatic radar system (MSRS) is considered an effective scheme to suppress mainlobe jamming, since it has higher spatial resolution enabling jamming cancellation from spatial domain. To develop electronic countermeasures against MSRS, a random array subset selection (RASS)jamming method is proposed in this paper. In the RASS jammer, elements of the array antenna are activated randomly, leading to stable mainlobe and random sidelobes, different from the traditional jammer that applies the complete antenna array enjoying constant mainlobe and sidelobes. We study the covariance matrix of jamming signals received by radars, and derive its rank, revealing that the covariance matrix is of full rank. We also calculate the output jamming to signal and noise ratio (JSNR) after the subspace-based jamming suppression methods used in MSRS under the proposed jamming method, which demonstrates that the full rank property invalidates such suppression methods. Numerical results verify our analytical deduction and exhibit the improved countermeasure performance of our proposed RASS jamming method compared to the traditional one.
http://w3id.org/mlsea/pwc/scientificWork/A%20Random%20Attention%20and%20Utility%20Model                                                                                  A Random Attention and Utility Model                                                                                  We generalize the stochastic revealed preference methodology of McFadden and Richter (1990) for finite choice sets to settings with limited consideration. Our approach is nonparametric and requires partial choice set variation. We impose a monotonicity condition on attention first proposed by Cattaneo et al. (2020) and a stability condition on the marginal distribution of preferences. Our framework is amenable to statistical testing. These new restrictions extend widely known parametric models of consideration with heterogeneous preferences.
http://w3id.org/mlsea/pwc/scientificWork/A%20Random%20Persistence%20Diagram%20Generator                                                                                  A Random Persistence Diagram Generator                                                                                  Topological data analysis (TDA) studies the shape patterns of data. Persistent homology is a widely used method in TDA that summarizes homological features of data at multiple scales and stores them in persistence diagrams (PDs). In this paper, we propose a random persistence diagram generator (RPDG) method that generates a sequence of random PDs from the ones produced by the data. RPDG is underpinned by a model based on pairwise interacting point processes, and a reversible jump Markov chain Monte Carlo (RJ-MCMC) algorithm. A first example, which is based on a synthetic dataset, demonstrates the efficacy of RPDG and provides a comparison with another method for sampling PDs. A second example demonstrates the utility of RPDG to solve a materials science problem given a real dataset of small sample size.
http://w3id.org/mlsea/pwc/scientificWork/A%20Rank%20based%20Adaptive%20Mutation%20in%20Genetic%20Algorithm                                                                                  A Rank based Adaptive Mutation in Genetic Algorithm                                                                                  Traditionally Genetic Algorithm has been used for optimization of unimodal and multimodal functions. Earlier researchers worked with constant probabilities of GA control operators like crossover, mutation etc. for tuning the optimization in specific domains. Recent advancements in this field witnessed adaptive approach in probability determination. In Adaptive mutation primarily poor individuals are utilized to explore state space, so mutation probability is usually generated proportionally to the difference between fitness of best chromosome and itself (fMAX - f). However, this approach is susceptible to nature of fitness distribution during optimization. This paper presents an alternate approach of mutation probability generation using chromosome rank to avoid any susceptibility to fitness distribution. Experiments are done to compare results of simple genetic algorithm (SGA) with constant mutation probability and adaptive approaches within a limited resource constraint for unimodal, multimodal functions and Travelling Salesman Problem (TSP). Measurements are done for average best fitness, number of generations evolved and percentage of global optimum achievements out of several trials. The results demonstrate that the rank-based adaptive mutation approach is superior to fitness-based adaptive approach as well as SGA in a multimodal problem space.
http://w3id.org/mlsea/pwc/scientificWork/A%20Rate-Splitting%20Strategy%20to%20Enable%20Joint%20Radar%20Sensing%20and%20Communication%20with%20Partial%20CSIT                                                                                  A Rate-Splitting Strategy to Enable Joint Radar Sensing and Communication with Partial CSIT                                                                                  In order to manage the increasing interference between radar and communication systems, joint radar and communication (RadCom) systems have attracted increased attention in recent years, with the studies so far considering the assumption of perfect Channel State Information at the Transmitter (CSIT). However, such an assumption is unrealistic and neglects the inevitable CSIT errors that need to be considered to fully exploit the multi-antenna processing and interference management capabilities of a joint RadCom system. In this work, a joint RadCom system is designed which marries the capabilities of a Multiple-Input Multiple-Output (MIMO) radar with Rate-Splitting Multiple Access (RSMA), a powerful downlink communications scheme based on linearly precoded Rate-Splitting (RS) to partially decode multi-user interference (MUI) and partially treat it as noise. In this way, the RadCom precoders are optimized in the presence of partial CSIT to simultaneously maximize the Average Weighted Sum-Rate (AWSR) under QoS rate constraints and minimize the RadCom Beampattern Squared Error (BSE) against an ideal MIMO radar beampattern. Simulation results demonstrate that RSMA provides the RadCom with more robustness, flexibility and user rate fairness compared to the baseline joint RadCom system based on Space Division Multiple Access (SDMA).
http://w3id.org/mlsea/pwc/scientificWork/A%20Rational%20Inattention%20Theory%20of%20Echo%20Chamber                                                                                  A Rational Inattention Theory of Echo Chamber                                                                                  We develop a rational inattention theory of echo chamber, whereby players gather information about an uncertain state by allocating limited attention capacities across biased primary sources and the other players. The resulting Poisson attention network transmits information from the primary source to a player either directly or indirectly through the other players. Rational inattention generates heterogeneous demands for information among players who are initially biased towards different decisions. In an echo chamber equilibrium, each player restricts attention to his own-biased source and like-minded friends, as the latter attend to the same primary source as his, and so could serve as secondary sources in case the information transmission from the primary source to him is disrupted. We provide sufficient conditions that give rise to echo chamber equilibria, characterize the attention networks within echo chambers, and use our results to inform the design and regulation of information platforms.
http://w3id.org/mlsea/pwc/scientificWork/A%20Rational%20Inattention%20Theory%20of%20Echo%20Chamber                                                                                  A Rational Inattention Theory of Echo Chamber                                                                                  We develop a rational inattention theory of echo chamber, whereby players gather information about an uncertain state by allocating limited attention capacities across biased primary sources and other players. The resulting Poisson attention network transmits information from the primary source to a player either directly, or indirectly through the other players. Rational inattention generates heterogeneous demands for information among players who are biased toward different decisions. In an echo chamber equilibrium, each player restricts attention to his own-biased source and like-minded friends, as the latter attend to the same primary source as his, and so could serve as secondary sources in case the information transmission from the primary source to him is disrupted. We provide sufficient conditions that give rise to echo chamber equilibria, characterize the attention networks inside echo chambers, and use our results to inform the design and regulation of information platforms.
http://w3id.org/mlsea/pwc/scientificWork/A%20Real-Time%20Closed-Form%20Model%20for%20Nonlinearity%20Modeling%20in%20Ultra-Wide-Band%20Optical%20Fiber%20Links%20Accounting%20for%20Inter-channel%20Stimulated%20Raman%20Scattering%20and%20Co-Propagating%20Raman%20Amplification                                                                                  A Real-Time Closed-Form Model for Nonlinearity Modeling in Ultra-Wide-Band Optical Fiber Links Accounting for Inter-channel Stimulated Raman Scattering and Co-Propagating Raman Amplification                                                                                  In this paper, we present a novel closed-form model (CFM) for accurate and fast evaluation of nonlinear interference in modern ultrawideband coherent optical fiber communication systems. Starting from the Gaussian noise model (GN model), using reasonable approximations and machine-learning optimized improvements, we achieve an accurate CFM capable of handling ultrawide band (C+L or wider) optical fiber systems in the presence of Inter-channel Stimulated Raman Scattering (ISRS) and forward-pumped Raman amplification.
http://w3id.org/mlsea/pwc/scientificWork/A%20Realistic%20Evaluation%20of%20Semi-Supervised%20Learning%20for%20Fine-Grained%20Classification                                                                                  A Realistic Evaluation of Semi-Supervised Learning for Fine-Grained Classification                                                                                  We evaluate the effectiveness of semi-supervised learning (SSL) on a realistic benchmark where data exhibits considerable class imbalance and contains images from novel classes. Our benchmark consists of two fine-grained classification datasets obtained by sampling classes from the Aves and Fungi taxonomy. We find that recently proposed SSL methods provide significant benefits, and can effectively use out-of-class data to improve performance when deep networks are trained from scratch. Yet their performance pales in comparison to a transfer learning baseline, an alternative approach for learning from a few examples. Furthermore, in the transfer setting, while existing SSL methods provide improvements, the presence of out-of-class is often detrimental. In this setting, standard fine-tuning followed by distillation-based self-training is the most robust. Our work suggests that semi-supervised learning with experts on realistic datasets may require different strategies than those currently prevalent in the literature.
http://w3id.org/mlsea/pwc/scientificWork/A%20Receding%20Horizon%20Approach%20for%20Simultaneous%20Active%20Learning%20and%20Control%20using%20Gaussian%20Processes                                                                                  A Receding Horizon Approach for Simultaneous Active Learning and Control using Gaussian Processes                                                                                  This paper proposes a receding horizon active learning and control problem for dynamical systems in which Gaussian Processes (GPs) are utilized to model the system dynamics. The active learning objective in the optimization problem is presented by the exact conditional differential entropy of GP predictions at multiple steps ahead, which is equivalent to the log determinant of the GP posterior covariance matrix. The resulting non-convex and complex optimization problem is solved by the Sequential Convex Programming algorithm that exploits the first-order approximations of non-convex functions. Simulation results of an autonomous racing car example verify that using the proposed method can significantly improve data quality for model learning while solving time is highly promising for real-time applications.
http://w3id.org/mlsea/pwc/scientificWork/A%20Recipe%20for%20Global%20Convergence%20Guarantee%20in%20Deep%20Neural%20Networks                                                                                  A Recipe for Global Convergence Guarantee in Deep Neural Networks                                                                                  Existing global convergence guarantees of (stochastic) gradient descent do not apply to practical deep networks in the practical regime of deep learning beyond the neural tangent kernel (NTK) regime. This paper proposes an algorithm, which is ensured to have global convergence guarantees in the practical regime beyond the NTK regime, under a verifiable condition called the expressivity condition. The expressivity condition is defined to be both data-dependent and architecture-dependent, which is the key property that makes our results applicable for practical settings beyond the NTK regime. On the one hand, the expressivity condition is theoretically proven to hold data-independently for fully-connected deep neural networks with narrow hidden layers and a single wide layer. On the other hand, the expressivity condition is numerically shown to hold data-dependently for deep (convolutional) ResNet with batch normalization with various standard image datasets. We also show that the proposed algorithm has generalization performances comparable with those of the heuristic algorithm, with the same hyper-parameters and total number of iterations. Therefore, the proposed algorithm can be viewed as a step towards providing theoretical guarantees for deep learning in the practical regime.
http://w3id.org/mlsea/pwc/scientificWork/A%20Recipe%20for%20Social%20Media%20Analysis                                                                                  A Recipe for Social Media Analysis                                                                                  The Ubiquitous nature of smartphones has significantly increased the use of social media platforms, such as Facebook, Twitter, TikTok, and LinkedIn, etc., among the public, government, and businesses. Facebook generated ~70 billion USD in 2019 in advertisement revenues alone, a ~27% increase from the previous year. Social media has also played a strong role in outbreaks of social protests responsible for political changes in different countries. As we can see from the above examples, social media plays a big role in business intelligence and international politics. In this paper, we present and discuss a high-level functional intelligence model (recipe) of Social Media Analysis (SMA). This model synthesizes the input data and uses operational intelligence to provide actionable recommendations. In addition, it also matches the synthesized function of the experiences and learning gained from the environment. The SMA model presented is independent of the application domain, and can be applied to different domains, such as Education, Healthcare and Government, etc. Finally, we also present some of the challenges faced by SMA and how the SMA model presented in this paper solves them.
http://w3id.org/mlsea/pwc/scientificWork/A%20Recursive%20Logit%20Model%20with%20Choice%20Aversion%20and%20Its%20Application%20to%20Transportation%20Networks                                                                                  A Recursive Logit Model with Choice Aversion and Its Application to Transportation Networks                                                                                  We propose a recursive logit model which captures the notion of choice aversion by imposing a penalty term that accounts for the dimension of the choice set at each node of the transportation network. We make three contributions. First, we show that our model overcomes the correlation problem between routes, a common pitfall of traditional logit models, and that the choice aversion model can be seen as an alternative to these models. Second, we show how our model can generate violations of regularity in the path choice probabilities. In particular, we show that removing edges in the network may decrease the probability for existing paths. Finally, we show that under the presence of choice aversion, adding edges to the network can make users worse off. In other words, a type of Braess's paradox can emerge outside of congestion and can be characterized in terms of a parameter that measures users' degree of choice aversion. We validate these contributions by estimating this parameter over GPS traffic data captured on a real-world transportation network.
http://w3id.org/mlsea/pwc/scientificWork/A%20Recursive%20Measure%20of%20Voting%20Power%20that%20Satisfies%20Reasonable%20Postulates                                                                                  A Recursive Measure of Voting Power that Satisfies Reasonable Postulates                                                                                  We design a recursive measure of voting power based on partial as well as full voting efficacy. Classical measures, by contrast, incorporate solely full efficacy. We motivate our design by representing voting games using a division lattice and via the notion of random walks in stochastic processes, and show the viability of our recursive measure by proving it satisfies a plethora of postulates that any reasonable voting measure should satisfy. These include the iso-invariance, dummy, dominance, donation, minimum-power bloc, and quarrel postulates.
http://w3id.org/mlsea/pwc/scientificWork/A%20Reduction-Based%20Framework%20for%20Conservative%20Bandits%20and%20Reinforcement%20Learning                                                                                  A Reduction-Based Framework for Conservative Bandits and Reinforcement Learning                                                                                  In this paper, we present a reduction-based framework for conservative bandits and RL, in which our core technique is to calculate the necessary and sufficient budget obtained from running the baseline policy. For lower bounds, we improve the existing lower bound for conservative multi-armed bandits and obtain new lower bounds for conservative linear bandits, tabular RL and low-rank MDP, through a black-box reduction that turns a certain lower bound in the nonconservative setting into a new lower bound in the conservative setting. For upper bounds, in multi-armed bandits, linear bandits and tabular RL, our new upper bounds tighten or match existing ones with significantly simpler analyses. We also obtain a new upper bound for conservative low-rank MDP.
http://w3id.org/mlsea/pwc/scientificWork/A%20Refined%20Inertial%20DC%20Algorithm%20for%20DC%20Programming                                                                                  A Refined Inertial DC Algorithm for DC Programming                                                                                  In this paper we consider the difference-of-convex (DC) programming problems, whose objective function is the difference of two convex functions. The classical DC Algorithm (DCA) is well-known for solving this kind of problems, which generally returns a critical point. Recently, an inertial DC algorithm (InDCA) equipped with heavy-ball inertial-force procedure was proposed in de Oliveira et al. (Set-Valued and Variational Analysis 27(4):895--919, 2019), which potentially helps to improve both the convergence speed and the solution quality. Based on InDCA, we propose a refined inertial DC algorithm (RInDCA) equipped with enlarged inertial step-size compared with InDCA. Empirically, larger step-size accelerates the convergence. We demonstrate the subsequential convergence of our refined version to a critical point. In addition, by assuming the Kurdyka-{ L}ojasiewicz (KL) property of the objective function, we establish the sequential convergence of RInDCA. Numerical simulations on checking copositivity of matrices and image denoising problem show the benefit of larger step-size.
http://w3id.org/mlsea/pwc/scientificWork/A%20Reflection%20on%20the%20Structure%20and%20Process%20of%20the%20Web%20of%20Data                                                                                  A Reflection on the Structure and Process of the Web of Data                                                                                  The Web community has introduced a set of standards and technologies for representing, querying, and manipulating a globally distributed data structure known as the Web of Data. The proponents of the Web of Data envision much of the world's data being interrelated and openly accessible to the general public. This vision is analogous in many ways to the Web of Documents of common knowledge, but instead of making documents and media openly accessible, the focus is on making data openly accessible. In providing data for public use, there has been a stimulated interest in a movement dubbed Open Data. Open Data is analogous in many ways to the Open Source movement. However, instead of focusing on software, Open Data is focused on the legal and licensing issues around publicly exposed data. Together, various technological and legal tools are laying the groundwork for the future of global-scale data management on the Web. As of today, in its early form, the Web of Data hosts a variety of data sets that include encyclopedic facts, drug and protein data, metadata on music, books and scholarly articles, social network representations, geospatial information, and many other types of information. The size and diversity of the Web of Data is a demonstration of the flexibility of the underlying standards and the overall feasibility of the project as a whole. The purpose of this article is to provide a review of the technological underpinnings of the Web of Data as well as some of the hurdles that need to be overcome if the Web of Data is to emerge as the defacto medium for data representation, distribution, and ultimately, processing.
http://w3id.org/mlsea/pwc/scientificWork/A%20Reinforcement%20Learning%20Approach%20for%20an%20IRS-assisted%20NOMA%20Network                                                                                  A Reinforcement Learning Approach for an IRS-assisted NOMA Network                                                                                  This letter investigates a sum rate maximizationproblem in an intelligent reflective surface (IRS) assisted non-orthogonal multiple access (NOMA) downlink network. Specif-ically, the sum rate of all the users is maximized by jointlyoptimizing the beams at the base station and the phase shiftat the IRS. The deep reinforcement learning (DRL), which hasachieved massive successes, is applied to solve this sum ratemaximization problem. In particular, an algorithm based on thedeep deterministic policy gradient (DDPG) is proposed. Both therandom channel case and the fixed channel case are studied inthis letter. The simulation result illustrates that the DDPG basedalgorithm has the competitive performance on both case.
http://w3id.org/mlsea/pwc/scientificWork/A%20Reinforcement%20Learning%20Environment%20For%20Job-Shop%20Scheduling                                                                                  A Reinforcement Learning Environment For Job-Shop Scheduling                                                                                  Scheduling is a fundamental task occurring in various automated systems applications, e.g., optimal schedules for machines on a job shop allow for a reduction of production costs and waste. Nevertheless, finding such schedules is often intractable and cannot be achieved by Combinatorial Optimization Problem (COP) methods within a given time limit. Recent advances of Deep Reinforcement Learning (DRL) in learning complex behavior enable new COP application possibilities. This paper presents an efficient DRL environment for Job-Shop Scheduling -- an important problem in the field. Furthermore, we design a meaningful and compact state representation as well as a novel, simple dense reward function, closely related to the sparse make-span minimization criteria used by COP methods. We demonstrate that our approach significantly outperforms existing DRL methods on classic benchmark instances, coming close to state-of-the-art COP approaches.
http://w3id.org/mlsea/pwc/scientificWork/A%20Reinforcement%20Learning%20Environment%20for%20Multi-Service%20UAV-enabled%20Wireless%20Systems                                                                                  A Reinforcement Learning Environment for Multi-Service UAV-enabled Wireless Systems                                                                                  We design a multi-purpose environment for autonomous UAVs offering different communication services in a variety of application contexts (e.g., wireless mobile connectivity services, edge computing, data gathering). We develop the environment, based on OpenAI Gym framework, in order to simulate different characteristics of real operational environments and we adopt the Reinforcement Learning to generate policies that maximize some desired performance.The quality of the resulting policies are compared with a simple baseline to evaluate the system and derive guidelines to adopt this technique in different use cases. The main contribution of this paper is a flexible and extensible OpenAI Gym environment, which allows to generate, evaluate, and compare policies for autonomous multi-drone systems in multi-service applications. This environment allows for comparative evaluation and benchmarking of different approaches in a variety of application contexts.
http://w3id.org/mlsea/pwc/scientificWork/A%20Reinforcement%20Learning%20Environment%20for%20Polyhedral%20Optimizations                                                                                  A Reinforcement Learning Environment for Polyhedral Optimizations                                                                                  The polyhedral model allows a structured way of defining semantics-preserving transformations to improve the performance of a large class of loops. Finding profitable points in this space is a hard problem which is usually approached by heuristics that generalize from domain-expert knowledge. Existing problem formulations in state-of-the-art heuristics depend on the shape of particular loops, making it hard to leverage generic and more powerful optimization techniques from the machine learning domain. In this paper, we propose PolyGym, a shape-agnostic formulation for the space of legal transformations in the polyhedral model as a Markov Decision Process (MDP). Instead of using transformations, the formulation is based on an abstract space of possible schedules. In this formulation, states model partial schedules, which are constructed by actions that are reusable across different loops. With a simple heuristic to traverse the space, we demonstrate that our formulation is powerful enough to match and outperform state-of-the-art heuristics. On the Polybench benchmark suite, we found transformations that led to a speedup of 3.39x over LLVM O3, which is 1.83x better than the speedup achieved by ISL. Our generic MDP formulation enables using reinforcement learning to learn optimization policies over a wide range of loops. This also contributes to the emerging field of machine learning in compilers, as it exposes a novel problem formulation that can push the limits of existing methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20Reinforcement%20Learning-based%20Economic%20Model%20Predictive%20Control%20Framework%20for%20Autonomous%20Operation%20of%20Chemical%20Reactors                                                                                  A Reinforcement Learning-based Economic Model Predictive Control Framework for Autonomous Operation of Chemical Reactors                                                                                  Economic model predictive control (EMPC) is a promising methodology for optimal operation of dynamical processes that has been shown to improve process economics considerably. However, EMPC performance relies heavily on the accuracy of the process model used. As an alternative to model-based control strategies, reinforcement learning (RL) has been investigated as a model-free control methodology, but issues regarding its safety and stability remain an open research challenge. This work presents a novel framework for integrating EMPC and RL for online model parameter estimation of a class of nonlinear systems. In this framework, EMPC optimally operates the closed loop system while maintaining closed loop stability and recursive feasibility. At the same time, to optimize the process, the RL agent continuously compares the measured state of the process with the model's predictions (nominal states), and modifies model parameters accordingly. The major advantage of this framework is its simplicity; state-of-the-art RL algorithms and EMPC schemes can be employed with minimal modifications. The performance of the proposed framework is illustrated on a network of reactions with challenging dynamics and practical significance. This framework allows control, optimization, and model correction to be performed online and continuously, making autonomous reactor operation more attainable.
http://w3id.org/mlsea/pwc/scientificWork/A%20Reinforcement-Learning-Based%20Energy-Efficient%20Framework%20for%20Multi-Task%20Video%20Analytics%20Pipeline                                                                                  A Reinforcement-Learning-Based Energy-Efficient Framework for Multi-Task Video Analytics Pipeline                                                                                  Deep-learning-based video processing has yielded transformative results in recent years. However, the video analytics pipeline is energy-intensive due to high data rates and reliance on complex inference algorithms, which limits its adoption in energy-constrained applications. Motivated by the observation of high and variable spatial redundancy and temporal dynamics in video data streams, we design and evaluate an adaptive-resolution optimization framework to minimize the energy use of multi-task video analytics pipelines. Instead of heuristically tuning the input data resolution of individual tasks, our framework utilizes deep reinforcement learning to dynamically govern the input resolution and computation of the entire video analytics pipeline. By monitoring the impact of varying resolution on the quality of high-dimensional video analytics features, hence the accuracy of video analytics results, the proposed end-to-end optimization framework learns the best non-myopic policy for dynamically controlling the resolution of input video streams to globally optimize energy efficiency. Governed by reinforcement learning, optical flow is incorporated into the framework to minimize unnecessary spatio-temporal redundancy that leads to re-computation, while preserving accuracy. The proposed framework is applied to video instance segmentation which is one of the most challenging computer vision tasks, and achieves better energy efficiency than all baseline methods of similar accuracy on the YouTube-VIS dataset.
http://w3id.org/mlsea/pwc/scientificWork/A%20Remote%20Carrier%20Synchronization%20Technique%20for%20Coherent%20Distributed%20Remote%20Sensing%20Systems                                                                                  A Remote Carrier Synchronization Technique for Coherent Distributed Remote Sensing Systems                                                                                  Phase, frequency, and time synchronization are crucial requirements for many applications, such as multi-static remote sensing and communication systems. Moreover, the synchronization solution becomes even more challenging when the nodes are orbiting or flying on airborne or spaceborne platforms. This paper compares the available technologies used for the synchronization and coordination of nodes in distributed remote sensing applications. Additionally, this paper proposes a general system model and identifies preliminary guidelines and critical elements for implementing the synchronization mechanisms exploiting the inter-satellite communication link. The distributed phase synchronization loop introduced in this work deals with the self-interference in a full-duplex point to point scenario by transmitting two carriers at each node. All carriers appear with different frequency offsets around a central frequency, called the application central-frequency or the beamforming frequency. This work includes a detailed analysis of the proposed algorithm and the required simulations to verify its performance for different phase noise, AWGN, and Doppler shift scenarios.
http://w3id.org/mlsea/pwc/scientificWork/A%20Replication%20Study%20of%20Dense%20Passage%20Retriever                                                                                  A Replication Study of Dense Passage Retriever                                                                                  Text retrieval using learned dense representations has recently emerged as a promising alternative to 'traditional' text retrieval using sparse bag-of-words representations. One recent work that has garnered much attention is the dense passage retriever (DPR) technique proposed by Karpukhin et al. (2020) for end-to-end open-domain question answering. We present a replication study of this work, starting with model checkpoints provided by the authors, but otherwise from an independent implementation in our group's Pyserini IR toolkit and PyGaggle neural text ranking library. Although our experimental results largely verify the claims of the original paper, we arrived at two important additional findings that contribute to a better understanding of DPR: First, it appears that the original authors under-report the effectiveness of the BM25 baseline and hence also dense--sparse hybrid retrieval results. Second, by incorporating evidence from the retriever and an improved answer span scoring technique, we are able to improve end-to-end question answering effectiveness using exactly the same models as in the original work.
http://w3id.org/mlsea/pwc/scientificWork/A%20Reputation%20Game%20Simulation%3A%20Emergent%20Social%20Phenomena%20from%20Information%20Theory                                                                                  A Reputation Game Simulation: Emergent Social Phenomena from Information Theory                                                                                  Reputation is a central element of social communications, be it with human or artificial intelligence (AI), and as such can be the primary target of malicious communication strategies. There is already a vast amount of literature on trust networks addressing this issue and proposing ways to simulate these networks dynamics using Bayesian principles and involving Theory of Mind models. The main issue for these simulations is usually the amount of information that can be stored and is usually solved by discretising variables and using hard thresholds. Here we propose a novel approach to the way information is updated that accounts for knowledge uncertainty and is closer to reality. In our game, agents use information compression techniques to capture their complex environment and store it in their finite memories. The loss of information that results from this leads to emergent phenomena, such as echo chambers, self-deception, deception symbiosis, and freezing of group opinions. Various malicious strategies of agents are studied for their impact on group sociology, like sycophancy, egocentricity, pathological lying, and aggressiveness. Even though our modeling could be made more complex, our set-up can already provide insights into social interactions and can be used to investigate the effects of various communication strategies and find ways to counteract malicious ones. Eventually this work should help to safeguard the design of non-abusive AI systems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Research%20on%20Cross-sectional%20Return%20Dispersion%20and%20Volatility%20of%20US%20Stock%20Market%20during%20COVID-19                                                                                  A Research on Cross-sectional Return Dispersion and Volatility of US Stock Market during COVID-19                                                                                  We studied the volatility and cross-sectional return dispersion effect of S&P Health Care Sector under the covid-19 epidemic. We innovatively used the Google index to proxy the impact of the epidemic and modeled the volatility. We also studied the influencing factors of the log-return of S&P Energy Sector and S&P Health Care Sector. We found that volatility is significantly affected by both the epidemic and cross-sectional return dispersion, and the coefficients in front of them are all positive, which means that the herding behaviour did not exist and as the cross-sectional return dispersion increases and the epidemic becomes more severe, the volatility of stock returns is also increasing. We also found that the epidemic has a significant negative impact on the return of the energy sector, and finally we provided our suggestions to investors.
http://w3id.org/mlsea/pwc/scientificWork/A%20Residual%20Network%20based%20Deep%20Learning%20Model%20for%20Detection%20of%20COVID-19%20from%20Cough%20Sounds                                                                                  A Residual Network based Deep Learning Model for Detection of COVID-19 from Cough Sounds                                                                                  The present work proposes a deep-learning-based approach for the classification of COVID-19 coughs from non-COVID-19 coughs and that can be used as a low-resource-based tool for early detection of the onset of such respiratory diseases. The proposed system uses the ResNet-50 architecture, a popularly known Convolutional Neural Network (CNN) for image recognition tasks, fed with the log-Mel spectrums of the audio data to discriminate between the two types of coughs. For the training and validation of the proposed deep learning model, this work utilizes the Track-1 dataset provided by the DiCOVA Challenge 2021 organizers. Additionally, to increase the number of COVID-positive samples and to enhance variability in the training data, it has also utilized a large open-source database of COVID-19 coughs collected by the EPFL CoughVid team. Our developed model has achieved an average validation AUC of 98.88%. Also, applying this model on the Blind Test Set released by the DiCOVA Challenge, the system has achieved a Test AUC of 75.91%, Test Specificity of 62.50%, and Test Sensitivity of 80.49%. Consequently, this submission has secured 16th position in the DiCOVA Challenge 2021 leader-board.
http://w3id.org/mlsea/pwc/scientificWork/A%20Result%20based%20Portable%20Framework%20for%20Spoken%20Language%20Understanding                                                                                  A Result based Portable Framework for Spoken Language Understanding                                                                                  Spoken language understanding (SLU), which is a core component of the task-oriented dialogue system, has made substantial progress in the research of single-turn dialogue. However, the performance in multi-turn dialogue is still not satisfactory in the sense that the existing multi-turn SLU methods have low portability and compatibility for other single-turn SLU models. Further, existing multi-turn SLU methods do not exploit the historical predicted results when predicting the current utterance, which wastes helpful information. To gap those shortcomings, in this paper, we propose a novel Result-based Portable Framework for SLU (RPFSLU). RPFSLU allows most existing single-turn SLU models to obtain the contextual information from multi-turn dialogues and takes full advantage of predicted results in the dialogue history during the current prediction. Experimental results on the public dataset KVRET have shown that all SLU models in baselines acquire enhancement by RPFSLU on multi-turn SLU tasks.
http://w3id.org/mlsea/pwc/scientificWork/A%20Reversed%20and%20Shift%20Sparse%20Array%20Scheme%20based%20on%20the%20Difference%20and%20Sum%20Co-array                                                                                  A Reversed and Shift Sparse Array Scheme based on the Difference and Sum Co-array                                                                                  The reversed and shift (RAS) sparse array scheme, which is based on the difference and sum co-array (DSCA) and remarkably enhances the capability of identifying sources, is proposed. For the original nested array (NA) or co-prime array (CPA), there exists a large overlap between its difference co-array and sum co-array, which prevents it from obtaining high degrees of freedom (DOFs). Motivated by this fact, the RAS scheme is designed for reducing the overlap while increasing the DOFs for direction of arrival (DOA) estimation. The proposed scheme is effective for both NA and CPA. The closed-form expression for the relationship between the number of physical sensors and the number of consecutive DSCA sensors is derived. Compared with some representative DSCA based sparse arrays, the proposed one can achieve longer consecutive virtual array. Simulation experiments are carried out to exhibit the enhanced DOA estimation performance of RAS scheme.
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20of%20Anonymization%20for%20Healthcare%20Data                                                                                  A Review of Anonymization for Healthcare Data                                                                                  Mining health data can lead to faster medical decisions, improvement in the quality of treatment, disease prevention, reduced cost, and it drives innovative solutions within the healthcare sector. However, health data is highly sensitive and subject to regulations such as the General Data Protection Regulation (GDPR), which aims to ensure patient's privacy. Anonymization or removal of patient identifiable information, though the most conventional way, is the first important step to adhere to the regulations and incorporate privacy concerns. In this paper, we review the existing anonymization techniques and their applicability to various types (relational and graph-based) of health data. Besides, we provide an overview of possible attacks on anonymized data. We illustrate via a reconstruction attack that anonymization though necessary, is not sufficient to address patient privacy and discuss methods for protecting against such attacks. Finally, we discuss tools that can be used to achieve anonymization.
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20of%20Assistive%20Technologies%20for%20Activities%20of%20Daily%20Living%20of%20Elderly                                                                                  A Review of Assistive Technologies for Activities of Daily Living of Elderly                                                                                  One of the distinct features of this century has been the population of older adults which has been on a constant rise. Elderly people have several needs and requirements due to physical disabilities, cognitive issues, weakened memory and disorganized behavior, that they face with increasing age. The extent of these limitations also differs according to the varying diversities in elderly, which include age, gender, background, experience, skills, knowledge and so on. These varying needs and challenges with increasing age, limits abilities of older adults to perform Activities of Daily Living (ADLs) in an independent manner. To add to it, the shortage of caregivers creates a looming need for technology-based services for elderly people, to assist them in performing their daily routine tasks to sustain their independent living and active aging. To address these needs, this work consists of making three major contributions in this field. First, it provides a rather comprehensive review of assisted living technologies aimed at helping elderly people to perform ADLs. Second, the work discusses the challenges identified through this review, that currently exist in the context of implementation of assisted living services for elderly care in Smart Homes and Smart Cities. Finally, the work also outlines an approach for implementation, extension and integration of the existing works in this field for development of a much-needed framework that can provide personalized assistance and user-centered behavior interventions to elderly as per their varying and ever-changing needs.
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20of%20Autonomous%20Road%20Vehicle%20Integrated%20Approaches%20to%20an%20Emergency%20Obstacle%20Avoidance%20Maneuver                                                                                  A Review of Autonomous Road Vehicle Integrated Approaches to an Emergency Obstacle Avoidance Maneuver                                                                                  As passenger vehicle technologies have advanced, so have their capabilities to avoid obstacles, especially with developments in tires, suspensions, steering, as well as safety technologies like ABS, ESC, and more recently, ADAS systems. However, environments around passenger vehicles have also become more complex, and dangerous. There have previously been studies that outline driver tendencies and performance capabilities when attempting to avoid obstacles while driving passenger vehicles. Now that autonomous vehicles are being developed with obstacle avoidance capabilities, it is important to target performance that meets or exceeds that of human drivers. This manuscript highlights systems that are crucial for an emergency obstacle avoidance maneuver (EOAM) and identifies the state-of-the-art for each of the related systems, while considering the nuances of traveling at highway speeds. Some of the primary EOAM-related systems/areas that are discussed in this review are: general path planning methods, system hierarchies, decision-making, trajectory generation, and trajectory-tracking control methods. After concluding remarks, suggestions for future work which could lead to an ideal EOAM development, are discussed.
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20of%20Confidentiality%20Threats%20Against%20Embedded%20Neural%20Network%20Models                                                                                  A Review of Confidentiality Threats Against Embedded Neural Network Models                                                                                  Utilization of Machine Learning (ML) algorithms, especially Deep Neural Network (DNN) models, becomes a widely accepted standard in many domains more particularly IoT-based systems. DNN models reach impressive performances in several sensitive fields such as medical diagnosis, smart transport or security threat detection, and represent a valuable piece of Intellectual Property. Over the last few years, a major trend is the large-scale deployment of models in a wide variety of devices. However, this migration to embedded systems is slowed down because of the broad spectrum of attacks threatening the integrity, confidentiality and availability of embedded models. In this review, we cover the landscape of attacks targeting the confidentiality of embedded DNN models that may have a major impact on critical IoT systems, with a particular focus on model extraction and data leakage. We highlight the fact that Side-Channel Analysis (SCA) is a relatively unexplored bias by which model's confidentiality can be compromised. Input data, architecture or parameters of a model can be extracted from power or electromagnetic observations, testifying a real need from a security point of view.
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20of%20Cross-Domain%20Text-to-SQL%20Models                                                                                  A Review of Cross-Domain Text-to-SQL Models                                                                                  WikiSQL and Spider, the large-scale cross-domain text-to-SQL datasets, have attracted much attention from the research community. The leaderboards of WikiSQL and Spider show that many researchers propose their models trying to solve the text-to-SQL problem. This paper first divides the top models in these two leaderboards into two paradigms. We then present details not mentioned in their original paper by evaluating the key components, including schema linking, pretrained word embeddings, and reasoning assistance modules. Based on the analysis of these models, we want to promote understanding of the text-to-SQL field and find out some interesting future works, for example, it is worth studying the text-to-SQL problem in an environment where it is more challenging to build schema linking and also worth studying combing the advantage of each model toward text-to-SQL.
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20of%20Deep%20Reinforcement%20Learning%20for%20Smart%20Building%20Energy%20Management                                                                                  A Review of Deep Reinforcement Learning for Smart Building Energy Management                                                                                  Global buildings account for about 30% of the total energy consumption and carbon emission, raising severe energy and environmental concerns. Therefore, it is significant and urgent to develop novel smart building energy management (SBEM) technologies for the advance of energy-efficient and green buildings. However, it is a nontrivial task due to the following challenges. Firstly, it is generally difficult to develop an explicit building thermal dynamics model that is both accurate and efficient enough for building control. Secondly, there are many uncertain system parameters (e.g., renewable generation output, outdoor temperature, and the number of occupants). Thirdly, there are many spatially and temporally coupled operational constraints. Fourthly, building energy optimization problems can not be solved in real-time by traditional methods when they have extremely large solution spaces. Fifthly, traditional building energy management methods have respective applicable premises, which means that they have low versatility when confronted with varying building environments. With the rapid development of Internet of Things technology and computation capability, artificial intelligence technology find its significant competence in control and optimization. As a general artificial intelligence technology, deep reinforcement learning (DRL) is promising to address the above challenges. Notably, the recent years have seen the surge of DRL for SBEM. However, there lacks a systematic overview of different DRL methods for SBEM. To fill the gap, this paper provides a comprehensive review of DRL for SBEM from the perspective of system scale. In particular, we identify the existing unresolved issues and point out possible future research directions.
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20of%20Disease%20and%20Development                                                                                  A Review of Disease and Development                                                                                  Acemoglu and Johnson (2007) put forward the unprecedented view that health improvement has no significant effect on income growth. To arrive at this conclusion, they constructed predicted mortality as an instrumental variable based on the WHO international disease interventions to analyse this problem. I replicate the process of their research and eliminate some biases in their estimate. In addition, and more importantly, we argue that the construction of their instrumental variable contains a violation of the exclusion restriction of their instrumental variable. This negative correlation between health improvement and income growth still lacks an accurate causal explanation, according to which the instrumental variable they constructed increases reverse causality bias instead of eliminating it.
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20of%20Explainable%20Artificial%20Intelligence%20in%20Manufacturing                                                                                  A Review of Explainable Artificial Intelligence in Manufacturing                                                                                  The implementation of Artificial Intelligence (AI) systems in the manufacturing domain enables higher production efficiency, outstanding performance, and safer operations, leveraging powerful tools such as deep learning and reinforcement learning techniques. Despite the high accuracy of these models, they are mostly considered black boxes: they are unintelligible to the human. Opaqueness affects trust in the system, a factor that is critical in the context of decision-making. We present an overview of Explainable Artificial Intelligence (XAI) techniques as a means of boosting the transparency of models. We analyze different metrics to evaluate these techniques and describe several application scenarios in the manufacturing domain.
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20of%20Formal%20Methods%20applied%20to%20Machine%20Learning                                                                                  A Review of Formal Methods applied to Machine Learning                                                                                  We review state-of-the-art formal methods applied to the emerging field of the verification of machine learning systems. Formal methods can provide rigorous correctness guarantees on hardware and software systems. Thanks to the availability of mature tools, their use is well established in the industry, and in particular to check safety-critical applications as they undergo a stringent certification process. As machine learning is becoming more popular, machine-learned components are now considered for inclusion in critical systems. This raises the question of their safety and their verification. Yet, established formal methods are limited to classic, i.e. non machine-learned software. Applying formal methods to verify systems that include machine learning has only been considered recently and poses novel challenges in soundness, precision, and scalability. We first recall established formal methods and their current use in an exemplar safety-critical field, avionic software, with a focus on abstract interpretation based techniques as they provide a high level of scalability. This provides a golden standard and sets high expectations for machine learning verification. We then provide a comprehensive and detailed review of the formal methods developed so far for machine learning, highlighting their strengths and limitations. The large majority of them verify trained neural networks and employ either SMT, optimization, or abstract interpretation techniques. We also discuss methods for support vector machines and decision tree ensembles, as well as methods targeting training and data preparation, which are critical but often neglected aspects of machine learning. Finally, we offer perspectives for future research directions towards the formal verification of machine learning systems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20of%20Human%20Evaluation%20for%20Style%20Transfer                                                                                  A Review of Human Evaluation for Style Transfer                                                                                  This paper reviews and summarizes human evaluation practices described in 97 style transfer papers with respect to three main evaluation aspects: style transfer, meaning preservation, and fluency. In principle, evaluations by human raters should be the most reliable. However, in style transfer papers, we find that protocols for human evaluations are often underspecified and not standardized, which hampers the reproducibility of research in this field and progress toward better human and automatic evaluation methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20of%20Lithium-Ion%20Battery%20Models%20in%20Techno-economic%20Analyses%20of%20Power%20Systems                                                                                  A Review of Lithium-Ion Battery Models in Techno-economic Analyses of Power Systems                                                                                  The penetration of the lithium-ion battery energy storage system (BESS) into the power system environment occurs at a colossal rate worldwide. This is mainly because it is considered as one of the major tools to decarbonize, digitalize, and democratize the electricity grid. The economic viability and technical reliability of projects with batteries require appropriate assessment because of high capital expenditures, deterioration in charging/discharging performance and uncertainty with regulatory policies. Most of the power system economic studies employ a simple power-energy representation coupled with an empirical description of degradation to model the lithium-ion battery. This approach to modelling may result in violations of the safe operation and misleading estimates of the economic benefits. Recently, the number of publications on techno-economic analysis of BESS with more details on the lithium-ion battery performance has increased. The aim of this review paper is to explore these publications focused on the grid-scale BESS applications and to discuss the impacts of using more sophisticated modelling approaches. First, an overview of the three most popular battery models is given, followed by a review of the applications of such models. The possible directions of future research of employing detailed battery models in power systems' techno-economic studies are then explored.
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20of%20Machine%20Learning%20Classification%20Using%20Quantum%20Annealing%20for%20Real-world%20Applications                                                                                  A Review of Machine Learning Classification Using Quantum Annealing for Real-world Applications                                                                                  Optimizing the training of a machine learning pipeline helps in reducing training costs and improving model performance. One such optimizing strategy is quantum annealing, which is an emerging computing paradigm that has shown potential in optimizing the training of a machine learning model. The implementation of a physical quantum annealer has been realized by D-Wave systems and is available to the research community for experiments. Recent experimental results on a variety of machine learning applications using quantum annealing have shown interesting results where the performance of classical machine learning techniques is limited by limited training data and high dimensional features. This article explores the application of D-Wave's quantum annealer for optimizing machine learning pipelines for real-world classification problems. We review the application domains on which a physical quantum annealer has been used to train machine learning classifiers. We discuss and analyze the experiments performed on the D-Wave quantum annealer for applications such as image recognition, remote sensing imagery, computational biology, and particle physics. We discuss the possible advantages and the problems for which quantum annealing is likely to be advantageous over classical computation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20of%20Wind%20Speed%20and%20Wind%20Power%20Forecasting%20Techniques                                                                                  A Review of Wind Speed and Wind Power Forecasting Techniques                                                                                  Forecasting a particular variable can depend upon temporal or spatial scale. Temporal variations that indicate variations with time, reflect the stochasticity present in the variable. Spatial variation usually are dominant in climatology and meteorology. Temporal scale for a variable can be modeled in terms of time-series. A time series is a successively ordered sequence of numerical data points, and can be taken on any variable changing with time. Wind speed forecasting applications lie majorly in the area of electricity market clearing, economic load dispatch and scheduling, and sometimes to provide ancillary support. Thus, a proper classification based on the prediction horizon i.e. the duration of prediction becomes important for various transmission system operators.
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20of%20the%20Non-Invasive%20Techniques%20for%20Monitoring%20Different%20Aspects%20of%20Sleep                                                                                  A Review of the Non-Invasive Techniques for Monitoring Different Aspects of Sleep                                                                                  Quality sleep is very important for a healthy life. Nowadays, many people around the world are not getting enough sleep which is having negative impacts on their lifestyles. Studies are being conducted for sleep monitoring and have now become an important tool for understanding sleep behavior. The gold standard method for sleep analysis is polysomnography (PSG) conducted in a clinical environment but this method is both expensive and complex for long-term use. With the advancements in the field of sensors and the introduction of off-the-shelf technologies, unobtrusive solutions are becoming common as alternatives for in-home sleep monitoring. Various solutions have been proposed using both wearable and non-wearable methods which are cheap and easy to use for in-home sleep monitoring. In this paper, we present a comprehensive survey of the latest research works (2015 and after) conducted in various categories of sleep monitoring including sleep stage classification, sleep posture recognition, sleep disorders detection, and vital signs monitoring. We review the latest works done using the non-invasive approach and cover both wearable and non-wearable methods. We discuss the design approaches and key attributes of the work presented and provide an extensive analysis based on 10 key factors, to give a comprehensive overview of the recent developments and trends in all four categories of sleep monitoring. We also present some publicly available datasets for different categories of sleep monitoring. In the end, we discuss several open issues and provide future research directions in the area of sleep monitoring.
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20on%20Bio-Cyber%20Interfaces%20for%20Intrabody%20Molecular%20Communications%20Systems                                                                                  A Review on Bio-Cyber Interfaces for Intrabody Molecular Communications Systems                                                                                  The recent advancements in bio-engineering and wireless communications systems have motivated researchers to propose novel applications for telemedicine, therapeutics and human health monitoring. For instance, through wireless medical telemetry a healthcare worker can remotely measure biological signals and control certain processes in the organism required for the maintenance of the patient's health state. This technology can be further extended to use Bio-Nano devices to promote a real-time monitoring of the human health and storage of the gathered data in the cloud. This brings new challenges and opportunities for the development of biosensing network, which will depend on the extension of the current intrabody devices functionalities. In this paper we will cover the recent progress made on implantable micro-scale devices and introduce the perspective of improve them to foster the development of new theranostics based on data collected at the nanoscale level.
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20on%20Explainability%20in%20Multimodal%20Deep%20Neural%20Nets                                                                                  A Review on Explainability in Multimodal Deep Neural Nets                                                                                  Artificial Intelligence techniques powered by deep neural nets have achieved much success in several application domains, most significantly and notably in the Computer Vision applications and Natural Language Processing tasks. Surpassing human-level performance propelled the research in the applications where different modalities amongst language, vision, sensory, text play an important role in accurate predictions and identification. Several multimodal fusion methods employing deep learning models are proposed in the literature. Despite their outstanding performance, the complex, opaque and black-box nature of the deep neural nets limits their social acceptance and usability. This has given rise to the quest for model interpretability and explainability, more so in the complex tasks involving multimodal AI methods. This paper extensively reviews the present literature to present a comprehensive survey and commentary on the explainability in multimodal deep neural nets, especially for the vision and language tasks. Several topics on multimodal AI and its applications for generic domains have been covered in this paper, including the significance, datasets, fundamental building blocks of the methods and techniques, challenges, applications, and future trends in this domain
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20on%20Oracle%20Issues%20in%20Machine%20Learning                                                                                  A Review on Oracle Issues in Machine Learning                                                                                  Machine learning contrasts with traditional software development in that the oracle is the data, and the data is not always a correct representation of the problem that machine learning tries to model. We present a survey of the oracle issues found in machine learning and state-of-the-art solutions for dealing with these issues. These include lines of research for differential testing, metamorphic testing, and test coverage. We also review some recent improvements to robustness during modeling that reduce the impact of oracle issues, as well as tools and frameworks for assisting in testing and discovering issues specific to the dataset.
http://w3id.org/mlsea/pwc/scientificWork/A%20Review%20on%20Semi-Supervised%20Relation%20Extraction                                                                                  A Review on Semi-Supervised Relation Extraction                                                                                  Relation extraction (RE) plays an important role in extracting knowledge from unstructured text but requires a large amount of labeled corpus. To reduce the expensive annotation efforts, semisupervised learning aims to leverage both labeled and unlabeled data. In this paper, we review and compare three typical methods in semi-supervised RE with deep learning or meta-learning: self-ensembling, which forces consistent under perturbations but may confront insufficient supervision; self-training, which iteratively generates pseudo labels and retrain itself with the enlarged labeled set; dual learning, which leverages a primal task and a dual task to give mutual feedback. Mean-teacher (Tarvainen and Valpola, 2017), LST (Li et al., 2019), and DualRE (Lin et al., 2019) are elaborated as the representatives to alleviate the weakness of these three methods, respectively.
http://w3id.org/mlsea/pwc/scientificWork/A%20Riemannian%20Newton%20Trust-Region%20Method%20for%20Fitting%20Gaussian%20Mixture%20Models                                                                                  A Riemannian Newton Trust-Region Method for Fitting Gaussian Mixture Models                                                                                  Gaussian Mixture Models are a powerful tool in Data Science and Statistics that are mainly used for clustering and density approximation. The task of estimating the model parameters is in practice often solved by the Expectation Maximization (EM) algorithm which has its benefits in its simplicity and low per-iteration costs. However, the EM converges slowly if there is a large share of hidden information or overlapping clusters. Recent advances in manifold optimization for Gaussian Mixture Models have gained increasing interest. We introduce an explicit formula for the Riemannian Hessian for Gaussian Mixture Models. On top, we propose a new Riemannian Newton Trust-Region method which outperforms current approaches both in terms of runtime and number of iterations. We apply our method on clustering problems and density approximation tasks. Our method is very powerful for data with a large share of hidden information compared to existing methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20Riemannian%20smoothing%20steepest%20descent%20method%20for%20non-Lipschitz%20optimization%20on%20submanifolds                                                                                  A Riemannian smoothing steepest descent method for non-Lipschitz optimization on submanifolds                                                                                  In this paper, we propose a Riemannian smoothing steepest descent method to minimize a nonconvex and non-Lipschitz function on submanifolds. The generalized subdifferentials on Riemannian manifold and the Riemannian gradient sub-consistency are defined and discussed. We prove that any accumulation point of the sequence generated by the Riemannian smoothing steepest descent method is a stationary point associated with the smoothing function employed in the method, which is necessary for the local optimality of the original non-Lipschitz problem. Under the Riemannian gradient sub-consistency condition, we also prove that any accumulation point is a Riemannian limiting stationary point of the original non-Lipschitz problem. Numerical experiments are conducted to demonstrate the efficiency of the proposed method.
http://w3id.org/mlsea/pwc/scientificWork/A%20Rigid%20Registration%20Method%20in%20TEVAR                                                                                  A Rigid Registration Method in TEVAR                                                                                  Since the mapping relationship between definitized intra-interventional X-ray and undefined pre-interventional Computed Tomography(CT) is uncertain, auxiliary positioning devices or body markers, such as medical implants, are commonly used to determine this relationship. However, such approaches can not be widely used in clinical due to the complex realities. To determine the mapping relationship, and achieve a initializtion post estimation of human body without auxiliary equipment or markers, proposed method applies image segmentation and deep feature matching to directly match the X-ray and CT images. As a result, the well-trained network can directly predict the spatial correspondence between arbitrary X-ray and CT. The experimental results show that when combining our approach with the conventional approach, the achieved accuracy and speed can meet the basic clinical intervention needs, and it provides a new direction for intra-interventional registration.
http://w3id.org/mlsea/pwc/scientificWork/A%20Rigorous%20Information-Theoretic%20Definition%20of%20Redundancy%20and%20Relevancy%20in%20Feature%20Selection%20Based%20on%20%28Partial%29%20Information%20Decomposition                                                                                  A Rigorous Information-Theoretic Definition of Redundancy and Relevancy in Feature Selection Based on (Partial) Information Decomposition                                                                                  Selecting a minimal feature set that is maximally informative about a target variable is a central task in machine learning and statistics. Information theory provides a powerful framework for formulating feature selection algorithms -- yet, a rigorous, information-theoretic definition of feature relevancy, which accounts for feature interactions such as redundant and synergistic contributions, is still missing. We argue that this lack is inherent to classical information theory which does not provide measures to decompose the information a set of variables provides about a target into unique, redundant, and synergistic contributions. Such a decomposition has been introduced only recently by the partial information decomposition (PID) framework. Using PID, we clarify why feature selection is a conceptually difficult problem when approached using information theory and provide a novel definition of feature relevancy and redundancy in PID terms. From this definition, we show that the conditional mutual information (CMI) maximizes relevancy while minimizing redundancy and propose an iterative, CMI-based algorithm for practical feature selection. We demonstrate the power of our CMI-based algorithm in comparison to the unconditional mutual information on benchmark examples and provide corresponding PID estimates to highlight how PID allows to quantify information contribution of features and their interactions in feature-selection problems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Road-map%20to%20Robot%20Task%20Execution%20with%20the%20Functional%20Object-Oriented%20Network                                                                                  A Road-map to Robot Task Execution with the Functional Object-Oriented Network                                                                                  Following work on joint object-action representations, the functional object-oriented network (FOON) was introduced as a knowledge graph representation for robots. Taking the form of a bipartite graph, a FOON contains symbolic or high-level information that would be pertinent to a robot's understanding of its environment and tasks in a way that mirrors human understanding of actions. In this work, we outline a road-map for future development of FOON and its application in robotic systems for task planning as well as knowledge acquisition from demonstration. We propose preliminary ideas to show how a FOON can be created in a real-world scenario with a robot and human teacher in a way that can jointly augment existing knowledge in a FOON and teach a robot the skills it needs to replicate the demonstrated actions and solve a given manipulation problem.
http://w3id.org/mlsea/pwc/scientificWork/A%20Robotic%20Approach%20towards%20Quantifying%20Epipelagic%20Bound%20Plastic%20Using%20Deep%20Visual%20Models                                                                                  A Robotic Approach towards Quantifying Epipelagic Bound Plastic Using Deep Visual Models                                                                                  The quantification of positively buoyant marine plastic debris is critical to understanding how plastic litter accumulates across the world's oceans and is also crucial to identifying hotspots for targeted cleanup efforts. Currently, the most common method to quantify marine plastic is using manta trawls for manual sampling. However, this method is cost-intensive and requires human labor. This study removes the need for manual sampling by using an autonomous method using neural networks and computer vision models, which trained on images captured from various layers of the ocean column to perform real-time plastic quantification. The best performing model has a Mean Average Precision of 85% and an F1-Score of 0.89 while maintaining near real-time processing speeds ~2 ms/img.
http://w3id.org/mlsea/pwc/scientificWork/A%20Robust%20CACC%20Scheme%20Against%20Cyberattacks%20Via%20Multiple%20Vehicle-to-Vehicle%20Networks                                                                                  A Robust CACC Scheme Against Cyberattacks Via Multiple Vehicle-to-Vehicle Networks                                                                                  Cooperative Adaptive Cruise Control (CACC) is a vehicular technology that allows groups of vehicles on the highway to form in closely-coupled automated platoons to increase highway capacity and safety, and decrease fuel consumption and CO2 emissions. The underlying mechanism behind CACC is the use of Vehicle-to-Vehicle (V2V) wireless communication networks to transmit acceleration commands to adjacent vehicles in the platoon. However, the use of V2V networks leads to increased vulnerabilities against faults and cyberattacks at the communication channels. Communication networks serve as new access points for malicious agents trying to deteriorate the platooning performance or even cause crashes. Here, we address the problem of increasing robustness of CACC schemes against cyberattacks by the use of multiple V2V networks and a data fusion algorithm. The idea is to transmit acceleration commands multiple times through different communication networks (channels) to create redundancy at the receiver side. We exploit this redundancy to obtain attack-free estimates of acceleration commands. To accomplish this, we propose a data-fusion algorithm that takes data from all channels, returns an estimate of the true acceleration command, and isolates compromised channels. Note, however, that using estimated data for control introduces uncertainty into the loop and thus decreases performance. To minimize performance degradation, we propose a robust $H_{ infty}$ controller that reduces the joint effect of estimation errors and sensor/channel noise in the platooning performance (tracking performance and string stability). We present simulation results to illustrate the performance of our approach.
http://w3id.org/mlsea/pwc/scientificWork/A%20Robust%20Model%20for%20Trust%20Evaluation%20during%20Interactions%20between%20Agents%20in%20a%20Sociable%20Environment                                                                                  A Robust Model for Trust Evaluation during Interactions between Agents in a Sociable Environment                                                                                  Trust evaluation is an important topic in both research and applications in sociable environments. This paper presents a model for trust evaluation between agents by the combination of direct trust, indirect trust through neighbouring links and the reputation of an agent in the environment (i.e. social network) to provide the robust evaluation. Our approach is typology independent from social network structures and in a decentralized manner without a central controller, so it can be used in broad domains.
http://w3id.org/mlsea/pwc/scientificWork/A%20Robust%20Score-Driven%20Filter%20for%20Multivariate%20Time%20Series                                                                                  A Robust Score-Driven Filter for Multivariate Time Series                                                                                  A multivariate score-driven filter is developed to extract signals from noisy vector processes. By assuming that the conditional location vector from a multivariate Student's t distribution changes over time, we construct a robust filter which is able to overcome several issues that naturally arise when modeling heavy-tailed phenomena and, more in general, vectors of dependent non-Gaussian time series. We derive conditions for stationarity and invertibility and estimate the unknown parameters by maximum likelihood (ML). Strong consistency and asymptotic normality of the estimator are proved and the finite sample properties are illustrated by a Monte-Carlo study. From a computational point of view, analytical formulae are derived, which consent to develop estimation procedures based on the Fisher scoring method. The theory is supported by a novel empirical illustration that shows how the model can be effectively applied to estimate consumer prices from home scanner data.
http://w3id.org/mlsea/pwc/scientificWork/A%20Robust%20and%20Accurate%20Deep%20Learning%20based%20Pattern%20Recognition%20Framework%20for%20Upper%20Limb%20Prosthesis%20using%20sEMG                                                                                  A Robust and Accurate Deep Learning based Pattern Recognition Framework for Upper Limb Prosthesis using sEMG                                                                                  In EMG based pattern recognition (EMG-PR), deep learning-based techniques have become more prominent for their self-regulating capability to extract discriminant features from large data-sets. Moreover, the performance of traditional machine learning-based methods show limitation to categorize over a certain number of classes and degrades over a period of time. In this paper, an accurate, robust, and fast convolutional neural network-based framework for EMG pattern identification is presented. To assess the performance of the proposed system, five publicly available and benchmark data-sets of upper limb activities were used. This data-set contains 49 to 52 upper limb motions (NinaPro DB1, NinaPro DB2, and NinaPro DB3), Data with force variation, and data with arm position variation for intact and amputated subjects. The classification accuracies of 91.11% (53 classes), 89.45% (49 classes), 81.67% (49 classes of amputees), 95.67% (6 classes with force variation), and 99.11% (8 classes with arm position variation) have been observed during the testing and validation. The performance of the proposed system is compared with the state of art techniques in the literature. The findings demonstrate that classification accuracy and time complexity have improved significantly. Keras, TensorFlow's high-level API for constructing deep learning models, was used for signal pre-processing and deep-learning-based algorithms. The suggested method was run on an Intel 3.5GHz Core i7, 7th Gen CPU with 8GB DDR4 RAM.
http://w3id.org/mlsea/pwc/scientificWork/A%20Robust%20and%20Generalized%20Framework%20for%20Adversarial%20Graph%20Embedding                                                                                  A Robust and Generalized Framework for Adversarial Graph Embedding                                                                                  Graph embedding is essential for graph mining tasks. With the prevalence of graph data in real-world applications, many methods have been proposed in recent years to learn high-quality graph embedding vectors various types of graphs. However, most existing methods usually randomly select the negative samples from the original graph to enhance the training data without considering the noise. In addition, most of these methods only focus on the explicit graph structures and cannot fully capture complex semantics of edges such as various relationships or asymmetry. In order to address these issues, we propose a robust and generalized framework for adversarial graph embedding based on generative adversarial networks. Inspired by generative adversarial network, we propose a robust and generalized framework for adversarial graph embedding, named AGE. AGE generates the fake neighbor nodes as the enhanced negative samples from the implicit distribution, and enables the discriminator and generator to jointly learn each node's robust and generalized representation. Based on this framework, we propose three models to handle three types of graph data and derive the corresponding optimization algorithms, i.e., UG-AGE and DG-AGE for undirected and directed homogeneous graphs, respectively, and HIN-AGE for heterogeneous information networks. Extensive experiments show that our methods consistently and significantly outperform existing state-of-the-art methods across multiple graph mining tasks, including link prediction, node classification, and graph reconstruction.
http://w3id.org/mlsea/pwc/scientificWork/A%20Rule%20Mining-Based%20Advanced%20Persistent%20Threats%20Detection%20System                                                                                  A Rule Mining-Based Advanced Persistent Threats Detection System                                                                                  Advanced persistent threats (APT) are stealthy cyber-attacks that are aimed at stealing valuable information from target organizations and tend to extend in time. Blocking all APTs is impossible, security experts caution, hence the importance of research on early detection and damage limitation. Whole-system provenance-tracking and provenance trace mining are considered promising as they can help find causal relationships between activities and flag suspicious event sequences as they occur. We introduce an unsupervised method that exploits OS-independent features reflecting process activity to detect realistic APT-like attacks from provenance traces. Anomalous processes are ranked using both frequent and rare event associations learned from traces. Results are then presented as implications which, since interpretable, help leverage causality in explaining the detected anomalies. When evaluated on Transparent Computing program datasets (DARPA), our method outperformed competing approaches.
http://w3id.org/mlsea/pwc/scientificWork/A%20SEIRUC%20mathematical%20model%20for%20transmission%20dynamics%20of%20COVID-19                                                                                  A SEIRUC mathematical model for transmission dynamics of COVID-19                                                                                  The world is still fighting against COVID-19, which has been lasting for more than a year. Till date, it has been a greatest challenge to human beings in fighting against COVID-19 since, the pathogen SARS-COV-2 that causes COVID-19 has significant biological and transmission characteristics when compared to SARS-COV and MERS-COV pathogens. In spite of many control strategies that are implemented to reduce the disease spread, there is a rise in the number of infected cases around the world. Hence, a mathematical model which can describe the real nature and impact of COVID-19 is necessary for the better understanding of disease transmission dynamics of COVID-19. This article proposes a new compartmental SEIRUC mathematical model, which includes the new state called convalesce (C). The basic reproduction number $ mathcal{R}_0$ is identified for the proposed model. The stability analysis are performed for the disease free equilibrium ($ mathcal{E}_0$) as well for the endemic equilibrium ($ mathcal{E}_*$) by using the Routh-Hurwitz criterion. The graphical illustrations of the proposed mathematical results are provided to validate the theoretical results.
http://w3id.org/mlsea/pwc/scientificWork/A%20STDP-based%20Encoding%20Algorithm%20for%20Associative%20and%20Composite%20Data                                                                                  A STDP-based Encoding Algorithm for Associative and Composite Data                                                                                  Spike-timing-dependent plasticity(STDP) is a biological process of synaptic modification caused by the difference of firing order and timing between neurons. One of the neurodynamical roles of STDP is to form a macroscopic geometrical structure in the neuronal state space in response to a periodic input. This work proposes a practical memory model based on STDP that can store and retrieve high-dimensional associative data. The model combines STDP dynamics with an encoding scheme for distributed representations and can handle multiple composite data in a continuous manner. In the auto-associative memory task where a group of images is continuously streamed to the model, the images are successfully retrieved from an oscillating neural state whenever a proper cue is given. In the second task that deals with semantic memories embedded from sentences, the results show that words can recall multiple sentences simultaneously or one exclusively, depending on their grammatical relations.
http://w3id.org/mlsea/pwc/scientificWork/A%20Sample-Based%20Training%20Method%20for%20Distantly%20Supervised%20Relation%20Extraction%20with%20Pre-Trained%20Transformers                                                                                  A Sample-Based Training Method for Distantly Supervised Relation Extraction with Pre-Trained Transformers                                                                                  Multiple instance learning (MIL) has become the standard learning paradigm for distantly supervised relation extraction (DSRE). However, due to relation extraction being performed at bag level, MIL has significant hardware requirements for training when coupled with large sentence encoders such as deep transformer neural networks. In this paper, we propose a novel sampling method for DSRE that relaxes these hardware requirements. In the proposed method, we limit the number of sentences in a batch by randomly sampling sentences from the bags in the batch. However, this comes at the cost of losing valid sentences from bags. To alleviate the issues caused by random sampling, we use an ensemble of trained models for prediction. We demonstrate the effectiveness of our approach by using our proposed learning setting to fine-tuning BERT on the widely NYT dataset. Our approach significantly outperforms previous state-of-the-art methods in terms of AUC and P@N metrics.
http://w3id.org/mlsea/pwc/scientificWork/A%20Scalable%20256-Elements%20E-Band%20Phased-Array%20Transceiver%20for%20Broadband%20Communication                                                                                  A Scalable 256-Elements E-Band Phased-Array Transceiver for Broadband Communication                                                                                  For E-band wireless communications, a high gain steerable antenna with sub-arrays is desired to reduce the implementation complexity. This paper presents an E-band communication link with 256-elements antennas based on 8-elements sub-arrays and four beam-forming chips in silicon germanium (SiGe) bipolar complementary metal-oxide-semiconductor (BiCMOS), which is packaged on a 19-layer low temperature co-fired ceramic (LTCC) substrate. After the design and manufacture of the 256-elements antenna, a fast near-field calibration method is proposed for calibration, where a single near-field measurement is required. Then near-field to far-field (NFFF) transform and far-field to near-field (FFNF) transform are used for the bore-sight calibration. The comparison with high frequency structure simulator (HFSS) is utilized for the non-bore-sight calibration. Verified on the 256-elements antenna, the beam-forming performance measured in the chamber is in good agreement with the simulations. The communication in the office environment is also realized using a fifth generation (5G) new radio (NR) system, whose bandwidth is 400 megahertz (MHz) and waveform format is orthogonal frequency division multiplexing (OFDM) with 120 kilohertz (kHz) sub-carrier spacing.
http://w3id.org/mlsea/pwc/scientificWork/A%20Scalable%20Algorithm%20for%20Anomaly%20Detection%20via%20Learning-Based%20Controlled%20Sensing                                                                                  A Scalable Algorithm for Anomaly Detection via Learning-Based Controlled Sensing                                                                                  We address the problem of sequentially selecting and observing processes from a given set to find the anomalies among them. The decision-maker observes one process at a time and obtains a noisy binary indicator of whether or not the corresponding process is anomalous. In this setting, we develop an anomaly detection algorithm that chooses the process to be observed at a given time instant, decides when to stop taking observations, and makes a decision regarding the anomalous processes. The objective of the detection algorithm is to arrive at a decision with an accuracy exceeding a desired value while minimizing the delay in decision making. Our algorithm relies on a Markov decision process defined using the marginal probability of each process being normal or anomalous, conditioned on the observations. We implement the detection algorithm using the deep actor-critic reinforcement learning framework. Unlike prior work on this topic that has exponential complexity in the number of processes, our algorithm has computational and memory requirements that are both polynomial in the number of processes. We demonstrate the efficacy of our algorithm using numerical experiments by comparing it with the state-of-the-art methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20Scalable%20Second%20Order%20Method%20for%20Ill-Conditioned%20Matrix%20Completion%20from%20Few%20Samples                                                                                  A Scalable Second Order Method for Ill-Conditioned Matrix Completion from Few Samples                                                                                  We propose an iterative algorithm for low-rank matrix completion that can be interpreted as an iteratively reweighted least squares (IRLS) algorithm, a saddle-escaping smoothing Newton method or a variable metric proximal gradient method applied to a non-convex rank surrogate. It combines the favorable data-efficiency of previous IRLS approaches with an improved scalability by several orders of magnitude. We establish the first local convergence guarantee from a minimal number of samples for that class of algorithms, showing that the method attains a local quadratic convergence rate. Furthermore, we show that the linear systems to be solved are well-conditioned even for very ill-conditioned ground truth matrices. We provide extensive experiments, indicating that unlike many state-of-the-art approaches, our method is able to complete very ill-conditioned matrices with a condition number of up to $10^{10}$ from few samples, while being competitive in its scalability.
http://w3id.org/mlsea/pwc/scientificWork/A%20Scalable%20and%20Reproducible%20System-on-Chip%20Simulation%20for%20Reinforcement%20Learning                                                                                  A Scalable and Reproducible System-on-Chip Simulation for Reinforcement Learning                                                                                  Deep Reinforcement Learning (DRL) underlies in a simulated environment and optimizes objective goals. By extending the conventional interaction scheme, this paper proffers gym-ds3, a scalable and reproducible open environment tailored for a high-fidelity Domain-Specific System-on-Chip (DSSoC) application. The simulation corroborates to schedule hierarchical jobs onto heterogeneous System-on-Chip (SoC) processors and bridges the system to reinforcement learning research. We systematically analyze the representative SoC simulator and discuss the primary challenging aspects that the system (1) continuously generates indefinite jobs at a rapid injection rate, (2) optimizes complex objectives, and (3) operates in steady-state scheduling. We provide exemplary snippets and experimentally demonstrate the run-time performances on different schedulers that successfully mimic results achieved from the standard DS3 framework and real-world embedded systems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Schottky-Diode-Based%20Wake-Up%20Receiver%20for%20IoT%20Applications                                                                                  A Schottky-Diode-Based Wake-Up Receiver for IoT Applications                                                                                  This paper presents an always-on low-power wake-up receiver (WuRx) that activates the remainder of the system when a wake-up signal is detected. The proposed receiver has two phases of waking up. The first phase uses an integrated CMOS Schottky diodes to detect the signal power at a low bias current. The approach dissipates low quiescent power and allows the reuse of the design in multiple frequency bands with only modifying the matching network. In the second phase, a data-locked startable oscillator is proposed to correlate the received data with a target signature. This design eliminates the area and power dissipation of an external crystal oscillator and only turns on when the second phase is activated. By correlating to a target signature, the second phase also reduces the probability of a false alarm (PFA) that would otherwise wake up the high-power bulk of the system. The two-phase approach leads to significant reduction in average power consumption when compared to a single-phase design. This implementation targets sub-ms wake-up latency and operates in the unlicensed band at a 750-MHz carrier frequency with a data rate of 200 kbps. The design achieves $ sim$8.45pJ/bit and $<$-50 dBm of input sensitivity and average power of 1.69$ mu$W. The system is implemented in 65-nm CMOS technology and occupies an area of 1mm$ times$0.75mm.
http://w3id.org/mlsea/pwc/scientificWork/A%20Search%20Engine%20for%20Scientific%20Publications%3A%20a%20Cybersecurity%20Case%20Study                                                                                  A Search Engine for Scientific Publications: a Cybersecurity Case Study                                                                                  Cybersecurity is a very challenging topic of research nowadays, as digitalization increases the interaction of people, software and services on the Internet by means of technology devices and networks connected to it. The field is broad and has a lot of unexplored ground under numerous disciplines such as management, psychology, and data science. Its large disciplinary spectrum and many significant research topics generate a considerable amount of information, making it hard for us to find what we are looking for when researching a particular subject. This work proposes a new search engine for scientific publications which combines both information retrieval and reading comprehension algorithms to extract answers from a collection of domain-specific documents. The proposed solution although being applied to the context of cybersecurity exhibited great generalization capabilities and can be easily adapted to perform under other distinct knowledge domains.
http://w3id.org/mlsea/pwc/scientificWork/A%20Self-Boosting%20Framework%20for%20Automated%20Radiographic%20Report%20Generation                                                                                  A Self-Boosting Framework for Automated Radiographic Report Generation                                                                                   Automated radiographic report generation is a challenging task since it requires to generate paragraphs describing fine-grained visual differences of cases, especially for those between the diseased and the healthy. Existing image captioning methods commonly target at generic images, and lack mechanism to meet this requirement. To bridge this gap, in this paper, we propose a self-boosting framework that improves radiographic report generation based on the cooperation of the main task of report generation and anauxiliary task of image-text matching. The two tasks are built as the two branches of a network model and influence each other in a cooperative way. On one hand, the image-text matching branch helps to learn highly text-correlated visual features for the report generation branch to output high quality reports. One the other hand, the improved reports produced by the report generation branch provideadditional harder samples for the image-text matching task and enforce the latter to improve itself by learning better visual and text feature representations. This, in turn, helps improve the report generation branch again. These two branches are jointly trained to help improve each other iteratively and progressively, so that the whole model is self-boosted without requiring any external resources. Additionally, in the loss function, our model evaluates the quality of the generated reports not only on the word similarity as common approaches do (via minimizing a cross-entropy loss), but also on the feature similarity at high-level, while the latter is provided by the text-encoder of the image-text matching branch. Experimental results demonstrate the effectiveness of our method on two public datasets, showing its superior performance over other state-of-the-art medical report generation methods. 
http://w3id.org/mlsea/pwc/scientificWork/A%20Self-Supervised%20Auxiliary%20Loss%20for%20Deep%20RL%20in%20Partially%20Observable%20Settings                                                                                  A Self-Supervised Auxiliary Loss for Deep RL in Partially Observable Settings                                                                                  In this work we explore an auxiliary loss useful for reinforcement learning in environments where strong performing agents are required to be able to navigate a spatial environment. The auxiliary loss proposed is to minimize the classification error of a neural network classifier that predicts whether or not a pair of states sampled from the agents current episode trajectory are in order. The classifier takes as input a pair of states as well as the agent's memory. The motivation for this auxiliary loss is that there is a strong correlation with which of a pair of states is more recent in the agents episode trajectory and which of the two states is spatially closer to the agent. Our hypothesis is that learning features to answer this question encourages the agent to learn and internalize in memory representations of states that facilitate spatial reasoning. We tested this auxiliary loss on a navigation task in a gridworld and achieved 9.6% increase in accumulative episode reward compared to a strong baseline approach.
http://w3id.org/mlsea/pwc/scientificWork/A%20Self-Supervised%20Framework%20for%20Function%20Learning%20and%20Extrapolation                                                                                  A Self-Supervised Framework for Function Learning and Extrapolation                                                                                  Understanding how agents learn to generalize -- and, in particular, to extrapolate -- in high-dimensional, naturalistic environments remains a challenge for both machine learning and the study of biological agents. One approach to this has been the use of function learning paradigms, which allow peoples' empirical patterns of generalization for smooth scalar functions to be described precisely. However, to date, such work has not succeeded in identifying mechanisms that acquire the kinds of general purpose representations over which function learning can operate to exhibit the patterns of generalization observed in human empirical studies. Here, we present a framework for how a learner may acquire such representations, that then support generalization -- and extrapolation in particular -- in a few-shot fashion. Taking inspiration from a classic theory of visual processing, we construct a self-supervised encoder that implements the basic inductive bias of invariance under topological distortions. We show the resulting representations outperform those from other models for unsupervised time series learning in several downstream function learning tasks, including extrapolation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Self-supervised%20Method%20for%20Entity%20Alignment                                                                                  A Self-supervised Method for Entity Alignment                                                                                  Entity alignment, aiming to identify equivalent entities across different knowledge graphs (KGs), is a fundamental problem for constructing large-scale KGs. Over the course of its development, supervision has been considered necessary for accurate alignments. Inspired by the recent progress of self-supervised learning, we explore the extent to which we can get rid of supervision for entity alignment. Existing supervised methods for this task focus on pulling each pair of positive (labeled) entities close to each other. However, our analysis suggests that the learning of entity alignment can actually benefit more from pushing sampled (unlabeled) negatives far away than pulling positive aligned pairs close. We present SelfKG by leveraging this discovery to design a contrastive learning strategy across two KGs. Extensive experiments on benchmark datasets demonstrate that SelfKG without supervision can match or achieve comparable results with state-of-the-art supervised baselines. The performance of SelfKG demonstrates self-supervised learning offers great potential for entity alignment in KGs.
http://w3id.org/mlsea/pwc/scientificWork/A%20Semantic%20Model%20for%20Interacting%20Cyber-Physical%20Systems                                                                                  A Semantic Model for Interacting Cyber-Physical Systems                                                                                  We propose a component-based semantic model for Cyber-Physical Systems (CPSs) wherein the notion of a component abstracts the internal details of both cyber and physical processes, to expose a uniform semantic model of their externally observable behaviors expressed as sets of sequences of observations. We introduce algebraic operations on such sequences to model different kinds of component composition. These composition operators yield the externally observable behavior of their resulting composite components through specifications of interactions of the behaviors of their constituent components, as they, e.g., synchronize with or mutually exclude each other's alternative behaviors. Our framework is expressive enough to allow articulation of properties that coordinate desired interactions among composed components within the framework, also as component behavior. We demonstrate the usefulness of our formalism through examples of coordination properties in a CPS consisting of two robots interacting through shared physical resources.
http://w3id.org/mlsea/pwc/scientificWork/A%20Semantic%20Segmentation%20Network%20for%20Urban-Scale%20Building%20Footprint%20Extraction%20Using%20RGB%20Satellite%20Imagery                                                                                  A Semantic Segmentation Network for Urban-Scale Building Footprint Extraction Using RGB Satellite Imagery                                                                                  Urban areas consume over two-thirds of the world's energy and account for more than 70 percent of global CO2 emissions. As stated in IPCC's Global Warming of 1.5C report, achieving carbon neutrality by 2050 requires a clear understanding of urban geometry. High-quality building footprint generation from satellite images can accelerate this predictive process and empower municipal decision-making at scale. However, previous Deep Learning-based approaches face consequential issues such as scale invariance and defective footprints, partly due to ever-present class-wise imbalance. Additionally, most approaches require supplemental data such as point cloud data, building height information, and multi-band imagery - which has limited availability and are tedious to produce. In this paper, we propose a modified DeeplabV3+ module with a Dilated Res-Net backbone to generate masks of building footprints from three-channel RGB satellite imagery only. Furthermore, we introduce an F-Beta measure in our objective function to help the model account for skewed class distributions and prevent false-positive footprints. In addition to F-Beta, we incorporate an exponentially weighted boundary loss and use a cross-dataset training strategy to further increase the quality of predictions. As a result, we achieve state-of-the-art performances across three public benchmarks and demonstrate that our RGB-only method produces higher quality visual results and is agnostic to the scale, resolution, and urban density of satellite imagery.
http://w3id.org/mlsea/pwc/scientificWork/A%20Semantic-based%20Method%20for%20Unsupervised%20Commonsense%20Question%20Answering                                                                                  A Semantic-based Method for Unsupervised Commonsense Question Answering                                                                                  Unsupervised commonsense question answering is appealing since it does not rely on any labeled task data. Among existing work, a popular solution is to use pre-trained language models to score candidate choices directly conditioned on the question or context. However, such scores from language models can be easily affected by irrelevant factors, such as word frequencies, sentence structures, etc. These distracting factors may not only mislead the model to choose a wrong answer but also make it oversensitive to lexical perturbations in candidate answers. In this paper, we present a novel SEmantic-based Question Answering method (SEQA) for unsupervised commonsense question answering. Instead of directly scoring each answer choice, our method first generates a set of plausible answers with generative models (e.g., GPT-2), and then uses these plausible answers to select the correct choice by considering the semantic similarity between each plausible answer and each choice. We devise a simple, yet sound formalism for this idea and verify its effectiveness and robustness with extensive experiments. We evaluate the proposed method on four benchmark datasets, and our method achieves the best results in unsupervised settings. Moreover, when attacked by TextFooler with synonym replacement, SEQA demonstrates much less performance drops than baselines, thereby indicating stronger robustness.
http://w3id.org/mlsea/pwc/scientificWork/A%20Semi-Personalized%20System%20for%20User%20Cold%20Start%20Recommendation%20on%20Music%20Streaming%20Apps                                                                                  A Semi-Personalized System for User Cold Start Recommendation on Music Streaming Apps                                                                                  Music streaming services heavily rely on recommender systems to improve their users' experience, by helping them navigate through a large musical catalog and discover new songs, albums or artists. However, recommending relevant and personalized content to new users, with few to no interactions with the catalog, is challenging. This is commonly referred to as the user cold start problem. In this applied paper, we present the system recently deployed on the music streaming service Deezer to address this problem. The solution leverages a semi-personalized recommendation strategy, based on a deep neural network architecture and on a clustering of users from heterogeneous sources of information. We extensively show the practical impact of this system and its effectiveness at predicting the future musical preferences of cold start users on Deezer, through both offline and online large-scale experiments. Besides, we publicly release our code as well as anonymized usage data from our experiments. We hope that this release of industrial resources will benefit future research on user cold start recommendation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Semi-Supervised%20Classification%20Method%20of%20Apicomplexan%20Parasites%20and%20Host%20Cell%20Using%20Contrastive%20Learning%20Strategy                                                                                  A Semi-Supervised Classification Method of Apicomplexan Parasites and Host Cell Using Contrastive Learning Strategy                                                                                  A common shortfall of supervised learning for medical imaging is the greedy need for human annotations, which is often expensive and time-consuming to obtain. This paper proposes a semi-supervised classification method for three kinds of apicomplexan parasites and non-infected host cells microscopic images, which uses a small number of labeled data and a large number of unlabeled data for training. There are two challenges in microscopic image recognition. The first is that salient structures of the microscopic images are more fuzzy and intricate than natural images' on a real-world scale. The second is that insignificant textures, like background staining, lightness, and contrast level, vary a lot in samples from different clinical scenarios. To address these challenges, we aim to learn a distinguishable and appearance-invariant representation by contrastive learning strategy. On one hand, macroscopic images, which share similar shape characteristics in morphology, are introduced to contrast for structure enhancement. On the other hand, different appearance transformations, including color distortion and flittering, are utilized to contrast for texture elimination. In the case where only 1% of microscopic images are labeled, the proposed method reaches an accuracy of 94.90% in a generalized testing set.
http://w3id.org/mlsea/pwc/scientificWork/A%20Semi-parametric%20Realized%20Joint%20Value-at-Risk%20and%20Expected%20Shortfall%20Regression%20Framework                                                                                  A Semi-parametric Realized Joint Value-at-Risk and Expected Shortfall Regression Framework                                                                                  A new realized conditional autoregressive Value-at-Risk (VaR) framework is proposed, through incorporating a measurement equation into the original quantile regression model. The framework is further extended by employing various Expected Shortfall (ES) components, to jointly estimate and forecast VaR and ES. The measurement equation models the contemporaneous dependence between the realized measure (i.e., Realized Variance and Realized Range) and the latent conditional ES. An adaptive Bayesian Markov Chain Monte Carlo method is employed for estimation and forecasting, the properties of which are assessed and compared with maximum likelihood through a simulation study. In a comprehensive forecasting study on 1% and 2.5 % quantile levels, the proposed models are compared to a range of parametric, non-parametric and semi-parametric models, based on 7 market indices and 7 individual assets. One-day-ahead VaR and ES forecasting results favor the proposed models, especially when incorporating the sub-sampled Realized Variance and the sub-sampled Realized Range in the model.
http://w3id.org/mlsea/pwc/scientificWork/A%20Semi-supervised%20Multi-task%20Learning%20Approach%20to%20Classify%20Customer%20Contact%20Intents                                                                                  A Semi-supervised Multi-task Learning Approach to Classify Customer Contact Intents                                                                                  In the area of customer support, understanding customers' intents is a crucial step. Machine learning plays a vital role in this type of intent classification. In reality, it is typical to collect confirmation from customer support representatives (CSRs) regarding the intent prediction, though it can unnecessarily incur prohibitive cost to ask CSRs to assign existing or new intents to the mis-classified cases. Apart from the confirmed cases with and without intent labels, there can be a number of cases with no human curation. This data composition (Positives + Unlabeled + multiclass Negatives) creates unique challenges for model development. In response to that, we propose a semi-supervised multi-task learning paradigm. In this manuscript, we share our experience in building text-based intent classification models for a customer support service on an E-commerce website. We improve the performance significantly by evolving the model from multiclass classification to semi-supervised multi-task learning by leveraging the negative cases, domain- and task-adaptively pretrained ALBERT on customer contact texts, and a number of un-curated data with no labels. In the evaluation, the final model boosts the average AUC ROC by almost 20 points compared to the baseline finetuned multiclass classification ALBERT model.
http://w3id.org/mlsea/pwc/scientificWork/A%20Semidefinite%20Programming%20Approach%20to%20Discrete-time%20Infinite%20Horizon%20Persistent%20Monitoring                                                                                  A Semidefinite Programming Approach to Discrete-time Infinite Horizon Persistent Monitoring                                                                                  We investigate the problem of persistent monitoring, where a mobile agent has to survey multiple targets in an environment in order to estimate their internal states. These internal states evolve with linear stochastic dynamics and the agent can observe them with a linear observation model. However, the signal to noise ratio is a monotonically decreasing function of the distance between the agent and the target. The goal is to minimize the uncertainty in the state estimates over the infinite horizon. We show that, for a periodic trajectory with fixed cycle length, the problem can be formulated as a set of semidefinite programs. We design a scheme that leverages the spatial configuration of the targets to guide the search over this set of optimization problems in order to provide efficient trajectories. Results are compared to a state of the art approach and we obtain improvements of up to 91% in terms of cost in a simple scenario, with much lower computational time.
http://w3id.org/mlsea/pwc/scientificWork/A%20Sensorless%20Control%20System%20for%20an%20Implantable%20Heart%20Pump%20using%20a%20Real-time%20Deep%20Convolutional%20Neural%20Network                                                                                  A Sensorless Control System for an Implantable Heart Pump using a Real-time Deep Convolutional Neural Network                                                                                  Left ventricular assist devices (LVADs) are mechanical pumps, which can be used to support heart failure (HF) patients as bridge to transplant and destination therapy. To automatically adjust the LVAD speed, a physiological control system needs to be designed to respond to variations of patient hemodynamics across a variety of clinical scenarios. These control systems require pressure feedback signals from the cardiovascular system. However, there are no suitable long-term implantable sensors available. In this study, a novel real-time deep convolutional neural network (CNN) for estimation of preload based on the LVAD flow was proposed. A new sensorless adaptive physiological control system for an LVAD pump was developed using the full dynamic form of model free adaptive control (FFDL-MFAC) and the proposed preload estimator to maintain the patient conditions in safe physiological ranges. The CNN model for preload estimation was trained and evaluated through 10-fold cross validation on 100 different patient conditions and the proposed sensorless control system was assessed on a new testing set of 30 different patient conditions across six different patient scenarios. The proposed preload estimator was extremely accurate with a correlation coefficient of 0.97, root mean squared error of 0.84 mmHg, reproducibility coefficient of 1.56 mmHg, coefficient of variation of 14.44 %, and bias of 0.29 mmHg for the testing dataset. The results also indicate that the proposed sensorless physiological controller works similarly to the preload-based physiological control system for LVAD using measured preload to prevent ventricular suction and pulmonary congestion. This study shows that the LVADs can respond appropriately to changing patient states and physiological demands without the need for additional pressure or flow measurements.
http://w3id.org/mlsea/pwc/scientificWork/A%20Sentence-level%20Hierarchical%20BERT%20Model%20for%20Document%20Classification%20with%20Limited%20Labelled%20Data                                                                                  A Sentence-level Hierarchical BERT Model for Document Classification with Limited Labelled Data                                                                                  Training deep learning models with limited labelled data is an attractive scenario for many NLP tasks, including document classification. While with the recent emergence of BERT, deep learning language models can achieve reasonably good performance in document classification with few labelled instances, there is a lack of evidence in the utility of applying BERT-like models on long document classification. This work introduces a long-text-specific model -- the Hierarchical BERT Model (HBM) -- that learns sentence-level features of the text and works well in scenarios with limited labelled data. Various evaluation experiments have demonstrated that HBM can achieve higher performance in document classification than the previous state-of-the-art methods with only 50 to 200 labelled instances, especially when documents are long. Also, as an extra benefit of HBM, the salient sentences identified by learned HBM are useful as explanations for labelling documents based on a user study.
http://w3id.org/mlsea/pwc/scientificWork/A%20Sequence-to-Set%20Network%20for%20Nested%20Named%20Entity%20Recognition                                                                                  A Sequence-to-Set Network for Nested Named Entity Recognition                                                                                  Named entity recognition (NER) is a widely studied task in natural language processing. Recently, a growing number of studies have focused on the nested NER. The span-based methods, considering the entity recognition as a span classification task, can deal with nested entities naturally. But they suffer from the huge search space and the lack of interactions between entities. To address these issues, we propose a novel sequence-to-set neural network for nested NER. Instead of specifying candidate spans in advance, we provide a fixed set of learnable vectors to learn the patterns of the valuable spans. We utilize a non-autoregressive decoder to predict the final set of entities in one pass, in which we are able to capture dependencies between entities. Compared with the sequence-to-sequence method, our model is more suitable for such unordered recognition task as it is insensitive to the label order. In addition, we utilize the loss function based on bipartite matching to compute the overall training loss. Experimental results show that our proposed model achieves state-of-the-art on three nested NER corpora: ACE 2004, ACE 2005 and KBP 2017. The code is available at https://github.com/zqtan1024/sequence-to-set.
http://w3id.org/mlsea/pwc/scientificWork/A%20Sequential%20Variational%20Mode%20Decomposition%20Method                                                                                  A Sequential Variational Mode Decomposition Method                                                                                  In this paper, we introduce a sequential variational mode decomposition method to separate non-stationary mixed signals successively. This method is inspired by the variational method, and can precisely recover the original components one by one from the raw mixture without prior knowing or assuming the number of components. And in such a way, the mode number also can be determined during the separation procedure. Such character brings great convenience for real application and differs from the current VMD method. Furthermore, we also conduct a principal elongation for the mixture signal before the decomposing operation. By applying such an approach, the end effect can be reduced to a low level compared with the VMD method. To obtain higher accuracy, a refinement process has been introduced after gross extraction. Combined these techniques together, the final decomposition result implies a significant improvement compared with the VMD method and EMD method.
http://w3id.org/mlsea/pwc/scientificWork/A%20Service-oriented%20Metro%20Traffic%20Regulation%20Method%20for%20Improving%20Operation%20Performance                                                                                  A Service-oriented Metro Traffic Regulation Method for Improving Operation Performance                                                                                  For high-density metro traffic, nowadays the time-variant passenger flow is the main cause of train delays and stranded passengers. Typically, the main objective of automatic metro traffic regulation methods is to minimize the delay time of trains while passengers' satisfaction is not considered. Instead, in this work, a novel framework that integrates a passenger flow module (PFM) and a train operation module (TOM) is proposed with the aim of simultaneously minimizing traffic delays and passengers' discomfort. In particular, the PFM is devoted to the optimization of the headway time in case of platform overcrowding, so as to reduce the passengers waiting time at platforms and increase the load rate of trains; while the TOM is devoted to the minimization of trains' delays. The two modules interact with each other so that the headway time is automatically adjusted when a platform is overcrowded, and the train traffic is immediately regulated according to the new headway time. As a result, the number of passengers on the platform and their total waiting time can be significantly reduced. Numerical results are provided to show the effectiveness of the proposed method in improving the operation performance while minimizing the passengers' discomfort.
http://w3id.org/mlsea/pwc/scientificWork/A%20Shape-Aware%20Retargeting%20Approach%20to%20Transfer%20Human%20Motion%20and%20Appearance%20in%20Monocular%20Videos                                                                                  A Shape-Aware Retargeting Approach to Transfer Human Motion and Appearance in Monocular Videos                                                                                  Transferring human motion and appearance between videos of human actors remains one of the key challenges in Computer Vision. Despite the advances from recent image-to-image translation approaches, there are several transferring contexts where most end-to-end learning-based retargeting methods still perform poorly. Transferring human appearance from one actor to another is only ensured when a strict setup has been complied, which is generally built considering their training regime's specificities. In this work, we propose a shape-aware approach based on a hybrid image-based rendering technique that exhibits competitive visual retargeting quality compared to state-of-the-art neural rendering approaches. The formulation leverages the user body shape into the retargeting while considering physical constraints of the motion in 3D and the 2D image domain. We also present a new video retargeting benchmark dataset composed of different videos with annotated human motions to evaluate the task of synthesizing people's videos, which can be used as a common base to improve tracking the progress in the field. The dataset and its evaluation protocols are designed to evaluate retargeting methods in more general and challenging conditions. Our method is validated in several experiments, comprising publicly available videos of actors with different shapes, motion types, and camera setups. The dataset and retargeting code are publicly available to the community at: https://www.verlab.dcc.ufmg.br/retargeting-motion.
http://w3id.org/mlsea/pwc/scientificWork/A%20Shared-Private%20Representation%20Model%20with%20Coarse-to-Fine%20Extraction%20for%20Target%20Sentiment%20Analysis                                                                                  A Shared-Private Representation Model with Coarse-to-Fine Extraction for Target Sentiment Analysis                                                                                  Target sentiment analysis aims to detect opinion targets along with recognizing their sentiment polarities from a sentence. Some models with span-based labeling have achieved promising results in this task. However, the relation between the target extraction task and the target classification task has not been well exploited. Besides, the span-based target extraction algorithm has a poor performance on target phrases due to the maximum target length setting or length penalty factor. To address these problems, we propose a novel framework of Shared-Private Representation Model (SPRM) with a coarse-to-fine extraction algorithm. For jointly learning target extraction and classification, we design a Shared-Private Network, which encodes not only shared information for both tasks but also private information for each task. To avoid missing correct target phrases, we also propose a heuristic coarse-to-fine extraction algorithm that first gets the approximate interval of the targets by matching the nearest predicted start and end indexes and then extracts the targets by adopting an extending strategy. Experimental results show that our model achieves state-of-the-art performance.
http://w3id.org/mlsea/pwc/scientificWork/A%20Short%20Note%20on%20the%20Relationship%20of%20Information%20Gain%20and%20Eluder%20Dimension                                                                                  A Short Note on the Relationship of Information Gain and Eluder Dimension                                                                                  Eluder dimension and information gain are two widely used methods of complexity measures in bandit and reinforcement learning. Eluder dimension was originally proposed as a general complexity measure of function classes, but the common examples of where it is known to be small are function spaces (vector spaces). In these cases, the primary tool to upper bound the eluder dimension is the elliptic potential lemma. Interestingly, the elliptic potential lemma also features prominently in the analysis of linear bandits/reinforcement learning and their nonparametric generalization, the information gain. We show that this is not a coincidence -- eluder dimension and information gain are equivalent in a precise sense for reproducing kernel Hilbert spaces.
http://w3id.org/mlsea/pwc/scientificWork/A%20Short%20Survey%20of%20Pre-trained%20Language%20Models%20for%20Conversational%20AI-A%20NewAge%20in%20NLP                                                                                  A Short Survey of Pre-trained Language Models for Conversational AI-A NewAge in NLP                                                                                  Building a dialogue system that can communicate naturally with humans is a challenging yet interesting problem of agent-based computing. The rapid growth in this area is usually hindered by the long-standing problem of data scarcity as these systems are expected to learn syntax, grammar, decision making, and reasoning from insufficient amounts of task-specific dataset. The recently introduced pre-trained language models have the potential to address the issue of data scarcity and bring considerable advantages by generating contextualized word embeddings. These models are considered counterpart of ImageNet in NLP and have demonstrated to capture different facets of language such as hierarchical relations, long-term dependency, and sentiment. In this short survey paper, we discuss the recent progress made in the field of pre-trained language models. We also deliberate that how the strengths of these language models can be leveraged in designing more engaging and more eloquent conversational agents. This paper, therefore, intends to establish whether these pre-trained models can overcome the challenges pertinent to dialogue systems, and how their architecture could be exploited in order to overcome these challenges. Open challenges in the field of dialogue systems have also been deliberated.
http://w3id.org/mlsea/pwc/scientificWork/A%20Shuffling%20Framework%20for%20Local%20Differential%20Privacy                                                                                  A Shuffling Framework for Local Differential Privacy                                                                                  ldp deployments are vulnerable to inference attacks as an adversary can link the noisy responses to their identity and subsequently, auxiliary information using the order of the data. An alternative model, shuffle DP, prevents this by shuffling the noisy responses uniformly at random. However, this limits the data learnability -- only symmetric functions (input order agnostic) can be learned. In this paper, we strike a balance and show that systematic shuffling of the noisy responses can thwart specific inference attacks while retaining some meaningful data learnability. To this end, we propose a novel privacy guarantee, d-sigma-privacy, that captures the privacy of the order of a data sequence. d-sigma-privacy allows tuning the granularity at which the ordinal information is maintained, which formalizes the degree the resistance to inference attacks trading it off with data learnability. Additionally, we propose a novel shuffling mechanism that can achieve name-privacy and demonstrate the practicality of our mechanism via evaluation on real-world datasets.
http://w3id.org/mlsea/pwc/scientificWork/A%20Siamese%20CNN%20Architecture%20for%20Learning%20Chinese%20Sentence%20Similarity                                                                                  A Siamese CNN Architecture for Learning Chinese Sentence Similarity                                                                                  This paper presents a deep neural architecture which applies the siamese convolutional neural network sharing model parameters for learning a semantic similarity metric between two sentences. In addition, two different similarity metrics (i.e., the Cosine Similarity and Manhattan similarity) are compared based on this architecture. Our experiments in binary similarity classification for Chinese sentence pairs show that the proposed siamese convolutional architecture with Manhattan similarity outperforms the baselines (i.e., the siamese Long Short-Term Memory architecture and the siamese Bidirectional Long Short-Term Memory architecture) by 8.7 points in accuracy.
http://w3id.org/mlsea/pwc/scientificWork/A%20Signal-Centric%20Perspective%20on%20the%20Evolution%20of%20Symbolic%20Communication                                                                                  A Signal-Centric Perspective on the Evolution of Symbolic Communication                                                                                  The evolution of symbolic communication is a longstanding open research question in biology. While some theories suggest that it originated from sub-symbolic communication (i.e., iconic or indexical), little experimental evidence exists on how organisms can actually evolve to define a shared set of symbols with unique interpretable meaning, thus being capable of encoding and decoding discrete information. Here, we use a simple synthetic model composed of sender and receiver agents controlled by Continuous-Time Recurrent Neural Networks, which are optimized by means of neuro-evolution. We characterize signal decoding as either regression or classification, with limited and unlimited signal amplitude. First, we show how this choice affects the complexity of the evolutionary search, and leads to different levels of generalization. We then assess the effect of noise, and test the evolved signaling system in a referential game. In various settings, we observe agents evolving to share a dictionary of symbols, with each symbol spontaneously associated to a 1-D unique signal. Finally, we analyze the constellation of signals associated to the evolved signaling systems and note that in most cases these resemble a Pulse Amplitude Modulation system.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20Approach%20for%20Zero-Shot%20Learning%20based%20on%20Triplet%20Distribution%20Embeddings                                                                                  A Simple Approach for Zero-Shot Learning based on Triplet Distribution Embeddings                                                                                  Given the semantic descriptions of classes, Zero-Shot Learning (ZSL) aims to recognize unseen classes without labeled training data by exploiting semantic information, which contains knowledge between seen and unseen classes. Existing ZSL methods mainly use vectors to represent the embeddings to the semantic space. Despite the popularity, such vector representation limits the expressivity in terms of modeling the intra-class variability for each class. We address this issue by leveraging the use of distribution embeddings. More specifically, both image embeddings and class embeddings are modeled as Gaussian distributions, where their similarity relationships are preserved through the use of triplet constraints. The key intuition which guides our approach is that for each image, the embedding of the correct class label should be closer than that of any other class label. Extensive experiments on multiple benchmark data sets show that the proposed method achieves highly competitive results for both traditional ZSL and more challenging Generalized Zero-Shot Learning (GZSL) settings.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20Approach%20to%20Increase%20the%20Maximum%20Allowable%20Transmission%20Interval                                                                                  A Simple Approach to Increase the Maximum Allowable Transmission Interval                                                                                  When designing Networked Control Systems (NCS), the maximum allowable transmission interval (MATI) is an important quantity, as it provides the admissible time between two transmission instants. An efficient procedure to compute a bound on the MATI such that stability can be guaranteed for general nonlinear NCS is the emulation of a continuous-time controller. In this paper, we present a simple but efficient modification to the well-established emulation-based approach from Carnevale et al. (2007) to derive a bound on the MATI. Whilst only minor technical changes are required, the proposed modification can lead to significant improvements for the MATI bound as compared to Carnevale et al. (2007). We revisit two numerical examples from literature and demonstrate that the improvement may amount to more than 100%.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20Baseline%20for%20Semi-supervised%20Semantic%20Segmentation%20with%20Strong%20Data%20Augmentation                                                                                  A Simple Baseline for Semi-supervised Semantic Segmentation with Strong Data Augmentation                                                                                  Recently, significant progress has been made on semantic segmentation. However, the success of supervised semantic segmentation typically relies on a large amount of labelled data, which is time-consuming and costly to obtain. Inspired by the success of semi-supervised learning methods in image classification, here we propose a simple yet effective semi-supervised learning framework for semantic segmentation. We demonstrate that the devil is in the details: a set of simple design and training techniques can collectively improve the performance of semi-supervised semantic segmentation significantly. Previous works [3, 27] fail to employ strong augmentation in pseudo label learning efficiently, as the large distribution change caused by strong augmentation harms the batch normalisation statistics. We design a new batch normalisation, namely distribution-specific batch normalisation (DSBN) to address this problem and demonstrate the importance of strong augmentation for semantic segmentation. Moreover, we design a self correction loss which is effective in noise resistance. We conduct a series of ablation studies to show the effectiveness of each component. Our method achieves state-of-the-art results in the semi-supervised settings on the Cityscapes and Pascal VOC datasets.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20Bound%20for%20Resilient%20Submodular%20Maximization%20with%20Curvature                                                                                  A Simple Bound for Resilient Submodular Maximization with Curvature                                                                                  Resilient submodular maximization refers to the combinatorial problems studied by Nemhauser and Fisher and asks how to maximize an objective given a number of adversarial removals. For example, one application of this problem is multi-robot sensor planning with adversarial attacks. However, more general applications of submodular maximization are also relevant. Tzoumas et al. obtain near-optimal solutions to this problem by taking advantage of a property called curvature to produce a mechanism which makes certain bait elements interchangeable with other elements of the solution that are produced via typical greedy means. This document demonstrates that -- at least in theory -- applying the method for selection of bait elements to the entire solution can improve that guarantee on solution quality.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20Fix%20to%20Mahalanobis%20Distance%20for%20Improving%20Near-OOD%20Detection                                                                                  A Simple Fix to Mahalanobis Distance for Improving Near-OOD Detection                                                                                  Mahalanobis distance (MD) is a simple and popular post-processing method for detecting out-of-distribution (OOD) inputs in neural networks. We analyze its failure modes for near-OOD detection and propose a simple fix called relative Mahalanobis distance (RMD) which improves performance and is more robust to hyperparameter choice. On a wide selection of challenging vision, language, and biology OOD benchmarks (CIFAR-100 vs CIFAR-10, CLINC OOD intent detection, Genomics OOD), we show that RMD meaningfully improves upon MD performance (by up to 15% AUROC on genomics OOD).
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20Geometric%20Method%20for%20Cross-Lingual%20Linguistic%20Transformations%20with%20Pre-trained%20Autoencoders                                                                                  A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders                                                                                  Powerful sentence encoders trained for multiple languages are on the rise. These systems are capable of embedding a wide range of linguistic properties into vector representations. While explicit probing tasks can be used to verify the presence of specific linguistic properties, it is unclear whether the vector representations can be manipulated to indirectly steer such properties. For efficient learning, we investigate the use of a geometric mapping in embedding space to transform linguistic properties, without any tuning of the pre-trained sentence encoder or decoder. We validate our approach on three linguistic properties using a pre-trained multilingual autoencoder and analyze the results in both monolingual and cross-lingual settings.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20Model%20of%20Monetary%20Policy%20under%20Phillips-Curve%20Causal%20Disagreements                                                                                  A Simple Model of Monetary Policy under Phillips-Curve Causal Disagreements                                                                                  I study a static textbook model of monetary policy and relax the conventional assumption that the private sector has rational expectations. Instead, the private sector forms inflation forecasts according to a misspecified subjective model that disagrees with the central bank's (true) model over the causal underpinnings of the Phillips Curve. Following the AI/Statistics literature on Bayesian Networks, I represent the private sector's model by a direct acyclic graph (DAG). I show that when the private sector's model reverses the direction of causality between inflation and output, the central bank's optimal policy can exhibit an attenuation effect that is sensitive to the noisiness of the true inflation-output equations.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20Recipe%20for%20Multilingual%20Grammatical%20Error%20Correction                                                                                  A Simple Recipe for Multilingual Grammatical Error Correction                                                                                  This paper presents a simple recipe to train state-of-the-art multilingual Grammatical Error Correction (GEC) models. We achieve this by first proposing a language-agnostic method to generate a large number of synthetic examples. The second ingredient is to use large-scale multilingual language models (up to 11B parameters). Once fine-tuned on language-specific supervised sets we surpass the previous state-of-the-art results on GEC benchmarks in four languages: English, Czech, German and Russian. Having established a new set of baselines for GEC, we make our results easily reproducible and accessible by releasing a cLang-8 dataset. It is produced by using our best model, which we call gT5, to clean the targets of a widely used yet noisy lang-8 dataset. cLang-8 greatly simplifies typical GEC training pipelines composed of multiple fine-tuning stages -- we demonstrate that performing a single fine-tuning step on cLang-8 with the off-the-shelf language models yields further accuracy improvements over an already top-performing gT5 model for English.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20Text-based%20Relevant%20Location%20Prediction%20Method%20using%20Knowledge%20Base                                                                                  A Simple Text-based Relevant Location Prediction Method using Knowledge Base                                                                                  In this paper, we propose a simple method to predict salient locations from news article text using a knowledge base (KB). The proposed method uses a dictionary of locations created from the KB to identify occurrences of locations in the text and uses the hierarchical information between entities in the KB for assigning appropriate saliency scores to regions. It allows prediction at arbitrary region units and has only a few hyperparameters that need to be tuned. We show using manually annotated news articles that the proposed method improves the f-measure by { textgreater} 0.12 compared to multiple baselines.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20Three-Step%20Approach%20for%20the%20Automatic%20Detection%20of%20Exaggerated%20Statements%20in%20Health%20Science%20News                                                                                  A Simple Three-Step Approach for the Automatic Detection of Exaggerated Statements in Health Science News                                                                                  There is a huge difference between a scientific journal reporting {`}wine consumption might be correlated to cancer{'}, and a media outlet publishing {`}wine causes cancer{'} citing the journal{'}s results. The above example is a typical case of a scientific statement being exaggerated as an outcome of the rising problem of media manipulation. Given a pair of statements (say one from the source journal article and the other from the news article covering the results published in the journal), is it possible to ascertain with some confidence whether one is an exaggerated version of the other? This paper presents a surprisingly simple yet rational three-step approach that performs best for this task. We solve the task by breaking it into three sub-tasks as follows {--} (a) given a statement from a scientific paper or press release, we first extract relation phrases (e.g., {`}causes{'} versus {`}might be correlated to{'}) connecting the dependent (e.g., {`}cancer{'}) and the independent ({`}wine{'}) variable, (b) classify the strength of the relationship phrase extracted and (c) compare the strengths of the relation phrases extracted from the statements to identify whether one statement contains an exaggerated version of the other, and to what extent. Through rigorous experiments, we demonstrate that our simple approach by far outperforms baseline models that compare state-of-the-art embedding of the statement pairs through a binary classifier or recast the problem as a textual entailment task, which appears to be a very natural choice in this settings.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20Voting%20Mechanism%20for%20Online%20Sexist%20Content%20Identification                                                                                  A Simple Voting Mechanism for Online Sexist Content Identification                                                                                  This paper presents the participation of the MiniTrue team in the EXIST 2021 Challenge on the sexism detection in social media task for English and Spanish. Our approach combines the language models with a simple voting mechanism for the sexist label prediction. For this, three BERT based models and a voting function are used. Experimental results show that our final model with the voting function has achieved the best results among our four models, which means that our voting mechanism brings an extra benefit to our system. Nevertheless, we also observe that our system is robust to data sources and languages.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20and%20Effective%20Positional%20Encoding%20for%20Transformers                                                                                  A Simple and Effective Positional Encoding for Transformers                                                                                  Transformer models are permutation equivariant. To supply the order and type information of the input tokens, position and segment embeddings are usually added to the input. Recent works proposed variations of positional encodings with relative position encodings achieving better performance. Our analysis shows that the gain actually comes from moving positional information to attention layer from the input. Motivated by this, we introduce Decoupled Positional Attention for Transformers (DIET), a simple yet effective mechanism to encode position and segment information into the Transformer models. The proposed method has faster training and inference time, while achieving competitive performance on GLUE, XTREME and WMT benchmarks. We further generalize our method to long-range transformers and show performance gain.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20and%20Effective%20Usage%20of%20Word%20Clusters%20for%20CBOW%20Model                                                                                  A Simple and Effective Usage of Word Clusters for CBOW Model                                                                                  We propose a simple and effective method for incorporating word clusters into the Continuous Bag-of-Words (CBOW) model. Specifically, we propose to replace infrequent input and output words in CBOW model with their clusters. The resulting cluster-incorporated CBOW model produces embeddings of frequent words and a small amount of cluster embeddings, which will be fine-tuned in downstream tasks. We empirically show our replacing method works well on several downstream tasks. Through our analysis, we show that our method might be also useful for other similar models which produce word embeddings.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20and%20Efficient%20Probabilistic%20Language%20model%20for%20Code-Mixed%20Text                                                                                  A Simple and Efficient Probabilistic Language model for Code-Mixed Text                                                                                  The conventional natural language processing approaches are not accustomed to the social media text due to colloquial discourse and non-homogeneous characteristics. Significantly, the language identification in a multilingual document is ascertained to be a preceding subtask in several information extraction applications such as information retrieval, named entity recognition, relation extraction, etc. The problem is often more challenging in code-mixed documents wherein foreign languages words are drawn into base language while framing the text. The word embeddings are powerful language modeling tools for representation of text documents useful in obtaining similarity between words or documents. We present a simple probabilistic approach for building efficient word embedding for code-mixed text and exemplifying it over language identification of Hindi-English short test messages scrapped from Twitter. We examine its efficacy for the classification task using bidirectional LSTMs and SVMs and observe its improved scores over various existing code-mixed embeddings
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20and%20General%20Debiased%20Machine%20Learning%20Theorem%20with%20Finite%20Sample%20Guarantees                                                                                  A Simple and General Debiased Machine Learning Theorem with Finite Sample Guarantees                                                                                  Debiased machine learning is a meta algorithm based on bias correction and sample splitting to calculate confidence intervals for functionals, i.e. scalar summaries, of machine learning algorithms. For example, an analyst may desire the confidence interval for a treatment effect estimated with a neural network. We provide a nonasymptotic debiased machine learning theorem that encompasses any global or local functional of any machine learning algorithm that satisfies a few simple, interpretable conditions. Formally, we prove consistency, Gaussian approximation, and semiparametric efficiency by finite sample arguments. The rate of convergence is $n^{-1/2}$ for global functionals, and it degrades gracefully for local functionals. Our results culminate in a simple set of conditions that an analyst can use to translate modern learning theory rates into traditional statistical inference. The conditions reveal a general double robustness property for ill posed inverse problems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20and%20Practical%20Approach%20to%20Improve%20Misspellings%20in%20OCR%20Text                                                                                  A Simple and Practical Approach to Improve Misspellings in OCR Text                                                                                  The focus of our paper is the identification and correction of non-word errors in OCR text. Such errors may be the result of incorrect insertion, deletion, or substitution of a character, or the transposition of two adjacent characters within a single word. Or, it can be the result of word boundary problems that lead to run-on errors and incorrect-split errors. The traditional N-gram correction methods can handle single-word errors effectively. However, they show limitations when dealing with split and merge errors. In this paper, we develop an unsupervised method that can handle both errors. The method we develop leads to a sizable improvement in the correction rates. This tutorial paper addresses very difficult word correction problems - namely incorrect run-on and split errors - and illustrates what needs to be considered when addressing such problems. We outline a possible approach and assess its success on a limited study.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20and%20Reliable%20Formula%20for%20Assessment%20of%20Maximum%20Volumetric%20Productivities%20in%20Photobioreactors                                                                                  A Simple and Reliable Formula for Assessment of Maximum Volumetric Productivities in Photobioreactors                                                                                  This paper establishes and discusses the consistency and the range of applicability of a simple, but general and predictive analytical formula, enabling to easily assess the maximum volumetric biomass growth rates (the productivities) in several kinds of photobioreactors with more or less 15 percents of deviation. Experimental validations are performed on photobioreactors of very different conceptions and designs, cultivating the cyanobacterium Arthrospira platensis, on a wide range of volumes and hemispherical incident light fluxes. The practical usefulness of the proposed formula is demonstrated by the fact that it appears completely independent of the characteristics of the material phase (as the type of reactor, the kind of mixing, the biomass concentration), according to the first principle of thermodynamics and to the Gauss-Ostrogradsky theorem. Its ability to give the maximum (only) kinetic performance of photobioreactors cultivating many different photoautotrophic strains (cyanobacteria, green algae, eukaryotic microalgae) is theoretically discussed but experimental results are reported to a future work of the authors or to any other contribution arising from the scientific community working in the field of photobioreactor engineering and potentially interested by this approach.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20and%20Strong%20Baseline%20for%20Universal%20Targeted%20Attacks%20on%20Siamese%20Visual%20Tracking                                                                                  A Simple and Strong Baseline for Universal Targeted Attacks on Siamese Visual Tracking                                                                                  Siamese trackers are shown to be vulnerable to adversarial attacks recently. However, the existing attack methods craft the perturbations for each video independently, which comes at a non-negligible computational cost. In this paper, we show the existence of universal perturbations that can enable the targeted attack, e.g., forcing a tracker to follow the ground-truth trajectory with specified offsets, to be video-agnostic and free from inference in a network. Specifically, we attack a tracker by adding a universal imperceptible perturbation to the template image and adding a fake target, i.e., a small universal adversarial patch, into the search images adhering to the predefined trajectory, so that the tracker outputs the location and size of the fake target instead of the real target. Our approach allows perturbing a novel video to come at no additional cost except the mere addition operations -- and not require gradient optimization or network inference. Experimental results on several datasets demonstrate that our approach can effectively fool the Siamese trackers in a targeted attack manner. We show that the proposed perturbations are not only universal across videos, but also generalize well across different trackers. Such perturbations are therefore doubly universal, both with respect to the data and the network architectures. We will make our code publicly available.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simple%20yet%20Universal%20Strategy%20for%20Online%20Convex%20Optimization                                                                                  A Simple yet Universal Strategy for Online Convex Optimization                                                                                  Recently, several universal methods have been proposed for online convex optimization, and attain minimax rates for multiple types of convex functions simultaneously. However, they need to design and optimize one surrogate loss for each type of functions, which makes it difficult to exploit the structure of the problem and utilize the vast amount of existing algorithms. In this paper, we propose a simple strategy for universal online convex optimization, which avoids these limitations. The key idea is to construct a set of experts to process the original online functions, and deploy a meta-algorithm over the emph{linearized} losses to aggregate predictions from experts. Specifically, we choose Adapt-ML-Prod to track the best expert, because it has a second-order bound and can be used to leverage strong convexity and exponential concavity. In this way, we can plug in off-the-shelf online solvers as black-box experts to deliver problem-dependent regret bounds. Furthermore, our strategy inherits the theoretical guarantee of any expert designed for strongly convex functions and exponentially concave functions, up to a double logarithmic factor. For general convex functions, it maintains the minimax optimality and also achieves a small-loss bound.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simulated%20Experiment%20to%20Explore%20Robotic%20Dialogue%20Strategies%20for%20People%20with%20Dementia                                                                                  A Simulated Experiment to Explore Robotic Dialogue Strategies for People with Dementia                                                                                  People with Alzheimer's disease and related dementias (ADRD) often show the problem of repetitive questioning, which brings a great burden on persons with ADRD (PwDs) and their caregivers. Conversational robots hold promise of coping with this problem and hence alleviating the burdens on caregivers. In this paper, we proposed a partially observable markov decision process (POMDP) model for the PwD-robot interaction in the context of repetitive questioning, and used Q-learning to learn an adaptive conversation strategy (i.e., rate of follow-up question and difficulty of follow-up question) towards PwDs with different cognitive capabilities and different engagement levels. The results indicated that Q-learning was helpful for action selection for the robot. This may be a useful step towards the application of conversational social robots to cope with repetitive questioning in PwDs.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simulation%20Study%20to%20Evaluate%20the%20Performance%20of%20the%20Cauchy%20Proximal%20Operator%20in%20Despeckling%20SAR%20Images%20of%20the%20Sea%20Surface                                                                                  A Simulation Study to Evaluate the Performance of the Cauchy Proximal Operator in Despeckling SAR Images of the Sea Surface                                                                                  The analysis of ocean surface is widely performed using synthetic aperture radar (SAR) imagery as it yields information for wide areas under challenging weather conditions, during day or night, etc. Speckle noise constitutes however the main reason for reduced performance in applications such as classification, ship detection, target tracking and so on. This paper presents an investigation into the despeckling of SAR images of the ocean that include ship wake structures, via sparse regularisation using the Cauchy proximal operator. We propose a closed-form expression for calculating the proximal operator for the Cauchy prior, which makes it applicable in generic proximal splitting algorithms. In our experiments, we simulate SAR images of moving vessels and their wakes. The performance of the proposed method is evaluated in comparison to the L1 and TV norm regularisation functions. The results show a superior performance of the proposed method for all the utilised images generated.
http://w3id.org/mlsea/pwc/scientificWork/A%20Simulation-Optimization%20Technique%20for%20Service%20Level%20Analysis%20in%20Conjunction%20with%20Reorder%20Point%20Estimation%20and%20Lead-Time%20consideration%3A%20A%20Case%20Study%20in%20Sea%20Port                                                                                  A Simulation-Optimization Technique for Service Level Analysis in Conjunction with Reorder Point Estimation and Lead-Time consideration: A Case Study in Sea Port                                                                                  This study offers a step-by-step practical procedure from the analysis of the current status of the spare parts inventory system to advanced service-level analysis by virtue of simulation-optimization technique for a real-world case study associated with a seaport. The remarkable variety and immense diversity on one hand, and extreme complexities not only in consumption patterns but in the supply of spare parts in an international port with technically advance port operator machinery, on the other hand, have convinced the managers to deal with this issue in a structural framework. The huge available data require cleaning and classification to properly process them and derive reorder point (ROP) estimation, reorder quantity (ROQ) estimation, and associated service level analysis. Finally, from 247000 items used in 9 years long, 1416 inventory items are elected as a result of ABC analysis integrating with the Analytic Hierarchy Process (AHP), which led to the main items that need to be kept under strict inventory control. The ROPs and the pertinent quantities are simulated by Arena software for all the main items, each of which took approximately 30 minutes run-time on a personal computer to determine near-optimal estimations.
http://w3id.org/mlsea/pwc/scientificWork/A%20Single-Layer%20Asymmetric%20RNN%3A%20Potential%20Low%20Hardware%20Complexity%20Linear%20Equation%20Solver                                                                                  A Single-Layer Asymmetric RNN: Potential Low Hardware Complexity Linear Equation Solver                                                                                  A single layer neural network for the solution of linear equations is presented. The proposed circuit is based on the standard Hopfield model albeit with the added flexibility that the interconnection weight matrix need not be symmetric. This results in an asymmetric Hopfield neural network capable of solving linear equations. PSPICE simulation results are given which verify the theoretical predictions. Experimental results for circuits set up to solve small problems further confirm the operation of the proposed circuit.
http://w3id.org/mlsea/pwc/scientificWork/A%20Sketch-Based%20Neural%20Model%20for%20Generating%20Commit%20Messages%20from%20Diffs                                                                                  A Sketch-Based Neural Model for Generating Commit Messages from Diffs                                                                                  Commit messages have an important impact in software development, especially when working in large teams. Multiple developers who have a different style of writing may often be involved in the same project. For this reason, it may be difficult to maintain a strict pattern of writing informative commit messages, with the most frequent issue being that these messages are not descriptive enough. In this paper we apply neural machine translation (NMT) techniques to convert code diffs into commit messages and we present an improved sketch-based encoder for this task. We split the approach into three parts. Firstly, we focus on finding a more suitable NMT baseline for this problem. Secondly, we show that the performance of the NMT models can be improved by training on examples containing a specific file type. Lastly, we introduce a novel sketch-based neural model inspired by recent approaches used for code generation and we show that the sketch-based encoder significantly outperforms existing state of the art solutions. The results highlight that this improvement is relevant especially for Java source code files, by examining two different datasets introduced in recent years for this task.
http://w3id.org/mlsea/pwc/scientificWork/A%20Sliding-Window%20Approach%20to%20Automatic%20Creation%20of%20Meeting%20Minutes                                                                                  A Sliding-Window Approach to Automatic Creation of Meeting Minutes                                                                                  Meeting minutes record any subject matters discussed, decisions reached and actions taken at meetings. The importance of minuting cannot be overemphasized in a time when a significant number of meetings take place in the virtual space. In this paper, we present a sliding window approach to automatic generation of meeting minutes. It aims to tackle issues associated with the nature of spoken text, including lengthy transcripts and lack of document structure, which make it difficult to identify salient content to be included in the meeting minutes. Our approach combines a sliding window and a neural abstractive summarizer to navigate through the transcripts to find salient content. The approach is evaluated on transcripts of natural meeting conversations, where we compare results obtained for human transcripts and two versions of automatic transcripts and discuss how and to what extent the summarizer succeeds at capturing salient content.
http://w3id.org/mlsea/pwc/scientificWork/A%20Small-Gain%20Theorem%20for%20Discrete-Time%20Convergent%20Systems%20and%20Its%20Applications                                                                                  A Small-Gain Theorem for Discrete-Time Convergent Systems and Its Applications                                                                                  Convergent, contractive or incremental stability properties of nonlinear systems have attracted interest for control tasks such as observer design, output regulation and synchronization. The convergence property plays a central role in the neuromorphic (brain-inspired) computing of reservoir computing, which seeks to harness the information processing capability of nonlinear systems. This paper presents a small-gain theorem for discrete-time output-feedback interconnected systems to be uniformly input-to-output convergent (UIOC) with outputs converging to a bounded reference output uniquely determined by the input. A small-gain theorem for interconnected time-varying discrete-time uniform input-to-output stable systems that could be of separate interest is also presented as an intermediate result. Applications of the UIOC small-gain theorem are illustrated in the design of observer-based controllers and interconnected nonlinear classical and quantum dynamical systems (as reservoir computers) for black-box system identification.
http://w3id.org/mlsea/pwc/scientificWork/A%20Smartphone%20based%20Application%20for%20Skin%20Cancer%20Classification%20Using%20Deep%20Learning%20with%20Clinical%20Images%20and%20Lesion%20Information                                                                                  A Smartphone based Application for Skin Cancer Classification Using Deep Learning with Clinical Images and Lesion Information                                                                                  Over the last decades, the incidence of skin cancer, melanoma and non-melanoma, has increased at a continuous rate. In particular for melanoma, the deadliest type of skin cancer, early detection is important to increase patient prognosis. Recently, deep neural networks (DNNs) have become viable to deal with skin cancer detection. In this work, we present a smartphone-based application to assist on skin cancer detection. This application is based on a Convolutional Neural Network(CNN) trained on clinical images and patients demographics, both collected from smartphones. Also, as skin cancer datasets are imbalanced, we present an approach, based on the mutation operator of Differential Evolution (DE) algorithm, to balance data. In this sense, beyond provides a flexible tool to assist doctors on skin cancer screening phase, the method obtains promising results with a balanced accuracy of 85% and a recall of 96%.
http://w3id.org/mlsea/pwc/scientificWork/A%20Solution%20to%20The%20Non-linearity%20of%20Electro-Optic%20Modulation%20Synthetic%20Aperture%20Lidar                                                                                  A Solution to The Non-linearity of Electro-Optic Modulation Synthetic Aperture Lidar                                                                                  Synthetic aperture laser radar has higher resolution, so requires higher modulated bandwidth. Because the data volume of chirp or pulse coding schemes is too large, it brings much pressure to data acquisition and data processing. So, we can use dechirp to reduce the amount of data. However, there is a seriously non-linear problem in phase modulation, which strictly prohibited the dechirp usage. Therefore, in order to solve the above problems, we propose a method based on Electro-Optic Modulators to achieve a large-bandwidth chirp signal. Meanwhile, give a method to resolve the seriously non-linear problems. Firstly, by properly setting the amplitude of the input signal of electro-optic modulators, the finite order signal coefficients are guaranteed to have an absolute advantage. Then, an ideal chirp signal can be obtained by filtering through an optical filter. Finally, the feasibility of the scheme is proved by theory and experiments.
http://w3id.org/mlsea/pwc/scientificWork/A%20Spacecraft%20Dataset%20for%20Detection%2C%20Segmentation%20and%20Parts%20Recognition                                                                                  A Spacecraft Dataset for Detection, Segmentation and Parts Recognition                                                                                  Virtually all aspects of modern life depend on space technology. Thanks to the great advancement of computer vision in general and deep learning-based techniques in particular, over the decades, the world witnessed the growing use of deep learning in solving problems for space applications, such as self-driving robot, tracers, insect-like robot on cosmos and health monitoring of spacecraft. These are just some prominent examples that has advanced space industry with the help of deep learning. However, the success of deep learning models requires a lot of training data in order to have decent performance, while on the other hand, there are very limited amount of publicly available space datasets for the training of deep learning models. Currently, there is no public datasets for space-based object detection or instance segmentation, partly because manually annotating object segmentation masks is very time consuming as they require pixel-level labelling, not to mention the challenge of obtaining images from space. In this paper, we aim to fill this gap by releasing a dataset for spacecraft detection, instance segmentation and part recognition. The main contribution of this work is the development of the dataset using images of space stations and satellites, with rich annotations including bounding boxes of spacecrafts and masks to the level of object parts, which are obtained with a mixture of automatic processes and manual efforts. We also provide evaluations with state-of-the-art methods in object detection and instance segmentation as a benchmark for the dataset. The link for downloading the proposed dataset can be found on https://github.com/Yurushia1998/SatelliteDataset.
http://w3id.org/mlsea/pwc/scientificWork/A%20Span%20Extraction%20Approach%20for%20Information%20Extraction%20on%20Visually-Rich%20Documents                                                                                  A Span Extraction Approach for Information Extraction on Visually-Rich Documents                                                                                  Information extraction (IE) for visually-rich documents (VRDs) has achieved SOTA performance recently thanks to the adaptation of Transformer-based language models, which shows the great potential of pre-training methods. In this paper, we present a new approach to improve the capability of language model pre-training on VRDs. Firstly, we introduce a new query-based IE model that employs span extraction instead of using the common sequence labeling approach. Secondly, to further extend the span extraction formulation, we propose a new training task that focuses on modelling the relationships among semantic entities within a document. This task enables target spans to be extracted recursively and can be used to pre-train the model or as an IE downstream task. Evaluation on three datasets of popular business documents (invoices, receipts) shows that our proposed method achieves significant improvements compared to existing models. The method also provides a mechanism for knowledge accumulation from multiple downstream IE tasks.
http://w3id.org/mlsea/pwc/scientificWork/A%20Span-Based%20Model%20for%20Joint%20Overlapped%20and%20Discontinuous%20Named%20Entity%20Recognition                                                                                  A Span-Based Model for Joint Overlapped and Discontinuous Named Entity Recognition                                                                                  Research on overlapped and discontinuous named entity recognition (NER) has received increasing attention. The majority of previous work focuses on either overlapped or discontinuous entities. In this paper, we propose a novel span-based model that can recognize both overlapped and discontinuous entities jointly. The model includes two major steps. First, entity fragments are recognized by traversing over all possible text spans, thus, overlapped entities can be recognized. Second, we perform relation classification to judge whether a given pair of entity fragments to be overlapping or succession. In this way, we can recognize not only discontinuous entities, and meanwhile doubly check the overlapped entities. As a whole, our model can be regarded as a relation extraction paradigm essentially. Experimental results on multiple benchmark datasets (i.e., CLEF, GENIA and ACE05) show that our model is highly competitive for overlapped and discontinuous NER.
http://w3id.org/mlsea/pwc/scientificWork/A%20Spatio-temporal%20Attention-based%20Model%20for%20Infant%20Movement%20Assessment%20from%20Videos                                                                                  A Spatio-temporal Attention-based Model for Infant Movement Assessment from Videos                                                                                  The absence or abnormality of fidgety movements of joints or limbs is strongly indicative of cerebral palsy in infants. Developing computer-based methods for assessing infant movements in videos is pivotal for improved cerebral palsy screening. Most existing methods use appearance-based features and are thus sensitive to strong but irrelevant signals caused by background clutter or a moving camera. Moreover, these features are computed over the whole frame, thus they measure gross whole body movements rather than specific joint/limb motion. Addressing these challenges, we develop and validate a new method for fidgety movement assessment from consumer-grade videos using human poses extracted from short clips. Human poses capture only relevant motion profiles of joints and limbs and are thus free from irrelevant appearance artifacts. The dynamics and coordination between joints are modeled using spatio-temporal graph convolutional networks. Frames and body parts that contain discriminative information about fidgety movements are selected through a spatio-temporal attention mechanism. We validate the proposed model on the cerebral palsy screening task using a real-life consumer-grade video dataset collected at an Australian hospital through the Cerebral Palsy Alliance, Australia. Our experiments show that the proposed method achieves the ROC-AUC score of 81.87%, significantly outperforming existing competing methods with better interpretability.
http://w3id.org/mlsea/pwc/scientificWork/A%20Specification-Guided%20Framework%20for%20Temporal%20Logic%20Control%20of%20Nonlinear%20Systems                                                                                  A Specification-Guided Framework for Temporal Logic Control of Nonlinear Systems                                                                                  This paper proposes a specification-guided framework for control of nonlinear systems with linear temporal logic (LTL) specifications. In contrast with well-known abstraction-based methods, the proposed framework directly characterizes the winning set, i.e., the set of initial conditions from which a given LTL formula can be realized, over the continuous state space of the system via a monotonic operator. Following this characterization, an algorithm is proposed to practically approximate the operator via an adaptive interval subdivision scheme, which yields a finite-memory control strategy. We show that the proposed algorithm is sound for full LTL specifications, and robustly complete for specifications recognizable by deterministic B 'uchi automata (DBA), the latter in the sense that control strategies can be found whenever the given specification can be satisfied with additional bounded disturbances. Without having to compute and store the abstraction and the resulting product system with the DBA, the proposed method is more memory efficient, which is demonstrated by complexity analysis and performance tests. A pre-processing stage is also devised to reduce computational cost via a decomposition of the specification. We show that the proposed method can effectively solve real-world control problems such as jet engine compressor control and motion planning for manipulators and mobile robots.
http://w3id.org/mlsea/pwc/scientificWork/A%20Spectral%20Estimation%20Framework%20for%20Phase%20Retrieval%20via%20Bregman%20Divergence%20Minimization                                                                                  A Spectral Estimation Framework for Phase Retrieval via Bregman Divergence Minimization                                                                                  In this paper, we develop a novel framework to optimally design spectral estimators for phase retrieval given measurements realized from an arbitrary model. We begin by deconstructing spectral methods, and identify the fundamental mechanisms that inherently promote the accuracy of estimates. We then propose a general formalism for spectral estimation as approximate Bregman loss minimization in the range of the lifted forward model that is tractable by a search over rank-1, PSD matrices. Essentially, by the Bregman loss approach we transcend the Euclidean sense alignment based similarity measure between phaseless measurements in favor of appropriate divergence metrics over $ mathbb{R}^M_+$. To this end, we derive spectral methods that perform approximate minimization of KL-divergence, and the Itakura-Saito distance over phaseless measurements by using element-wise sample processing functions. As a result, our formulation relates and extends existing results on model dependent design of optimal sample processing functions in the literature to a model independent sense of optimality. Numerical simulations confirm the effectiveness of our approach in problem settings under synthetic and real data sets.
http://w3id.org/mlsea/pwc/scientificWork/A%20Spectral-Spatial-Dependent%20Global%20Learning%20Framework%20for%20Insufficient%20and%20Imbalanced%20Hyperspectral%20Image%20Classification                                                                                  A Spectral-Spatial-Dependent Global Learning Framework for Insufficient and Imbalanced Hyperspectral Image Classification                                                                                  Deep learning techniques have been widely applied to hyperspectral image (HSI) classification and have achieved great success. However, the deep neural network model has a large parameter space and requires a large number of labeled data. Deep learning methods for HSI classification usually follow a patchwise learning framework. Recently, a fast patch-free global learning (FPGA) architecture was proposed for HSI classification according to global spatial context information. However, FPGA has difficulty extracting the most discriminative features when the sample data is imbalanced. In this paper, a spectral-spatial dependent global learning (SSDGL) framework based on global convolutional long short-term memory (GCL) and global joint attention mechanism (GJAM) is proposed for insufficient and imbalanced HSI classification. In SSDGL, the hierarchically balanced (H-B) sampling strategy and the weighted softmax loss are proposed to address the imbalanced sample problem. To effectively distinguish similar spectral characteristics of land cover types, the GCL module is introduced to extract the long short-term dependency of spectral features. To learn the most discriminative feature representations, the GJAM module is proposed to extract attention areas. The experimental results obtained with three public HSI datasets show that the SSDGL has powerful performance in insufficient and imbalanced sample problems and is superior to other state-of-the-art methods. Code can be obtained at: https://github.com/dengweihuan/SSDGL.
http://w3id.org/mlsea/pwc/scientificWork/A%20Spectrally%20Efficient%20Linear%20Polarization%20Coding%20Scheme%20for%20Fiber%20Nonlinearity%20Compensation%20in%20CO-OFDM%20Systems                                                                                  A Spectrally Efficient Linear Polarization Coding Scheme for Fiber Nonlinearity Compensation in CO-OFDM Systems                                                                                  In this paper, we propose a linear polarization coding scheme (LPC) combined with the phase conjugated twin signals (PCTS) technique, referred to as LPC-PCTS, for fiber nonlinearity mitigation in coherent optical orthogonal frequency division multiplexing (CO-OFDM) systems. The LPC linearly combines the data symbols on the adjacent subcarriers of the OFDM symbol, one at full amplitude and the other at half amplitude. The linearly coded data is then transmitted as phase conjugate pairs on the same subcarriers of the two OFDM symbols on the two orthogonal polarizations. The nonlinear distortions added to these subcarriers are essentially anti-correlated, since they carry phase conjugate pairs of data. At the receiver, the coherent superposition of the information symbols received on these pairs of subcarriers eventually leads to the cancellation of the nonlinear distortions. We conducted numerical simulation of a single channel 200 Gb/s CO-OFDM system employing the LPCPCTS technique. The results show that a Q-factor improvement of 2.3 dB and 1.7 dB with and without the dispersion symmetry, respectively, when compared to the recently proposed phase conjugated subcarrier coding (PCSC) technique, at an average launch power of 3 dBm. In addition, our proposed LPCPCTS technique shows a significant performance improvement when compared to the 16-quadrature amplitude modulation (QAM) with phase conjugated twin waves (PCTW) scheme, at the same spectral efficiency, for an uncompensated transmission distance of 2800 km.
http://w3id.org/mlsea/pwc/scientificWork/A%20Spiking%20Neural%20Network%20for%20Image%20Segmentation                                                                                  A Spiking Neural Network for Image Segmentation                                                                                  We seek to investigate the scalability of neuromorphic computing for computer vision, with the objective of replicating non-neuromorphic performance on computer vision tasks while reducing power consumption. We convert the deep Artificial Neural Network (ANN) architecture U-Net to a Spiking Neural Network (SNN) architecture using the Nengo framework. Both rate-based and spike-based models are trained and optimized for benchmarking performance and power, using a modified version of the ISBI 2D EM Segmentation dataset consisting of microscope images of cells. We propose a partitioning method to optimize inter-chip communication to improve speed and energy efficiency when deploying multi-chip networks on the Loihi neuromorphic chip. We explore the advantages of regularizing firing rates of Loihi neurons for converting ANN to SNN with minimum accuracy loss and optimized energy consumption. We propose a percentile based regularization loss function to limit the spiking rate of the neuron between a desired range. The SNN is converted directly from the corresponding ANN, and demonstrates similar semantic segmentation as the ANN using the same number of neurons and weights. However, the neuromorphic implementation on the Intel Loihi neuromorphic chip is over 2x more energy-efficient than conventional hardware (CPU, GPU) when running online (one image at a time). These power improvements are achieved without sacrificing the task performance accuracy of the network, and when all weights (Loihi, CPU, and GPU networks) are quantized to 8 bits.
http://w3id.org/mlsea/pwc/scientificWork/A%20Splicing%20Approach%20to%20Best%20Subset%20of%20Groups%20Selection                                                                                  A Splicing Approach to Best Subset of Groups Selection                                                                                  Best subset of groups selection (BSGS) is the process of selecting a small part of non-overlapping groups to achieve the best interpretability on the response variable. It has attracted increasing attention and has far-reaching applications in practice. However, due to the computational intractability of BSGS in high-dimensional settings, developing efficient algorithms for solving BSGS remains a research hotspot. In this paper,we propose a group-splicing algorithm that iteratively detects the relevant groups and excludes the irrelevant ones. Moreover, coupled with a novel group information criterion, we develop an adaptive algorithm to determine the optimal model size. Under mild conditions, it is certifiable that our algorithm can identify the optimal subset of groups in polynomial time with high probability. Finally, we demonstrate the efficiency and accuracy of our methods by comparing them with several state-of-the-art algorithms on both synthetic and real-world datasets.
http://w3id.org/mlsea/pwc/scientificWork/A%20State%20Representation%20Dueling%20Network%20for%20Deep%20Reinforcement%20Learning                                                                                  A State Representation Dueling Network for Deep Reinforcement Learning                                                                                  In recent years there have been many successes in boosting the performance of Deep Q-Networks (DQN). Dueling DQN uses simple dueling architecture but significantly improves the performance of DQN [1]. However, Dueling DQN is only concerned about dueling in estimating Q-values. In this paper, we introduce a state representation dueling network, which provides an auxiliary task designed to be combined with other reinforcement learning algorithms to improve the performance of Deep RL. The state representation dueling network is designed to be beneficial for solving reinforcement learning tasks with high dimensional observation, such as camera input. The experiment shows that adding the state representation dueling network to Dueling DQN improves both the training speed and performance of Dueling DQN in CartPole environment.
http://w3id.org/mlsea/pwc/scientificWork/A%20State-of-the-art%20Survey%20of%20Artificial%20Neural%20Networks%20for%20Whole-slide%20Image%20Analysis%3Afrom%20Popular%20Convolutional%20Neural%20Networks%20to%20Potential%20Visual%20Transformers                                                                                  A State-of-the-art Survey of Artificial Neural Networks for Whole-slide Image Analysis:from Popular Convolutional Neural Networks to Potential Visual Transformers                                                                                  To increase the objectivity and accuracy of pathologists' work, artificial neural network(ANN) methods have been generally needed in the segmentation, classification, and detection of histopathological WSI. In this paper, WSI analysis methods based on ANN are reviewed. Firstly, the development status of WSI and ANN methods is introduced. Secondly, we summarize the common ANN methods. Next, we discuss publicly available WSI datasets and evaluation metrics. These ANN architectures for WSI processing are divided into classical neural networks and deep neural networks(DNNs) and then analyzed. Finally, the application prospect of the analytical method in this field is discussed. The important potential method is Visual Transformers.
http://w3id.org/mlsea/pwc/scientificWork/A%20State-of-the-art%20Survey%20of%20Object%20Detection%20Techniques%20in%20Microorganism%20Image%20Analysis%3A%20From%20Classical%20Methods%20to%20Deep%20Learning%20Approaches                                                                                  A State-of-the-art Survey of Object Detection Techniques in Microorganism Image Analysis: From Classical Methods to Deep Learning Approaches                                                                                  Microorganisms play a vital role in human life. Therefore, microorganism detection is of great significance to human beings. However, the traditional manual microscopic detection methods have the disadvantages of long detection cycle, low detection accuracy in large orders, and great difficulty in detecting uncommon microorganisms. Therefore, it is meaningful to apply computer image analysis technology to the field of microorganism detection. Computer image analysis can realize high-precision and high-efficiency detection of microorganisms. In this review, first,we analyse the existing microorganism detection methods in chronological order, from traditional image processing and traditional machine learning to deep learning methods. Then, we analyze and summarize these existing methods and introduce some potential methods, including visual transformers. In the end, the future development direction and challenges of microorganism detection are discussed. In general, we have summarized 142 related technical papers from 1985 to the present. This review will help researchers have a more comprehensive understanding of the development process, research status, and future trends in the field of microorganism detection and provide a reference for researchers in other fields.
http://w3id.org/mlsea/pwc/scientificWork/A%20Statistical%20Analysis%20of%20Summarization%20Evaluation%20Metrics%20using%20Resampling%20Methods                                                                                  A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods                                                                                  The quality of a summarization evaluation metric is quantified by calculating the correlation between its scores and human annotations across a large number of summaries. Currently, it is unclear how precise these correlation estimates are, nor whether differences between two metrics' correlations reflect a true difference or if it is due to mere chance. In this work, we address these two problems by proposing methods for calculating confidence intervals and running hypothesis tests for correlations using two resampling methods, bootstrapping and permutation. After evaluating which of the proposed methods is most appropriate for summarization through two simulation experiments, we analyze the results of applying these methods to several different automatic evaluation metrics across three sets of human annotations. We find that the confidence intervals are rather wide, demonstrating high uncertainty in the reliability of automatic metrics. Further, although many metrics fail to show statistical improvements over ROUGE, two recent works, QAEval and BERTScore, do in some evaluation settings.
http://w3id.org/mlsea/pwc/scientificWork/A%20Statistical%20Model%20for%20Melody%20Reduction                                                                                  A Statistical Model for Melody Reduction                                                                                  A commonly-cited reason for the poor performance of automatic chord estimation (ACE) systems within music information retrieval (MIR) is that non-chord tones (i.e., notes outside the supporting harmony) contribute to error during the labeling process. Despite the prevalence of machine learning approaches in MIR, there are cases where alternative approaches provide a simpler alternative while allowing for insights into musicological practices. In this project, we present a statistical model for predicting chord tones based on music theory rules. Our model is currently focused on predicting chord tones in classical music, since composition in this style is highly constrained, theoretically making the placement of chord tones highly predictable. Indeed, music theorists have labeling systems for every variety of non-chord tone, primarily classified by the note's metric position and intervals of approach and departure. Using metric position, duration, and melodic intervals as predictors, we build a statistical model for predicting chord tones using the TAVERN dataset. While our probabilistic approach is similar to other efforts in the domain of automatic harmonic analysis, our focus is on melodic reduction rather than predicting harmony. However, we hope to pursue applications for ACE in the future. Finally, we implement our melody reduction model using an existing symbolic visualization tool, to assist with melody reduction and non-chord tone identification for computational musicology researchers and music theorists.
http://w3id.org/mlsea/pwc/scientificWork/A%20Step%20Toward%20More%20Inclusive%20People%20Annotations%20for%20Fairness                                                                                  A Step Toward More Inclusive People Annotations for Fairness                                                                                  The Open Images Dataset contains approximately 9 million images and is a widely accepted dataset for computer vision research. As is common practice for large datasets, the annotations are not exhaustive, with bounding boxes and attribute labels for only a subset of the classes in each image. In this paper, we present a new set of annotations on a subset of the Open Images dataset called the MIAP (More Inclusive Annotations for People) subset, containing bounding boxes and attributes for all of the people visible in those images. The attributes and labeling methodology for the MIAP subset were designed to enable research into model fairness. In addition, we analyze the original annotation methodology for the person class and its subclasses, discussing the resulting patterns in order to inform future annotation efforts. By considering both the original and exhaustive annotation sets, researchers can also now study how systematic patterns in training annotations affect modeling.
http://w3id.org/mlsea/pwc/scientificWork/A%20Stochastic%20Alternating%20Balance%20%24k%24-Means%20Algorithm%20for%20Fair%20Clustering                                                                                  A Stochastic Alternating Balance $k$-Means Algorithm for Fair Clustering                                                                                  In the application of data clustering to human-centric decision-making systems, such as loan applications and advertisement recommendations, the clustering outcome might discriminate against people across different demographic groups, leading to unfairness. A natural conflict occurs between the cost of clustering (in terms of distance to cluster centers) and the balance representation of all demographic groups across the clusters, leading to a bi-objective optimization problem that is nonconvex and nonsmooth. To determine the complete trade-off between these two competing goals, we design a novel stochastic alternating balance fair $k$-means (SAfairKM) algorithm, which consists of alternating classical mini-batch $k$-means updates and group swap updates. The number of $k$-means updates and the number of swap updates essentially parameterize the weight put on optimizing each objective function. Our numerical experiments show that the proposed SAfairKM algorithm is robust and computationally efficient in constructing well-spread and high-quality Pareto fronts both on synthetic and real datasets.
http://w3id.org/mlsea/pwc/scientificWork/A%20Stochastic%20Composite%20Augmented%20Lagrangian%20Method%20For%20Reinforcement%20Learning                                                                                  A Stochastic Composite Augmented Lagrangian Method For Reinforcement Learning                                                                                  In this paper, we consider the linear programming (LP) formulation for deep reinforcement learning. The number of the constraints depends on the size of state and action spaces, which makes the problem intractable in large or continuous environments. The general augmented Lagrangian method suffers the double-sampling obstacle in solving the LP. Namely, the conditional expectations originated from the constraint functions and the quadratic penalties in the augmented Lagrangian function impose difficulties in sampling and evaluation. Motivated from the updates of the multipliers, we overcome the obstacles in minimizing the augmented Lagrangian function by replacing the intractable conditional expectations with the multipliers. Therefore, a deep parameterized augment Lagrangian method is proposed. Furthermore, the replacement provides a promising breakthrough to integrate the two steps in the augmented Lagrangian method into a single constrained problem. A general theoretical analysis shows that the solutions generated from a sequence of the constrained optimizations converge to the optimal solution of the LP if the error is controlled properly. A theoretical analysis on the quadratic penalty algorithm under neural tangent kernel setting shows the residual can be arbitrarily small if the parameter in network and optimization algorithm is chosen suitably. Preliminary experiments illustrate that our method is competitive to other state-of-the-art algorithms.
http://w3id.org/mlsea/pwc/scientificWork/A%20Stochastic%20Sequential%20Quadratic%20Optimization%20Algorithm%20for%20Nonlinear%20Equality%20Constrained%20Optimization%20with%20Rank-Deficient%20Jacobians                                                                                  A Stochastic Sequential Quadratic Optimization Algorithm for Nonlinear Equality Constrained Optimization with Rank-Deficient Jacobians                                                                                  A sequential quadratic optimization algorithm is proposed for solving smooth nonlinear equality constrained optimization problems in which the objective function is defined by an expectation of a stochastic function. The algorithmic structure of the proposed method is based on a step decomposition strategy that is known in the literature to be widely effective in practice, wherein each search direction is computed as the sum of a normal step (toward linearized feasibility) and a tangential step (toward objective decrease in the null space of the constraint Jacobian). However, the proposed method is unique from others in the literature in that it both allows the use of stochastic objective gradient estimates and possesses convergence guarantees even in the setting in which the constraint Jacobians may be rank deficient. The results of numerical experiments demonstrate that the algorithm offers superior performance when compared to popular alternatives.
http://w3id.org/mlsea/pwc/scientificWork/A%20Stock%20Prediction%20Model%20Based%20on%20DCNN                                                                                  A Stock Prediction Model Based on DCNN                                                                                  The prediction of a stock price has always been a challenging issue, as its volatility can be affected by many factors such as national policies, company financial reports, industry performance, and investor sentiment etc.. In this paper, we present a prediction model based on deep CNN and the candle charts, the continuous time stock information is processed. According to different information richness, prediction time interval and classification method, the original data is divided into multiple categories as the training set of CNN. In addition, the convolutional neural network is used to predict the stock market and analyze the difference in accuracy under different classification methods. The results show that the method has the best performance when the forecast time interval is 20 days. Moreover, the Moving Average Convergence Divergence and three kinds of moving average are added as input. This method can accurately predict the stock trend of the US NDAQ exchange for 92.2%. Meanwhile, this article distinguishes three conventional classification methods to provide guidance for future research.
http://w3id.org/mlsea/pwc/scientificWork/A%20Streaming%20End-to-End%20Framework%20For%20Spoken%20Language%20Understanding                                                                                  A Streaming End-to-End Framework For Spoken Language Understanding                                                                                  End-to-end spoken language understanding (SLU) has recently attracted increasing interest. Compared to the conventional tandem-based approach that combines speech recognition and language understanding as separate modules, the new approach extracts users' intentions directly from the speech signals, resulting in joint optimization and low latency. Such an approach, however, is typically designed to process one intention at a time, which leads users to take multiple rounds to fulfill their requirements while interacting with a dialogue system. In this paper, we propose a streaming end-to-end framework that can process multiple intentions in an online and incremental way. The backbone of our framework is a unidirectional RNN trained with the connectionist temporal classification (CTC) criterion. By this design, an intention can be identified when sufficient evidence has been accumulated, and multiple intentions can be identified sequentially. We evaluate our solution on the Fluent Speech Commands (FSC) dataset and the intent detection accuracy is about 97 % on all multi-intent settings. This result is comparable to the performance of the state-of-the-art non-streaming models, but is achieved in an online and incremental way. We also employ our model to a keyword spotting task using the Google Speech Commands dataset and the results are also highly promising.
http://w3id.org/mlsea/pwc/scientificWork/A%20Strong%20Baseline%20for%20Vehicle%20Re-Identification                                                                                  A Strong Baseline for Vehicle Re-Identification                                                                                  Vehicle Re-Identification (Re-ID) aims to identify the same vehicle across different cameras, hence plays an important role in modern traffic management systems. The technical challenges require the algorithms must be robust in different views, resolution, occlusion and illumination conditions. In this paper, we first analyze the main factors hindering the Vehicle Re-ID performance. We then present our solutions, specifically targeting the dataset Track 2 of the 5th AI City Challenge, including (1) reducing the domain gap between real and synthetic data, (2) network modification by stacking multi heads with attention mechanism, (3) adaptive loss weight adjustment. Our method achieves 61.34% mAP on the private CityFlow testset without using external dataset or pseudo labeling, and outperforms all previous works at 87.1% mAP on the Veri benchmark. The code is available at https://github.com/cybercore-co-ltd/track2_aicity_2021.
http://w3id.org/mlsea/pwc/scientificWork/A%20Stronger%20Baseline%20for%20Ego-Centric%20Action%20Detection                                                                                  A Stronger Baseline for Ego-Centric Action Detection                                                                                  This technical report analyzes an egocentric video action detection method we used in the 2021 EPIC-KITCHENS-100 competition hosted in CVPR2021 Workshop. The goal of our task is to locate the start time and the end time of the action in the long untrimmed video, and predict action category. We adopt sliding window strategy to generate proposals, which can better adapt to short-duration actions. In addition, we show that classification and proposals are conflict in the same network. The separation of the two tasks boost the detection performance with high efficiency. By simply employing these strategy, we achieved 16.10 % performance on the test set of EPIC-KITCHENS-100 Action Detection challenge using a single model, surpassing the baseline method by 11.7 % in terms of average mAP.
http://w3id.org/mlsea/pwc/scientificWork/A%20Structural%20Model%20of%20Business%20Card%20Exchange%20Networks                                                                                  A Structural Model of Business Card Exchange Networks                                                                                  Social and professional networks affect labor market dynamics, knowledge diffusion and new business creation. To understand the determinants of how these networks are formed in the first place, we analyze a unique dataset of business cards exchanges among a sample of over 240,000 users of the multi-platform contact management and professional social networking tool for individuals Eight. We develop a structural model of network formation with strategic interactions, and we estimate users' payoffs that depend on the composition of business relationships, as well as indirect business interactions. We allow heterogeneity of users in both observable and unobservable characteristics to affect how relationships form and are maintained. The model's stationary equilibrium delivers a likelihood that is a mixture of exponential random graph models that we can characterize in closed-form. We overcome several econometric and computational challenges in estimation, by exploiting a two-step estimation procedure, variational approximations and minorization-maximization methods. Our algorithm is scalable, highly parallelizable and makes efficient use of computer memory to allow estimation in massive networks. We show that users payoffs display homophily in several dimensions, e.g. location; furthermore, users unobservable characteristics also display homophily.
http://w3id.org/mlsea/pwc/scientificWork/A%20Structure-Aware%20Relation%20Network%20for%20Thoracic%20Diseases%20Detection%20and%20Segmentation                                                                                  A Structure-Aware Relation Network for Thoracic Diseases Detection and Segmentation                                                                                  Instance level detection and segmentation of thoracic diseases or abnormalities are crucial for automatic diagnosis in chest X-ray images. Leveraging on constant structure and disease relations extracted from domain knowledge, we propose a structure-aware relation network (SAR-Net) extending Mask R-CNN. The SAR-Net consists of three relation modules: 1. the anatomical structure relation module encoding spatial relations between diseases and anatomical parts. 2. the contextual relation module aggregating clues based on query-key pair of disease RoI and lung fields. 3. the disease relation module propagating co-occurrence and causal relations into disease proposals. Towards making a practical system, we also provide ChestX-Det, a chest X-Ray dataset with instance-level annotations (boxes and masks). ChestX-Det is a subset of the public dataset NIH ChestX-ray14. It contains ~3500 images of 13 common disease categories labeled by three board-certified radiologists. We evaluate our SAR-Net on it and another dataset DR-Private. Experimental results show that it can enhance the strong baseline of Mask R-CNN with significant improvements. The ChestX-Det is released at https://github.com/Deepwise-AILab/ChestX-Det-Dataset.
http://w3id.org/mlsea/pwc/scientificWork/A%20Student-Teacher%20Architecture%20for%20Dialog%20Domain%20Adaptation%20under%20the%20Meta-Learning%20Setting                                                                                  A Student-Teacher Architecture for Dialog Domain Adaptation under the Meta-Learning Setting                                                                                  Numerous new dialog domains are being created every day while collecting data for these domains is extremely costly since it involves human interactions. Therefore, it is essential to develop algorithms that can adapt to different domains efficiently when building data-driven dialog models. The most recent researches on domain adaption focus on giving the model a better initialization, rather than optimizing the adaptation process. We propose an efficient domain adaptive task-oriented dialog system model, which incorporates a meta-teacher model to emphasize the different impacts between generated tokens with respect to the context. We first train our base dialog model and meta-teacher model adversarially in a meta-learning setting on rich-resource domains. The meta-teacher learns to quantify the importance of tokens under different contexts across different domains. During adaptation, the meta-teacher guides the dialog model to focus on important tokens in order to achieve better adaptation efficiency. We evaluate our model on two multi-domain datasets, MultiWOZ and Google Schema-Guided Dialogue, and achieve state-of-the-art performance.
http://w3id.org/mlsea/pwc/scientificWork/A%20Study%20On%20the%20Effects%20of%20Pre-processing%20On%20Spatio-temporal%20Action%20Recognition%20Using%20Spiking%20Neural%20Networks%20Trained%20with%20STDP                                                                                  A Study On the Effects of Pre-processing On Spatio-temporal Action Recognition Using Spiking Neural Networks Trained with STDP                                                                                  There has been an increasing interest in spiking neural networks in recent years. SNNs are seen as hypothetical solutions for the bottlenecks of ANNs in pattern recognition, such as energy efficiency. But current methods such as ANN-to-SNN conversion and back-propagation do not take full advantage of these networks, and unsupervised methods have not yet reached a success comparable to advanced artificial neural networks. It is important to study the behavior of SNNs trained with unsupervised learning methods such as spike-timing dependent plasticity (STDP) on video classification tasks, including mechanisms to model motion information using spikes, as this information is critical for video understanding. This paper presents multiple methods of transposing temporal information into a static format, and then transforming the visual information into spikes using latency coding. These methods are paired with two types of temporal fusion known as early and late fusion, and are used to help the spiking neural network in capturing the spatio-temporal features from videos. In this paper, we rely on the network architecture of a convolutional spiking neural network trained with STDP, and we test the performance of this network when challenged with action recognition tasks. Understanding how a spiking neural network responds to different methods of movement extraction and representation can help reduce the performance gap between SNNs and ANNs. In this paper we show the effect of the similarity in the shape and speed of certain actions on action recognition with spiking neural networks, we also highlight the effectiveness of some methods compared to others.
http://w3id.org/mlsea/pwc/scientificWork/A%20Study%20imbalance%20handling%20by%20various%20data%20sampling%20methods%20in%20binary%20classification                                                                                  A Study imbalance handling by various data sampling methods in binary classification                                                                                  The purpose of this research report is to present the our learning curve and the exposure to the Machine Learning life cycle, with the use of a Kaggle binary classification data set and taking to explore various techniques from pre-processing to the final optimization and model evaluation, also we highlight on the data imbalance issue and we discuss the different methods of handling that imbalance on the data level by over-sampling and under sampling not only to reach a balanced class representation but to improve the overall performance. This work also opens some gaps for future work.
http://w3id.org/mlsea/pwc/scientificWork/A%20Study%20into%20Pre-training%20Strategies%20for%20Spoken%20Language%20Understanding%20on%20Dysarthric%20Speech                                                                                  A Study into Pre-training Strategies for Spoken Language Understanding on Dysarthric Speech                                                                                  End-to-end (E2E) spoken language understanding (SLU) systems avoid an intermediate textual representation by mapping speech directly into intents with slot values. This approach requires considerable domain-specific training data. In low-resource scenarios this is a major concern, e.g., in the present study dealing with SLU for dysarthric speech. Pretraining part of the SLU model for automatic speech recognition targets helps but no research has shown to which extent SLU on dysarthric speech benefits from knowledge transferred from other dysarthric speech tasks. This paper investigates the efficiency of pre-training strategies for SLU tasks on dysarthric speech. The designed SLU system consists of a TDNN acoustic model for feature encoding and a capsule network for intent and slot decoding. The acoustic model is pre-trained in two stages: initialization with a corpus of normal speech and finetuning on a mixture of dysarthric and normal speech. By introducing the intelligibility score as a metric of the impairment severity, this paper quantitatively analyzes the relation between generalization and pathology severity for dysarthric speech.
http://w3id.org/mlsea/pwc/scientificWork/A%20Study%20into%20patient%20similarity%20through%20representation%20learning%20from%20medical%20records                                                                                  A Study into patient similarity through representation learning from medical records                                                                                  Patient similarity assessment, which identifies patients similar to a given patient, can help improve medical care. The assessment can be performed using Electronic Medical Records (EMRs). Patient similarity measurement requires converting heterogeneous EMRs into comparable formats to calculate their distance. While versatile document representation learning methods have been developed in recent years, it is still unclear how complex EMR data should be processed to create the most useful patient representations. This study presents a new data representation method for EMRs that takes the information in clinical narratives into account. To address the limitations of previous approaches in handling complex parts of EMR data, an unsupervised method is proposed for building a patient representation, which integrates unstructured data with structured data extracted from patients' EMRs. In order to model the extracted data, we employed a tree structure that captures the temporal relations of multiple medical events from EMR. We processed clinical notes to extract symptoms, signs, and diseases using different tools such as medspaCy, MetaMap, and scispaCy and mapped entities to the Unified Medical Language System (UMLS). After creating a tree data structure, we utilized two novel relabeling methods for the non-leaf nodes of the tree to capture two temporal aspects of the extracted events. By traversing the tree, we generated a sequence that could create an embedding vector for each patient. The comprehensive evaluation of the proposed method for patient similarity and mortality prediction tasks demonstrated that our proposed model leads to lower mean squared error (MSE), higher precision, and normalized discounted cumulative gain (NDCG) relative to baselines.
http://w3id.org/mlsea/pwc/scientificWork/A%20Study%20of%20Incorporating%20Articulatory%20Movement%20Information%20in%20Speech%20Enhancement                                                                                  A Study of Incorporating Articulatory Movement Information in Speech Enhancement                                                                                  Although deep learning algorithms are widely used for improving speech enhancement (SE) performance, the performance remains limited under highly challenging conditions, such as unseen noise or noise signals having low signal-to-noise ratios (SNRs). This study provides a pilot investigation on a novel multimodal audio-articulatory-movement SE (AAMSE) model to enhance SE performance under such challenging conditions. Articulatory movement features and acoustic signals were used as inputs to waveform-mapping-based and spectral-mapping-based SE systems with three fusion strategies. In addition, an ablation study was conducted to evaluate SE performance using a limited number of articulatory movement sensors. Experimental results confirm that, by combining the modalities, the AAMSE model notably improves the SE performance in terms of speech quality and intelligibility, as compared to conventional audio-only SE baselines.
http://w3id.org/mlsea/pwc/scientificWork/A%20Study%20of%20UK%20Household%20Wealth%20through%20Empirical%20Analysis%20and%20a%20Non-linear%20Kesten%20Process                                                                                  A Study of UK Household Wealth through Empirical Analysis and a Non-linear Kesten Process                                                                                  We study the wealth distribution of UK households through a detailed analysis of data from wealth surveys and rich lists, and propose a non-linear Kesten process to model the dynamics of household wealth. The main features of our model are that we focus on wealth growth and disregard exchange, and that the rate of return on wealth is increasing with wealth. The linear case with wealth-independent return rate has been well studied, leading to a log-normal wealth distribution in the long time limit which is essentially independent of initial conditions. We find through theoretical analysis and simulations that the non-linearity in our model leads to more realistic power-law tails, and can explain an apparent two-tailed structure in the empirical wealth distribution of the UK and other countries. Other realistic features of our model include an increase in inequality over time, and a stronger dependence on initial conditions compared to linear models.
http://w3id.org/mlsea/pwc/scientificWork/A%20Study%20of%20the%20Mathematics%20of%20Deep%20Learning                                                                                  A Study of the Mathematics of Deep Learning                                                                                  'Deep Learning'/'Deep Neural Nets' is a technological marvel that is now increasingly deployed at the cutting-edge of artificial intelligence tasks. This dramatic success of deep learning in the last few years has been hinged on an enormous amount of heuristics and it has turned out to be a serious mathematical challenge to be able to rigorously explain them. In this thesis, submitted to the Department of Applied Mathematics and Statistics, Johns Hopkins University we take several steps towards building strong theoretical foundations for these new paradigms of deep-learning. In chapter 2 we show new circuit complexity theorems for deep neural functions and prove classification theorems about these function spaces which in turn lead to exact algorithms for empirical risk minimization for depth 2 ReLU nets. We also motivate a measure of complexity of neural functions to constructively establish the existence of high-complexity neural functions. In chapter 3 we give the first algorithm which can train a ReLU gate in the realizable setting in linear time in an almost distribution free set up. In chapter 4 we give rigorous proofs towards explaining the phenomenon of autoencoders being able to do sparse-coding. In chapter 5 we give the first-of-its-kind proofs of convergence for stochastic and deterministic versions of the widely used adaptive gradient deep-learning algorithms, RMSProp and ADAM. This chapter also includes a detailed empirical study on autoencoders of the hyper-parameter values at which modern algorithms have a significant advantage over classical acceleration based methods. In the last chapter 6 we give new and improved PAC-Bayesian bounds for the risk of stochastic neural nets. This chapter also includes an experimental investigation revealing new geometric properties of the paths in weight space that are traced out by the net during the training.
http://w3id.org/mlsea/pwc/scientificWork/A%20Study%20of%20the%20Quality%20of%20Wikidata                                                                                  A Study of the Quality of Wikidata                                                                                  Wikidata has been increasingly adopted by many communities for a wide variety of applications, which demand high-quality knowledge to deliver successful results. In this paper, we develop a framework to detect and analyze low-quality statements in Wikidata by shedding light on the current practices exercised by the community. We explore three indicators of data quality in Wikidata, based on: 1) community consensus on the currently recorded knowledge, assuming that statements that have been removed and not added back are implicitly agreed to be of low quality; 2) statements that have been deprecated; and 3) constraint violations in the data. We combine these indicators to detect low-quality statements, revealing challenges with duplicate entities, missing triples, violated type rules, and taxonomic distinctions. Our findings complement ongoing efforts by the Wikidata community to improve data quality, aiming to make it easier for users and editors to find and correct mistakes.
http://w3id.org/mlsea/pwc/scientificWork/A%20Study%20on%20the%20Efficiency%20of%20the%20Indian%20Stock%20Market                                                                                  A Study on the Efficiency of the Indian Stock Market                                                                                  The efficiency of the stock market has a significant impact on the potential return on investment. An efficient market eliminates the possibility of arbitrage and unexploited profit opportunities. This study analyzes the weak form efficiency of the Indian Stock market based on the two major Indian stock exchanges, viz., BSE and NSE. The daily closing values of Sensex and Nifty indices for the period from April 2010 to March 2019 are used to perform the Runs test, the Autocorrelation test, and the Autoregression test. The study confirms that the Indian Stock market is weak form inefficient and can thus be outperformed.
http://w3id.org/mlsea/pwc/scientificWork/A%20Stylistic%20Analysis%20of%20Honest%20Deception%3A%20The%20Case%20of%20Seinfeld%20TV%20Series%20Sitcom                                                                                  A Stylistic Analysis of Honest Deception: The Case of Seinfeld TV Series Sitcom                                                                                  Language is a powerful tool if used in the correct manner. It is the major mode of communication, and using the correct choice of words and styles can serve to have a long-lasting impact. Stylistics is the study of the use of various language styles in communication to pass a message with a bigger impact or to communicate indirectly. Stylistic analysis, therefore, is the study of the use of linguistic styles in texts to determine how a style has been used, what is communicated and how it is communicated. Honest deception is the use of a choice of words to imply something different from the literal meaning. A person listening or reading a text where honest deception has been used and with a literal understanding may completely miss out on the point. This is because the issue of honesty and falsehood arises. However, it would be better to understand that honest deception is used with the intention of having a lasting impact rather than to deceive the readers, viewers or listeners. The major styles used in honest deception are hyperboles, litotes, irony and sarcasm. The Seinfeld Sitcom TV series was a situational TV comedy show aired from 1990 to 1998. the show attempts to bring to the understanding the daily life of a comedian and how comedian views life experiences and convert them into hilarious jokes. It also shows Jerry's struggle with getting the right partner from the many women who come into his life. Reflecting on honest deception in the Seinfeld sitcom TV series, this paper is going to investigate how honest deception has been used in the series, why it has been used and what is being communicated. The study is going to use a recapitulative form to give a better analysis and grouping of the different styles used in honest deception throughout the series.
http://w3id.org/mlsea/pwc/scientificWork/A%20Sub-Layered%20Hierarchical%20Pyramidal%20Neural%20Architecture%20for%20Facial%20Expression%20Recognition                                                                                  A Sub-Layered Hierarchical Pyramidal Neural Architecture for Facial Expression Recognition                                                                                  In domains where computational resources and labeled data are limited, such as in robotics, deep networks with millions of weights might not be the optimal solution. In this paper, we introduce a connectivity scheme for pyramidal architectures to increase their capacity for learning features. Experiments on facial expression recognition of unseen people demonstrate that our approach is a potential candidate for applications with restricted resources, due to good generalization performance and low computational cost. We show that our approach generalizes as well as convolutional architectures in this task but uses fewer trainable parameters and is more robust for low-resolution faces.
http://w3id.org/mlsea/pwc/scientificWork/A%20Subjective%20Study%20on%20Videos%20at%20Various%20Bit%20Depths                                                                                  A Subjective Study on Videos at Various Bit Depths                                                                                  Bit depth adaptation, where the bit depth of a video sequence is reduced before transmission and up-sampled during display, can potentially reduce data rates with limited impact on perceptual quality. In this context, we conducted a subjective study on a UHD video database, BVI-BD, to explore the relationship between bit depth and visual quality. In this work, three bit depth adaptation methods are investigated, including linear scaling, error diffusion, and a novel adaptive Gaussian filtering approach. The results from a subjective experiment indicate that above a critical bit depth, bit depth adaptation has no significant impact on perceptual quality, while reducing the amount information that is required to be transmitted. Below the critical bit depth, advanced adaptation methods can be used to retain `good' visual quality (on average) down to around 2 bits per color channel for the outlined experimental setup - a large reduction compared to the typically used 8 bits per color channel. A selection of image quality metrics were subsequently bench-marked on the subjective data, and analysis indicates that a bespoke quality metric is required for bit depth adaptation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Subspace-based%20Approach%20for%20Dimensionality%20Reduction%20and%20Important%20Variable%20Selection                                                                                  A Subspace-based Approach for Dimensionality Reduction and Important Variable Selection                                                                                  An analysis of high-dimensional data can offer a detailed description of a system but is often challenged by the curse of dimensionality. General dimensionality reduction techniques can alleviate such difficulty by extracting a few important features, but they are limited due to the lack of interpretability and connectivity to actual decision making associated with each physical variable. Variable selection techniques, as an alternative, can maintain the interpretability, but they often involve a greedy search that is susceptible to failure in capturing important interactions or a metaheuristic search that requires extensive computations. This research proposes a new method that produces subspaces, reduced-dimensional physical spaces, based on a randomized search and leverages an ensemble of critical subspace-based models, achieving dimensionality reduction and variable selection. When applied to high-dimensional data collected from the failure prediction of a composite/metal hybrid structure exhibiting complex progressive damage failure under loading, the proposed method outperforms the existing and potential alternatives in prediction and important variable selection.
http://w3id.org/mlsea/pwc/scientificWork/A%20Sufficient%20Condition%20to%20Guarantee%20Non-Simultaneous%20Charging%20and%20Discharging%20of%20Household%20Battery%20Energy%20Storage                                                                                  A Sufficient Condition to Guarantee Non-Simultaneous Charging and Discharging of Household Battery Energy Storage                                                                                  In this letter, we model the day-ahead price-based demand response of a residential household with battery energy storage and other controllable loads, as a convex optimization problem. Further using duality theory and Karush-Kuhn-Tucker optimality conditions, we derive a sufficient criterion which guarantees non-simultaneous charging and discharging of the battery energy storage, without explicitly modelling it as a constraint
http://w3id.org/mlsea/pwc/scientificWork/A%20Surface%20Geometry%20Model%20for%20LiDAR%20Depth%20Completion                                                                                  A Surface Geometry Model for LiDAR Depth Completion                                                                                  LiDAR depth completion is a task that predicts depth values for every pixel on the corresponding camera frame, although only sparse LiDAR points are available. Most of the existing state-of-the-art solutions are based on deep neural networks, which need a large amount of data and heavy computations for training the models. In this letter, a novel non-learning depth completion method is proposed by exploiting the local surface geometry that is enhanced by an outlier removal algorithm. The proposed surface geometry model is inspired by the observation that most pixels with unknown depth have a nearby LiDAR point. Therefore, it is assumed those pixels share the same surface with the nearest LiDAR point, and their respective depth can be estimated as the nearest LiDAR depth value plus a residual error. The residual error is calculated by using a derived equation with several physical parameters as input, including the known camera intrinsic parameters, estimated normal vector, and offset distance on the image plane. The proposed method is further enhanced by an outlier removal algorithm that is designed to remove incorrectly mapped LiDAR points from occluded regions. On KITTI dataset, the proposed solution achieves the best error performance among all existing non-learning methods and is comparable to the best self-supervised learning method and some supervised learning methods. Moreover, since outlier points from occluded regions is a commonly existing problem, the proposed outlier removal algorithm is a general preprocessing step that is applicable to many robotic systems with both camera and LiDAR sensors.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20Of%20Regression%20Algorithms%20And%20Connections%20With%20Deep%20Learning                                                                                  A Survey Of Regression Algorithms And Connections With Deep Learning                                                                                  Regression has attracted immense interest lately due to its effectiveness in tasks like predicting values. And Regression is of widespread use in multiple fields such as Economics, Finance, Business, Biology and so on. While considerable studies have proposed some impressive models, few of them have provided a whole picture regarding how and to what extent Regression has developed. With the aim of aiding beginners in understanding the relationships among different Regression algorithms, this paper characterizes a broad and thoughtful selection of recent regression algorithms, providing an organized and comprehensive overview of existing work and models utilized frequently. In this paper, the relationship between Regression and Deep Learning is also discussed and a conclusion can be drawn that Deep Learning can be more powerful as an combination with Regression models in the future.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20of%20Applied%20Machine%20Learning%20Techniques%20for%20Optical%20OFDM%20based%20Networks                                                                                  A Survey of Applied Machine Learning Techniques for Optical OFDM based Networks                                                                                  In this survey, we analyze the newest machine learning (ML) techniques for optical orthogonal frequency division multiplexing (O-OFDM)-based optical communications. ML has been proposed to mitigate channel and transceiver imperfections. For instance, ML can improve the signal quality under low modulation extinction ratio or can tackle both determinist and stochastic-induced nonlinearities such as parametric noise amplification in long-haul transmission. The proposed ML algorithms for O-OFDM can in particularly tackle inter-subcarrier nonlinear effects such as four-wave mixing and cross-phase modulation. In essence, these ML techniques could be beneficial for any multi-carrier approach (e.g. filter bank modulation). Supervised and unsupervised ML techniques are analyzed in terms of both O-OFDM transmission performance and computational complexity for potential real-time implementation. We indicate the strict conditions under which a ML algorithm should perform classification, regression or clustering. The survey also discusses open research issues and future directions towards the ML implementation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20of%20Data%20Augmentation%20Approaches%20for%20NLP                                                                                  A Survey of Data Augmentation Approaches for NLP                                                                                  Data augmentation has recently seen increased interest in NLP due to more work in low-resource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data. Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in a structured manner. We first introduce and motivate data augmentation for NLP, and then discuss major methodologically representative approaches. Next, we highlight techniques that are used for popular NLP applications and tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape of existing literature in data augmentation for NLP and motivate additional work in this area. We also present a GitHub repository with a paper list that will be continuously updated at https://github.com/styfeng/DataAug4NLP
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20of%20Deep%20Reinforcement%20Learning%20Algorithms%20for%20Motion%20Planning%20and%20Control%20of%20Autonomous%20Vehicles                                                                                  A Survey of Deep Reinforcement Learning Algorithms for Motion Planning and Control of Autonomous Vehicles                                                                                  In this survey, we systematically summarize the current literature on studies that apply reinforcement learning (RL) to the motion planning and control of autonomous vehicles. Many existing contributions can be attributed to the pipeline approach, which consists of many hand-crafted modules, each with a functionality selected for the ease of human interpretation. However, this approach does not automatically guarantee maximal performance due to the lack of a system-level optimization. Therefore, this paper also presents a growing trend of work that falls into the end-to-end approach, which typically offers better performance and smaller system scales. However, their performance also suffers from the lack of expert data and generalization issues. Finally, the remaining challenges applying deep RL algorithms on autonomous driving are summarized, and future research directions are also presented to tackle these challenges.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20of%20Forex%20and%20Stock%20Price%20Prediction%20Using%20Deep%20Learning                                                                                  A Survey of Forex and Stock Price Prediction Using Deep Learning                                                                                  The prediction of stock and foreign exchange (Forex) had always been a hot and profitable area of study. Deep learning application had proven to yields better accuracy and return in the field of financial prediction and forecasting. In this survey we selected papers from the DBLP database for comparison and analysis. We classified papers according to different deep learning methods, which included: Convolutional neural network (CNN), Long Short-Term Memory (LSTM), Deep neural network (DNN), Recurrent Neural Network (RNN), Reinforcement Learning, and other deep learning methods such as HAN, NLP, and Wavenet. Furthermore, this paper reviewed the dataset, variable, model, and results of each article. The survey presented the results through the most used performance metrics: RMSE, MAPE, MAE, MSE, accuracy, Sharpe ratio, and return rate. We identified that recent models that combined LSTM with other methods, for example, DNN, are widely researched. Reinforcement learning and other deep learning method yielded great returns and performances. We conclude that in recent years the trend of using deep-learning based method for financial modeling is exponentially rising.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20of%20Knowledge%20Tracing                                                                                  A Survey of Knowledge Tracing                                                                                  High-quality education is one of the keys to achieving a more sustainable world. In contrast to traditional face-to-face classroom education, online education enables us to record and research a large amount of learning data for offering intelligent educational services. Knowledge Tracing (KT), which aims to monitor students' evolving knowledge state in learning, is the fundamental task to support these intelligent services. In recent years, an increasing amount of research is focused on this emerging field and considerable progress has been made. In this survey, we categorize existing KT models from a technical perspective and investigate these models in a systematic manner. Subsequently, we review abundant variants of KT models that consider more strict learning assumptions from three phases: before, during, and after learning. To better facilitate researchers and practitioners working on this field, we open source two algorithm libraries: EduData for downloading and preprocessing KT-related datasets, and EduKTM with extensible and unified implementation of existing mainstream KT models. Moreover, the development of KT cannot be separated from its applications, therefore we further present typical KT applications in different scenarios. Finally, we discuss some potential directions for future research in this fast-growing field.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20of%20Modern%20Deep%20Learning%20based%20Object%20Detection%20Models                                                                                  A Survey of Modern Deep Learning based Object Detection Models                                                                                  Object Detection is the task of classification and localization of objects in an image or video. It has gained prominence in recent years due to its widespread applications. This article surveys recent developments in deep learning based object detectors. Concise overview of benchmark datasets and evaluation metrics used in detection is also provided along with some of the prominent backbone architectures used in recognition tasks. It also covers contemporary lightweight classification models used on edge devices. Lastly, we compare the performances of these architectures on multiple metrics.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20of%20Performance%20Optimization%20in%20Neural%20Network-Based%20Video%20Analytics%20Systems                                                                                  A Survey of Performance Optimization in Neural Network-Based Video Analytics Systems                                                                                  Video analytics systems perform automatic events, movements, and actions recognition in a video and make it possible to execute queries on the video. As a result of a large number of video data that need to be processed, optimizing the performance of video analytics systems has become an important research topic. Neural networks are the state-of-the-art for performing video analytics tasks such as video annotation and object detection. Prior survey papers consider application-specific video analytics techniques that improve accuracy of the results; however, in this survey paper, we provide a review of the techniques that focus on optimizing the performance of Neural Network-Based Video Analytics Systems.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20of%20Recent%20Abstract%20Summarization%20Techniques                                                                                  A Survey of Recent Abstract Summarization Techniques                                                                                  This paper surveys several recent abstract summarization methods: T5, Pegasus, and ProphetNet. We implement the systems in two languages: English and Indonesian languages. We investigate the impact of pre-training models (one T5, three Pegasuses, three ProphetNets) on several Wikipedia datasets in English and Indonesian language and compare the results to the Wikipedia systems' summaries. The T5-Large, the Pegasus-XSum, and the ProphetNet-CNNDM provide the best summarization. The most significant factors that influence ROUGE performance are coverage, density, and compression. The higher the scores, the better the summary. Other factors that influence the ROUGE scores are the pre-training goal, the dataset's characteristics, the dataset used for testing the pre-trained model, and the cross-lingual function. Several suggestions to improve this paper's limitation are: 1) assure that the dataset used for the pre-training model must sufficiently large, contains adequate instances for handling cross-lingual purpose; 2) Advanced process (finetuning) shall be reasonable. We recommend using the large dataset consists of comprehensive coverage of topics from many languages before implementing advanced processes such as the train-infer-train procedure to the zero-shot translation in the training stage of the pre-training model.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20of%20Transformers                                                                                  A Survey of Transformers                                                                                  Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20of%20the%20Individual-Based%20Model%20applied%20in%20Biomedical%20and%20Epidemiology                                                                                  A Survey of the Individual-Based Model applied in Biomedical and Epidemiology                                                                                  Individual-based model (IBM) has been used to simulate and to design control strategies for dynamic systems that are subject to stochasticity and heterogeneity, such as infectious diseases. In the IBM, an individual is represented by a set of specific characteristics that may change dynamically over time. This feature allows a more realistic analysis of the spread of an epidemic. This paper presents a literature survey of IBM applied to biomedical and epidemiology research. The main goal is to present existing techniques, advantages and future perspectives in the development of the model. We evaluated 89 articles, which mostly analyze interventions aimed at endemic infections. In addition to the review, an overview of IBM is presented as an alternative to complement or replace compartmental models, such as the SIR (Susceptible-Infected-Recovered) model. Numerical simulations also illustrate the capabilities of IBM, as well as some limitations regarding the effects of discretization. We show that similar side-effects of discretization scheme for compartmental models may also occur in IBM, which requires careful attention.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20on%20Accuracy-oriented%20Neural%20Recommendation%3A%20From%20Collaborative%20Filtering%20to%20Information-rich%20Recommendation                                                                                  A Survey on Accuracy-oriented Neural Recommendation: From Collaborative Filtering to Information-rich Recommendation                                                                                  Influenced by the great success of deep learning in computer vision and language understanding, research in recommendation has shifted to inventing new recommender models based on neural networks. In recent years, we have witnessed significant progress in developing neural recommender models, which generalize and surpass traditional recommender models owing to the strong representation power of neural networks. In this survey paper, we conduct a systematic review on neural recommender models from the perspective of recommendation modeling with the accuracy goal, aiming to summarize this field to facilitate researchers and practitioners working on recommender systems. Specifically, based on the data usage during recommendation modeling, we divide the work into collaborative filtering and information-rich recommendation: 1) collaborative filtering, which leverages the key source of user-item interaction data; 2) content enriched recommendation, which additionally utilizes the side information associated with users and items, like user profile and item knowledge graph; and 3) temporal/sequential recommendation, which accounts for the contextual information associated with an interaction, such as time, location, and the past interactions. After reviewing representative work for each type, we finally discuss some promising directions in this field.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20on%20Adversarial%20Image%20Synthesis                                                                                  A Survey on Adversarial Image Synthesis                                                                                  Generative Adversarial Networks (GANs) have been extremely successful in various application domains. Adversarial image synthesis has drawn increasing attention and made tremendous progress in recent years because of its wide range of applications in many computer vision and image processing problems. Among the many applications of GAN, image synthesis is the most well-studied one, and research in this area has already demonstrated the great potential of using GAN in image synthesis. In this paper, we provide a taxonomy of methods used in image synthesis, review different models for text-to-image synthesis and image-to-image translation, and discuss some evaluation metrics as well as possible future research directions in image synthesis with GAN.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20on%20Anomaly%20Detection%20for%20Technical%20Systems%20using%20LSTM%20Networks                                                                                  A Survey on Anomaly Detection for Technical Systems using LSTM Networks                                                                                  Anomalies represent deviations from the intended system operation and can lead to decreased efficiency as well as partial or complete system failure. As the causes of anomalies are often unknown due to complex system dynamics, efficient anomaly detection is necessary. Conventional detection approaches rely on statistical and time-invariant methods that fail to address the complex and dynamic nature of anomalies. With advances in artificial intelligence and increasing importance for anomaly detection and prevention in various domains, artificial neural network approaches enable the detection of more complex anomaly types while considering temporal and contextual characteristics. In this article, a survey on state-of-the-art anomaly detection using deep neural and especially long short-term memory networks is conducted. The investigated approaches are evaluated based on the application scenario, data and anomaly types as well as further metrics. To highlight the potential of upcoming anomaly detection techniques, graph-based and transfer learning approaches are also included in the survey, enabling the analysis of heterogeneous data as well as compensating for its shortage and improving the handling of dynamic processes.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20on%20Complex%20Knowledge%20Base%20Question%20Answering%3A%20Methods%2C%20Challenges%20and%20Solutions                                                                                  A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions                                                                                  Knowledge base question answering (KBQA) aims to answer a question over a knowledge base (KB). Recently, a large number of studies focus on semantically or syntactically complicated questions. In this paper, we elaborately summarize the typical challenges and solutions for complex KBQA. We begin with introducing the background about the KBQA task. Next, we present the two mainstream categories of methods for complex KBQA, namely semantic parsing-based (SP-based) methods and information retrieval-based (IR-based) methods. We then review the advanced methods comprehensively from the perspective of the two categories. Specifically, we explicate their solutions to the typical challenges. Finally, we conclude and discuss some promising directions for future research.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20on%20Deep%20Domain%20Adaptation%20for%20LiDAR%20Perception                                                                                  A Survey on Deep Domain Adaptation for LiDAR Perception                                                                                  Scalable systems for automated driving have to reliably cope with an open-world setting. This means, the perception systems are exposed to drastic domain shifts, like changes in weather conditions, time-dependent aspects, or geographic regions. Covering all domains with annotated data is impossible because of the endless variations of domains and the time-consuming and expensive annotation process. Furthermore, fast development cycles of the system additionally introduce hardware changes, such as sensor types and vehicle setups, and the required knowledge transfer from simulation. To enable scalable automated driving, it is therefore crucial to address these domain shifts in a robust and efficient manner. Over the last years, a vast amount of different domain adaptation techniques evolved. There already exists a number of survey papers for domain adaptation on camera images, however, a survey for LiDAR perception is absent. Nevertheless, LiDAR is a vital sensor for automated driving that provides detailed 3D scans of the vehicle's surroundings. To stimulate future research, this paper presents a comprehensive review of recent progress in domain adaptation methods and formulates interesting research questions specifically targeted towards LiDAR perception.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20on%20Deep%20Learning%20Event%20Extraction%3A%20Approaches%20and%20Applications                                                                                  A Survey on Deep Learning Event Extraction: Approaches and Applications                                                                                  Event extraction (EE) is a crucial research task for promptly apprehending event information from massive textual data. With the rapid development of deep learning, EE based on deep learning technology has become a research hotspot. Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This article fills the research gap by reviewing the state-of-the-art approaches, especially focusing on the general domain EE based on deep learning models. We introduce a new literature classification of current general domain EE research according to the task definition. Afterward, we summarize the paradigm and models of EE approaches, and then discuss each of them in detail. As an important aspect, we summarize the benchmarks that support tests of predictions and evaluation metrics. A comprehensive comparison among different approaches is also provided in this survey. Finally, we conclude by summarizing future research directions facing the research area.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20on%20Deep%20Learning%20Technique%20for%20Video%20Segmentation                                                                                  A Survey on Deep Learning Technique for Video Segmentation                                                                                  Video segmentation -- partitioning video frames into multiple segments or objects -- plays a critical role in a broad range of practical applications, from enhancing visual effects in movie, to understanding scenes in autonomous driving, to creating virtual background in video conferencing. Recently, with the renaissance of connectionism in computer vision, there has been an influx of deep learning based approaches for video segmentation that have delivered compelling performance. In this survey, we comprehensively review two basic lines of research -- generic object segmentation (of unknown categories) in videos, and video semantic segmentation -- by introducing their respective task settings, background concepts, perceived need, development history, and main challenges. We also offer a detailed overview of representative literature on both methods and datasets. We further benchmark the reviewed methods on several well-known datasets. Finally, we point out open issues in this field, and suggest opportunities for further research. We also provide a public website to continuously track developments in this fast advancing field: https://github.com/tfzhou/VS-Survey.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20on%20Federated%20Learning%20and%20its%20Applications%20for%20Accelerating%20Industrial%20Internet%20of%20Things                                                                                  A Survey on Federated Learning and its Applications for Accelerating Industrial Internet of Things                                                                                  Federated learning (FL) brings collaborative intelligence into industries without centralized training data to accelerate the process of Industry 4.0 on the edge computing level. FL solves the dilemma in which enterprises wish to make the use of data intelligence with security concerns. To accelerate industrial Internet of things with the further leverage of FL, existing achievements on FL are developed from three aspects: 1) define terminologies and elaborate a general framework of FL for accommodating various scenarios; 2) discuss the state-of-the-art of FL on fundamental researches including data partitioning, privacy preservation, model optimization, local model transportation, personalization, motivation mechanism, platform & tools, and benchmark; 3) discuss the impacts of FL from the economic perspective. To attract more attention from industrial academia and practice, a FL-transformed manufacturing paradigm is presented, and future research directions of FL are given and possible immediate applications in Industry 4.0 domain are also proposed.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20on%20Human-aware%20Robot%20Navigation                                                                                  A Survey on Human-aware Robot Navigation                                                                                  Intelligent systems are increasingly part of our everyday lives and have been integrated seamlessly to the point where it is difficult to imagine a world without them. Physical manifestations of those systems on the other hand, in the form of embodied agents or robots, have so far been used only for specific applications and are often limited to functional roles (e.g. in the industry, entertainment and military fields). Given the current growth and innovation in the research communities concerned with the topics of robot navigation, human-robot-interaction and human activity recognition, it seems like this might soon change. Robots are increasingly easy to obtain and use and the acceptance of them in general is growing. However, the design of a socially compliant robot that can function as a companion needs to take various areas of research into account. This paper is concerned with the navigation aspect of a socially-compliant robot and provides a survey of existing solutions for the relevant areas of research as well as an outlook on possible future directions.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20on%20Machine%20Learning%20Algorithms%20for%20Applications%20in%20Cognitive%20Radio%20Networks                                                                                  A Survey on Machine Learning Algorithms for Applications in Cognitive Radio Networks                                                                                  In this paper, we present a survey on the utility of machine learning (ML) algorithms for applications in cognitive radio networks (CRN). We start with a high-level overview of some of the major challenges in CRNs, and mention the ML architectures and algorithms that can be used to alleviate them. In particular, our focus is on two fundamental applications in CRNs, namely spectrum sensing -- with non-cooperative and cooperative scenarios, and dynamic spectrum access -- with spectrum auction and prediction. We present a detailed study of recent advancements in the field of ML in CRNs for these applications, and briefly discuss the set of challenges in real-time implementation of ML techniques for CRNs.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20on%20Millimeter-Wave%20Beamforming%20Enabled%20UAV%20Communications%20and%20Networking                                                                                  A Survey on Millimeter-Wave Beamforming Enabled UAV Communications and Networking                                                                                  Unmanned aerial vehicles (UAVs) have found widespread commercial, civilian, and military applications. Wireless communication has always been one of the core technologies for UAV. However, the communication capacity is becoming a bottleneck for UAV to support more challenging application scenarios. The heavily-occupied sub-6 GHz frequency band is not sufficient to meet the ultra high-data-traffic requirements. The utilization of the millimeter-wave (mmWave) frequency bands is a promising direction for UAV communications, where large antenna arrays can be packed in a small area on the UAV to perform three-dimensional (3D) beamforming. On the other hand, UAVs serving as aerial access points or relays can significantly enhance the coverage and quality of service of the terrestrial mmWave cellular networks. In this paper, we provide a comprehensive survey on mmWave beamforming enabled UAV communications and networking. The technical potential of and challenges for mmWave-UAV communications are presented first. Then, we provide an overview on relevant mmWave antenna structures and channel modeling. Subsequently, the technologies and solutions for UAV-connected mmWave cellular networks and mmWave-UAV ad hoc networks are reviewed, respectively. Finally, we present open issues and promising directions for future research in mmWave beamforming enabled UAV communications and networking.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20on%20Natural%20Language%20Video%20Localization                                                                                  A Survey on Natural Language Video Localization                                                                                  Natural language video localization (NLVL), which aims to locate a target moment from a video that semantically corresponds to a text query, is a novel and challenging task. Toward this end, in this paper, we present a comprehensive survey of the NLVL algorithms, where we first propose the pipeline of NLVL, and then categorize them into supervised and weakly-supervised methods, following by the analysis of the strengths and weaknesses of each kind of methods. Subsequently, we present the dataset, evaluation protocols and the general performance analysis. Finally, the possible perspectives are obtained by summarizing the existing methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20on%20Neural%20Speech%20Synthesis                                                                                  A Survey on Neural Speech Synthesis                                                                                  Text to speech (TTS), or speech synthesis, which aims to synthesize intelligible and natural speech given text, is a hot research topic in speech, language, and machine learning communities and has broad applications in the industry. As the development of deep learning and artificial intelligence, neural network-based TTS has significantly improved the quality of synthesized speech in recent years. In this paper, we conduct a comprehensive survey on neural TTS, aiming to provide a good understanding of current research and future trends. We focus on the key components in neural TTS, including text analysis, acoustic models and vocoders, and several advanced topics, including fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc. We further summarize resources related to TTS (e.g., datasets, opensource implementations) and discuss future research directions. This survey can serve both academic researchers and industry practitioners working on TTS.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20on%20Optimal%20Transport%20for%20Machine%20Learning%3A%20Theory%20and%20Applications                                                                                  A Survey on Optimal Transport for Machine Learning: Theory and Applications                                                                                  Optimal Transport (OT) theory has seen an increasing amount of attention from the computer science community due to its potency and relevance in modeling and machine learning. It introduces means that serve as powerful ways to compare probability distributions with each other, as well as producing optimal mappings to minimize cost functions. In this survey, we present a brief introduction and history, a survey of previous work and propose directions of future study. We will begin by looking at the history of optimal transport and introducing the founders of this field. We then give a brief glance into the algorithms related to OT. Then, we will follow up with a mathematical formulation and the prerequisites to understand OT. These include Kantorovich duality, entropic regularization, KL Divergence, and Wassertein barycenters. Since OT is a computationally expensive problem, we then introduce the entropy-regularized version of computing optimal mappings, which allowed OT problems to become applicable in a wide range of machine learning problems. In fact, the methods generated from OT theory are competitive with the current state-of-the-art methods. We follow this up by breaking down research papers that focus on image processing, graph learning, neural architecture search, document representation, and domain adaptation. We close the paper with a small section on future research. Of the recommendations presented, three main problems are fundamental to allow OT to become widely applicable but rely strongly on its mathematical formulation and thus are hardest to answer. Since OT is a novel method, there is plenty of space for new research, and with more and more competitive methods (either on an accuracy level or computational speed level) being created, the future of applied optimal transport is bright as it has become pervasive in machine learning.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20on%20Reinforcement%20Learning-Aided%20Caching%20in%20Mobile%20Edge%20Networks                                                                                  A Survey on Reinforcement Learning-Aided Caching in Mobile Edge Networks                                                                                  Mobile networks are experiencing tremendous increase in data volume and user density. An efficient technique to alleviate this issue is to bring the data closer to the users by exploiting the caches of edge network nodes, such as fixed or mobile access points and even user devices. Meanwhile, the fusion of machine learning and wireless networks offers a viable way for network optimization as opposed to traditional optimization approaches which incur high complexity, or fail to provide optimal solutions. Among the various machine learning categories, reinforcement learning operates in an online and autonomous manner without relying on large sets of historical data for training. In this survey, reinforcement learning-aided mobile edge caching is presented, aiming at highlighting the achieved network gains over conventional caching approaches. Taking into account the heterogeneity of sixth generation (6G) networks in various wireless settings, such as fixed, vehicular and flying networks, learning-aided edge caching is presented, departing from traditional architectures. Furthermore, a categorization according to the desirable performance metric, such as spectral, energy and caching efficiency, average delay, and backhaul and fronthaul offloading is provided. Finally, several open issues are discussed, targeting to stimulate further interest in this important research field.
http://w3id.org/mlsea/pwc/scientificWork/A%20Survey%20on%20sentiment%20analysis%20in%20Persian%3A%20A%20Comprehensive%20System%20Perspective%20Covering%20Challenges%20and%20Advances%20in%20Resources%2C%20and%20Methods                                                                                  A Survey on sentiment analysis in Persian: A Comprehensive System Perspective Covering Challenges and Advances in Resources, and Methods                                                                                  Social media has been remarkably grown during the past few years. Nowadays, posting messages on social media websites has become one of the most popular Internet activities. The vast amount of user-generated content has made social media the most extensive data source of public opinion. Sentiment analysis is one of the techniques used to analyze user-generated data. The Persian language has specific features and thereby requires unique methods and models to be adopted for sentiment analysis, which are different from those in English language. Sentiment analysis in each language has specified prerequisites; hence, the direct use of methods, tools, and resources developed for English language in Persian has its limitations. The main target of this paper is to provide a comprehensive literature survey for state-of-the-art advances in Persian sentiment analysis. In this regard, the present study aims to investigate and compare the previous sentiment analysis studies on Persian texts and describe contributions presented in articles published in the last decade. First, the levels, approaches, and tasks for sentiment analysis are described. Then, a detailed survey of the sentiment analysis methods used for Persian texts is presented, and previous relevant works on Persian Language are discussed. Moreover, we present in this survey the authentic and published standard sentiment analysis resources and advances that have been done for Persian sentiment analysis. Finally, according to the state-of-the-art development of English sentiment analysis, some issues and challenges not being addressed in Persian texts are listed, and some guidelines and trends are provided for future research on Persian texts. The paper provides information to help new or established researchers in the field as well as industry developers who aim to deploy an operational complete sentiment analysis system.
http://w3id.org/mlsea/pwc/scientificWork/A%20Swarm%20Variant%20for%20the%20Schr%C3%B6dinger%20Solver                                                                                  A Swarm Variant for the Schrödinger Solver                                                                                  This paper introduces application of the Exponentially Averaged Momentum Particle Swarm Optimization (EM-PSO) as a derivative-free optimizer for Neural Networks. It adopts PSO's major advantages such as search space exploration and higher robustness to local minima compared to gradient-descent optimizers such as Adam. Neural network based solvers endowed with gradient optimization are now being used to approximate solutions to Differential Equations. Here, we demonstrate the novelty of EM-PSO in approximating gradients and leveraging the property in solving the Schr 'odinger equation, for the Particle-in-a-Box problem. We also provide the optimal set of hyper-parameters supported by mathematical proofs, suited for our algorithm.
http://w3id.org/mlsea/pwc/scientificWork/A%20Symbolic%20Regression%20Method%20for%20Dynamic%20Modeling%20and%20Control%20of%20Quadrotor%20UAVs                                                                                  A Symbolic Regression Method for Dynamic Modeling and Control of Quadrotor UAVs                                                                                  This paper presents a mathematic dynamic model of a quadrotor unmanned aerial vehicle (QUAV) by using the symbolic regression approach and then proposes a hierarchical control scheme for trajectory tracking. The symbolic regression approach is capable of constructing analytical quadrotor dynamic equations only through the collected data, which relieves the burden of first principle modeling. To facilitate position tracking control of a QUAV, the design of controller can be decomposed into two parts: a proportional-integral controller for the position subsystem is first designed to obtain the desired horizontal position and the backstepping method for the attitude subsystem is developed to ensure that the Euler angles and the altitude can fast converge to the reference values. The effectiveness is verified through experiments on a benchmark multicopter simulator.
http://w3id.org/mlsea/pwc/scientificWork/A%20Synchronized%20Reprojection-based%20Model%20for%203D%20Human%20Pose%20Estimation                                                                                  A Synchronized Reprojection-based Model for 3D Human Pose Estimation                                                                                  3D human pose estimation is still a challenging problem despite the large amount of work that has been done in this field. Generally, most methods directly use neural networks and ignore certain constraints (e.g., reprojection constraints and joint angle and bone length constraints). This paper proposes a weakly supervised GAN-based model for 3D human pose estimation that considers 3D information along with 2D information simultaneously, in which a reprojection network is employed to learn the mapping of the distribution from 3D poses to 2D poses. In particular, we train the reprojection network and the generative adversarial network synchronously. Furthermore, inspired by the typical kinematic chain space (KCS) matrix, we propose a weighted KCS matrix, which is added into the discriminator's input to impose joint angle and bone length constraints. The experimental results on Human3.6M show that our method outperforms state-of-the-art methods by approximately 24.7 %.
http://w3id.org/mlsea/pwc/scientificWork/A%20Syntax-Guided%20Edit%20Decoder%20for%20Neural%20Program%20Repair                                                                                  A Syntax-Guided Edit Decoder for Neural Program Repair                                                                                  Automated Program Repair (APR) helps improve the efficiency of software development and maintenance. Recent APR techniques use deep learning, particularly the encoder-decoder architecture, to generate patches. Though existing DL-based APR approaches have proposed different encoder architectures, the decoder remains to be the standard one, which generates a sequence of tokens one by one to replace the faulty statement. This decoder has multiple limitations: 1) allowing to generate syntactically incorrect programs, 2) inefficiently representing small edits, and 3) not being able to generate project-specific identifiers. In this paper, we propose Recoder, a syntax-guided edit decoder with placeholder generation. Recoder is novel in multiple aspects: 1) Recoder generates edits rather than modified code, allowing efficient representation of small edits; 2) Recoder is syntax-guided, with the novel provider/decider architecture to ensure the syntactic correctness of the patched program and accurate generation; 3) Recoder generates placeholders that could be instantiated as project-specific identifiers later. We conduct experiments to evaluate Recoder on 395 bugs from Defects4J v1.2 and 420 additional bugs from Defects4J v2.0. Our results show that Recoder repairs 53 bugs on Defects4J v1.2, which achieves 21.4% improvement over the previous state-of-the-art approach for single-hunk bugs (TBar). Importantly, to our knowledge, Recoder is the first DL-based APR approach that has outperformed the traditional APR approaches on this dataset. Furthermore, Recoder also repairs 19 bugs on the additional bugs from Defects4J v2.0, which is 137.5% more than TBar (8 bugs) and 850% more than SimFix (2 bugs). This result suggests that Recoder has better generalizability than existing APR approaches.
http://w3id.org/mlsea/pwc/scientificWork/A%20System%20Model-Based%20Approach%20for%20the%20Control%20of%20Power%20Park%20Modules%20for%20Grid%20Voltage%20and%20Frequency%20Services                                                                                  A System Model-Based Approach for the Control of Power Park Modules for Grid Voltage and Frequency Services                                                                                  A new control approach is proposed for the grid insertion of Power Park Modules (PPMs). It allows full participation of these modules to ancillary services. This means that, not only their control have some positive impact on the grid frequency and voltage dynamics, but they can effectively participate to existing primary and secondary control loops together with the classic thermal/inertia synchronous generators and fulfill the same specifications both from the control and contractual points of view. To achieve such level of performances, a system approach based on an innovatory control model is proposed. The latter control model drops classic hypothesis for separation of voltage and frequency dynamics used till now in order to gather these dynamics into a small size model. From the system point of view, dynamics are grouped by time-scales of phenomena in the proposed control model. This results in more performant controls in comparison to classic approaches which orient controls to physical actuators (control of grid side converter and of generator side converter). Also, this allows coordination between control of converters and generator or, in case of multimachines specifications, among several PPMs. From the control synthesis point of view, classic robust approaches are used (like, e.g., H-infinity synthesis). Implementation and validation tests are presented for wind PPMs but the approach holds for any other type of PPM. These results will be further used to control the units of the new concept of Dynamic Virtual Power Plant introduced in the H2020 POSYTYF project.
http://w3id.org/mlsea/pwc/scientificWork/A%20System%20for%20Traded%20Control%20Teleoperation%20of%20Manipulation%20Tasks%20using%20Intent%20Prediction%20from%20Hand%20Gestures                                                                                  A System for Traded Control Teleoperation of Manipulation Tasks using Intent Prediction from Hand Gestures                                                                                  This paper presents a teleoperation system that includes robot perception and intent prediction from hand gestures. The perception module identifies the objects present in the robot workspace and the intent prediction module which object the user likely wants to grasp. This architecture allows the approach to rely on traded control instead of direct control: we use hand gestures to specify the goal objects for a sequential manipulation task, the robot then autonomously generates a grasping or a retrieving motion using trajectory optimization. The perception module relies on the model-based tracker to precisely track the 6D pose of the objects and makes use of a state of the art learning-based object detection and segmentation method, to initialize the tracker by automatically detecting objects in the scene. Goal objects are identified from user hand gestures using a trained a multi-layer perceptron classifier. After presenting all the components of the system and their empirical evaluation, we present experimental results comparing our pipeline to a direct traded control approach (i.e., one that does not use prediction) which shows that using intent prediction allows to bring down the overall task execution time.
http://w3id.org/mlsea/pwc/scientificWork/A%20Systematic%20Collection%20of%20Medical%20Image%20Datasets%20for%20Deep%20Learning                                                                                  A Systematic Collection of Medical Image Datasets for Deep Learning                                                                                  The astounding success made by artificial intelligence (AI) in healthcare and other fields proves that AI can achieve human-like performance. However, success always comes with challenges. Deep learning algorithms are data-dependent and require large datasets for training. The lack of data in the medical imaging field creates a bottleneck for the application of deep learning to medical image analysis. Medical image acquisition, annotation, and analysis are costly, and their usage is constrained by ethical restrictions. They also require many resources, such as human expertise and funding. That makes it difficult for non-medical researchers to have access to useful and large medical data. Thus, as comprehensive as possible, this paper provides a collection of medical image datasets with their associated challenges for deep learning research. We have collected information of around three hundred datasets and challenges mainly reported between 2013 and 2020 and categorized them into four categories: head & neck, chest & abdomen, pathology & blood, and ``others''. Our paper has three purposes: 1) to provide a most up to date and complete list that can be used as a universal reference to easily find the datasets for clinical image analysis, 2) to guide researchers on the methodology to test and evaluate their methods' performance and robustness on relevant datasets, 3) to provide a ``route'' to relevant algorithms for the relevant medical topics, and challenge leaderboards.
http://w3id.org/mlsea/pwc/scientificWork/A%20Systematic%20Investigation%20of%20KB-Text%20Embedding%20Alignment%20at%20Scale                                                                                  A Systematic Investigation of KB-Text Embedding Alignment at Scale                                                                                  Knowledge bases (KBs) and text often contain complementary knowledge: KBs store structured knowledge that can support long range reasoning, while text stores more comprehensive and timely knowledge in an unstructured way. Separately embedding the individual knowledge sources into vector spaces has demonstrated tremendous successes in encoding the respective knowledge, but how to jointly embed and reason with both knowledge sources to fully leverage the complementary information is still largely an open problem. We conduct a large-scale, systematic investigation of aligning KB and text embeddings for joint reasoning. We set up a novel evaluation framework with two evaluation tasks, few-shot link prediction and analogical reasoning, and evaluate an array of KB-text embedding alignment methods. We also demonstrate how such alignment can infuse textual information into KB embeddings for more accurate link prediction on emerging entities and events, using COVID-19 as a case study.
http://w3id.org/mlsea/pwc/scientificWork/A%20Systematic%20Literature%20Review%20on%20Process-Aware%20Recommender%20Systems                                                                                  A Systematic Literature Review on Process-Aware Recommender Systems                                                                                  Considering processes of a business in a recommender system is highly advantageous. Although most studies in the business process analysis domain are of descriptive and predictive nature, the feasibility of constructing a process-aware recommender system is assessed in a few works. One reason can be the lack of knowledge on process mining potential for recommendation problems. Therefore, this paper aims to identify and analyze the published studies on process-aware recommender system techniques in business process management and process mining domain. A systematic review was conducted on 33 academic articles published between 2008 and 2020 according to several aspects. In this regard, we provide a state-of-the-art review with critical details and researchers with a better perception of which path to pursue in this field. Moreover, based on a knowledge base and holistic perspective, we discuss some research gaps and open challenges in this field.
http://w3id.org/mlsea/pwc/scientificWork/A%20Systems%20Theory%20of%20Transfer%20Learning                                                                                  A Systems Theory of Transfer Learning                                                                                  Existing frameworks for transfer learning are incomplete from a systems theoretic perspective. They place emphasis on notions of domain and task, and neglect notions of structure and behavior. In doing so, they limit the extent to which formalism can be carried through into the elaboration of their frameworks. Herein, we use Mesarovician systems theory to define transfer learning as a relation on sets and subsequently characterize the general nature of transfer learning as a mathematical construct. We interpret existing frameworks in terms of ours and go beyond existing frameworks to define notions of transferability, transfer roughness, and transfer distance. Importantly, despite its formalism, our framework avoids the detailed mathematics of learning theory or machine learning solution methods without excluding their consideration. As such, we provide a formal, general systems framework for modeling transfer learning that offers a rigorous foundation for system design and analysis.
http://w3id.org/mlsea/pwc/scientificWork/A%20Tale%20of%20Two%20Lexica%20Testing%20Computational%20Hypotheses%20with%20Deep%20Convolutional%20Neural%20Networks                                                                                  A Tale of Two Lexica Testing Computational Hypotheses with Deep Convolutional Neural Networks                                                                                  Gow's (2012) dual lexicon model suggests that the primary purpose of words is to mediate the mappings between acoustic-phonetic input and other forms of linguistic representation. Motivated by evidence from functional imaging, aphasia, and behavioral results, the model argues for the existence of two parallel wordform stores: the dorsal and ventral processing streams. In this paper, we tested the hypothesis that the complex, but systematic mapping between sound and articulation in the dorsal stream poses different computational pressures on feature sets than the more arbitrary mapping between sound and meaning. To test this hypothesis, we created two deep convolutional neural networks (CNNs). While the dorsal network was trained to identify individual spoken words, the ventral network was trained to map them onto semantic classes. We then extracted patterns of network activation from the penultimate level of each network and tested how well features generated by the network supported generalization to linguistic categorization associated with the dorsal versus ventral processing streams. Our preliminary results showed both models successfully learned their tasks. Secondary generalization testing showed the ventral CNN outperformed the dorsal CNN on a semantic task: concreteness classification, while the dorsal CNN outperformed the ventral CNN on articulation tasks: classification by onset phoneme class and syllable length. These results are consistent with the hypothesis that the divergent processing demands of the ventral and dorsal processing streams impose computational pressures for the development of multiple lexica.
http://w3id.org/mlsea/pwc/scientificWork/A%20Tale%20of%20Two%20Tails%3A%20A%20Model-free%20Approach%20to%20Estimating%20Disaster%20Risk%20Premia%20and%20Testing%20Asset%20Pricing%20Models                                                                                  A Tale of Two Tails: A Model-free Approach to Estimating Disaster Risk Premia and Testing Asset Pricing Models                                                                                  I introduce a model-free methodology to assess the impact of disaster risk on the market return. Using S&P500 returns and the risk-neutral quantile function derived from option prices, I employ quantile regression to estimate local differences between the conditional physical and risk-neutral distributions. The results indicate substantial disparities primarily in the left-tail, reflecting the influence of disaster risk on the equity premium. These differences vary over time and persist beyond crisis periods. On average, the bottom 5% of returns contribute to 17% of the equity premium, shedding light on the Peso problem. I also find that disaster risk increases the stochastic discount factor's volatility. Using a lower bound observed from option prices on the left-tail difference between the physical and risk-neutral quantile functions, I obtain similar results, reinforcing the robustness of my findings.
http://w3id.org/mlsea/pwc/scientificWork/A%20Targeted%20Assessment%20of%20Incremental%20Processing%20in%20Neural%20LanguageModels%20and%20Humans                                                                                  A Targeted Assessment of Incremental Processing in Neural LanguageModels and Humans                                                                                  We present a targeted, scaled-up comparison of incremental processing in humans and neural language models by collecting by-word reaction time data for sixteen different syntactic test suites across a range of structural phenomena. Human reaction time data comes from a novel online experimental paradigm called the Interpolated Maze task. We compare human reaction times to by-word probabilities for four contemporary language models, with different architectures and trained on a range of data set sizes. We find that across many phenomena, both humans and language models show increased processing difficulty in ungrammatical sentence regions with human and model `accuracy' scores (a la Marvin and Linzen(2018)) about equal. However, although language model outputs match humans in direction, we show that models systematically under-predict the difference in magnitude of incremental processing difficulty between grammatical and ungrammatical sentences. Specifically, when models encounter syntactic violations they fail to accurately predict the longer reaction times observed in the human data. These results call into question whether contemporary language models are approaching human-like performance for sensitivity to syntactic violations.
http://w3id.org/mlsea/pwc/scientificWork/A%20Task-Motion%20Planning%20Framework%20Using%20Iteratively%20Deepened%20AND%2FOR%20Graph%20Networks                                                                                  A Task-Motion Planning Framework Using Iteratively Deepened AND/OR Graph Networks                                                                                  We present an approach for Task-Motion Planning (TMP) using Iterative Deepened AND/OR Graph Networks (TMP-IDAN) that uses an AND/OR graph network based novel abstraction for compactly representing the task-level states and actions. While retrieving a target object from clutter, the number of object re-arrangements required to grasp the target is not known ahead of time. To address this challenge, in contrast to traditional AND/OR graph-based planners, we grow the AND/OR graph online until the target grasp is feasible and thereby obtain a network of AND/OR graphs. The AND/OR graph network allows faster computations than traditional task planners. We validate our approach and evaluate its capabilities using a Baxter robot and a state-of-the-art robotics simulator in several challenging non-trivial cluttered table-top scenarios. The experiments show that our approach is readily scalable to increasing number of objects and different degrees of clutter.
http://w3id.org/mlsea/pwc/scientificWork/A%20Technical%20Critique%20of%20Some%20Parts%20of%20the%20Free%20Energy%20Principle                                                                                  A Technical Critique of Some Parts of the Free Energy Principle                                                                                  We summarize the original formulation of the free energy principle, and highlight some technical issues. We discuss how these issues affect related results involving generalised coordinates and, where appropriate, mention consequences for and reveal, up to now unacknowledged, differences to newer formulations of the free energy principle. In particular, we reveal that various definitions of the 'Markov blanket' proposed in different works are not equivalent. We show that crucial steps in the free energy argument which involve rewriting the equations of motion of systems with Markov blankets, are not generally correct without additional (previously unstated) assumptions. We prove by counterexample that the original free energy lemma, when taken at face value, is wrong. We show further that this free energy lemma, when it does hold, implies equality of variational density and ergodic conditional density. The interpretation in terms of Bayesian inference hinges on this point, and we hence conclude that it is not sufficiently justified. Additionally, we highlight that the variational densities presented in newer formulations of the free energy principle and lemma are parameterised by different variables than in older works, leading to a substantially different interpretation of the theory. Note that we only highlight some specific problems in the discussed publications. These problems do not rule out conclusively that the general ideas behind the free energy principle are worth pursuing.
http://w3id.org/mlsea/pwc/scientificWork/A%20Technical%20Question%20Answering%20System%20with%20Transfer%20Learning                                                                                  A Technical Question Answering System with Transfer Learning                                                                                  In recent years, the need for community technical question-answering sites has increased significantly. However, it is often expensive for human experts to provide timely and helpful responses on those forums. We develop TransTQA, which is a novel system that offers automatic responses by retrieving proper answers based on correctly answered similar questions in the past. TransTQA is built upon a siamese ALBERT network, which enables it to respond quickly and accurately. Furthermore, TransTQA adopts a standard deep transfer learning strategy to improve its capability of supporting multiple technical domains.
http://w3id.org/mlsea/pwc/scientificWork/A%20Template-guided%20Hybrid%20Pointer%20Network%20for%20Knowledge-basedTask-oriented%20Dialogue%20Systems                                                                                  A Template-guided Hybrid Pointer Network for Knowledge-basedTask-oriented Dialogue Systems                                                                                  Most existing neural network based task-oriented dialogue systems follow encoder-decoder paradigm, where the decoder purely depends on the source texts to generate a sequence of words, usually suffering from instability and poor readability. Inspired by the traditional template-based generation approaches, we propose a template-guided hybrid pointer network for the knowledge-based task-oriented dialogue system, which retrieves several potentially relevant answers from a pre-constructed domain-specific conversational repository as guidance answers, and incorporates the guidance answers into both the encoding and decoding processes. Specifically, we design a memory pointer network model with a gating mechanism to fully exploit the semantic correlation between the retrieved answers and the ground-truth response. We evaluate our model on four widely used task-oriented datasets, including one simulated and three manually created datasets. The experimental results demonstrate that the proposed model achieves significantly better performance than the state-of-the-art methods over different automatic evaluation metrics.
http://w3id.org/mlsea/pwc/scientificWork/A%20Text%20Extraction-Based%20Smart%20Knowledge%20Graph%20Composition%20for%20Integrating%20Lessons%20Learned%20during%20the%20Microchip%20Design                                                                                  A Text Extraction-Based Smart Knowledge Graph Composition for Integrating Lessons Learned during the Microchip Design                                                                                  The production of microchips is a complex and thus well documented process. Therefore, available textual data about the production can be overwhelming in terms of quantity. This affects the visibility and retrieval of a certain piece of information when it is most needed. In this paper, we propose a dynamic approach to interlink the information extracted from multisource production-relevant documents through the creation of a knowledge graph. This graph is constructed in order to support searchability and enhance user's access to large-scale production information. Text mining methods are firstly utilized to extract data from multiple documentation sources. Document relations are then mined and extracted for the composition of the knowledge graph. Graph search functionality is then supported with a recommendation use-case to enhance users' access to information that is related to the initial documents. The proposed approach is tailored to and tested on microchip design-relevant documents. It enhances the visibility and findability of previous design-failure-cases during the process of a new chip design.
http://w3id.org/mlsea/pwc/scientificWork/A%20Theoretical%20Analysis%20of%20Learning%20with%20Noisily%20Labeled%20Data                                                                                  A Theoretical Analysis of Learning with Noisily Labeled Data                                                                                  Noisy labels are very common in deep supervised learning. Although many studies tend to improve the robustness of deep training for noisy labels, rare works focus on theoretically explaining the training behaviors of learning with noisily labeled data, which is a fundamental principle in understanding its generalization. In this draft, we study its two phenomena, clean data first and phase transition, by explaining them from a theoretical viewpoint. Specifically, we first show that in the first epoch training, the examples with clean labels will be learned first. We then show that after the learning from clean data stage, continuously training model can achieve further improvement in testing error when the rate of corrupted class labels is smaller than a certain threshold; otherwise, extensively training could lead to an increasing testing error.
http://w3id.org/mlsea/pwc/scientificWork/A%20Theoretical-Empirical%20Approach%20to%20Estimating%20Sample%20Complexity%20of%20DNNs                                                                                  A Theoretical-Empirical Approach to Estimating Sample Complexity of DNNs                                                                                  This paper focuses on understanding how the generalization error scales with the amount of the training data for deep neural networks (DNNs). Existing techniques in statistical learning require computation of capacity measures, such as VC dimension, to provably bound this error. It is however unclear how to extend these measures to DNNs and therefore the existing analyses are applicable to simple neural networks, which are not used in practice, e.g., linear or shallow ones or otherwise multi-layer perceptrons. Moreover, many theoretical error bounds are not empirically verifiable. We derive estimates of the generalization error that hold for deep networks and do not rely on unattainable capacity measures. The enabling technique in our approach hinges on two major assumptions: i) the network achieves zero training error, ii) the probability of making an error on a test point is proportional to the distance between this point and its nearest training point in the feature space and at a certain maximal distance (that we call radius) it saturates. Based on these assumptions we estimate the generalization error of DNNs. The obtained estimate scales as O(1/( delta N^{1/d})), where N is the size of the training data and is parameterized by two quantities, the effective dimensionality of the data as perceived by the network (d) and the aforementioned radius ( delta), both of which we find empirically. We show that our estimates match with the experimentally obtained behavior of the error on multiple learning tasks using benchmark data-sets and realistic models. Estimating training data requirements is essential for deployment of safety critical applications such as autonomous driving etc. Furthermore, collecting and annotating training data requires a huge amount of financial, computational and human resources. Our empirical estimates will help to efficiently allocate resources.
http://w3id.org/mlsea/pwc/scientificWork/A%20Theory%20of%20%27Auction%20as%20a%20Search%27%20in%20speculative%20markets                                                                                  A Theory of 'Auction as a Search' in speculative markets                                                                                  The tatonnement process in high frequency order driven markets is modeled as a search by buyers for sellers and vice-versa. We propose a total order book model, comprising limit orders and latent orders, in the absence of a market maker. A zero intelligence approach of agents is employed using a diffusion-drift-reaction model, to explain the trading through continuous auctions (price and volume). The search (levy or brownian) for transaction price is the primary diffusion mechanism with other behavioural dynamics in the model inspired from foraging, chemotaxis and robotic search. Analytic and asymptotic analysis is provided for several scenarios and examples. Numerical simulation of the model extends our understanding of the relative performance between brownian, superdiffusive and ballistic search in the model.
http://w3id.org/mlsea/pwc/scientificWork/A%20Theory%20of%20Choice%20Bracketing%20under%20Risk                                                                                  A Theory of Choice Bracketing under Risk                                                                                  Aggregating risks from multiple sources can be complex and demanding, and decision makers usually adopt heuristics to simplify the evaluation process. This paper axiomatizes two closed related and yet different heuristics, narrow bracketing and correlation neglect, by relaxing the independence axiom in the expected utility theory. The flexibility of our framework allows for applications in various economic problems. First, our model can explain the experimental evidence of narrow bracketing over monetary gambles. Second, when one source represents background risk, we can accommodate Rabin (2000)'s critique and explain risk aversion over small gambles. Finally, when different sources represent consumptions in different periods, we unify three seemingly distinct models of time preferences and propose a novel model that simultaneously satisfies indifference to temporal resolution of uncertainty, separation of time and risk preferences, and recursivity in the domain of lotteries. As a direct application to macroeconomics and finance, we provide an alternative to Epstein and Zin (1989) which avoids the unreasonably high timing premium discussed in Epstein, Farhi, and Strzalecki (2014).
http://w3id.org/mlsea/pwc/scientificWork/A%20Theory%20of%20Language%20Learning                                                                                  A Theory of Language Learning                                                                                  A theory of language learning is described, which uses Bayesian induction of feature structures (scripts) and script functions. Each word sense in a language is mentally represented by an m-script, a script function which embodies all the syntax and semantics of the word. M-scripts form a fully-lexicalised unification grammar, which can support adult language. Each word m-script can be learnt robustly from about six learning examples. The theory has been implemented as a computer model, which can bootstrap-learn a language from zero vocabulary. The Bayesian learning mechanism is (1) Capable: to learn arbitrarily complex meanings and syntactic structures; (2) Fast: learning these structures from a few examples each; (3) Robust: learning in the presence of much irrelevant noise, and (4) Self-repairing: able to acquire implicit negative evidence, using it to learn exceptions. Children learning language are clearly all of (1) - (4), whereas connectionist theories fail on (1) and (2), and symbolic theories fail on (3) and (4). The theory is in good agreement with many key facts of language acquisition, including facts which are problematic for other theories. It is compared with over 100 key cross-linguistic findings about acquisition of the lexicon, phrase structure, morphology, complementation and control, auxiliaries, verb argument structures, gaps and movement - in nearly all cases giving unforced agreement without extra assumptions.
http://w3id.org/mlsea/pwc/scientificWork/A%20Theory%20of%20Neural%20Tangent%20Kernel%20Alignment%20and%20Its%20Influence%20on%20Training                                                                                  A Theory of Neural Tangent Kernel Alignment and Its Influence on Training                                                                                  The training dynamics and generalization properties of neural networks (NN) can be precisely characterized in function space via the neural tangent kernel (NTK). Structural changes to the NTK during training reflect feature learning and underlie the superior performance of networks outside of the static kernel regime. In this work, we seek to theoretically understand kernel alignment, a prominent and ubiquitous structural change that aligns the NTK with the target function. We first study a toy model of kernel evolution in which the NTK evolves to accelerate training and show that alignment naturally emerges from this demand. We then study alignment mechanism in deep linear networks and two layer ReLU networks. These theories provide good qualitative descriptions of kernel alignment and specialization in practical networks and identify factors in network architecture and data structure that drive kernel alignment. In nonlinear networks with multiple outputs, we identify the phenomenon of kernel specialization, where the kernel function for each output head preferentially aligns to its own target function. Together, our results provide a mechanistic explanation of how kernel alignment emerges during NN training and a normative explanation of how it benefits training.
http://w3id.org/mlsea/pwc/scientificWork/A%20Theory%20of%20Updating%20Ambiguous%20Information                                                                                  A Theory of Updating Ambiguous Information                                                                                  We introduce a new updating rule, the conditional maximum likelihood rule (CML) for updating ambiguous information. The CML formula replaces the likelihood term in Bayes' rule with the maximal likelihood of the given signal conditional on the state. We show that CML satisfies a new axiom, increased sensitivity after updating, while other updating rules do not. With CML, a decision maker's posterior is unaffected by the order in which independent signals arrive. CML also accommodates recent experimental findings on updating signals of unknown accuracy and has simple predictions on learning with such signals. We show that an information designer can almost achieve her maximal payoff with a suitable ambiguous information structure whenever the agent updates according to CML.
http://w3id.org/mlsea/pwc/scientificWork/A%20Theory%20of%20the%20Distortion-Perception%20Tradeoff%20in%20Wasserstein%20Space                                                                                  A Theory of the Distortion-Perception Tradeoff in Wasserstein Space                                                                                  The lower the distortion of an estimator, the more the distribution of its outputs generally deviates from the distribution of the signals it attempts to estimate. This phenomenon, known as the perception-distortion tradeoff, has captured significant attention in image restoration, where it implies that fidelity to ground truth images comes at the expense of perceptual quality (deviation from statistics of natural images). However, despite the increasing popularity of performing comparisons on the perception-distortion plane, there remains an important open question: what is the minimal distortion that can be achieved under a given perception constraint? In this paper, we derive a closed form expression for this distortion-perception (DP) function for the mean squared-error (MSE) distortion and the Wasserstein-2 perception index. We prove that the DP function is always quadratic, regardless of the underlying distribution. This stems from the fact that estimators on the DP curve form a geodesic in Wasserstein space. In the Gaussian setting, we further provide a closed form expression for such estimators. For general distributions, we show how these estimators can be constructed from the estimators at the two extremes of the tradeoff: The global MSE minimizer, and a minimizer of the MSE under a perfect perceptual quality constraint. The latter can be obtained as a stochastic transformation of the former.
http://w3id.org/mlsea/pwc/scientificWork/A%20Theory-Driven%20Self-Labeling%20Refinement%20Method%20for%20Contrastive%20Representation%20Learning                                                                                  A Theory-Driven Self-Labeling Refinement Method for Contrastive Representation Learning                                                                                  For an image query, unsupervised contrastive learning labels crops of the same image as positives, and other image crops as negatives. Although intuitive, such a native label assignment strategy cannot reveal the underlying semantic similarity between a query and its positives and negatives, and impairs performance, since some negatives are semantically similar to the query or even share the same semantic class as the query. In this work, we first prove that for contrastive learning, inaccurate label assignment heavily impairs its generalization for semantic instance discrimination, while accurate labels benefit its generalization. Inspired by this theory, we propose a novel self-labeling refinement approach for contrastive learning. It improves the label quality via two complementary modules: (i) self-labeling refinery (SLR) to generate accurate labels and (ii) momentum mixup (MM) to enhance similarity between query and its positive. SLR uses a positive of a query to estimate semantic similarity between a query and its positive and negatives, and combines estimated similarity with vanilla label assignment in contrastive learning to iteratively generate more accurate and informative soft labels. We theoretically show that our SLR can exactly recover the true semantic labels of label-corrupted data, and supervises networks to achieve zero prediction error on classification tasks. MM randomly combines queries and positives to increase semantic similarity between the generated virtual queries and their positives so as to improves label accuracy. Experimental results on CIFAR10, ImageNet, VOC and COCO show the effectiveness of our method. PyTorch code and model will be released online.
http://w3id.org/mlsea/pwc/scientificWork/A%20Thermodynamic%20based%20and%20Data%20Driven%20Hybrid%20Network%20for%20Gas%20Turbine%20Modeling                                                                                  A Thermodynamic based and Data Driven Hybrid Network for Gas Turbine Modeling                                                                                  The on-wing engine performance is difficult to track for thermodynamic models because of its inaccurate component maps, and also difficult for data driven methods for their over-fitting to measurement errors. So, we propose a thermodynamic based and data driven hybrid network for gas turbine modeling. Different from thermodynamic models, our network reconstructs the component characteristics in a data-driven way to take component degeneration and individual difference into consideration. Moreover, different from data driven methods, in the training phase, physical based equations and the analytical mathematical description are used to ensure that the optimization converges to the gas turbine's dynamics. A huge number of relaxed quasi steady state flight data to 26970 is used to train and test our hybrid network. The result shows that the accuracy of our hybrid network can reach about 7% measured by max T6 relative error, 5% better than map fitting based thermodynamic model and 8% better than pure data driven method with similar model volume.
http://w3id.org/mlsea/pwc/scientificWork/A%20Time-Temperature%20Dataset%20for%20the%20Strawberry%20Cold%20Chain%20Across%20Multiple%20Shipments%20and%20Locations                                                                                  A Time-Temperature Dataset for the Strawberry Cold Chain Across Multiple Shipments and Locations                                                                                  This article describes location aware temperature profiles from six strawberry shipments across the continental United States. Three pallets were instrumented in each shipment with three vertically placed loggers to take a longitudinal and latitudinal snapshot of 9 strategically different locations (including the top, middle and bottom layers of the pallets placed in the back, middle and the front of the shipping container) for a combined 54 measurement points across shipments of varying lengths. The sensors were instrumented in the field, right at the point of harvest, recorded temperatures every every 5 to 10 minutes depending on the shipment, and uploaded their data periodically via cellular radios on each device. The data is a result of significant collaboration between stakeholders from farmers to distributors to retailers to academics, which can play an important role for researchers and educators in food engineering, cold-chain, machine learning, and data mining, as well as in other disciplines related to food and transportation.
http://w3id.org/mlsea/pwc/scientificWork/A%20Token-level%20Reference-free%20Hallucination%20Detection%20Benchmark%20for%20Free-form%20Text%20Generation                                                                                  A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation                                                                                  Large pretrained generative models like GPT-3 often suffer from hallucinating non-existent or incorrect content, which undermines their potential merits in real applications. Existing work usually attempts to detect these hallucinations based on a corresponding oracle reference at a sentence or document level. However ground-truth references may not be readily available for many free-form text generation applications, and sentence- or document-level detection may fail to provide the fine-grained signals that would prevent fallacious content in real time. As a first step to addressing these issues, we propose a novel token-level, reference-free hallucination detection task and an associated annotated dataset named HaDes (HAllucination DEtection dataSet). To create this dataset, we first perturb a large number of text segments extracted from English language Wikipedia, and then verify these with crowd-sourced annotations. To mitigate label imbalance during annotation, we utilize an iterative model-in-loop strategy. We conduct comprehensive data analyses and create multiple baseline models.
http://w3id.org/mlsea/pwc/scientificWork/A%20Toolbox%20for%20Construction%20and%20Analysis%20of%20Speech%20Datasets                                                                                  A Toolbox for Construction and Analysis of Speech Datasets                                                                                  Automatic Speech Recognition and Text-to-Speech systems are primarily trained in a supervised fashion and require high-quality, accurately labeled speech datasets. In this work, we examine common problems with speech data and introduce a toolbox for the construction and interactive error analysis of speech datasets. The construction tool is based on K 'urzinger et al. work, and, to the best of our knowledge, the dataset exploration tool is the world's first open-source tool of this kind. We demonstrate how to apply these tools to create a Russian speech dataset and analyze existing speech datasets (Multilingual LibriSpeech, Mozilla Common Voice). The tools are open sourced as a part of the NeMo framework.
http://w3id.org/mlsea/pwc/scientificWork/A%20Topological%20Approach%20to%20Compare%20Document%20Semantics%20Based%20on%20a%20New%20Variant%20of%20Syntactic%20N-grams                                                                                  A Topological Approach to Compare Document Semantics Based on a New Variant of Syntactic N-grams                                                                                  This paper delivers a new perspective of thinking and utilizing syntactic n-grams (sn-grams). Sn-grams are a type of non-linear n-grams which have been playing a critical role in many NLP tasks. Introducing sn-grams to comparing document semantics thus is an appealing application, and few studies have reported progress at this. However, when proceeding on this application, we found three major issues of sn-grams: lack of significance, being sensitive to word orders and failing on capture indirect syntactic relations. To address these issues, we propose a new variant of sn-grams named generalized phrases (GPs). Then based on GPs we propose a topological approach, named DSCoH, to compute document semantic similarities. DSCoH has been extensively tested on the document semantics comparison and the document clustering tasks. The experimental results show that DSCoH can outperform state-of-the-art embedding-based methods.
http://w3id.org/mlsea/pwc/scientificWork/A%20Topology-Switching%20Coalitional%20Control%20and%20Observation%20Scheme%20with%20Stability%20Guarantees                                                                                  A Topology-Switching Coalitional Control and Observation Scheme with Stability Guarantees                                                                                  In this paper a coalitional control and observation scheme is presented in which the coalitions are changed online by enabling and disabling communication links. Transitions between coalitions are made to best balance overall system performance and communication costs. Linear Matrix Inequalities are used to design the controller and observer, guaranteeing stability of the switching system. Simulation results for vehicle platoon control are presented to illustrate the proposed method.
http://w3id.org/mlsea/pwc/scientificWork/A%20Track-Before-Detect%20Approach%20to%20Multi-Target%20Tracking%20on%20Automotive%20Radar%20Sensor%20Data                                                                                  A Track-Before-Detect Approach to Multi-Target Tracking on Automotive Radar Sensor Data                                                                                  In recent years, Bayes filter methods in the labeled random finite set formulation have become increasingly powerful in the multi-target tracking domain. One of the latest outcomes is the Generalized Labeled Multi-Bernoulli (GLMB) filter which allows for stable cardinality and target state estimation as well as target identification in a unified framework. In contrast to the initial context of the GLMB filter, this paper makes use of it in the Track-Before-Detect (TBD) scheme and thus, avoids information loss due to thresholding and other data preprocessing steps. This paper provides a TBD GLMB filter design under the separable likelihood assumption that can be applied to real world scenarios and data in the automotive radar context. Its applicability to real sensor data is demonstrated in an exemplary scenario. To the best of the authors' knowledge, the GLMB filter is applied to real radar data in a TBD framework for the first time.
http://w3id.org/mlsea/pwc/scientificWork/A%20Transactive%20Energy%20Market%20Framework%20Considering%20Network%20Constraints%20and%20Fairness                                                                                  A Transactive Energy Market Framework Considering Network Constraints and Fairness                                                                                  The continuous penetration of distributed energy resources (DER) in the electric power grid is driving a new paradigm shift towards transactive energy system (TES), an active and more sustainable system characterized by distributed generation and energy exchanges among consumers and producers in the network. This transition, however, comes with challenges such as dealing with the nonlinear and non-convex power flows of the system, determining an optimal transaction price to maximize overall system welfare, and ensuring fairness for all participants. In this paper, we propose a three-stage transactive energy framework that aims to address these challenges. In the first stage, the cost without trading is calculated which will serve as the reference in the profit maximization problem in the next stage. DER dispatch, power flows and initial transaction payments/incentives of the participants will then be determined in the second stage. A benefit allocation algorithm is applied in the third control stage to determine the optimal transaction price and final payments/incentives that will ensure fairness for trading participants. The proposed framework was tested in an IEEE 33-bus system and results show that fair benefits are given for all participants during trading and the system operates within the network and economic constraints.
http://w3id.org/mlsea/pwc/scientificWork/A%20Transductive%20Multi-Head%20Model%20for%20Cross-Domain%20Few-Shot%20Learning                                                                                  A Transductive Multi-Head Model for Cross-Domain Few-Shot Learning                                                                                  In this paper, we present a new method, Transductive Multi-Head Few-Shot learning (TMHFS), to address the Cross-Domain Few-Shot Learning (CD-FSL) challenge. The TMHFS method extends the Meta-Confidence Transduction (MCT) and Dense Feature-Matching Networks (DFMN) method [2] by introducing a new prediction head, i.e, an instance-wise global classification network based on semantic information, after the common feature embedding network. We train the embedding network with the multiple heads, i.e,, the MCT loss, the DFMN loss and the semantic classifier loss, simultaneously in the source domain. For the few-shot learning in the target domain, we first perform fine-tuning on the embedding network with only the semantic global classifier and the support instances, and then use the MCT part to predict labels of the query set with the fine-tuned embedding network. Moreover, we further exploit data augmentation techniques during the fine-tuning and test stages to improve the prediction performance. The experimental results demonstrate that the proposed methods greatly outperform the strong baseline, fine-tuning, on four different target domains.
http://w3id.org/mlsea/pwc/scientificWork/A%20Transformer-based%20Cross-modal%20Fusion%20Model%20with%20Adversarial%20Training%20for%20VQA%20Challenge%202021                                                                                  A Transformer-based Cross-modal Fusion Model with Adversarial Training for VQA Challenge 2021                                                                                  In this paper, inspired by the successes of visionlanguage pre-trained models and the benefits from training with adversarial attacks, we present a novel transformerbased cross-modal fusion modeling by incorporating the both notions for VQA challenge 2021. Specifically, the proposed model is on top of the architecture of VinVL model [19], and the adversarial training strategy [4] is applied to make the model robust and generalized. Moreover, two implementation tricks are also used in our system to obtain better results. The experiments demonstrate that the novel framework can achieve 76.72% on VQAv2 test-std set.
http://w3id.org/mlsea/pwc/scientificWork/A%20Tunable%20Model%20for%20Graph%20Generation%20Using%20LSTM%20and%20Conditional%20VAE                                                                                  A Tunable Model for Graph Generation Using LSTM and Conditional VAE                                                                                  With the development of graph applications, generative models for graphs have been more crucial. Classically, stochastic models that generate graphs with a pre-defined probability of edges and nodes have been studied. Recently, some models that reproduce the structural features of graphs by learning from actual graph data using machine learning have been studied. However, in these conventional studies based on machine learning, structural features of graphs can be learned from data, but it is not possible to tune features and generate graphs with specific features. In this paper, we propose a generative model that can tune specific features, while learning structural features of a graph from data. With a dataset of graphs with various features generated by a stochastic model, we confirm that our model can generate a graph with specific features.
http://w3id.org/mlsea/pwc/scientificWork/A%20Turing%20Test%20for%20Transparency                                                                                  A Turing Test for Transparency                                                                                  A central goal of explainable artificial intelligence (XAI) is to improve the trust relationship in human-AI interaction. One assumption underlying research in transparent AI systems is that explanations help to better assess predictions of machine learning (ML) models, for instance by enabling humans to identify wrong predictions more efficiently. Recent empirical evidence however shows that explanations can have the opposite effect: When presenting explanations of ML predictions humans often tend to trust ML predictions even when these are wrong. Experimental evidence suggests that this effect can be attributed to how intuitive, or human, an AI or explanation appears. This effect challenges the very goal of XAI and implies that responsible usage of transparent AI methods has to consider the ability of humans to distinguish machine generated from human explanations. Here we propose a quantitative metric for XAI methods based on Turing's imitation game, a Turing Test for Transparency. A human interrogator is asked to judge whether an explanation was generated by a human or by an XAI method. Explanations of XAI methods that can not be detected by humans above chance performance in this binary classification task are passing the test. Detecting such explanations is a requirement for assessing and calibrating the trust relationship in human-AI interaction. We present experimental results on a crowd-sourced text classification task demonstrating that even for basic ML models and XAI approaches most participants were not able to differentiate human from machine generated explanations. We discuss ethical and practical implications of our results for applications of transparent ML.
http://w3id.org/mlsea/pwc/scientificWork/A%20Tutorial%20on%205G%20NR%20V2X%20Communications                                                                                  A Tutorial on 5G NR V2X Communications                                                                                  The Third Generation Partnership Project (3GPP) has recently published its Release 16 that includes the first Vehicle to-Everything (V2X) standard based on the 5G New Radio (NR) air interface. 5G NR V2X introduces advanced functionalities on top of the 5G NR air interface to support connected and automated driving use cases with stringent requirements. This paper presents an in-depth tutorial of the 3GPP Release 16 5G NR V2X standard for V2X communications, with a particular focus on the sidelink, since it is the most significant part of 5G NR V2X. The main part of the paper is an in-depth treatment of the key aspects of 5G NR V2X: the physical layer, the resource allocation, the quality of service management, the enhancements introduced to the Uu interface and the mobility management for V2N (Vehicle to Network) communications, as well as the co-existence mechanisms between 5G NR V2X and LTE V2X. We also review the use cases, the system architecture, and describe the evaluation methodology and simulation assumptions for 5G NR V2X. Finally, we provide an outlook on possible 5G NR V2X enhancements, including those identified within Release 17.
http://w3id.org/mlsea/pwc/scientificWork/A%20Tutorial%20on%20Evaluation%20Metrics%20used%20in%20Natural%20Language%20Generation                                                                                  A Tutorial on Evaluation Metrics used in Natural Language Generation                                                                                  The advent of Deep Learning and the availability of large scale datasets has accelerated research on Natural Language Generation with a focus on newer tasks and better models. With such rapid progress, it is vital to assess the extent of scientific progress made and identify the areas/components that need improvement. To accomplish this in an automatic and reliable manner, the NLP community has actively pursued the development of automatic evaluation metrics. Especially in the last few years, there has been an increasing focus on evaluation metrics, with several criticisms of existing metrics and proposals for several new metrics. This tutorial presents the evolution of automatic evaluation metrics to their current state along with the emerging trends in this field by specifically addressing the following questions: (i) What makes NLG evaluation challenging? (ii) Why do we need automatic evaluation metrics? (iii) What are the existing automatic evaluation metrics and how can they be organised in a coherent taxonomy? (iv) What are the criticisms and shortcomings of existing metrics? (v) What are the possible future directions of research?
http://w3id.org/mlsea/pwc/scientificWork/A%20Twin%20Neural%20Model%20for%20Uplift                                                                                  A Twin Neural Model for Uplift                                                                                  Uplift is a particular case of conditional treatment effect modeling. Such models deal with cause-and-effect inference for a specific factor, such as a marketing intervention or a medical treatment. In practice, these models are built on individual data from randomized clinical trials where the goal is to partition the participants into heterogeneous groups depending on the uplift. Most existing approaches are adaptations of random forests for the uplift case. Several split criteria have been proposed in the literature, all relying on maximizing heterogeneity. However, in practice, these approaches are prone to overfitting. In this work, we bring a new vision to uplift modeling. We propose a new loss function defined by leveraging a connection with the Bayesian interpretation of the relative risk. Our solution is developed for a specific twin neural network architecture allowing to jointly optimize the marginal probabilities of success for treated and control individuals. We show that this model is a generalization of the uplift logistic interaction model. We modify the stochastic gradient descent algorithm to allow for structured sparse solutions. This helps training our uplift models to a great extent. We show our proposed method is competitive with the state-of-the-art in simulation setting and on real data from large scale randomized experiments.
http://w3id.org/mlsea/pwc/scientificWork/A%20Two-Level%20Simulation-Assisted%20Sequential%20Distribution%20System%20Restoration%20Model%20With%20Frequency%20Dynamics%20Constraints                                                                                  A Two-Level Simulation-Assisted Sequential Distribution System Restoration Model With Frequency Dynamics Constraints                                                                                  This paper proposes a service restoration model for unbalanced distribution systems and inverter-dominated microgrids (MGs), in which frequency dynamics constraints are developed to optimize the amount of load restoration and guarantee the dynamic performance of system frequency response during the restoration process. After extreme events, the damaged distribution systems can be sectionalized into several isolated MGs to restore critical loads and tripped non-black start distributed generations (DGs) by black start DGs. However, the high penetration of inverter-based DGs reduces the system inertia, which results in low-inertia issues and large frequency fluctuation during the restoration process. To address this challenge, we propose a two-level simulation-assisted sequential service restoration model, which includes a mixed integer linear programming (MILP)-based optimization model and a transient simulation model. The proposed MILP model explicitly incorporates the frequency response into constraints, by interfacing with transient simulation of inverter-dominated MGs. Numerical results on a modified IEEE 123-bus system have validated that the frequency dynamic performance of the proposed service restoration model are indeed improved.
http://w3id.org/mlsea/pwc/scientificWork/A%20Two-Population%20Mortality%20Model%20to%20Assess%20Longevity%20Basis%20Risk                                                                                  A Two-Population Mortality Model to Assess Longevity Basis Risk                                                                                  Index-based hedging solutions are used to transfer the longevity risk to the capital markets. However, mismatches between the liability of the hedger and the hedging instrument cause longevity basis risk. Therefore, an appropriate two-population model to measure and assess the longevity basis risk is required. In this paper, we aim to construct a two-population mortality model to provide an effective hedge against the longevity basis risk. The reference population is modelled by using the Lee-Carter model with the renewal process and exponential jumps proposed by 'Ozen and c{S}ahin (2020) and the dynamics of the book population are specified. The analysis based on the UK mortality data indicates that the proposed model for the reference population and the common age effect model for the book population provide a better fit compared to the other models considered in the paper. Different two-population models are used to investigate the impact of the sampling risk on the index-based hedge as well as to analyse the risk reduction regarding hedge effectiveness. The results show that the proposed model provides a significant risk reduction when mortality jumps and the sampling risk are taken into account.
http://w3id.org/mlsea/pwc/scientificWork/A%20Two-Stage%20Attentive%20Network%20for%20Single%20Image%20Super-Resolution                                                                                  A Two-Stage Attentive Network for Single Image Super-Resolution                                                                                  Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-resolution (SISR) and contribute remarkable progress. However, most of the existing CNNs-based SISR methods do not adequately explore contextual information in the feature extraction stage and pay little attention to the final high-resolution (HR) image reconstruction step, hence hindering the desired SR performance. To address the above two issues, in this paper, we propose a two-stage attentive network (TSAN) for accurate SISR in a coarse-to-fine manner. Specifically, we design a novel multi-context attentive block (MCAB) to make the network focus on more informative contextual features. Moreover, we present an essential refined attention block (RAB) which could explore useful cues in HR space for reconstructing fine-detailed HR image. Extensive evaluations on four benchmark datasets demonstrate the efficacy of our proposed TSAN in terms of quantitative metrics and visual effects. Code is available at https://github.com/Jee-King/TSAN.
http://w3id.org/mlsea/pwc/scientificWork/A%20Two-Stage%20Coordinative%20Zonal%20Volt%2FVAR%20Control%20Scheme%20for%20Distribution%20Systems%20with%20High%20Inverter-based%20Resources                                                                                  A Two-Stage Coordinative Zonal Volt/VAR Control Scheme for Distribution Systems with High Inverter-based Resources                                                                                  This paper presents a two-stage zonal Volt/VAR control scheme for coordinating inverter-based resources (IBR) with utility-owned voltage regulators (VR) to regulate voltage in unbalanced 3-phase distribution systems. First, correlations between nodal voltages are derived from nodal voltage sensitivity studies. Then, the feeder is partitioned into non-overlapping, weakly-coupled voltage control zones based on nodal voltage correlations. IBR are used in the first stage to regulate voltage changes continuously and VR are used in the second stage to regulate large voltage deviations. An online VR voltage setpoint tuning strategy is developed to reduce excessive tap changes and avoid large voltage fluctuations without retrofitting existing VR controllers. In addition, the proposed algorithm uses real-time voltage measurements only from the critical nodes (typically less than 4% of total nodes) to reduce the sensing and communication needs. Actual distribution feeder topologies and load and PV time-series data are used to verify the performance of the algorithm. Because the method is a rule-based approach, it runs extremely fast, requires fewer measurements, and requires no retrofit to the existing VR control mechanisms. Simulation results show that the performance of the proposed method in terms of voltage control results and average numbers of VR tap changes are satisfactory.
http://w3id.org/mlsea/pwc/scientificWork/A%20Two-Stage%20Variable%20Selection%20Approach%20for%20Correlated%20High%20Dimensional%20Predictors                                                                                  A Two-Stage Variable Selection Approach for Correlated High Dimensional Predictors                                                                                  When fitting statistical models, some predictors are often found to be correlated with each other, and functioning together. Many group variable selection methods are developed to select the groups of predictors that are closely related to the continuous or categorical response. These existing methods usually assume the group structures are well known. For example, variables with similar practical meaning, or dummy variables created by categorical data. However, in practice, it is impractical to know the exact group structure, especially when the variable dimensional is large. As a result, the group variable selection results may be selected. To solve the challenge, we propose a two-stage approach that combines a variable clustering stage and a group variable stage for the group variable selection problem. The variable clustering stage uses information from the data to find a group structure, which improves the performance of the existing group variable selection methods. For ultrahigh dimensional data, where the predictors are much larger than observations, we incorporated a variable screening method in the first stage and shows the advantages of such an approach. In this article, we compared and discussed the performance of four existing group variable selection methods under different simulation models, with and without the variable clustering stage. The two-stage method shows a better performance, in terms of the prediction accuracy, as well as in the accuracy to select active predictors. An athlete's data is also used to show the advantages of the proposed method.
http://w3id.org/mlsea/pwc/scientificWork/A%20Two-Stage%20Wavelet%20Decomposition%20Method%20for%20Instantaneous%20Power%20Quality%20Indices%20Estimation%20Considering%20Interharmonics%20and%20Transient%20Disturbances                                                                                  A Two-Stage Wavelet Decomposition Method for Instantaneous Power Quality Indices Estimation Considering Interharmonics and Transient Disturbances                                                                                  As the complexity increases in modern power systems, power quality analysis considering interharmonics has become a challenging and important task. This paper proposes a novel decomposition and estimation method for instantaneous power quality indices (PQIs) monitoring in single-phase and three-phase systems with interharmonics and transient disturbances. To separate the interharmonic components, a set of new scaling filter and wavelet filter with narrow transition bands are designed for the undecimated wavelet packet transform (UWPT). Further, a two-stage decomposition method for multi-tone voltage and current signals is proposed. The Hilbert transform (HT) is applied to calculate the instantaneous amplitude and phase of each frequency component, which accordingly allows the monitoring of different PQI parameters. Numerical tests are conducted to check the performance of the proposed method. The test results show that compared to other conventional approaches, instantaneous PQIs estimated by the proposed method present significant advances for tracking transitory changes in power systems, and could be considered as a helpful tool for high-accuracy PQ detections.
http://w3id.org/mlsea/pwc/scientificWork/A%20Two-Step%20Framework%20for%20Arbitrage-Free%20Prediction%20of%20the%20Implied%20Volatility%20Surface                                                                                  A Two-Step Framework for Arbitrage-Free Prediction of the Implied Volatility Surface                                                                                  We propose a two-step framework for predicting the implied volatility surface over time without static arbitrage. In the first step, we select features to represent the surface and predict them over time. In the second step, we use the predicted features to construct the implied volatility surface using a deep neural network (DNN) model by incorporating constraints that prevent static arbitrage. We consider three methods to extract features from the implied volatility data: principal component analysis, variational autoencoder and sampling the surface, and we predict these features using LSTM. Using a long time series of implied volatility data for S &P500 index options to train our models, we find two feature construction methods, sampling the surface and variational autoencoders combined with DNN for surface construction, are the best performers in out-of-sample prediction. In particular, they outperform a classical method substantially. Furthermore, the DNN model for surface construction not only removes static arbitrage, but also significantly reduces the prediction error compared with a standard interpolation method. Our framework can also be used to simulate the dynamics of the implied volatility surface without static arbitrage.
http://w3id.org/mlsea/pwc/scientificWork/A%20Two-branch%20Neural%20Network%20for%20Non-homogeneous%20Dehazing%20via%20Ensemble%20Learning                                                                                  A Two-branch Neural Network for Non-homogeneous Dehazing via Ensemble Learning                                                                                  Recently, there has been rapid and significant progress on image dehazing. Many deep learning based methods have shown their superb performance in handling homogeneous dehazing problems. However, we observe that even if a carefully designed convolutional neural network (CNN) can perform well on large-scaled dehazing benchmarks, the network usually fails on the non-homogeneous dehazing datasets introduced by NTIRE challenges. The reasons are mainly in two folds. Firstly, due to its non-homogeneous nature, the non-uniformly distributed haze is harder to be removed than the homogeneous haze. Secondly, the research challenge only provides limited data (there are only 25 training pairs in NH-Haze 2021 dataset). Thus, learning the mapping from the domain of hazy images to that of clear ones based on very limited data is extremely hard. To this end, we propose a simple but effective approach for non-homogeneous dehazing via ensemble learning. To be specific, we introduce a two-branch neural network to separately deal with the aforementioned problems and then map their distinct features by a learnable fusion tail. We show extensive experimental results to illustrate the effectiveness of our proposed method.
http://w3id.org/mlsea/pwc/scientificWork/A%20Two-stage%20Deep%20Network%20for%20High%20Dynamic%20Range%20Image%20Reconstruction                                                                                  A Two-stage Deep Network for High Dynamic Range Image Reconstruction                                                                                  Mapping a single exposure low dynamic range (LDR) image into a high dynamic range (HDR) is considered among the most strenuous image to image translation tasks due to exposure-related missing information. This study tackles the challenges of single-shot LDR to HDR mapping by proposing a novel two-stage deep network. Notably, our proposed method aims to reconstruct an HDR image without knowing hardware information, including camera response function (CRF) and exposure settings. Therefore, we aim to perform image enhancement task like denoising, exposure correction, etc., in the first stage. Additionally, the second stage of our deep network learns tone mapping and bit-expansion from a convex set of data samples. The qualitative and quantitative comparisons demonstrate that the proposed method can outperform the existing LDR to HDR works with a marginal difference. Apart from that, we collected an LDR image dataset incorporating different camera systems. The evaluation with our collected real-world LDR images illustrates that the proposed method can reconstruct plausible HDR images without presenting any visual artefacts. Code available: https://github. com/sharif-apu/twostageHDR_NTIRE21.
http://w3id.org/mlsea/pwc/scientificWork/A%20Two-stage%20Multi-modal%20Affect%20Analysis%20Framework%20for%20Children%20with%20Autism%20Spectrum%20Disorder                                                                                  A Two-stage Multi-modal Affect Analysis Framework for Children with Autism Spectrum Disorder                                                                                  Autism spectrum disorder (ASD) is a developmental disorder that influences the communication and social behavior of a person in a way that those in the spectrum have difficulty in perceiving other people's facial expressions, as well as presenting and communicating emotions and affect via their own faces and bodies. Some efforts have been made to predict and improve children with ASD's affect states in play therapy, a common method to improve children's social skills via play and games. However, many previous works only used pre-trained models on benchmark emotion datasets and failed to consider the distinction in emotion between typically developing children and children with autism. In this paper, we present an open-source two-stage multi-modal approach leveraging acoustic and visual cues to predict three main affect states of children with ASD's affect states (positive, negative, and neutral) in real-world play therapy scenarios, and achieved an overall accuracy of 72:40%. This work presents a novel way to combine human expertise and machine intelligence for ASD affect recognition by proposing a two-stage schema.
http://w3id.org/mlsea/pwc/scientificWork/A%20Typology%20of%20Data%20Anomalies                                                                                  A Typology of Data Anomalies                                                                                  Anomalies are cases that are in some way unusual and do not appear to fit the general patterns present in the dataset. Several conceptualizations exist to distinguish between different types of anomalies. However, these are either too specific to be generally applicable or so abstract that they neither provide concrete insight into the nature of anomaly types nor facilitate the functional evaluation of anomaly detection algorithms. With the recent criticism on 'black box' algorithms and analytics it has become clear that this is an undesirable situation. This paper therefore introduces a general typology of anomalies that offers a clear and tangible definition of the different types of anomalies in datasets. The typology also facilitates the evaluation of the functional capabilities of anomaly detection algorithms and as a framework assists in analyzing the conceptual levels of data, patterns and anomalies. Finally, it serves as an analytical tool for studying anomaly types from other typologies.
http://w3id.org/mlsea/pwc/scientificWork/A%20Unified%20Approach%20to%20Fair%20Online%20Learning%20via%20Blackwell%20Approachability                                                                                  A Unified Approach to Fair Online Learning via Blackwell Approachability                                                                                  We provide a setting and a general approach to fair online learning with stochastic sensitive and non-sensitive contexts. The setting is a repeated game between the Player and Nature, where at each stage both pick actions based on the contexts. Inspired by the notion of unawareness, we assume that the Player can only access the non-sensitive context before making a decision, while we discuss both cases of Nature accessing the sensitive contexts and Nature unaware of the sensitive contexts. Adapting Blackwell's approachability theory to handle the case of an unknown contexts' distribution, we provide a general necessary and sufficient condition for learning objectives to be compatible with some fairness constraints. This condition is instantiated on (group-wise) no-regret and (group-wise) calibration objectives, and on demographic parity as an additional constraint. When the objective is not compatible with the constraint, the provided framework permits to characterise the optimal trade-off between the two.
http://w3id.org/mlsea/pwc/scientificWork/A%20Unified%20Cognitive%20Learning%20Framework%20for%20Adapting%20to%20Dynamic%20Environment%20and%20Tasks                                                                                  A Unified Cognitive Learning Framework for Adapting to Dynamic Environment and Tasks                                                                                  Many machine learning frameworks have been proposed and used in wireless communications for realizing diverse goals. However, their incapability of adapting to the dynamic wireless environment and tasks and of self-learning limit their extensive applications and achievable performance. Inspired by the great flexibility and adaptation of primate behaviors due to the brain cognitive mechanism, a unified cognitive learning (CL) framework is proposed for the dynamic wireless environment and tasks. The mathematical framework for our proposed CL is established. Using the public and authoritative dataset, we demonstrate that our proposed CL framework has three advantages, namely, the capability of adapting to the dynamic environment and tasks, the self-learning capability and the capability of 'good money driving out bad money' by taking modulation recognition as an example. The proposed CL framework can enrich the current learning frameworks and widen the applications.
http://w3id.org/mlsea/pwc/scientificWork/A%20Unified%20Framework%20for%20Constructing%20Nonconvex%20Regularizations                                                                                  A Unified Framework for Constructing Nonconvex Regularizations                                                                                  Over the past decades, many individual nonconvex methods have been proposed to achieve better sparse recovery performance in various scenarios. However, how to construct a valid nonconvex regularization function remains open in practice. In this paper, we fill in this gap by presenting a unified framework for constructing the nonconvex regularization based on the probability density function. Meanwhile, a new nonconvex sparse recovery method constructed via the Weibull distribution is studied.
http://w3id.org/mlsea/pwc/scientificWork/A%20Unified%20Framework%20for%20Multilingual%20and%20Code-Mixed%20Visual%20Question%20Answering                                                                                  A Unified Framework for Multilingual and Code-Mixed Visual Question Answering                                                                                  In this paper, we propose an effective deep learning framework for multilingual and code- mixed visual question answering. The pro- posed model is capable of predicting answers from the questions in Hindi, English or Code- mixed (Hinglish: Hindi-English) languages. The majority of the existing techniques on Vi- sual Question Answering (VQA) focus on En- glish questions only. However, many applica- tions such as medical imaging, tourism, visual assistants require a multilinguality-enabled module for their widespread usages. As there is no available dataset in English-Hindi VQA, we firstly create Hindi and Code-mixed VQA datasets by exploiting the linguistic properties of these languages. We propose a robust tech- nique capable of handling the multilingual and code-mixed question to provide the answer against the visual information (image). To better encode the multilingual and code-mixed questions, we introduce a hierarchy of shared layers. We control the behaviour of these shared layers by an attention-based soft layer sharing mechanism, which learns how shared layers are applied in different ways for the dif- ferent languages of the question. Further, our model uses bi-linear attention with a residual connection to fuse the language and image fea- tures. We perform extensive evaluation and ablation studies for English, Hindi and Code- mixed VQA. The evaluation shows that the proposed multilingual model achieves state-of- the-art performance in all these settings.
http://w3id.org/mlsea/pwc/scientificWork/A%20Unified%20Framework%20for%20Specification%20Tests%20of%20Continuous%20Treatment%20Effect%20Models                                                                                  A Unified Framework for Specification Tests of Continuous Treatment Effect Models                                                                                  We propose a general framework for the specification testing of continuous treatment effect models. We assume a general residual function, which includes the average and quantile treatment effect models as special cases. The null models are identified under the unconfoundedness condition and contain a nonparametric weighting function. We propose a test statistic for the null model in which the weighting function is estimated by solving an expanding set of moment equations. We establish the asymptotic distributions of our test statistic under the null hypothesis and under fixed and local alternatives. The proposed test statistic is shown to be more efficient than that constructed from the true weighting function and can detect local alternatives deviated from the null models at the rate of $O(N^{-1/2})$. A simulation method is provided to approximate the null distribution of the test statistic. Monte-Carlo simulations show that our test exhibits a satisfactory finite-sample performance, and an application shows its practical value.
http://w3id.org/mlsea/pwc/scientificWork/A%20Unified%20Framework%20for%20Task-Driven%20Data%20Quality%20Management                                                                                  A Unified Framework for Task-Driven Data Quality Management                                                                                  High-quality data is critical to train performant Machine Learning (ML) models, highlighting the importance of Data Quality Management (DQM). Existing DQM schemes often cannot satisfactorily improve ML performance because, by design, they are oblivious to downstream ML tasks. Besides, they cannot handle various data quality issues (especially those caused by adversarial attacks) and have limited applications to only certain types of ML models. Recently, data valuation approaches (e.g., based on the Shapley value) have been leveraged to perform DQM; yet, empirical studies have observed that their performance varies considerably based on the underlying data and training process. In this paper, we propose a task-driven, multi-purpose, model-agnostic DQM framework, DataSifter, which is optimized towards a given downstream ML task, capable of effectively removing data points with various defects, and applicable to diverse models. Specifically, we formulate DQM as an optimization problem and devise a scalable algorithm to solve it. Furthermore, we propose a theoretical framework for comparing the worst-case performance of different DQM strategies. Remarkably, our results show that the popular strategy based on the Shapley value may end up choosing the worst data subset in certain practical scenarios. Our evaluation shows that DataSifter achieves and most often significantly improves the state-of-the-art performance over a wide range of DQM tasks, including backdoor, poison, noisy/mislabel data detection, data summarization, and data debiasing.
http://w3id.org/mlsea/pwc/scientificWork/A%20Unified%20Generative%20Framework%20for%20Aspect-Based%20Sentiment%20Analysis                                                                                  A Unified Generative Framework for Aspect-Based Sentiment Analysis                                                                                  Aspect-based Sentiment Analysis (ABSA) aims to identify the aspect terms, their corresponding sentiment polarities, and the opinion terms. There exist seven subtasks in ABSA. Most studies only focus on the subsets of these subtasks, which leads to various complicated ABSA models while hard to solve these subtasks in a unified framework. In this paper, we redefine every subtask target as a sequence mixed by pointer indexes and sentiment class indexes, which converts all ABSA subtasks into a unified generative formulation. Based on the unified formulation, we exploit the pre-training sequence-to-sequence model BART to solve all ABSA subtasks in an end-to-end framework. Extensive experiments on four ABSA datasets for seven subtasks demonstrate that our framework achieves substantial performance gain and provides a real unified end-to-end solution for the whole ABSA subtasks, which could benefit multiple tasks.
http://w3id.org/mlsea/pwc/scientificWork/A%20Unified%20Generative%20Framework%20for%20Various%20NER%20Subtasks                                                                                  A Unified Generative Framework for Various NER Subtasks                                                                                  Named Entity Recognition (NER) is the task of identifying spans that represent entities in sentences. Whether the entity spans are nested or discontinuous, the NER task can be categorized into the flat NER, nested NER, and discontinuous NER subtasks. These subtasks have been mainly solved by the token-level sequence labelling or span-level classification. However, these solutions can hardly tackle the three kinds of NER subtasks concurrently. To that end, we propose to formulate the NER subtasks as an entity span sequence generation task, which can be solved by a unified sequence-to-sequence (Seq2Seq) framework. Based on our unified framework, we can leverage the pre-trained Seq2Seq model to solve all three kinds of NER subtasks without the special design of the tagging schema or ways to enumerate spans. We exploit three types of entity representations to linearize entities into a sequence. Our proposed framework is easy-to-implement and achieves state-of-the-art (SoTA) or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets.

1000 Rows. -- 3012 msec.
