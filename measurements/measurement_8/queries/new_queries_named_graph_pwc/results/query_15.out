Result of /data/leuven/370/vsc37064/new_queries_named_graph_pwc/query_15.txt:
OpenLink Virtuoso Interactive SQL (Virtuoso)
Version 07.20.3240 as of Mar 11 2025
Type HELP; for help and EXIT; to exit.
Connected to OpenLink Virtuoso
Driver: 07.20.3240 OpenLink Virtuoso ODBC Driver
datasetName                                                                       description
LONG VARCHAR                                                                      LONG VARCHAR
_______________________________________________________________________________

1,995 People Face Images Data (Asian race)                                                                                  Description: 1,995 People Face Images Data (Asian race). For each subject, more than 20 images per person with frontal face were collected. This data can be used for face recognition and other tasks. Data size: 1,995 people, more than 20 images per person with frontal face Race distribution: Asian people
105,941 Images Natural Scenes OCR Data of 12 Languages                                                                                  Description: 105,941 Images Natural Scenes OCR Data of 12 Languages. The data covers 12 languages (6 Asian languages, 6 European languages), multiple natural scenes, multiple photographic angles. For annotation, line-level quadrilateral bounding box annotation and transcription for the texts were annotated in the data. The data can be used for tasks such as OCR of multi-language. Data size: 105,941 images, including Asian language family: Japanese 9,997 images, Korean 10,231 images, Indonesian 7,591 images, Malay 5,650 images, Vietnamese 8,822 images, Thai 9,645 images; European language family: French 10,015 images, German 7,213 images, Italian 8,824 images, Portuguese 7,754 images, Russian 10,376 images and Spanish 9,823 images Collecting environment: including shop plaque, stop board, poster, ticket, road sign, comic, cover picture, prompt/reminder, warning, packing instruction, menu, building sign, etc.
23 Pairs of Identical Twins Face Image Data                                                                                  Description： 23 Pairs of Identical Twins Face Image Data. The collecting scenes includes indoor and outdoor scenes. The subjects are Chinese males and females. The data diversity inlcudes multiple face angles, multiple face postures, close-up of eyes, multiple light conditions and multiple age groups. This dataset can be used for tasks such as twins' face recognition. Data size： 23 pairs, each person in a pair of identical twins has 40 images (20 indoor images, 20 outdoor images) Population distribution： race distribution: Asian (Chinese); gender distribution: male 9 pairs, female 14 pairs; age distribution: 12 pairs under 18 years old, 10 pairs aged from 18 to 40, 1 pairs over 40 years old
5,011 Images – Human Frontal face Data (Male)                                                                                  Description： 5,011 Images – Human Frontal face Data (Male). The data diversity includes multiple scenes, multiple ages and multiple races. This dataset includes 2,004 Caucasians , 3,007 Asians. This dataset can be used for tasks such as face detection, race detection, age detection, beard category classification. Data size： 5,011 people, one image per person Race distribution： 2,004 Caucasians , 3,007 Asians
A Dataset of Multispectral Potato Plants Images                                                                                  The dataset contains aerial agricultural images of a potato field with manual labels of healthy and stressed plant regions. The images were collected with a Parrot Sequoia multispectral camera carried by a 3DR Solo drone flying at an altitude of 3 meters. The dataset consists of RGB images with a resolution of 750×750 pixels, and spectral monochrome red, green, red-edge, and near-infrared images with a resolution of 416×416 pixels, and XML files with annotated bounding boxes of healthy and stressed potato crop.
ACL ARC citation contexts with DBLP ID                                                                                  
ACL Anthology Corpus with Full Text                                                                                  [![License](https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/) This repository provides full-text and metadata to the ACL anthology collection (80k articles/posters as of September 2022) also including .pdf files and grobid extractions of the pdfs. ## How is this different from what ACL anthology provides and what already exists? - We provide pdfs, full-text, references and other details extracted by grobid from the PDFs while [ACL Anthology](https://aclanthology.org/anthology+abstracts.bib.gz) only provides abstracts. - There exists a similar corpus call [ACL Anthology Network](https://clair.eecs.umich.edu/aan/about.php) but is now showing its age with just 23k papers from Dec 2016. ---- The goal is to keep this corpus updated and provide a comprehensive repository of the full ACL collection. This repository provides data for `80,013` ACL articles/posters - 1. 📖 All PDFs in ACL anthology : **size 45G** [download here](https://drive.google.com/file/d/1OGHyJrkaVpbrdbmxsDotG-tI3LiKyxuC/view?usp=sharing) 2. 🎓 All bib files in ACL anthology with abstracts : **size 172M** [download here](https://drive.google.com/file/d/1dJ-iE85moBv3iYG2LhRLT6KQyVkmllBg/view?usp=sharing) 3. 🏷️ Raw grobid extraction results on all the ACL anthology pdfs which includes full text and references : **size 3.6G** [download here](https://drive.google.com/file/d/1xC-K6__W3FCalIDBlDROeN4d4xh0IVry/view?usp=sharing) 4. 💾 Dataframe with extracted metadata (table below with details) and full text of the collection for analysis : **size 489M** [download here](https://drive.google.com/file/d/1CFCzNGlTls0H-Zcaem4Hg_ETj4ebhcDO/view?usp=sharing) | **Column name** | **Description** | | :----------------: | :---------------------------: | | `acl_id` | unique ACL id | | `abstract` | abstract extracted by GROBID | | `full_text` | full text extracted by GROBID | | `corpus_paper_id` | Semantic Scholar ID | | `pdf_hash` | sha1 hash of the pdf | | `numcitedby` | number of citations from S2 | | `url` | link of publication | | `publisher` | - | | `address` | Address of conference | | `year` | - | | `month` | - | | `booktitle` | - | | `author` | list of authors | | `title` | title of paper | | `pages` | - | | `doi` | - | | `number` | - | | `volume` | - | | `journal` | - | | `editor` | - | | `isbn` | - | ```python >>> import pandas as pd >>> df = pd.read_parquet('acl-publication-info.74k.parquet') >>> df acl_id abstract full_text corpus_paper_id pdf_hash ... number volume journal editor isbn 0 O02-2002 There is a need to measure word similarity whe... There is a need to measure word similarity whe... 18022704 0b09178ac8d17a92f16140365363d8df88c757d0 ... None None None None None 1 L02-1310 8220988 8d5e31610bc82c2abc86bc20ceba684c97e66024 ... None None None None None 2 R13-1042 Thread disentanglement is the task of separati... Thread disentanglement is the task of separati... 16703040 3eb736b17a5acb583b9a9bd99837427753632cdb ... None None None None None 3 W05-0819 In this paper, we describe a word alignment al... In this paper, we describe a word alignment al... 1215281 b20450f67116e59d1348fc472cfc09f96e348f55 ... None None None None None 4 L02-1309 18078432 011e943b64a78dadc3440674419821ee080f0de3 ... None None None None None ... ... ... ... ... ... ... ... ... ... ... ... 73280 P99-1002 This paper describes recent progress and the a... This paper describes recent progress and the a... 715160 ab17a01f142124744c6ae425f8a23011366ec3ee ... None None None None None 73281 P00-1009 We present an LFG-DOP parser which uses fragme... We present an LFG-DOP parser which uses fragme... 1356246 ad005b3fd0c867667118482227e31d9378229751 ... None None None None None 73282 P99-1056 The processes through which readers evoke ment... The processes through which readers evoke ment... 7277828 924cf7a4836ebfc20ee094c30e61b949be049fb6 ... None None None None None 73283 P99-1051 This paper examines the extent to which verb d... This paper examines the extent to which verb d... 1829043 6b1f6f28ee36de69e8afac39461ee1158cd4d49a ... None None None None None 73284 P00-1013 Spoken dialogue managers have benefited from u... Spoken dialogue managers have benefited from u... 10903652 483c818c09e39d9da47103fbf2da8aaa7acacf01 ... None None None None None [73285 rows x 21 columns] ``` The provided ACL id is consistent with S2 API as well - [https://api.semanticscholar.org/graph/v1/paper/ACL:P83-1025](https://api.semanticscholar.org/graph/v1/paper/ACL:P83-1025) The API can be used to fetch more information for each paper in the corpus. --- ## Text generation on Huggingface We fine-tuned the distilgpt2 model from huggingface using the full-text from this corpus. The model is trained for generation task. Text Generation Demo : <https://huggingface.co/shaurya0512/distilgpt2-finetune-acl22> Example: ```python >>> from transformers import AutoTokenizer, AutoModelForCausalLM >>> tokenizer = AutoTokenizer.from_pretrained('shaurya0512/distilgpt2-finetune-acl22') >>> model = AutoModelForCausalLM.from_pretrained('shaurya0512/distilgpt2-finetune-acl22') >>> >>> input_context = 'We introduce a new language representation' >>> input_ids = tokenizer.encode(input_context, return_tensors='pt') # encode input context >>> outputs = model.generate( ... input_ids=input_ids, max_length=128, temperature=0.7, repetition_penalty=1.2 ... ) # generate sequences >>> print(f'Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}') ``` ```text Generated: We introduce a new language representation for the task of sentiment classification. We propose an approach to learn representations from unlabeled data, which is based on supervised learning and can be applied in many applications such as machine translation (MT) or information retrieval systems where labeled text has been used by humans with limited training time but no supervision available at all. Our method achieves state-oftheart results using only one dataset per domain compared to other approaches that use multiple datasets simultaneously, including BERTScore(Devlin et al., 2019; Liu & Lapata, 2020b ) ; RoBERTa+LSTM + L2SRC - ``` ### TODO 1. ~~Link the acl corpus to semantic scholar(S2), sources like S2ORC~~ 2. Extract figures and captions from the ACL corpus using pdffigures - [scientific-figure-captioning](https://github.com/billchen0/scientific-figure-captioning) 3. Have a release schedule to keep the corpus updated. 4. ACL citation graph 5. ~~Enhance metadata with bib file mapping - include authors~~ 6. ~~Add citation counts for papers~~ 7. Use [ForeCite](https://github.com/allenai/ForeCite) to extract impactful keywords from the corpus 8. Link datasets using [paperswithcode](https://github.com/paperswithcode/paperswithcode-data)? - don't know how useful this is 9. Have some stats about the data - [linguistic-diversity](http://stats.aclrollingreview.org/submissions/linguistic-diversity/); [geo-diversity](http://stats.aclrollingreview.org/submissions/geo-diversity/); if possible [explorer](http://stats.aclrollingreview.org/submissions/explorer/) We are hoping that this corpus can be helpful for analysis relevant to the ACL community. **Please cite/star 🌟 this page if you use this corpus** ## Citing the ACL Anthology Corpus If you use this corpus in your research please use the following BibTeX entry: @Misc{acl_anthology_corpus, author = {Shaurya Rohatgi}, title = {ACL Anthology Corpus with Full Text}, howpublished = {Github}, year = {2022}, url = {https://github.com/shauryr/ACL-anthology-corpus} } [<img src='https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&logo=buy-me-a-coffee&logoColor=black'>](https://www.buymeacoffee.com/shauryrG) <!-- If you are feeling generous buy me a ☕ --> ## Acknowledgements We thank Semantic Scholar for providing access to the citation related data in this corpus. ## License ACL anthology corpus is released under the [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/). By using this corpus, you are agreeing to its usage terms.
Abstractive Text Summarization from Fanpage                                                                                  Fanpage dataset, containing news articles taken from Fanpage. There are two features: * source: Input news article. * target: Summary of the article.
Abstractive Text Summarization from Il Post                                                                                  IlPost dataset, containing news articles taken from IlPost. There are two features: * source: Input news article. * target: Summary of the article.
Aesthetics Text Corpus                                                                                  An exhaustive list of stop lemmas created from 12 corpora across multiple domains, consisting of over 13 million words, from which more than 200,000 lemmas were generated, and 11 publicly available stop word lists comprising over 1000 words, from which nearly 400 unique lemmas were generated. Source: [Novel Language Resources for Hindi: An Aesthetics Text Corpus and a Comprehensive Stop Lemma List](/paper/novel-language-resources-for-hindi-an)
Affective Text                                                                                  Affective Text (Test Corpus of SemEval 2007) by [Carlo Strapparava & Rada Mihalcea](https://www.aclweb.org/anthology/S07-1013/). Source: [Affective Text](https://github.com/sarnthil/unify-emotion-datasets/tree/master/datasets)
Aircraft Context Dataset                                                                                  The Aircraft Context Dataset, a composition of two inter-compatible large-scale and versatile image datasets focusing on manned aircraft and UAVs, is intended for training and evaluating classification, detection and segmentation models in aerial domains. Additionally, a set of relevant meta-parameters can be used to quantify dataset variability as well as the impact of environmental conditions on model performance.
AlexandreMotorImagery MOABB                                                                                  
An Amharic News Text classification Dataset                                                                                  In NLP, text classification is one of the primary problems we try to solve and its uses in language analyses are indisputable. The lack of labeled training data made it harder to do these tasks in low resource languages like Amharic. The task of collecting, labeling, annotating, and making valuable this kind of data will encourage junior researchers, schools, and machine learning practitioners to implement existing classification models in their language. In this short paper, we aim to introduce the Amharic text classification dataset that consists of more than 50k news articles that were categorized into 6 classes. This dataset is made available with easy baseline performances to encourage studies and better performance experiments.
Arabic Text Diacritization                                                                                  Extracted from the Tashkeela Corpus, the dataset consists of 55K lines containing about 2.3M words. Source: [https://github.com/AliOsm/arabic-text-diacritization](https://github.com/AliOsm/arabic-text-diacritization)
ArtImage                                                                                  **ArtImage** is a synthetic dataset of articulated object models of 5 categories from PartNet-Mobility for articulated object tasks in category level.
Audio de mosquitos Aedes Aegypti                                                                                  **Dataset Description:** The dataset comprises audio recordings of the wing beats of Aedes aegypti mosquitoes and others, conducted in a semi-controlled environment. It encompasses approximately 18,706 seconds of recording, with properly labeled samples. **Dataset Characteristics:** - **Data Type:** Audio recordings. - **Total Duration:** Approximately 18,706 seconds. - **Labeling:** Labeled samples. - **Environment:** Semi-controlled. - **Data Type:** Audio recordings. - **Total Duration:** Approximately 18,706 seconds. - **Species:** Aedes aegypti and others.
Audio demo files                                                                                  Audio files that supplement 'Treatise on Hearing: The Temporal Auditory Imaging Theory Inspired by Optics and Communication'.
AudioCaps                                                                                  **AudioCaps** is a dataset of sounds with event descriptions that was introduced for the task of audio captioning, with sounds sourced from the [AudioSet](https://paperswithcode.com/dataset/audioset) dataset. Annotators were provided the audio tracks together with category hints (and with additional video hints if needed). Source: [Audio Retrieval with Natural Language Queries](/paper/audio-retrieval-with-natural-language-queries) Image source: [https://audiocaps.github.io/](https://audiocaps.github.io/)
AudioSet                                                                                  Audioset is an audio event dataset, which consists of over 2M human-annotated 10-second video clips. These clips are collected from YouTube, therefore many of which are in poor-quality and contain multiple sound-sources. A hierarchical ontology of 632 event classes is employed to annotate these data, which means that the same sound could be annotated as different labels. For example, the sound of barking is annotated as Animal, Pets, and Dog. All the videos are split into Evaluation/Balanced-Train/Unbalanced-Train set. Source: [Curriculum Audiovisual Learning](https://arxiv.org/abs/2001.09414)
AudioSet CC                                                                                  The subset of audio samples from the AudioSet ontology which are licensed with Creative Commons. This set contains approximately 10,000 samples of 10s long clips, and is freely modifiable and distributable. Each clip has with it, its full label set and unique ID.
Autorickshaw Image Dataset | Niche Vehicle Dataset                                                                                  This dataset is an extremely challenging set of over 8000+ original Fire and Smoke images captured and crowdsourced from over 1200+ urban and rural areas, where each image is **manually reviewed and verified** by computer vision professionals at Datacluster Labs. ### **Dataset Features** - Dataset size : 8000+ - Captured by : Over 1200+ crowdsource contributors - Resolution : 99% images HD and above (1920x1080 and above) - Location : Captured with 800+ cities accross India - Diversity : Various lighting conditions like day, night, varied distances, view points etc. - Device used : Captured using mobile phones in 2021-2022 - Usage : Vehicle detection, Autorickshaw detection, Self driving, Indian vehicles, Number Plate detection, etc. ### Available Annotation formats COCO, YOLO, PASCAL-VOC, Tf-Record **To download full datasets or to submit a request for your dataset needs, please ping us at [sales@datacluster.ai](sales@datacluster.ai) Visit [www.datacluster.ai](www.datacluster.ai) to know more.** **Note**: All the images are manually captured and verified by a large contributor base on DataCluster platform
BIRD (BIg Bench for LaRge-scale Database Grounded Text-to-SQL Evaluation)                                                                                  BIRD (BIg Bench for LaRge-scale Database Grounded Text-to-SQL Evaluation) represents a pioneering, cross-domain dataset that examines the impact of extensive database contents on text-to-SQL parsing. BIRD contains over 12,751 unique question-SQL pairs and 95 big databases with a total size of 33.4 GB. It also covers more than 37 professional domains, such as blockchain, hockey, healthcare and education, etc.
BNCI 2014-001 Motor Imagery dataset.                                                                                  ## BNCI 2014-001 Motor Imagery dataset Dataset IIa from BCI Competition 4 [1]. ### Dataset Description This data set consists of EEG data from 9 subjects. The cue-based BCI paradigm consisted of four different motor imagery tasks, namely the imagination of movement of the left hand (class 1), right hand (class 2), both feet (class 3), and tongue (class 4). Two sessions on different days were recorded for each subject. Each session is comprised of 6 runs separated by short breaks. One run consists of 48 trials (12 for each of the four possible classes), yielding a total of 288 trials per session. The subjects were sitting in a comfortable armchair in front of a computer screen. At the beginning of a trial ( t = 0 s), a fixation cross appeared on the black screen. In addition, a short acoustic warning tone was presented. After two seconds ( t = 2 s), a cue in the form of an arrow pointing either to the left, right, down or up (corresponding to one of the four classes left hand, right hand, foot or tongue) appeared and stayed on the screen for 1.25 s. This prompted the subjects to perform the desired motor imagery task. No feedback was provided. The subjects were ask to carry out the motor imagery task until the fixation cross disappeared from the screen at t = 6 s. Twenty-two Ag/AgCl electrodes (with inter-electrode distances of 3.5 cm) were used to record the EEG; the montage is shown in Figure 3 left. All signals were recorded monopolarly with the left mastoid serving as reference and the right mastoid as ground. The signals were sampled with. 250 Hz and bandpass-filtered between 0.5 Hz and 100 Hz. The sensitivity of the amplifier was set to 100 μV . An additional 50 Hz notch filter was enabled to suppress line noise. ### References [1] Tangermann, M., Müller, K.R., Aertsen, A., Birbaumer, N., Braun, C., Brunner, C., Leeb, R., Mehring, C., Miller, K.J., Mueller-Putz, G. and Nolte, G., 2012. Review of the BCI competition IV. Frontiers in neuroscience, 6, p.55.
BNCI 2014-002 Motor Imagery dataset                                                                                  **Dataset Description** This data set consists of EEG data from 9 subjects. The cue-based BCI paradigm consisted of four different motor imagery tasks, namely the imag- ination of movement of the left hand (class 1), right hand (class 2), both feet (class 3), and tongue (class 4). Two sessions on different days were recorded for each subject. Each session is comprised of 6 runs separated by short breaks. One run consists of 48 trials (12 for each of the four possible classes), yielding a total of 288 trials per session. The subjects were sitting in a comfortable armchair in front of a computer screen. At the beginning of a trial ( t = 0 s), a fixation cross appeared on the black screen. In addition, a short acoustic warning tone was presented. After two seconds ( t = 2 s), a cue in the form of an arrow pointing either to the left, right, down or up (corresponding to one of the four classes left hand, right hand, foot or tongue) appeared and stayed on the screen for 1.25 s. This prompted the subjects to perform the desired motor imagery task. No feedback was provided. The subjects were ask to carry out the motor imagery task until the fixation cross disappeared from the screen at t = 6 s. Twenty-two Ag/AgCl electrodes (with inter-electrode distances of 3.5 cm) were used to record the EEG; the montage is shown in Figure 3 left. All signals were recorded monopolarly with the left mastoid serving as reference and the right mastoid as ground. The signals were sampled with. 250 Hz and bandpass-filtered between 0.5 Hz and 100 Hz. The sensitivity of the amplifier was set to 100 μV . An additional 50 Hz notch filter was enabled to suppress line noise References ---------- [1] Tangermann, M., Müller, K.R., Aertsen, A., Birbaumer, N., Braun, C., Brunner, C., Leeb, R., Mehring, C., Miller, K.J., Mueller-Putz, G. and Nolte, G., 2012. Review of the BCI competition IV. Frontiers in neuroscience, 6, p.55.
BNCI 2014-004 Motor Imagery dataset.                                                                                  **Dataset description** This data set consists of EEG data from 9 subjects of a study published in [1]_. The subjects were right-handed, had normal or corrected-to-normal vision and were paid for participating in the experiments. All volunteers were sitting in an armchair, watching a flat screen monitor placed approximately 1 m away at eye level. For each subject 5 sessions are provided, whereby the first two sessions contain training data without feedback (screening), and the last three sessions were recorded with feedback. Three bipolar recordings (C3, Cz, and C4) were recorded with a sampling frequency of 250 Hz.They were bandpass- filtered between 0.5 Hz and 100 Hz, and a notch filter at 50 Hz was enabled. The placement of the three bipolar recordings (large or small distances, more anterior or posterior) were slightly different for each subject (for more details see [1]). The electrode position Fz served as EEG ground. In addition to the EEG channels, the electrooculogram (EOG) was recorded with three monopolar electrodes. The cue-based screening paradigm consisted of two classes, namely the motor imagery (MI) of left hand (class 1) and right hand (class 2). Each subject participated in two screening sessions without feedback recorded on two different days within two weeks. Each session consisted of six runs with ten trials each and two classes of imagery. This resulted in 20 trials per run and 120 trials per session. Data of 120 repetitions of each MI class were available for each person in total. Prior to the first motor im- agery training the subject executed and imagined different movements for each body part and selected the one which they could imagine best (e. g., squeezing a ball or pulling a brake). Each trial started with a fixation cross and an additional short acoustic warning tone (1 kHz, 70 ms). Some seconds later a visual cue was presented for 1.25 seconds. Afterwards the subjects had to imagine the corresponding hand movement over a period of 4 seconds. Each trial was followed by a short break of at least 1.5 seconds. A randomized time of up to 1 second was added to the break to avoid adaptation For the three online feedback sessions four runs with smiley feedback were recorded, whereby each run consisted of twenty trials for each type of motor imagery. At the beginning of each trial (second 0) the feedback (a gray smiley) was centered on the screen. At second 2, a short warning beep (1 kHz, 70 ms) was given. The cue was presented from second 3 to 7.5. At second 7.5 the screen went blank and a random interval between 1.0 and 2.0 seconds was added to the trial.
BNCI 2015-001 Motor Imagery dataset                                                                                  **Dataset description** We acquired the EEG from three Laplacian derivations, 3.5 cm (center-to- center) around the electrode positions (according to International 10-20 System of Electrode Placement) C3 (FC3, C5, CP3 and C1), Cz (FCz, C1, CPz and C2) and C4 (FC4, C2, CP4 and C6). The acquisition hardware was a g.GAMMAsys active electrode system along with a g.USBamp amplifier (g.tec, Guger Tech- nologies OEG, Graz, Austria). The system sampled at 512 Hz, with a bandpass filter between 0.5 and 100 Hz and a notch filter at 50 Hz. The order of the channels in the data is FC3, FCz, FC4, C5, C3, C1, Cz, C2, C4, C6, CP3, CPz, CP4. The task for the user was to perform sustained right hand versus both feet movement imagery starting from the cue (second 3) to the end of the cross period (sec- ond 8). A trial started with 3 s of reference period, followed by a brisk audible cue and a visual cue (arrow right for right hand, arrow down for both feet) from second 3 to 4.25. The activity period, where the users received feedback, lasted from second 4 to 8. There was a random 2 to 3 s pause between the trials. References ---------- [1] J. Faller, C. Vidaurre, T. Solis-Escalante, C. Neuper and R. Scherer (2012). Autocalibration and recurrent adaptation: Towards a plug and play online ERD- BCI. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 20(3), 313-319.
BNCI 2015-004 Motor Imagery dataset                                                                                  **Dataset description** We provide EEG data recorded from nine users with disability (spinal cord injury and stroke) on two different days (sessions). Users performed, follow- ing a cue-guided experimental paradigm, five distinct mental tasks (MT). MTs include mental word association (condition WORD), mental subtraction (SUB), spatial navigation (NAV), right hand motor imagery (HAND) and feet motor imagery (FEET). Details on the experimental paradigm are summarized in Figure 1. The session for a single subject consisted of 8 runs resulting in 40 trials of each class for each day. One single experimental run consisted of 25 cues, with 5 of each mental task. Cues were presented in random order. EEG was recorded from 30 electrode channels placed on the scalp according to the international 10-20 system. Electrode positions included channels AFz, F7, F3, Fz, F4, F8, FC3, FCz, FC4, T3, C3, Cz, C4, T4, CP3, CPz,CP4, P7, P5, P3, P1, Pz, P2, P4, P6, P8, PO3, PO4, O1, and O2. Reference and ground were placed at the left and right mastoid, respectively. The g.tec GAMMAsys system with g.LADYbird active electrodes and two g.USBamp biosignal amplifiers (Guger Technolgies, Graz, Austria) was used for recording. EEG was band pass filtered 0.5-100 Hz (notch filter at 50 Hz) and sampled at a rate of 256 Hz. The duration of a single imagery trials is 10 s. At t = 0 s, a cross was presented in the middle of the screen. Participants were asked to relax and fixate the cross to avoid eye movements. At t = 3 s, a beep was sounded to get the participant’s attention. The cue indicating the requested imagery task, one out of five graphical symbols, was presented from t = 3 s to t = 4.25 s. At t = 10 s, a second beep was sounded and the fixation-cross disappeared, which indicated the end of the trial. A variable break (inter-trial-interval, ITI) lasting between 2.5 s and 3.5 s occurred before the start of the next trial. Participants were asked to avoid movements during the imagery period, and to move and blink during the ITI. Experimental runs began and ended with a blank screen (duration 4 s) References ---------- [1] Scherer R, Faller J, Friedrich EVC, Opisso E, Costa U, Kübler A, et al. (2015) Individually Adapted Imagery Improves Brain-Computer Interface Performance in End-Users with Disability. PLoS ONE 10(5). https://doi.org/10.1371/journal.pone.0123727
BOVText                                                                                  BOVText is a new large-scale benchmark dataset named Bilingual, Open World Video Text(BOVText), the first large-scale and multilingual benchmark for video text spotting in a variety of scenarios. All data are collected from KuaiShou and YouTube
BanglaLekhaImageCaptions                                                                                  This dataset consists of images and annotations in Bengali. The images are human annotated in Bengali by two adult native Bengali speakers. All popular image captioning datasets have a predominant western cultural bias with the annotations done in English. Using such datasets to train an image captioning system assumes that a good English to target language translation system exists and that the original dataset had elements of the target culture. Both these assumptions are false, leading to the need of a culturally relevant dataset in Bengali, to generate appropriate image captions of images relevant to the Bangladeshi and wider subcontinental context. The dataset presented consists of 9,154 images.
Benchmarking-Chinese-Text-Recognition                                                                                  This repository contains datasets and baselines for benchmarking Chinese text recognition. Please see the corresponding paper for more details regarding the datasets, baselines, the empirical study, etc. Highlights 🌟 All datasets are transformed to lmdb format for convenient usage. 🌟 The experimental results of all baselines are available at link with format (index [pred] [gt]). 🌟 The code and trained weights of all baselines are available at link for direct use.
BirdSoundsDenoising: Deep Visual Audio Denoising for Bird Sounds                                                                                  This is the dataset for BirdSoundsDenoising including training, validation and test.
Biwi 3D Audiovisual Corpus of Affective Communication - B3D(AC)^2                                                                                  **BIWI 3D** corpus comprises a total of 1109 sentences uttered by 14 native English speakers (6 males and 8 females). A real time 3D scanner and a professional microphone were used to capture the facial movements and the speech of the speakers. The dense dynamic face scans were acquired at 25 frames per second and the RMS error in the 3D reconstruction is about 0.5 mm. In order to ease automatic speech segmentation, we carried out the recordings in a anechoic room, with walls covered by sound wave-absorbing materials. Each sentence was recorded twice: - First, the speaker read the sentence from text, with a neutral expression. - Then, the speaker watched a clip extracted from a feature film where the sentence is acted by professional actors and the context is highly emotional. After rating the emotions induced by the video, the speaker repeated the sentence.
Bramble flower image dataset                                                                                  This dataset contains both the artificial and real flower images of bramble flowers. The real images were taken with a realsense D435 camera inside the West Virginia University greenhouse. All the flowers are annotated in YOLO format with bounding box and class name. The trained weights after training also have been provided. They can be used with the python script provided to detect the bramble flowers. Also the classifier can classify whether the flowers center is visible or hidden which will be helpful in precision pollination projects. Images are also augmented to make the task robust in various environmental conditions.
Burned Area Delineation from Satellite Imagery                                                                                  The dataset contains 73 satellite images of different forests damaged by wildfires across Europe with a resolution of up to 10m per pixel. Data were collected from the Sentinel-2 L2A satellite mission and the target labels were generated from the Copernicus Emergency Management Service (EMS) annotations, with five different severity levels, ranging from undamaged to completely destroyed.
Burr classification images                                                                                  Original images and images with RUSTICO filters applied Also a csv with classes is included
CAT: Context Adjustment Training                                                                                  CAT is a specialized dataset for co-saliency detection. This dataset is intended for both helping to assess the performance of vision algorithms and supporting research that aims to exploit large volumes of annotated data, e.g., for training deep neural networks. Scale & Features - A total number of 33500 image samples. - 280 semantic groups affiliated to 15 superclasses. - High-quality mask annotations. - Diverse visual context with multiple foreground objects.
CIFAKE: Real and AI-Generated Synthetic Images                                                                                  The quality of AI-generated images has rapidly increased, leading to concerns of authenticity and trustworthiness. CIFAKE is a dataset that contains 60,000 synthetically-generated images and 60,000 real images (collected from CIFAR-10). Can computer vision techniques be used to detect when an image is real or has been generated by AI? ##Dataset details The dataset contains two classes - REAL and FAKE. For REAL, we collected the images from Krizhevsky & Hinton's CIFAR-10 dataset For the FAKE images, we generated the equivalent of CIFAR-10 with Stable Diffusion version 1.4 There are 100,000 images for training (50k per class) and 20,000 for testing (10k per class) ##References If you use this dataset, you must cite the following sources Krizhevsky, A., & Hinton, G. (2009). Learning multiple layers of features from tiny images. Bird, J.J., Lotfi, A. (2023). CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images. arXiv preprint arXiv:2303.14126. Real images are from Krizhevsky & Hinton (2009), fake images are from Bird & Lotfi (2023). The Bird & Lotfi study is a preprint currently available on ArXiv and this description will be updated when the paper is published. ##License This dataset is published under the same MIT license as CIFAR-10: Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the 'Software'), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
CIFAR-10 Image Classification                                                                                  
CIFAR-10 image generation                                                                                  
COCO (image as query)                                                                                  
COCO Visual Question Answering (VQA) abstract images 1.0 open ended                                                                                  
COCO Visual Question Answering (VQA) real images 1.0 multiple choice                                                                                  
COCO Visual Question Answering (VQA) real images 1.0 open ended                                                                                  
COCO Visual Question Answering (VQA) real images 2.0 open ended                                                                                  
COCO-Text                                                                                  The **COCO-Text** dataset is a dataset for text detection and recognition. It is based on the MS COCO dataset, which contains images of complex everyday scenes. The COCO-Text dataset contains non-text images, legible text images and illegible text images. In total there are 22184 training images and 7026 validation images with at least one instance of legible text. Source: [Improving Text Proposals for Scene Images with Fully Convolutional Networks](https://arxiv.org/abs/1702.05089) Image Source: [https://vision.cornell.edu/se3/coco-text-2/](https://vision.cornell.edu/se3/coco-text-2/)
COVID-19 Image Data Collection                                                                                  Contains hundreds of frontal view X-rays and is the largest public resource for COVID-19 image and prognostic data, making it a necessary resource to develop and evaluate tools to aid in the treatment of COVID-19. Source: [COVID-19 Image Data Collection](/paper/covid-19-image-data-collection)
COVID19-CountryImage                                                                                  The Covid19-CountryImage dataset is a Twitter dataset which contains COVID-19-related tweets. Source: [https://github.com/thunlp/COVID19-CountryImage](https://github.com/thunlp/COVID19-CountryImage)
CSI Images                                                                                  The raw .mat data collected in two different scenarios are provided.
CUHK Image Cropping                                                                                  **CUHK Image Cropping** is a dataset for image cropping. The photos are of varying aesthetic quality and span a variety of image categories, including animal, architecture, human, landscape, night, plant and man-made objects. Each image is manually cropped by three expert photographers (graduate students in art whose primary medium is photography) to form three training sets. There are 1,000 photos in the dataset.
CelebV-Text                                                                                  **CelebV-Text** comprises 70,000 in-the-wild face video clips with diverse visual content, each paired with 20 texts generated using the proposed semi-automatic text generation strategy. The provided texts describes both static and dynamic attributes precisely. Source: [CelebV-Text: A Large-Scale Facial Text-Video Dataset](https://arxiv.org/pdf/2303.14717v1.pdf) Image Source: [CelebV-Text: A Large-Scale Facial Text-Video Dataset](https://arxiv.org/pdf/2303.14717v1.pdf)
ChCatExt                                                                                  ChCatExt is composed of BidAnn (bid announcement), FinAnn (financial announcement) and CreRat (credit rating report). It is designed for re-construct catalog trees from documents.
Chart-to-text                                                                                  **Chart-to-text** is a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types.
Chart2Text                                                                                  Chart2Text is a dataset that was crawled from 23,382 freely accessible pages from statista.com in early March of 2020, yielding a total of 8,305 charts, and associated summaries. For each chart, the chart image, the underlying data table, the title, the axis labels, and a human-written summary describing the statistic was downloaded. Source: [Chart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model](https://arxiv.org/pdf/2010.09142.pdf)
Chest ImaGenome                                                                                  Chest ImaGenome is a dataset with a scene graph data structure to describe 242,072 images. Local annotations are automatically produced using a joint rule-based natural language processing (NLP) and atlas-based bounding box detection pipeline. Through a radiologist constructed CXR ontology, the annotations for each CXR are connected as an anatomy-centered scene graph, useful for image-level reasoning and multimodal fusion applications. Overall, the following are provided: i) 1256 combinations of relation annotations between 29 CXR anatomical locations (objects with bounding box coordinates) and their attributes, structured as a scene graph per image, ii) over 670,000 localized comparison relations (for improved, worsened, or no change) between the anatomical locations across sequential exams, as well as ii) a manually annotated gold standard scene graph dataset from 500 unique patients. Description from: [Chest ImaGenome Dataset for Clinical Reasoning](https://paperswithcode.com/paper/chest-imagenome-dataset-for-clinical) Image source: [Chest ImaGenome Dataset for Clinical Reasoning](https://paperswithcode.com/paper/chest-imagenome-dataset-for-clinical)
Chest X-ray images                                                                                  Chest X-ray images for pneumonia detection.
Chinese Text in the Wild                                                                                  Chinese Text in the Wild is a dataset of Chinese text with about 1 million Chinese characters from 3850 unique ones annotated by experts in over 30000 street view images. This is a challenging dataset with good diversity containing planar text, raised text, text under poor illumination, distant text, partially occluded text, etc.
CoNLL 2017 Shared Task - Automatically Annotated Raw Texts and Word Embeddings                                                                                  Automatic segmentation, tokenization and morphological and syntactic annotations of raw texts in 45 languages, generated by UDPipe (http://ufal.mff.cuni.cz/udpipe), together with word embeddings of dimension 100 computed from lowercased texts by word2vec (https://code.google.com/archive/p/word2vec/).
Congolese Swahili – French parallel text corpora                                                                                  French sentences are sourced from Tatoeba repository and then translated into Congolese Swahili.
Construction Vehicle Image Dataset |Trucks|Tractor etc.                                                                                  This dataset is an extremely challenging set of over 20,000+ original Construction vehicle images captured and crowdsourced from over 600+ urban and rural areas, where each image is manually reviewed and verified by computer vision professionals at Datacluster Labs. - Dataset Features - Dataset size : 20,000+ - Captured by : Over 1000+ crowdsource contributors - Resolution : 100% of the images are HD and above (1920x1080 and above) - Location : Captured with 600+ cities accross India - Diversity : Various lighting conditions like day, night, varied distances, view points etc. - Device used : Captured using mobile phones in 2020-2022 - Usage : Construction site object detection, workplace safety monitoring, self driving systems, etc. - Available Annotation formats - COCO, YOLO, PASCAL-VOC, Tf-Record **To download full datasets or to submit a request for your dataset needs, please drop a mail on sales@datacluster.ai . Visit www.datacluster.ai to know more.** This dataset is collected by DataCluster Labs. To download full dataset or to submit a request for your new data collection needs, please drop a mail to: sales@datacluster.ai
Contextualised Polyseme Word Sense Dataset v2                                                                                  This is a revised and extended second version of a Contextualised Polyseme Word Sense Dataset. The dataset contains two human annotated measures of word sense similarity for polysemic target words used in contexts invoking different sense interpretations. The first set contains graded similarity judgements for highlighted target words displayed in two different contexts. The second set contains co-predication acceptability judgements for sentence constructions combining the sentence pairs from the first set.
Corrosion Image Data Set for Automating Scientific Assessment of Materials                                                                                  The study of material corrosion is an important research area, with corrosion degradation of metallic structures causing expenses up to 4% of the global domestic product annually along with major safety risks worldwide. Unfortunately, large-scale and timely scientific discovery of materials has been hindered by the lack of standardized corrosion experimental data in the public domain for developing machine learning models. Obtaining such data is challenging due to the expert knowledge and time required to conduct these scientific experiments and assess corrosion levels. We curate a novel dataset consisting of 600 images annotated with expert corrosion ratings obtained over 10 years of laboratory corrosion testing by material scientists. Based on this data set, we find that non-experts even when rigorously trained with domain guidelines to rate corrosion fail to match expert ratings. Challenges include limited data, image artifacts, and millimeter-precision corrosion. This motivates us to explore the viability of deep learning approaches to tackle this benchmark classification task. We study (i) convolutional neural networks powered with rich domain-specific image augmentation techniques tuned to our data, and (ii) a recent self-supervised representation learning approach either pretrained on ImageNet or trained on our data. We demonstrate that pretrained ResNet-18 and HR-Net models with tuned augmentations can reach up to 0.83 accuracy. With this corrosion data set, we open the door for the design of more advanced deep learning models to support this real-world task, while driving innovative new research to bridge computer vision and material innovation. [Disclaimer] By downloading this code and/or using this data, you agree to abide by all of the rules and regulations. 1. Researcher shall use the dataset only for non-commercial research and educational purposes. 2. The authors with Worcester Polytechnic Institute and US Army Research Lab make no representations or warranties regarding the dataset, including but not limited to warranties of non-infringement or fitness for a particular purpose. 3. Researcher accepts full responsibility for his or her use of the dataset and shall defend and indemnify the authors, including their employees, Trustees, officers and agents, against any and all claims arising from Researcher's use of the dataset, including but not limited to Researcher's use of any copies of copyrighted images that he or she may create from the dataset. 4. Researcher may provide research associates and colleagues with access to the dataset provided that they first agree to be bound by these terms and conditions. 5. The authors reserve the right to terminate Researcher's access to or usage of the dataset at any time. 6. If Researcher is employed by a for-profit, commercial entity, Researcher's employer shall also be bound by these terms and conditions, and Researcher hereby represents that he or she is fully authorized to enter into this agreement on behalf of such employer. 7. The law of the State of Massachusetts shall apply to all disputes under this agreement. [Citation] Anyone that uses this data set must cite and acknowledge our BMVC data set paper: @InProceedings{yin2021BMVC, author = {Yin, Biao and Josselyn, Nicholas and Considine, Thomas and Kelley, John and Rinderspacher, Berend and Jensen, Robert and Snyder, James and Zhang, Ziming and Rundensteiner, Elke}, title = {Corrosion Image Data Set for Automating Scientific Assessment of Materials}, booktitle = {British Machine Vision Conference (BMVC)}, year = {2021}}
CytoImageNet                                                                                  CytoImageNet is a large-scale pretraining dataset of microscopy images (890K, 894 classes). In the paper, CytoImageNet pretraining yielded features competitive to **and different** from ImageNet pretrained features on downstream microscopy tasks. * It was constructed from 40 openly available microscopy datasets. * Weak labels (from experimental metadata) were assigned to each image in the dataset. * Images are of varying sizes. The primary purpose of the dataset is to be used for pretraining as a pretext task for learning useful bioimage representations. However, it may be used for validation or exploratory analysis.
Dark Corner Artifact Masks for ISIC Images                                                                                  A set of over 4000 dark corner artifact / vignette masks that can be applied to ISIC skin lesion images to test for the effect of such artifacts in classification tasks.
Data-Efficient Image Classification (DEIC) Benchmark                                                                                  
Dataset for Post-OCR text correction in Sanskrit                                                                                  This dataset contains around 218K sentences, with 1.5 million words, from 30 different books designed for Post-OCR text correction. Source: [A Benchmark and Dataset for Post-OCR text correction in Sanskrit](https://arxiv.org/pdf/2211.07980v1.pdf) Image Source: [https://arxiv.org/pdf/2211.07980v1.pdf](https://arxiv.org/pdf/2211.07980v1.pdf)
Dataset of Context information for Zero Interaction Security                                                                                  We release both the processed data and evaluation results from our own experiments, and the underlying raw data that can be used for future experiments and schemes in the domain of Zero-Interaction Security. Find more details in the dataset description on Zenodo.
Dataset of Rendered Chess Game State Images                                                                                  This dataset contains 4,888 synthetic images of chess game states that occurred in games played by Magnus Carlsen. The images were rendered in Blender at different angles and lighting conditions.
Datasets for 3D shape reconstruction from 2D microscopy images                                                                                  Two single cell datsets for 3D shape reconstruction from 2D microscopy images used for our three previous publication’s, together with the respective model predictions.
DeepMind Cartpole Balance (Images)                                                                                  
DeepMind Cartpole Swingup (Images)                                                                                  
DeepMind Cheetah Run (Images)                                                                                  
DeepMind Cup Catch (Images)                                                                                  
DeepMind Finger Spin (Images)                                                                                  
DeepMind Walker Walk (Images)                                                                                  
EEG Motor Movement/Imagery Dataset                                                                                  This data set consists of over 1500 one- and two-minute EEG recordings, obtained from 109 volunteers.
ES-ImageNet                                                                                  **ES-ImageNet** is a large-scale event-stream dataset for SNNs and neuromorphic vision. It consists of about 1.3 M samples converted from ILSVRC2012 in 1000 different categories. ES-ImageNet is dozens of times larger than other neuromorphic classification datasets at present and completely generated by the software
Electromagnetic Calorimeter Shower Images                                                                                  Each HDF5 file has the following structure: `energy Dataset {100000, 1}` `layer_0 Dataset {100000, 3, 96}` `layer_1 Dataset {100000, 12, 12}` `layer_2 Dataset {100000, 12, 6}` `overflow Dataset {100000, 3}` In practice, each file is a collection of 100,000 calorimeter showers corresponding to the particle specified in the file name (eplus = positrons, gamma = photons, piplus = charged pions). The calorimeter we built is segmented longitudinally into three layer with different depths and granularities. In units of mm, the three layers have the following (eta, phi, z) dimensions: Layer 0: (5, 160, 90) | Layer 1: (40, 40, 347) | Layer 2: (80, 40, 43) In the HDF5 files, the `energy` entry specifies the true energy of the incoming particle in units of GeV. `layer_0`, `layer_1`, and `layer_2` represents the energy deposited in each layer of the calorimeter in an image data format. Given the segmentation of each calorimeter layer, these images have dimensions 3x96 (in layer 0), 12x12 (in layer 1), and 12x6 (in layer 3). The `overflow` contains the amount of energy that was deposited outside of the calorimeter section we are considering.
Electronics Object Image Dataset | Computer Parts                                                                                  This dataset is an extremely challenging set of over 5000+ original Electronic Items images captured and crowdsourced from over 1000+ urban and rural areas, where each image is **manually reviewed and verified** by computer vision professionals at Datacluster Labs. ### **Dataset Features** - Dataset size : 5000+ - Captured by : Over 1000+ crowdsource contributors - Resolution : 99% images HD and above (1920x1080 and above) - Location : Captured with 800+ cities accross India - Diversity : Various lighting conditions like day, night, varied distances, view points etc. - Device used : Captured using mobile phones in 2020-2021 - Applications : Electronics Detection, Daily item detection, Home Automation, etc. ### Available Annotation formats COCO, YOLO, PASCAL-VOC, Tf-Record **To download full datasets or to submit a request for your dataset needs, please ping us at [sales@datacluster.ai](sales@datacluster.ai) Visit [www.datacluster.ai](www.datacluster.ai) to know more.** **Note**: All the images are manually captured and verified by a large contributor base on DataCluster platform
EmoContext                                                                                  EmoContext consists of three-turn English Tweets. The emotion labels include happiness, sadness, anger and other.
Engineered Cardiac Microbundle Time-Lapse Microscopy Image Dataset                                                                                  The 'Microbundle Time-lapse Dataset' contains 24 experimental time-lapse images of cardiac microbundles using three distinct types of experimental testbed of beating lab grown hiPSC-based cardiac microbundles. Of the 24 experimental time-lapse images, 23 examples are brightfield videos, and a single example is a phase contrast video. We categorize the different experimental testbeds into 3 types, where 'Type 1' includes movies obtained from standard experimental microbundle platforms termed microbundle strain gauges [1,2,3]. We refer to data collected from non-standard platforms termed FibroTUGs [4] as 'Type 2' data, and 'Type 3' data represents a highly versatile and diverse nanofabricated experimental platform [5,6]. Within this dataset, we include 11 examples of 'Type 1' tissue, 7 examples of 'Type 2' tissue, and 6 examples of 'Type 3' tissue, totaling to 24 different examples of these experimental data. In addition to the raw videos shared in '.tif' format, we include the tissue masks, whether generated automatically via our computational pipeline [7] or manually via tracing in ImageJ [8], that were used to run the 'MicroBundleCompute' software [7] for analyzing these data. These masks are included within the 'masks' subfolders, where each mask text file is a two-dimensional array in which the tissue domain is denoted by “1” and the background domain is denoted by “0”. We include the 'mask.tif' files for visualization purposes only. In brief, this dataset was used to showcase the functionality of the 'MicroBundleCompute' analysis software [7] including pillar tracking and analysis of heterogeneous displacement and strain fields. To reproduce the results shown in [9], the manuscript introducing the 'MicroBundleCompute' software, only a single pre-processing step is required. Specifically, the single '.tif' file for each experiment needs to be converted into a series of individual images saved in the '.TIF' format in the 'movie' folder. References: [1] Boudou T, Legant WR, Mu A, Borochin MA, Thavandiran N, Radisic M, Zandstra PW, Epstein JA, Margulies KB, Chen CS. A microfabricated platform to measure and manipulate the mechanics of engineered cardiac microtissues. Tissue Engineering Part A. 2012 May 1;18(9-10):910-9. [2] Xu F, Zhao R, Liu AS, Metz T, Shi Y, Bose P, Reich DH. A microfabricated magnetic actuation device for mechanical conditioning of arrays of 3D microtissues. Lab on a Chip. 2015;15(11):2496-503. [3] Bielawski KS, Leonard A, Bhandari S, Murry CE, Sniadecki NJ. Real-time force and frequency analysis of engineered human heart tissue derived from induced pluripotent stem cells using magnetic sensing. Tissue Engineering Part C: Methods. 2016 Oct 1;22(10):932-40. [4] DePalma SJ, Davidson CD, Stis AE, Helms AS, Baker BM. Microenvironmental determinants of organized iPSC-cardiomyocyte tissues on synthetic fibrous matrices. Biomaterials science. 2021;9(1):93-107. [5] Jayne RK, Karakan MÇ, Zhang K, Pierce N, Michas C, Bishop DJ, Chen CS, Ekinci KL, White AE. Direct laser writing for cardiac tissue engineering: a microfluidic heart on a chip with integrated transducers. Lab on a Chip. 2021;21(9):1724-37. [6] Karakan MÇ. A Direct-Laser-Written Heart-on-a-Chip Platform for Generation and Stimulation of Engineered Heart Tissues (Doctoral dissertation, Boston University, 2023). [7] Kobeissi H, & Lejeune E (2023). MicroBundleCompute [Computer software]. https://github.com/HibaKob/MicroBundleCompute [8] Bourne R. Fundamentals of digital imaging in medicine. Springer Science & Business Media; 2010 Jan 18. [9] Kobeissi H, Jilberto, J, Karakan MÇ, Gao X, DePalma SJ, Das SL, Quach L, Urquia J, Baker BM, Chen CS, Nordsletten D, Lejeune E. MicroBundleCompute: Automated segmentation, tracking, and analysis of sub-domain deformation in cardiac microbundles, under review (2023).
EvalCrafter Text-to-Video (ECTV) Dataset                                                                                  This dataset contains around 10000 videos generated by various methods using the Prompt list. These videos have been evaluated using the innovative EvalCrafter framework, which assesses generative models across visual, content, and motion qualities using 17 objective metrics and subjective user opinions.
FFHQ-Text                                                                                  **FFHQ-Text** is a small-scale face image dataset with large-scale facial attributes, designed for text-to-face generation & manipulation, text-guided facial image manipulation, and other vision-related tasks. This dataset is an extension of the [NVIDIA Flickr-Faces-HQ Dataset (FFHQ)](https://github.com/NVlabs/ffhq-dataset), which is the selected top **760 female FFHQ images** that only contain one complete human face.
FICS PCB Image Collection (FPIC)                                                                                  Optical images of printed circuit boards as well as detailed annotations of any text, logos, and surface-mount devices (SMDs). There are several hundred samples spanning a wide variety of manufacturing locations, sizes, node technology, applications, and more. - pcb_image: Optical images of each PCB surface and rear, tagged with a unique identifier. - color_checker: Pallette to account for environmental illumination factors as well as a scale reference for the photo resolution. Each pcb image indicates which color checker it is associated with. - ocr_annotation: Optical Character Recognition annotations. This includes polygon boundaries around all relevant text on a PCB image. Whether the piece of text is on the board or a device, whether it is a logo or not, orientation, and more are noted within the columns of the csv. - smd_annotation: Surface-mount Device (SMD) annotations. This includes polygon boundaries around all relevant SMD devices such as resistors, capacitors, inductors, transistors, diodes, LEDs, and more. Along with each component, its associated silkscreen designator ('L', 'R', 'C', 'U', etc.) is recorded. - vtp_annotation: Vias, traces, and pins (VTP) annotations. These are regions of connectivity between SMDs on a PCB. Few annotations currently exist, this is considered in 'beta' mode currently. - metadata: Holds two files corresponding to information about image files. * pcb.csv holds information about the physical PCB samples such as their color, online item description, and any notes. * color_checker.csv indicates the pixels per millimeter (ppmm) of any image associated with that color checker, whether an X-Rite ColorChecker Passport or Nano was used, what camera performed the acquisition, and any relevant notes. Each annotation file is designed to be compatible with the S3A application (https://gitlab.com/ficsresearch/s3a or https://pypi.org/project/s3a/), a Python tool for visualizing polygon annotations on an image.
FLAIR (French Land cover from Aerospace ImageRy)                                                                                  The French National Institute of Geographical and Forest Information (IGN) has the mission to document and measure land-cover on French territory and provides referential geographical datasets, including high-resolution aerial images and topographic maps. The monitoring of land-cover plays a crucial role in land management and planning initiatives, which can have significant socio-economic and environmental impact. Together with remote sensing technologies, artificial intelligence (IA) promises to become a powerful tool in determining land-cover and its evolution. IGN is currently exploring the potential of IA in the production of high-resolution land cover maps. Notably, deep learning methods are employed to obtain a semantic segmentation of aerial images. However, territories as large as France imply heterogeneous contexts: variations in landscapes and image acquisition make it challenging to provide uniform, reliable and accurate results across all of France. The FLAIR-one dataset presented is part of the dataset currently used at IGN to establish the French national reference land cover map 'Occupation du sol `a grande 'echelle' (OCS- GE). It covers 810 km² and has 13 semantic classes.
Fishnet Open Images                                                                                  **Fishnet Open Images Database** is a large dataset of EM imagery for fish detection and fine-grained categorisation onboard commercial fishing vessels. The dataset consists of 86,029 images containing 34 object classes, making it the largest and most diverse public dataset of fisheries EM imagery to-date. It includes many of the characteristic challenges of EM data: visual similarity between species, skewed class distributions, harsh weather conditions, and chaotic crew activity.
Flickr Audio Caption Corpus                                                                                  The Flickr 8k Audio Caption Corpus contains 40,000 spoken captions of 8,000 natural images. It was collected in 2015 to investigate multimodal learning schemes for unsupervised speech pattern discovery. For a description of the corpus, see: D. Harwath and J. Glass, 'Deep Multimodal Semantic Embeddings for Speech and Images,' 2015 IEEE Automatic Speech Recognition and Understanding Workshop, pp. 237-244, Scottsdale, Arizona, USA, December 2015
Fongbe audio                                                                                  Fongbe Data collected by Fréjus A. A LALEYE This dataset contains Fongbe speech corpus with audio data and transcriptions. Source: [Fongbe dataset](https://github.com/laleye/ALFFA_PUBLIC/tree/master/ASR/FONGBE)
Full-Spectral Autofluorescence Lifetime Microscopic Images                                                                                  - The dataset contains full-spectral autofluorescence lifetime microscopic images (FS-FLIM) acquired on unstained ex-vivo human lung tissue, where 100 4D hypercubes of 256x256 (spatial resolution) x 32 (time bins) x 512 (spectral channels from 500nm to 780nm). This dataset associates with our paper 'Deep Learning-Assisted Co-registration of Full-Spectral Autofluorescence Lifetime Microscopic Images with H&E-Stained Histology Images' (https://arxiv.org/abs/2202.07755) and 'Full spectrum fluorescence lifetime imaging with 0.5 nm spectral and 50 ps temporal resolution' (https://doi.org/10.1038/s41467-021-26837-0). - The FS-FLIM images provide transformative insights into human lung cancer with extra-dimensional information. This will enable visual and precise detection of early lung cancer. With the methodology in our co-registration paper, FS-FLIM images can be registered with H&E-stained histology images, allowing characterisation of tumour and surrounding cells at a celluar level with absolute autofluorescence lifetime. - The dataset can be used for various purposes, including signal processing for optimal lifetime reconstruction, advanced image analysis for automatic feature extraction of lung cancer, and cellular-level characterisation of lung cancer with absolute label-free autofluorescence lifetime values. - The dataset is available on the University of Edinburgh's DataShare (https://doi.org/10.7488/ds/3099 and https://doi.org/10.7488/ds/3421)
FullTextPeerRead                                                                                  FullTextPeerRead is a dataset created by Jeong et al. for context-aware citation recommendation. It contains context sentences to cited references and paper metadata, which makes it a well-organized dataset for a context-aware paper recommendation. From paper: A context-aware citation recommendation model with BERT and graph convolutional networks
GLIB: image dataset                                                                                  data/images: data/images/Base : 132 screenshots of game1 & game2 with UI display issues from 466 test reports. data/images/Code : 9,412 screenshots of game1 & game2 with UI display issues generated by our Code augmentation method. data/images/Normal: 7,750 screenshots of game1 & game2 without UI display issues collected by randomly traversing the game scene. data/images/Rule(F) : 7,750 screenshots of game1 & game2 with UI display issues generated by our Rule(F) augmentation method. data/images/Rule(R) : 7,750 screenshots of game1 & game2 with UI display issues generated by our Rule(R) augmentation method. data/images/testDataSet : 192 screenshots with UI display issues from 466 test reports(exclude game1 & game2). data/data_csv: data/data_csv/Base : dataset for baseline method. data/data_csv/Code : dataset for our Code Augmentation method. data/data_csv/Rule(F) : dataset for our Rule(F) Augmentation method. data/data_csv/Rule(R) : dataset for our Rule(R) Augmentation method. data/data_csv/Code_plus_Rule(F) : dataset for our Code&Rule(F) Augmentation method. data/data_csv/Code_plus_Rule(R) : dataset for our Code&Rule(R) Augmentation method. data/data_csv/testDataSet : test dataset(normal image and real glitch images from 466 test reports).
GOD-Wiki, DIR-Wiki, ThingsEEG-Text.                                                                                  brain-image-text trimodal datasets
Grapevine Leaves Image Dataset                                                                                  KOKLU Murat (a), UNLERSEN M. Fahri (b), OZKAN Ilker Ali (a), ASLAN M. Fatih(c), SABANCI Kadir (c) (a) Department of Computer Engineering, Selcuk University, Turkey, Konya, Turkey (b) Department of Electrical and Electronics Engineering, Necmettin Erbakan University, Konya, Turkey (c) Department of Electrical-Electronic Engineering, Karamanoglu Mehmetbey University, Karaman, Turkey Citation Request : Koklu, M., Unlersen, M. F., Ozkan, I. A., Aslan, M. F., & Sabanci, K. (2022). A CNN-SVM study based on selected deep features for grapevine leaves classification. Measurement, 188, 110425. Doi:https://doi.org/10.1016/j.measurement.2021.110425 Link: https://doi.org/10.1016/j.measurement.2021.110425 https://www.kaggle.com/mkoklu42 DATASET: https://www.muratkoklu.com/datasets/ Highlights • Classification of five classes of grapevine leaves by MobileNetv2 CNN Model. • Classification of features using SVMs with different kernel functions. • Implementing a feature selection algorithm for high classification percentage. • Classification with highest accuracy using CNN-SVM Cubic model. Abstract: The main product of grapevines is grapes that are consumed fresh or processed. In addition, grapevine leaves are harvested once a year as a by-product. The species of grapevine leaves are important in terms of price and taste. In this study, deep learning-based classification is conducted by using images of grapevine leaves. For this purpose, images of 500 vine leaves belonging to 5 species were taken with a special self-illuminating system. Later, this number was increased to 2500 with data augmentation methods. The classification was conducted with a state-of-art CNN model fine-tuned MobileNetv2. As the second approach, features were extracted from pre-trained MobileNetv2′s Logits layer and classification was made using various SVM kernels. As the third approach, 1000 features extracted from MobileNetv2′s Logits layer were selected by the Chi-Squares method and reduced to 250. Then, classification was made with various SVM kernels using the selected features. The most successful method was obtained by extracting features from the Logits layer and reducing the feature with the Chi-Squares method. The most successful SVM kernel was Cubic. The classification success of the system has been determined as 97.60%. It was observed that feature selection increased the classification success although the number of features used in classification decreased. Keywords: Deep learning, Transfer learning, SVM, Grapevine leaves, Leaf identification
Grounded Image-Text dataset                                                                                  Click to add a brief description of the dataset (Markdown and LaTeX enabled). Provide: * a high-level explanation of the dataset characteristics * explain motivations and summary of its content * potential use cases of the dataset
HOPE-Image                                                                                  The NVIDIA HOPE datasets consist of RGBD images and video sequences with labeled 6-DoF poses for 28 toy grocery objects. The toy grocery objects are readily available for purchase and have ideal size and weight for robotic manipulation. 3D textured meshes for generating synthetic training data are provided. The HOPE-Image dataset shows the objects in 50 scenes from 10 household/office environments, and contains 188 test images taken in 8 environments, with a total of 40 scenes (unique camera and object poses). Up to 5 lighting variations are captured for each scene, including backlighting and angled direct lighting with cast shadows. Scenes are cluttered with varying levels of occlusion. An additional 50 validation images are included from 2 environments in 10 scene arrangements. Within each scene, up to 5 lighting variations are captured with the same camera and object poses. For example, the captures in `valid/scene_0000/*.json` all depict the same camera pose and arrangement of objects, but each individual capture (0000.json, 0001.json, ...) has a different lighting condition. For this reason, each image should be treated independently for purposes of pose prediction. The most favorable lighting condition for each scene is found in `image 0000.json`. Images were captured using a RealSense D415 RGBD camera. Systematic errors were observed in the depth values relative to the estimated distance of a calibration grid. To correct for this, depth frames are scaled by a factor of 0.98042517 before registering to RGB. Annotations were made manually using these corrected RGBD frames. NOTE: Only validation set annotations are included. Test annotations are managed by the BOP challenge.
HierText                                                                                  HierText is the first dataset featuring hierarchical annotations of text in natural scenes and documents. The dataset contains 11639 images selected from the [Open Images dataset](https://storage.googleapis.com/openimages/web/index.html), providing high quality word (~1.2M), line, and paragraph level annotations. Text lines are defined as connected sequences of words that are aligned in spatial proximity and are logically connected. Text lines that belong to the same semantic topic and are geometrically coherent form paragraphs. Images in HierText are rich in text, with average of more than 100 words per image.
High Altitude Georeferenced UAV Images                                                                                  The dataset contains 179 photographs taken by a UAV flying at 120 meters altitude. The photographs are high resolution georeferenced (altitude and lontitude) orthoimages. It was originally used for developing a visual-based localization algorithm for UAVs. The dataset could be used for training machine learning models for localization purposes or building maps.
Hindi Text Image Dataset | Hindi in the wild                                                                                  This dataset is an extremely challenging set of over 5000+ original Hindi text images captured and crowdsourced from over 700+ urban and rural areas, where each image is **manually reviewed and verified** by computer vision professionals at DataclusterLabs. ### **Dataset Features** - Dataset size : 5000+ - Captured by : Over 700+ crowdsource contributors - Resolution : 99% images HD and above (1920x1080 and above) - Location : Captured with 400+ cities accross India - Diversity : Various lighting conditions like day, night, varied distances, view points etc. - Device used : Captured using mobile phones in 2021-2022 - Usage : Hindi text detection, Hindi NLP, Text recognition, etc. ### Available Annotation formats COCO, YOLO, PASCAL-VOC, Tf-Record **To download full datasets or to submit a request for your dataset needs, please ping us at [sales@datacluster.ai](sales@datacluster.ai) Visit [www.datacluster.ai](www.datacluster.ai) to know more.** **Note**: All the images are manually captured and verified by a large contributor base on DataCluster platform
Historical Color Image Dataset                                                                                  The historical color image dataset is collected for the task of automatically estimating the age of historical color photos. Each image is annotated with its associated decade, where five decades from the 1930s to 1970s are considered. There are 265 images for each category Image Source: [Dating Historical Color Images](https://link.springer.com/chapter/10.1007/978-3-642-33783-3_36)
Human Protein Atlas Image                                                                                  
Human Wrist Image Dataset | Human Body Parts                                                                                  This dataset consists of images of wrist (with different kind of bands on it). ### **Introduction** Dataset consists of images of wrist captured using mobile phones in real-world scenario. Images were captured under wide variety of lighting conditions, weather, indoor and outdoor. This dataset can be used for Augmented Reality, Mixed Reality, Rakhi Detection, Wrist-watch Detection, Hand-band Detection, etc. ### **Dataset Features** - Captured by 3000+ unique users - Rich in diversity - Mobile phone view point - Various items on the wrist - Consists male and female wrists - HD Resolution - Various lighting conditions - Indoor and Outdoor scene ### **Dataset Features** - Classification and detection annotations available - Multiple category annotations possible - COCO, PASCAL VOC and YOLO formats **To download full datasets or to submit a request for your dataset needs, please ping us at [sales@datacluster.ai](sales@datacluster.ai) Visit [www.datacluster.ai](www.datacluster.ai) to know more.** **Note**: All the images are manually captured and verified by a large contributor base on DataCluster platform
Human-Annotated Sense-Disambiguated Word Contexts for Russian                                                                                  This dataset contains human-annotated sense identifiers for 2562 contexts of 20 words used in the RUSSE'2018 shared task on Word Sense Induction and Disambiguation for the Russian language. Assembled by Dmitry Ustalov in 2017. In particular, 80 pre-annotated contexts were used for training the human annotators, and 2562 contexts were annotated by humans such that each context was annotated by 9 different annotators. After the annotation, every context was additionally inspected (“curated”) by the organizers of the shared task.
INRIA Aerial Image Labeling                                                                                  The **INRIA Aerial Image Labeling** dataset is comprised of 360 RGB tiles of 5000×5000px with a spatial resolution of 30cm/px on 10 cities across the globe. Half of the cities are used for training and are associated to a public ground truth of building footprints. The rest of the dataset is used only for evaluation with a hidden ground truth. The dataset was constructed by combining public domain imagery and public domain official building footprints. Source: [Distance transform regression for spatially-aware deep semantic segmentation](https://arxiv.org/abs/1909.01671) Image Source: [https://project.inria.fr/aerialimagelabeling/](https://project.inria.fr/aerialimagelabeling/)
IRFL: Image Recognition of Figurative Language                                                                                  The IRFL dataset consists of idioms, similes, and metaphors with matching figurative and literal images, as well as two novel tasks of multimodal figurative understanding and preference. We collected figurative and literal images for textual idioms, metaphors, and similes using an automatic pipeline we created (idioms) and manually (metaphors + similes). We annotated the relations between these images and the figurative phrase they originated from. Using these images we created two novel tasks of figurative understanding and preference. The figurative understanding task evaluates Vision and Language Pre-Trained Models’ (VL-PTMs) ability to understand the relation between an image and a figurative phrase. The task is to choose the image that best visualizes the figurative phrase out of X candidates. The preference task examines VL-PTMs' preference for figurative images. In this task, the model needs to classify phrase images of different categories correctly based on their ranking by the model matching score. The best models achieve 22%, 30%, and 66% accuracy vs. humans 97%, 99.7%, and 100% on our understanding task for idioms, metaphors, and similes respectively. The best model achieved an F1 score of 61 on the preference task. Researchers are welcome to evaluate models on this dataset.
ISM Images                                                                                  Click to add a brief description of the dataset (Markdown and LaTeX enabled). https://github.com/ajaygunalan/BrightEyes-ISM/tree/main/compressive_ism/data Provide: * a high-level explanation of the dataset characteristics * explain motivations and summary of its content * potential use cases of the dataset
Image Aesthetics dataset                                                                                  The image aesthetic benchmark [18] consists of 10800 Flickr photos of four categories, i.e., “animals”, “urban”, “people” and “nature”, and is constructed originally to retrieve beautiful yet unpopular images in social networks. The ground truths of the photos in the benchmark are five aesthetic grades: “Unacceptable” - images with extremely low quality, out of focus or underexposed, “Flawed” - images with some technical flaws and without any artistic value, “Ordinary” - standard quality images without technical flaws, “Professional” - professional-quality images with some artistic value, and “Exceptional” - very appealing images showing both outstanding professional quality and high artistic value. Image Source: [OrdinalCLIP](https://arxiv.org/pdf/2206.02338.pdf)
Image Caption Quality Dataset                                                                                  Image Caption Quality Dataset is a dataset of crowdsourced ratings for machine-generated image captions. It contains more than 600k ratings of image-caption pairs. Source: [https://arxiv.org/abs/1909.03396](https://arxiv.org/abs/1909.03396)
Image Captioning on COCO Captions                                                                                  
Image Description Sequences                                                                                  A dataset of description sequences, a sequence of expressions that together are meant to single out one image from an (imagined) set of other similar images. These sequences were produced in a monological setting, but with the instruction to imagine they were provided to a partner who successively asked for more information (hence, tell me more).
Image Editing Request Dataset                                                                                  A new language-guided image editing dataset that contains a large number of real image pairs with corresponding editing instructions. Source: [Expressing Visual Relationships via Language](/paper/expressing-visual-relationships-via-language)
Image Memorability                                                                                  A database of images with measured probabilities that each picture will be remembered after a single view. Source: [Image Memorability](http://web.mit.edu/phillipi/Public/WhatMakesAnImageMemorable/)
Image Paragraph Captioning                                                                                  The Image Paragraph Captioning dataset allows researchers to benchmark their progress in generating paragraphs that tell a story about an image. The dataset contains 19,561 images from the [Visual Genome dataset](https://paperswithcode.com/dataset/visual-genome). Each image contains one paragraph. The training/val/test sets contains 14,575/2,487/2,489 images. Since all the images are also part of the Visual Genome dataset, each image also contains 50 region descriptions (short phrases describing parts of an image), 35 objects, 26 attributes and 21 relationships and 17 question-answer pairs. Source: [A Hierarchical Approach for Generating Descriptive Image Paragraphs](/paper/a-hierarchical-approach-for-generating) Image Source: [https://cs.stanford.edu/people/ranjaykrishna/im2p/index.html](https://cs.stanford.edu/people/ranjaykrishna/im2p/index.html)
Image and Video Advertisements                                                                                  The **Image and Video Advertisements** collection consists of an image dataset of 64,832 image ads, and a video dataset of 3,477 ads. The data contains rich annotations encompassing the topic and sentiment of the ads, questions and answers describing what actions the viewer is prompted to take and the reasoning that the ad presents to persuade the viewer ('What should I do according to this ad, and why should I do it? '), and symbolic references ads make (e.g. a dove symbolizes peace). Source: [Automatic Understanding of Image and Video Advertisements](/paper/automatic-understanding-of-image-and-video)
Image-Chat                                                                                  The IMAGE-CHAT dataset is a large collection of (image, style trait for speaker A, style trait for speaker B, dialogue between A & B) tuples that we collected using crowd-workers, Each dialogue consists of consecutive turns by speaker A and B. No particular constraints are placed on the kinds of utterance, only that we ask the speakers to both use the provided style trait, and to respond to the given image and dialogue history in an engaging way. The goal is not just to build a diagnostic dataset but a basis for training models that humans actually want to engage with.
Image-based size estimation of broccoli heads under varying degrees of occlusion                                                                                  This publicly available dataset contains 1613 RGB-D images of field-grown broccoli plants. The dataset also includes the polygon and circle annotations of the broccoli heads. The broccoli heads in the images were subject to various degrees of natural and man-made leaf occlusion. The images were acquired in July and August 2020 on a broccoli field in Sexbierum (The Netherlands). The broccoli cultivar was Ironman. The dataset belongs to the paper “Image-based size estimation of broccoli heads under varying degrees of occlusion”. This paper has been published at Biosystems Engineering journal: https://doi.org/10.1016/j.biosystemseng.2021.06.001Click to add a brief description of the dataset (Markdown and LaTeX enabled). Provide: * a high-level explanation of the dataset characteristics * explain motivations and summary of its content * potential use cases of the dataset
ImageCLEF-DA                                                                                  The **ImageCLEF-DA** dataset is a benchmark dataset for ImageCLEF 2014 domain adaptation challenge, which contains three domains: Caltech-256 (C), ImageNet ILSVRC 2012 (I) and Pascal VOC 2012 (P). For each domain, there are 12 categories and 50 images in each category. Source: [Domain-Symmetric Networks for Adversarial Domain Adaptation](https://arxiv.org/abs/1904.04663) Image Source: [https://www.imageclef.org/2014/adaptation](https://www.imageclef.org/2014/adaptation)
ImageCoDe                                                                                  Given 10 minimally contrastive (highly similar) images and a complex description for one of them, the task is to retrieve the correct image. The source of most images are videos and descriptions as well as retrievals come from human.
ImageNet                                                                                  The **ImageNet** dataset contains 14,197,122 annotated images according to the WordNet hierarchy. Since 2010 the dataset is used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a benchmark in image classification and object detection. The publicly released dataset contains a set of manually annotated training images. A set of test images is also released, with the manual annotations withheld. ILSVRC annotations fall into one of two categories: (1) image-level annotation of a binary label for the presence or absence of an object class in the image, e.g., “there are cars in this image” but “there are no tigers,” and (2) object-level annotation of a tight bounding box and class label around an object instance in the image, e.g., “there is a screwdriver centered at position (20,25) with width of 50 pixels and height of 30 pixels”. The ImageNet project does not own the copyright of the images, therefore only thumbnails and URLs of images are provided. * Total number of non-empty WordNet synsets: 21841 * Total number of images: 14197122 * Number of images with bounding box annotations: 1,034,908 * Number of synsets with SIFT features: 1000 * Number of images with SIFT features: 1.2 million Source: [ImageNet Large Scale Visual Recognition Challenge](https://arxiv.org/abs/1409.0575) Image Source: [https://cs.stanford.edu/people/karpathy/cnnembed/](https://cs.stanford.edu/people/karpathy/cnnembed/)
ImageNet (Fine-grained 6 Tasks)                                                                                  
ImageNet (finetuned)                                                                                  
ImageNet (linear)                                                                                  
ImageNet (non-targeted PGD, max perturbation=4)                                                                                  
ImageNet (targeted PGD, max perturbation=16)                                                                                  
ImageNet - 0-Shot                                                                                  
ImageNet - 0.2% labeled data                                                                                  
ImageNet - 1 labeled data per class                                                                                  
ImageNet - 1% labeled data                                                                                  
ImageNet - 1-shot                                                                                  
ImageNet - 10 steps                                                                                  
ImageNet - 10% labeled data                                                                                  
ImageNet - 10-shot                                                                                  
ImageNet - 2 labeled data per class                                                                                  
ImageNet - 5 labeled data per class                                                                                  
ImageNet - 5-shot                                                                                  
ImageNet - 500 classes + 10 steps of 50 classes                                                                                  
ImageNet - 500 classes + 5 steps of 100 classes                                                                                  
ImageNet - 500 classes + 50 steps of 10 classes                                                                                  
ImageNet - ResNet 50 - 90% sparsity                                                                                  
ImageNet 128x128                                                                                  
ImageNet 32x32                                                                                  
ImageNet 50 samples per class                                                                                  This ImageNet version contains only 50 training images per class while the original testing set remains unchanged. It is one of the datasets comprising the data-efficient image classification (DEIC) benchmark. It was proposed to challenge the generalization capabilities of modern image classifiers.
ImageNet 512x512                                                                                  
ImageNet 64x64                                                                                  
ImageNet C-OOD (class-out-of-distribution)                                                                                  This dataset was presented as part of the ICLR 2023 paper 𝘈 𝘧𝘳𝘢𝘮𝘦𝘸𝘰𝘳𝘬 𝘧𝘰𝘳 𝘣𝘦𝘯𝘤𝘩𝘮𝘢𝘳𝘬𝘪𝘯𝘨 𝘊𝘭𝘢𝘴𝘴-𝘰𝘶𝘵-𝘰𝘧-𝘥𝘪𝘴𝘵𝘳𝘪𝘣𝘶𝘵𝘪𝘰𝘯 𝘥𝘦𝘵𝘦𝘤𝘵𝘪𝘰𝘯 𝘢𝘯𝘥 𝘪𝘵𝘴 𝘢𝘱𝘱𝘭𝘪𝘤𝘢𝘵𝘪𝘰𝘯 𝘵𝘰 𝘐𝘮𝘢𝘨𝘦𝘕𝘦𝘵. It is a framework that, based on this dataset (a subset of the ImageNet-21k dataset) is able to generate a C-OOD (AKA open-set recognition) benchmark that covers a variety of difficulty levels. these benchmarks are tailored to the evaluated model. This approach provides a more accurate representation of the model’s own performance. The resulting difficulty levels of our framework allow benchmarking with respect to the difficulty levels most relevant to the task. For example, for a task with a high tolerance for risk (e.g., a task for an entertainment application), the performance of a model on a median difficulty level might be more important than on the hardest difficulty level (severity 10). The opposite might be true for some applications with a low tolerance for risk (e.g., medical applications), for which one requires the best performance to be attained even if the OOD is very hard to detect (severity 10). The paper in which the framework was introduced showed that detection algorithms do not always improve performance on all inputs equally, and could even hurt performance for specific difficulty levels and models. Choosing the combination of (model, detection algorithm) based only on the detection performance on all data may yield sub-optimal results for our specific desired level of difficulty.
ImageNet Detection                                                                                  
ImageNet ReaL                                                                                  
ImageNet ResNet-50 - 50 Epochs                                                                                  
ImageNet ResNet-50 - 60 Epochs                                                                                  
ImageNet ResNet-50 - 90 Epochs                                                                                  
ImageNet V2                                                                                  
ImageNet VID                                                                                  ImageNet VID is a large-scale public dataset for video object detection and contains more than 1M frames for training and more than 100k frames for validation.
ImageNet VIPriors subset                                                                                  The training and validation data are subsets of the training split of the Imagenet 2012. The test set is taken from the validation split of the Imagenet 2012 dataset. Each data set includes 50 images per class.
ImageNet ctest10k                                                                                  Colorization validation set for unconditional/conditional colorization tasks. Subset of the ImageNet validation images and excludes andy grayscale single-channel images.
ImageNet-10                                                                                  
ImageNet-100                                                                                  
ImageNet-100 - 50 classes + 10 steps of 5 classes                                                                                  
ImageNet-100 - 50 classes + 25 steps of 2 classes                                                                                  
ImageNet-100 - 50 classes + 5 steps of 10 classes                                                                                  
ImageNet-100 - 50 classes + 50 steps of 1 class                                                                                  
ImageNet-1K                                                                                  The ImageNet1K dataset, also known as ILSVRC 2012, is a subset of the larger ImageNet dataset. It is commonly used for pretraining deep learning models for computer vision tasks. Here are some key details about the ImageNet1K dataset: 1. It spans 1000 object classes. 2. It contains 1,281,167 training images, 50,000 validation images, and 100,000 test images. 3. The images in the dataset are organized according to the WordNet hierarchy. 4. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a “synonym set” or “synset”. ImageNet aims to provide on average 1000 images to illustrate each synset. 5. The images of each concept are quality-controlled and human-annotated.
ImageNet-1k vs Curated OODs (avg.)                                                                                  
ImageNet-1k vs NINCO                                                                                  The NINCO (No ImageNet Class Objects) dataset is introduced in the ICML 2023 paper In or Out? Fixing ImageNet Out-of-Distribution Detection Evaluation. The images in this dataset are free from objects that belong to any of the 1000 classes of ImageNet-1K (ILSVRC2012), which makes NINCO suitable for evaluating out-of-distribution detection on ImageNet-1K . The NINCO main dataset consists of 64 OOD classes with a total of 5879 samples. These OOD classes were selected to have no categorical overlap with any classes of ImageNet-1K. Each sample was inspected individually by the authors to not contain ID objects. Besides NINCO, included are (in the same .tar.gz file) truly OOD versions of 11 popular OOD datasets with in total 2715 OOD samples. Further included are 17 OOD unit-tests, with 400 samples each. Code for loading and evaluating on each of the three datasets is provided at https://github.com/j-cb/NINCO. When using NINCO, please consider citing (besides the bibtex given below) the following data sources that were used to create NINCO: Hendrycks et al.: ”Scaling out-of-distribution detection for real-world settings”, ICML, 2022. Bossard et al.: ”Food-101 – mining discriminative components with random forests”, ECCV 2014. Zhou et al.: ”Places: A 10 million image database for scene recognition”, IEEE PAMI 2017. Huang et al.: ”Mos: Towards scaling out-of-distribution detection for large semantic space”, CVPR 2021. Li et al.: ”Caltech 101 (1.0)”, 2022. Ismail et al.: ”MYNursingHome: A fully-labelled image dataset for indoor object classification.”, Data in Brief (V. 32) 2020. The iNaturalist project: https://www.inaturalist.org/ When using NINCO_popular_datasets_subsamples, additionally to the above, please consider citing: Cimpoi et al.: ”Describing textures in the wild”, CVPR 2014. Hendrycks et al.: ”Natural adversarial examples”, CVPR 2021. Wang et al.: ”Vim: Out-of-distribution with virtual-logit matching”, CVPR 2022. Bendale et al.: ”Towards Open Set Deep Networks”, CVPR 2016. Vaze et al.: ”Open-set Recognition: a Good Closed-set Classifier is All You Need?”, ICLR 2022. Wang et al.: ”Partial and Asymmetric Contrastive Learning for Out-of-Distribution Detection in Long-Tailed Recognition.” ICML, 2022. Galil et al.: “A framework for benchmarking Class-out-of-distribution detection and its application to ImageNet”, ICLR 2023.
ImageNet-1k vs OpenImage-O                                                                                  OpenImage-O is built for the ID dataset ImageNet-1k. It is manually annotated, comes with a naturally diverse distribution, and has a large scale. It is built to overcome several shortcomings of existing OOD benchmarks. OpenImage-O is image-by-image filtered from the test set of OpenImage-V3, which has been collected from Flickr without a predefined list of class names or tags, leading to natural class statistics and avoiding an initial design bias.
ImageNet-1k vs Places                                                                                  A benchmark dataset for out-of-distribution detection. ImageNet-1k is in-distribution, while Places is out-of-distribution.
ImageNet-1k vs SUN                                                                                  A benchmark dataset for out-of-distribution detection. ImageNet-1k is in-distribution, while SUN is out-of-distribution.
ImageNet-1k vs Textures                                                                                  A benchmark dataset for out-of-distribution detection. ImageNet-1k is in-distribution, while Textures is out-of-distribution.
ImageNet-1k vs iNaturalist                                                                                  A benchmark dataset for out-of-distribution detection. ImageNet-1k is in-distribution, while iNaturalist is out-of-distribution.
ImageNet-32                                                                                  Imagenet32 is a huge dataset made up of small images called the down-sampled version of Imagenet. Imagenet32 is composed of 1,281,167 training data and 50,000 test data with 1,000 labels. Source: [Self-supervised Knowledge Distillation Using Singular Value Decomposition](https://arxiv.org/abs/1807.06819) Image Source: [https://arxiv.org/pdf/1707.08819v3.pdf](https://arxiv.org/pdf/1707.08819v3.pdf)
ImageNet-50 (5 tasks)                                                                                   
ImageNet-64                                                                                  Imagenet64 is a massive dataset of small images called the down-sampled version of Imagenet. Imagenet64 comprises 1,281,167 training data and 50,000 test data with 1,000 labels. Source: [A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets](https://arxiv.org/abs/1707.08819v3) Image Source: [https://patrykchrabaszcz.github.io/Imagenet32/](https://patrykchrabaszcz.github.io/assets/img/Imagenet32/64x64.png)
ImageNet-9                                                                                  ImageNet-9 consists of images with different amounts of background and foreground signal, which you can use to measure the extent to which your models rely on image backgrounds. This dataset is helpful in testing the robustness of vision models with respect to their dependence on the backgrounds of images.
ImageNet-A                                                                                  The **ImageNet-A** dataset consists of real-world, unmodified, and naturally occurring examples that are misclassified by ResNet models. Source: [On Robustness and Transferability of Convolutional Neural Networks](https://arxiv.org/abs/2007.08558) Image Source: [https://github.com/hendrycks/natural-adv-examples](https://github.com/hendrycks/natural-adv-examples)
ImageNet-Atr                                                                                  We build a new evaluation set by adding spotting words to the images of ImageNet 2012 evaluation sets. There are 1,000 categories in ImageNet. For each category c, we find its most confusing category c*and spot the category name to every evaluation image. This evaluation set is challenging for many CLIP models. For example, OpenAI CLIP B-16 got a top-1 accuracy of as low as 32%, which is much lower than the original ImageNet evaluation set.
ImageNet-B                                                                                  We introduce diverse and realistic backgrounds into the images or color, texture, and adversarial changes in the background
ImageNet-C                                                                                  **ImageNet-C** is an open source data set that consists of algorithmically generated corruptions (blur, noise) applied to the ImageNet test-set. Source: [Selective Brain Damage: Measuring the Disparate Impact of Model Pruning](https://arxiv.org/abs/1911.05248) Image Source: [https://arxiv.org/pdf/1807.01697.pdf](https://arxiv.org/pdf/1807.01697.pdf)
ImageNet-Caltech                                                                                  
ImageNet-D                                                                                  ImageNet-D contains 4835 test images featuring diverse backgrounds (3,764), textures (498), and materials (573). Generated by diffusion models, ImageNet-D achieves superior image fidelity and collection efficiency than prior studies. Evaluation results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60%.
ImageNet-Hard                                                                                  ImageNet-Hard is a new benchmark that comprises 10,980 images collected from various existing ImageNet-scale benchmarks (ImageNet, ImageNet-V2, ImageNet-Sketch, ImageNet-C, ImageNet-R, ImageNet-ReaL, ImageNet-A, and ObjectNet). This dataset poses a significant challenge to state-of-the-art vision models as merely zooming in often fails to improve their ability to classify images correctly. As a result, even the most advanced models, such as CLIP-ViT-L/14@336px, struggle to perform well on this dataset, achieving a mere 2.02% accuracy.
ImageNet-LT                                                                                  **ImageNet Long-Tailed** is a subset of [/dataset/imagenet](ImageNet) dataset consisting of 115.8K images from 1000 categories, with maximally 1280 images per class and minimally 5 images per class. The additional classes of images in ImageNet-2010 are used as the open set. Source: [Large-Scale Long-Tailed Recognition in an Open World](https://arxiv.org/pdf/1904.05160v2.pdf)
ImageNet-LT-d                                                                                  
ImageNet-O                                                                                  ImageNet-O consists of images from classes that are not found in the ImageNet-1k dataset. It is used to test the robustness of vision models to out-of-distribution samples. It's reported using the AUPR metric.
ImageNet-P                                                                                  **ImageNet-P** consists of noise, blur, weather, and digital distortions. The dataset has validation perturbations; has difficulty levels; has CIFAR-10, Tiny ImageNet, ImageNet 64 × 64, standard, and Inception-sized editions; and has been designed for benchmarking not training networks. ImageNet-P departs from ImageNet-C by having perturbation sequences generated from each ImageNet validation image. Each sequence contains more than 30 frames, so to counteract an increase in dataset size and evaluation time only 10 common perturbations are used. Source: [Benchmarking Neural Network Robustness to Common Corruptions and Perturbations](https://arxiv.org/pdf/1903.12261.pdf)
ImageNet-Patch                                                                                  ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness against Adversarial Patches Adversarial patches are optimized contiguous pixel blocks in an input image that cause a machine-learning model to misclassify it. However, their optimization is computationally demanding, and requires careful hyperparameter tuning, potentially leading to suboptimal robustness evaluations. To overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machine-learning models against adversarial patches. It consists of a set of patches, optimized to generalize across different models, and readily applicable to ImageNet data after preprocessing them with affine transformations. This process enables an approximate yet faster robustness evaluation, leveraging the transferability of adversarial perturbations.
ImageNet-R                                                                                  ImageNet-R(endition) contains art, cartoons, deviantart, graffiti, embroidery, graphics, origami, paintings, patterns, plastic objects, plush objects, sculptures, sketches, tattoos, toys, and video game renditions of ImageNet classes. ImageNet-R has renditions of 200 ImageNet classes resulting in 30,000 images. Source: [The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization](/paper/the-many-faces-of-robustness-a-critical)
ImageNet-S                                                                                  Powered by the ImageNet dataset, unsupervised learning on large-scale data has made significant advances for classification tasks. There are two major challenges to allowing such an attractive learning modality for segmentation tasks: i) a large-scale benchmark for assessing algorithms is missing; ii) unsupervised shape representation learning is difficult. We propose a new problem of large-scale unsupervised semantic segmentation (LUSS) with a newly created benchmark dataset to track the research progress. Based on the ImageNet dataset, we propose the ImageNet-S dataset with 1.2 million training images and 50k high-quality semantic segmentation annotations for evaluation. Our benchmark has a high data diversity and a clear task objective. We also present a simple yet effective baseline method that works surprisingly well for LUSS. In addition, we benchmark related un/weakly/fully supervised methods accordingly, identifying the challenges and possible directions of LUSS.
ImageNet-S-300                                                                                  
ImageNet-S-50                                                                                  
ImageNet-Sketch                                                                                  ImageNet-Sketch data set consists of 50,889 images, approximately 50 images for each of the 1000 ImageNet classes. The data set is constructed with Google Image queries 'sketch of __', where __ is the standard class name. Only within the 'black and white' color scheme is searched. 100 images are initially queried for every class, and the pulled images are cleaned by deleting the irrelevant images and images that are for similar but different classes. For some classes, there are less than 50 images after manually cleaning, and then the data set is augmented by flipping and rotating the images. Source: [ImageNet-Sketch](https://github.com/HaohanWang/ImageNet-Sketch) Image Source: [https://github.com/HaohanWang/ImageNet-Sketch](https://github.com/HaohanWang/ImageNet-Sketch)
ImageNet-VidVRD                                                                                  ImageNet-VidVRD dataset contains 1,000 videos selected from ILVSRC2016-VID dataset based on whether the video contains clear visual relations. It is split into 800 training set and 200 test set, and covers common subject/objects of 35 categories and predicates of 132 categories. Ten people contributed to labeling the dataset, which includes object trajectory labeling and relation labeling. Since the ILVSRC2016-VID dataset has the object trajectory annotation for 30 categories already, we supplemented the annotations by labeling the remaining 5 categories. In order to save the labor of relation labeling, we labeled typical segments of the videos in the training set and the whole of the videos in the test set.
ImageNet-W                                                                                  ImageNet-W(atermark) is a test set to evaluate models’ reliance on the newly found watermark shortcut in ImageNet, which is used to predict the *carton* class. ImageNet-W is created by overlaying transparent watermarks on the ImageNet validation set. Two metrics are used to evaluate watermark shortcut reliance: (1) IN-W Gap: the top-1 accuracy drop from ImageNet to ImageNet-W, (2) Carton Gap: carton class accuracy increase from ImageNet to ImageNet-W. Combining ImageNet-W with previous out-of-distribution variants of ImageNet (e.g., Stylized ImageNet, ImageNet-R, ImageNet-9) forms a comprehensive suite of multi-shortcut evaluation on ImageNet.
ImageNet-X                                                                                  **ImageNet-X** is a set of human annotations pinpointing failure types for the popular ImageNet dataset. ImageNet-X labels distinguishing object factors such as pose, size, color, lighting, occlusions, co-occurences, etc. for each image in the validation set and a random subset of 12,000 training samples. It is designed to study the types of mistakes as a function of model's architecture, learning paradigm, and training procedures.
ImageNet-to-Bing                                                                                  
ImageNet100                                                                                  
ImageNet100 - 10 steps                                                                                  
ImageNet32                                                                                  
ImageNet64x64                                                                                  
ImageNetV2                                                                                  
ImageNet_CN                                                                                  transform the ImageNet-1K classification datatset for Chinese models by translating labels and prompts into Chinese.
ImageTBAD                                                                                  A dataset of A 3D Computed Tomography (CT) image dataset, ImageTBAD, for segmentation of Type-B Aortic Dissection is published. ImageTBAD contains 100 3D Computed Tomography (CT) images, which is of decent size compared with existing medical imaging datasets. ImageTBAD contains a total of 100 3D CTA images gathered from Guangdong Peoples' Hospital Data from January 1,2013 to April 23, 2019. Images are acquired from a variety of scanners (GE Medical Systems, Siemens, Philips), resulting in large variance in voxel size, resolution and imaging quality. All the images are pre-operative TBAD CTA images whose top and bottom are around the neck and the brachiocephalic vessels, respectively, in the axial view. The segmentation labeling is performed by a team of two cardiovascular radiologists who have extensive experience with TBAD. The segmentation labeling of each patient is fulfilled by one radiologist and checked by the other. The segmentation
Imagenet-dog-15                                                                                  
Imagenette                                                                                  **Imagenette** is a subset of 10 easily classified classes from Imagenet (bench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute). Source: [https://github.com/fastai/imagenette](https://github.com/fastai/imagenette) Image Source: [https://docs.fast.ai/tutorial.imagenette.html](https://docs.fast.ai/tutorial.imagenette.html)
Images from camera traps in the Jura and Ain counties (France)                                                                                  This dataset contains images taken from camera traps set up in the Jura and Ain counties in France. We use this dataset to illustrate the training of a deep learning algorithm with application to animal specie sidentification. See more here https://github.com/oliviergimenez/computo-deeplearning-occupany-lynx.
Images of Public Streetlights with Operational Monitoring using Computer Vision Techniques                                                                                  This dataset consists of ~350k JPEG images of streetlight columns installed on a public road infrastructure located in the city of Bristol, UK. Each streetlight is photographed by a Raspberry Pi Camera Module v1, installed on each lamppost, providing a unique camera placement, photographic angle, and distance from the streetlight. Several streetlights are partially obstructed by vegetation or are outside the Field of View (FoV) of the Raspberry Pi camera. Finally, the cameras facing the sky are susceptible to weather conditions (e.g., rain, snow, direct sunlight, etc.) that can partially or entirely alter the quality of the images taken. The above provides a unique and diverse dataset of images that can be used for training tools and machine learning models for inspection, monitoring and maintenance use-cases within Smart Cities applications.
Imagewoof                                                                                  **Imagewoof** is a subset of 10 dog breed classes from Imagenet. The breeds are: Australian terrier, Border terrier, Samoyed, Beagle, Shih-Tzu, English foxhound, Rhodesian ridgeback, Dingo, Golden retriever, Old English sheepdog. Source: [https://github.com/fastai/imagenette](https://github.com/fastai/imagenette) Image Source: [https://medium.com/@lessw/how-we-beat-the-fastai-leaderboard-score-by-19-77-a-cbb2338fab5c](https://medium.com/@lessw/how-we-beat-the-fastai-leaderboard-score-by-19-77-a-cbb2338fab5c)
Image网                                                                                  **Image网** (pronounced Imagewang; 网 means 'net' in Chinese) is an image classification dataset combined from [Imagenette](/dataset/imagenette) and [Imagewoof](/dataset/imagewoof) datasets in a way to make it into a semi-supervised unbalanced classification problem: * the validation set is the same as the validation set of Imagewoof; there are no Imagenette images in the validation set (they're all in the training set), * only 10% of Imagewoof images are in the training set. The remaining images are in the 'unsupervised' split. Source: [fast.ai](https://github.com/fastai/imagenette)
Indian Food Image Dataset                                                                                  This dataset is an extremely challenging set of over 5000+ original India food images captured and crowdsourced from over 800+ urban and rural areas, where each image is **manually reviewed and verified** by computer vision professionals at ****DC Labs. ### **Dataset Features** - Dataset size : 5000+ - Captured by : Over 800+ crowdsource contributors - Resolution : 99% images HD and above (1920x1080 and above) - Location : Captured with 800+ cities accross India - Diversity : Various lighting conditions like day, night, varied distances, view points, dimlight etc. - Device used : Captured using mobile phones in 2020-2021 - Usage : Indian food classification, Dish classification, Food plate detection, etc. ### Available Annotation formats COCO, YOLO, PASCAL-VOC, Tf-Record **To download full datasets or to submit a request for your dataset needs, please ping us at [sales@datacluster.ai](sales@datacluster.ai) Visit [www.datacluster.ai](www.datacluster.ai) to know more.** **Note**: All the images are manually captured and verified by a large contributor base on DataCluster platform
Indian Signboard Image Dataset | Text in Image                                                                                  ### **Introduction** The dataset consists of Indian traffic signs images for classification and detection. The images have been taken in varied weather conditions in daylight, evening and nights. The dataset has a wide variety of variations of illumination, distances, view points etc. This dataset represents a very challenging set of unstructured images of Indian traffic signboards. ### **Dataset Features** - Captured by 2000+ unique users - Covers wide variety of Indian traffic signs - Captured with 20+ cities accross India - Captured using mobile phones - Highly diverse - Various lighting conditions like day, night, - Outdoor scene with variety of view points ### **Dataset Features** - Classification and detection annotations available - Multiple category annotations possible - COCO, PASCAL VOC and YOLO formats **To download full datasets or to submit a request for your dataset needs, please ping us at [sales@datacluster.ai](sales@datacluster.ai) Visit [www.datacluster.ai](www.datacluster.ai) to know more.** **Note**: All the images are manually captured and verified by a large contributor base on DataCluster platform
Indian Traffic Sign Image Dataset                                                                                  ### **This dataset is collected by Datacluster Labs. To download full dataset or to submit a request for your new data collection needs, please drop a mail to:&nbsp;[sales@datacluster.ai](mailto:sales@datacluster.ai)** This dataset is an extremely challenging set of over 2000+ original Indian Traffic Sign images captured and crowdsourced from over 400+ urban and rural areas, where each image is **manually reviewed and verified** by computer vision professionals at DC Labs. ### **Dataset Features** - Dataset size : 2000+ - Captured by : Over 400+ crowdsource contributors - Resolution : 100% of images HD and above (1920x1080 and above) - Location : Captured with 400+ cities accross India - Diversity : Various lighting conditions like day, night, varied distances, view points etc. - Device used : Captured using mobile phones in 2020-2021 - Usage : Traffic sign detection, Self-driving systems, traffic detection, sign detection, etc. ### **Available Annotation formats** COCO, YOLO, PASCAL-VOC, Tf-Record *To download full datasets or to submit a request for your dataset needs, please drop a mail on sales@datacluster.ai . Visit&nbsp;[w](http://www.datacluster.in/)ww.datacluster.ai to know more.
InstructPix2Pix Image Editing Dataset                                                                                  ![Dataset Figure](https://raw.githubusercontent.com/timothybrooks/instruct-pix2pix/main/imgs/dataset.jpg) A dataset for image editing containing *>450k* samples of: 1. input image (with corresponding text caption describing the image) 2. text-based edit instruction 3. edited image (with corresponding text caption describing the image) This dataset is automatically generated using a combination of GPT-3 (for generating the text edits) and StableDiffusion+Prompt-To-Prompt (for generating the input & edited images). Full description of the dataset can be found in the paper: (https://www.timothybrooks.com/instruct-pix2pix/)
Intel Image Classification                                                                                  Context This is image data of Natural Scenes around the world. Content This Data contains around 25k images of size 150x150 distributed under 6 categories. {'buildings' -> 0, 'forest' -> 1, 'glacier' -> 2, 'mountain' -> 3, 'sea' -> 4, 'street' -> 5 } The Train, Test and Prediction data is separated in each zip files. There are around 14k images in Train, 3k in Test and 7k in Prediction. This data was initially published on https://datahack.analyticsvidhya.com by Intel to host a Image classification Challenge. Acknowledgements Thanks to https://datahack.analyticsvidhya.com for the challenge and Intel for the Data Photo by Jan Böttinger on Unsplash Inspiration Want to build powerful Neural network that can classify these images with more accuracy.
Iran's Built Heritage Binary Image Classification Dataset                                                                                  **Iran's Built Heritage Binary Image Classification Dataset** contains approximately 10,500 CHB images gathered from four different sources: i) The archives of Iran’s cultural heritage ministry ii) The author’s (M.B) personal archives iii) images captured on site by the authors (M.B) during the research process iv) pictures crawled from the Internet but kept it to a minimum as their distribution differed due to heavy edits and effects.
LIVE (Public-Domain Subjective Image Quality Database)                                                                                  The **LIVE Public-Domain Subjective Image Quality Database** is a resource developed by the Laboratory for Image and Video Engineering at the University of Texas at Austin. It contains a set of images and videos whose quality has been ranked by human subjects. This database is used in Quality Assessment (QA) research, which aims to make quality predictions that align with the subjective opinions of human observers. The database was created through an extensive experiment conducted in collaboration with the Department of Psychology at the University of Texas at Austin. The experiment involved obtaining scores from human subjects for many images distorted with different distortion types. The QA algorithm may be trained on part of this data set, and tested on the rest. The database is available to the research community free of charge. If you use these images in your research, the creators kindly ask that you reference their website and their papers. There are two releases of the database. Release 2 includes more distortion types and more subjects than Release 1. The distortions include JPEG-compressed images, JPEG2000-compressed images, Gaussian blur, and white noise.
LSMDC-Context                                                                                  The Large Scale Movie Description Challenge (LSMDC) - Context is an augmented version of the original LSMDC dataset with movie scripts as contextual text. Source: [https://github.com/primle/LSMDC-Context](https://github.com/primle/LSMDC-Context)
Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification                                                                                  Dataset of validated OCT and Chest X-Ray images described and analyzed in 'Deep learning-based classification and referral of treatable human diseases'. The OCT Images are split into a training set and a testing set of independent patients. OCT Images are labeled as (disease)-(randomized patient ID)-(image number by this patient) and split into 4 directories: CNV, DME, DRUSEN, and NORMAL.
Linked WikiText-2                                                                                  The Linked Wikitext-2 language modeling dataset contains over 2 million tokens from Wikipedia articles, along with annotations linking mentions to their corresponding entities and relations in Wikidata. It is designed to match as closely as possible the contents of the popular WikiText-2 dataset.
Logic2Text                                                                                  **Logic2Text** is a large-scale dataset with 10,753 descriptions involving common logic types paired with the underlying logical forms. The logical forms show diversified graph structure of free schema, which poses great challenges on the model's ability to understand the semantics. Source: [https://github.com/czyssrs/Logic2Text](https://github.com/czyssrs/Logic2Text)
Lombardia Sentinel-2 Image Time Series for Crop Mapping                                                                                  Usually, the information related to the crop types available in a given territory is annual information, that is, we only know the type of main crop grown over a year and we do not know any crops that have followed one another during the year and also we do not know when a particular crop is sown and when it is harvested. The main objective of this dataset is to create the basis for experimenting with suitable solutions to give a reliable answer to the above questions, or to propose models capable of producing dynamic segmentation maps that show when a crop begins to grow and when it is collected. Consequently, being able to understand if more than one crop has been grown in a territory within a year. In this dataset, we have 20 coverage classes as ground-truth values provided by Regine Lombardia. The mapping of the class labels used (see file lombardia-classes/classes25pc.txt) brings together some classes and provides the time intervals within which that category grows. The last two columns of the following table are respectively the date (month-day) of the start and end of the interval in which the class is visible during the construction of our dataset.
MULTEXT-East                                                                                  The **MULTEXT-East** resources are a multilingual dataset for language engineering research and development. It consists of the (1) MULTEXT-East morphosyntactic specifications, defining categories (parts-of-speech), their morphosyntactic features (attributes and values), and the compact MSD tagset representations; (2) morphosyntactic lexica, (3) the annotated parallel '1984' corpus; and (4) some comparable text and speech corpora. The specifications are available for the following macrolanguages, languages and language varieties: Albanian, Bulgarian, Chechen, Czech, Damaskini, English, Estonian, Hungarian, Macedonian, Persian, Polish, Resian, Romanian, Russian, Serbo-Croatian, Slovak, Slovene, Torlak, and Ukrainian, while the other resources are available for a subset of these languages.
Marine Microalgae Detection in Microscopy Images                                                                                  **Marine Microalgae Detection in Microscopy Images** dataset contains a total number of images in the dataset is 937 and all the objects in these images were annotated. The total number of annotated objects is 4201. The training set contains 537 images and the testing set contains 430 images. Source: [Marine Microalgae Detection in Microscopy Images: A New Dataset](https://arxiv.org/pdf/2211.07546v1.pdf) Image Source: [https://arxiv.org/pdf/2211.07546v1.pdf](https://arxiv.org/pdf/2211.07546v1.pdf)
Masks Dataset | Unattended Mask Images                                                                                  This dataset is an extremely challenging set of over 7000+ original Masks images captured and crowdsourced from over 1200+ urban and rural areas, where each image is **manually reviewed and verified** by computer vision professionals at ****DC Labs. ### **Dataset Features** - Dataset size : 7000+ - Captured by : Over 1200+ crowdsource contributors - Resolution : 99% images HD and above (1920x1080 and above) - Location : Captured with 900+ cities accross India - Diversity : Various lighting conditions like day, night, varied distances, view points etc. - Device used : Captured using mobile phones in 2020-2021 - Usage : Mask detection, Mask segregation, Trash Mask detection, etc. ### Available Annotation formats COCO, YOLO, PASCAL-VOC, Tf-Record **To download full datasets or to submit a request for your dataset needs, please ping us at [sales@datacluster.ai](sales@datacluster.ai) Visit [www.datacluster.ai](www.datacluster.ai) to know more.** **Note**: All the images are manually captured and verified by a large contributor base on DataCluster platform
MassiveText                                                                                  **MassiveText** is a collection of large English-language text datasets from multiple sources: web pages, books, news articles, and code. The data pipeline includes text quality filtering, removal of repetitious text, deduplication of similar documents, and removal of documents with significant test-set overlap. MassiveText contains 2.35 billion documents or about 10.5 TB of text. Usage: [Gopher](https://paperswithcode.com/paper/scaling-language-models-methods-analysis-1) is trained on 300B tokens (12.8% of the tokens in the dataset), so the authors sub-sample from MassiveText with sampling proportions specified per subset (books, news, etc.). These sampling proportions are tuned to maximize downstream performance. The largest sampling subset is the curated web-text corpus MassiveWeb, which is found to improve downstream performance relative to existing web-text datasets such as C4 (Raffel et al., 2020). Find Datasheets in the [Gopher paper](https://paperswithcode.com/paper/scaling-language-models-methods-analysis-1).
MedMCQA (with Context)                                                                                  
Medical Wiki Paralell Corpus for Medical Text Simplification                                                                                  A medical Wiki paralell corpus for medical text simplification.
Microscopy Images of Drosophila Wing                                                                                  Microscopy Images of the Drosophila Wing dataset are divided into two folders, Tumor/ No Tumor. The tumor folder has images of different stages of cancer, including both early and late stages. The organization of images was done in a way that the Tumor Folder has images that already have a Tumor or is going to develop cancer in the next few days. In contrast, the No Tumor Folder has images with no sign of cancer or a tiny tumor percentage that will be suppressed the following day.
Microsoft Academic Graph citation contexts from English CS papers with abstract                                                                                  
MineralImage5k                                                                                  We present a comprehensive dataset comprising a vast collection of raw mineral samples for the purpose of mineral recognition. The dataset encompasses more than 5,000 distinct mineral species and incorporates subsets for zero-shot and few-shot learning. In addition to the samples themselves, some entries in the dataset are accompanied by supplementary natural language descriptions, size measurements, and segmentation masks. For detailed information on each sample, please refer to the minerals_full.csv file.
Mini-ImageNet - 1-Shot Learning                                                                                  
Mini-ImageNet - 5-Shot Learning                                                                                  
Mini-ImageNet to CUB - 5 shot learning                                                                                  
Mini-ImageNet-CUB 5-way (1-shot)                                                                                  
Mini-ImageNet-CUB 5-way (5-shot)                                                                                  
Mini-Imagenet 10-way (1-shot)                                                                                  
Mini-Imagenet 10-way (5-shot)                                                                                  
Mini-Imagenet 20-way (1-shot)                                                                                  
Mini-Imagenet 20-way (5-shot)                                                                                  
Mini-Imagenet 5-way (1-shot)                                                                                  
Mini-Imagenet 5-way (1-shot)                                                                                   
Mini-Imagenet 5-way (10-shot)                                                                                  
Mini-Imagenet 5-way (5-shot)                                                                                  
Mivia Audio Events Dataset                                                                                  The **MIVIA audio events** data set is composed of a total of 6000 events for surveillance applications, namely glass breaking, gun shots and screams. The 6000 events are divided into a training set (composed of 4200 events) and a test set (composed of 1800 events). In audio surveillance applications, the events of interest (for instance a scream) can occur at different distances from the microphone that correspond to different levels of the signal-to-noise ratio. Moreover, in these applications the events are generally mixed with a complex background, usually composed of several types of different sounds depending on the specific environments both indoor and outdoor (household appliances, cheering of crowds, talking people, traffic jam, passing cars or motorbikes etc.). The data set is designed to provide each audio event at 6 different values of signal-to-noise ratio (namely 5dB, 10dB, 15dB, 20dB, 25dB and 30dB) and overimposed to different combinations of environmental sounds in order to simulate their occurrence in different ambiences.
Motor Imagery dataset                                                                                  From dataset repository for '2020 International BCI Competition': https://osf.io/pq7vb/?view_only=08e7108d89fd42bab2adbd6b98fb683d
Motor Imagery dataset from Cho et al 2017.                                                                                  **Dataset Description** We conducted a BCI experiment for motor imagery movement (MI movement) of the left and right hands with 52 subjects (19 females, mean age ± SD age = 24.8 ± 3.86 years); Each subject took part in the same experiment, and subject ID was denoted and indexed as s1, s2, ..., s52. Subjects s20 and s33 were both-handed, and the other 50 subjects were right-handed. EEG data were collected using 64 Ag/AgCl active electrodes. A 64-channel montage based on the international 10-10 system was used to record the EEG signals with 512 Hz sampling rates. The EEG device used in this experiment was the Biosemi ActiveTwo system. The BCI2000 system 3.0.2 was used to collect EEG data and present instructions (left hand or right hand MI). Furthermore, we recorded EMG as well as EEG simultaneously with the same system and sampling rate to check actual hand movements. Two EMG electrodes were attached to the flexor digitorum profundus and extensor digitorum on each arm. Subjects were asked to imagine the hand movement depending on the instruction given. Five or six runs were performed during the MI experiment. After each run, we calculated the classification accuracy over one run and gave the subject feedback to increase motivation. Between each run, a maximum 4-minute break was given depending on the subject 's demands.
Motor Imagery dataset from Ofner et al 2017                                                                                  ### Dataset description We recruited 15 healthy subjects aged between 22 and 40 years with a mean age of 27 years (standard deviation 5 years). Nine subjects were female, and all the subjects except s1 were right-handed. We measured each subject in two sessions on two different days, which were not separated by more than one week. In the first session the subjects performed ME, and MI in the second session. The subjects performed six movement types which were the same in both sessions and comprised of elbow flexion/extension, forearm supination/pronation and hand open/close; all with the right upper limb. All movements started at a neutral position: the hand half open, the lower arm extended to 120 degree and in a neutral rotation, i.e. thumb on the inner side. Additionally to the movement classes, a rest class was recorded in which subjects were instructed to avoid any movement and to stay in the starting position. In the ME session, we instructed subjects to execute sustained movements. In the MI session, we asked subjects to perform kinesthetic MI of the movements done in the ME session (subjects performed one ME run immediately before the MI session to support kinesthetic MI). The paradigm was trial-based and cues were displayed on a computer screen in front of the subjects, Fig 2 shows the sequence of the paradigm. At second 0, a beep sounded and a cross popped up on the computer screen (subjects were instructed to fixate their gaze on the cross). Afterwards, at second 2, a cue was presented on the computer screen, indicating the required task (one out of six movements or rest) to the subjects. At the end of the trial, subjects moved back to the starting position. In every session, we recorded 10 runs with 42 trials per run. We presented 6 movement classes and a rest class and recorded 60 trials per class in a session. ### References ---------- [1] Ofner, P., Schwarz, A., Pereira, J. and Müller-Putz, G.R., 2017. Upper limb movements can be decoded from the time-domain of low-frequency EEG. PloS one, 12(8), p.e0182578.
Motor Imagery dataset from Weibo et al 2014.                                                                                  Dataset from the article *Evaluation of EEG oscillatory patterns and cognitive process during simple and compound limb motor imagery* [1]_. It contains data recorded on 10 subjects, with 60 electrodes. This dataset was used to investigate the differences of the EEG patterns between simple limb motor imagery and compound limb motor imagery. Seven kinds of mental tasks have been designed, involving three tasks of simple limb motor imagery (left hand, right hand, feet), three tasks of compound limb motor imagery combining hand with hand/foot (both hands, left hand combined with right foot, right hand combined with left foot) and rest state. At the beginning of each trial (8 seconds), a white circle appeared at the center of the monitor. After 2 seconds, a red circle (preparation cue) appeared for 1 second to remind the subjects of paying attention to the character indication next. Then red circle disappeared and character indication (‘Left Hand’, ‘Left Hand & Right Foot’, et al) was presented on the screen for 4 seconds, during which the participants were asked to perform kinesthetic motor imagery rather than a visual type of imagery while avoiding any muscle movement. After 7 seconds, ‘Rest’ was presented for 1 second before next trial (Fig. 1(a)). The experiments were divided into 9 sections, involving 8 sections consisting of 60 trials each for six kinds of MI tasks (10 trials for each MI task in one section) and one section consisting of 80 trials for rest state. The sequence of six MI tasks was randomized. Intersection break was about 5 to 10 minutes. References ----------- [1] Yi, Weibo, et al. 'Evaluation of EEG oscillatory patterns and cognitive process during simple and compound limb motor imagery.' PloS one 9.12 (2014). https://doi.org/10.1371/journal.pone.0114853
Motor Imagery dataset from Zhou et al 2016.                                                                                  Dataset from the article *A Fully Automated Trial Selection Method for Optimization of Motor Imagery Based Brain-Computer Interface* [1]_. This dataset contains data recorded on 4 subjects performing 3 type of motor imagery: left hand, right hand and feet. Every subject went through three sessions, each of which contained two consecutive runs with several minutes inter-run breaks, and each run comprised 75 trials (25 trials per class). The intervals between two sessions varied from several days to several months. A trial started by a short beep indicating 1 s preparation time, and followed by a red arrow pointing randomly to three directions (left, right, or bottom) lasting for 5 s and then presented a black screen for 4 s. The subject was instructed to immediately perform the imagination tasks of the left hand, right hand or foot movement respectively according to the cue direction, and try to relax during the black screen. References ---------- [1] Zhou B, Wu X, Lv Z, Zhang L, Guo X (2016) A Fully Automated Trial Selection Method for Optimization of Motor Imagery Based Brain-Computer Interface. PLoS ONE 11(9). https://doi.org/10.1371/journal.pone.0162657
Motor Imagey Dataset from Shin et al 2017                                                                                  ## Data Acquisition EEG and NIRS data was collected in an ordinary bright room. EEG data was recorded by a multichannel BrainAmp EEG amplifier with thirty active electrodes (Brain Products GmbH, Gilching, Germany) with linked mastoids reference at 1000 Hz sampling rate. The EEG amplifier was also used to measure the electrooculogram (EOG), electrocardiogram (ECG) and respiration with a piezo based breathing belt. Thirty EEG electrodes were placed on a custom-made stretchy fabric cap (EASYCAP GmbH, Herrsching am Ammersee, Germany) and placed according to the international 10-5 system (AFp1, AFp2, AFF1h, AFF2h, AFF5h, AFF6h, F3, F4, F7, F8, FCC3h, FCC4h, FCC5h, FCC6h, T7, T8, Cz, CCP3h, CCP4h, CCP5h, CCP6h, Pz, P3, P4, P7, P8, PPO1h, PPO2h, POO1, POO2 and Fz for ground electrode). NIRS data was collected by NIRScout (NIRx GmbH, Berlin, Germany) at 12.5 Hz sampling rate. Each adjacent source-detector pair creates one physiological NIRS channel. Fourteen sources and sixteen detectors resulting in thirty-six physiological channels were placed at frontal (nine channels around Fp1, Fp2, and Fpz), motor (twelve channels around C3 and C4, respectively) and visual areas (three channels around Oz). The inter-optode distance was 30 mm. NIRS optodes were fixed on the same cap as the EEG electrodes. Ambient lights were sufficiently blocked by a firm contact between NIRS optodes and scalp and use of an opaque cap. EOG was recorded using two vertical (above and below left eye) and two horizontal (outer canthus of each eye) electrodes. ECG was recorded based on Einthoven triangle derivations I and II, and respiration was measured using a respiration belt on the lower chest. EOG, ECG and respiration were sampled at the same sampling rate of the EEG. ECG and respiration data were not analyzed in this study, but are provided along with the other signals. ## Experimental Procedure The subjects sat on a comfortable armchair in front of a 50-inch white screen. The distance between their heads and the screen was 1.6 m. They were asked not to move any part of the body during the data recording. The experiment consisted of three sessions of left and right hand MI (dataset A)and MA and baseline tasks (taking a rest without any thought) (dataset B) each. Each session comprised a 1 min pre-experiment resting period, 20 repetitions of the given task and a 1 min post-experiment resting period. The task started with 2 s of a visual introduction of the task, followed by 10 s of a task period and resting period which was given randomly from 15 to 17 s. At the beginning and end of the task period, a short beep (250 ms) was played. All instructions were displayed on the white screen by a video projector. MI and MA tasks were performed in separate sessions but in alternating order (i.e., sessions 1, 3 and 5 for MI (dataset A) and sessions 2, 4 and 6 for MA (dataset B)). Fig. 2 shows the schematic diagram of the experimental paradigm. Five sorts of motion artifacts induced by eye and head movements (dataset C) were measured. The motion artifacts were recorded after all MI and MA task recordings. The experiment did not include the pre- and post-experiment resting state periods. ## Motor Imagery (Dataset A) For motor imagery, subjects were instructed to perform haptic motor imagery (i.e. to imagine the feeling of opening and closing their hands as they were grabbing a ball) to ensure that actual motor imagery, not visual imagery, was performed. All subjects were naive to the MI experiment. For the visual instruction, a black arrow pointing to either the left or right side appeared at the center of the screen for 2 s. The arrow disappeared with a short beep sound and then a black fixation cross was displayed during the task period. The subjects were asked to imagine hand gripping (opening and closing their hands) in a 1 Hz pace. This pace was shown to and repeated by the subjects by performing real hand gripping before the experiment. Motor imagery was performed continuously over the task period. The task period was finished with a short beep sound and a 'STOP' displayed for 1s on the screen. The fixation cross was displayed again during the rest period and the subjects were asked to gaze at it to minimize their eye movements. This process was repeated twenty times in a single session (ten trials per condition in a single session; thirty trials in the whole sessions). In a single session, motor imagery tasks were performed on the basis of ten subsequent blocks randomly consisting of one of two conditions: Either first left and then right hand motor imagery or vice versa. ## References [1] Shin, J., von Lühmann, A., Blankertz, B., Kim, D.W., Jeong, J., Hwang, H.J. and Müller, K.R., 2017. Open access dataset for EEG+NIRS single-trial classification. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 25(10), pp.1735-1745. [2] GNU General Public License, Version 3 `<https://www.gnu.org/licenses/gpl-3.0.txt>`_
Multi-Spectral Stereo Dataset (RGB, NIR, thermal images, LiDAR, GPS/IMU)                                                                                  Abstract: We introduce the multi-spectral stereo (MS2) outdoor dataset, including stereo RGB, stereo NIR, stereo thermal, stereo LiDAR data, and GPS/IMU information. Our dataset provides rectified and synchronized 184K data pairs taken from city, residential, road, campus, and suburban areas in the morning, daytime, and nighttime under clear-sky, cloudy, and rainy conditions. We designed the dataset to explore various computer vision algorithms from multi-spectral sensor data to achieve high-level performance, reliability, and robustness against challenging environments. MS2 dataset provides: * 1. (Synchronized) Stereo RGB images / Stereo NIR images / Stereo thermal images * 2. (Synchronized) Stereo LiDAR scans / GPS/IMU navigation data * 3. Projected depth map (in RGB, NIR, thermal image planes) * 4. Odometry data (in RGB, NIR, thermal cameras, and LiDAR coordinates)
Multi-domain Image Characteristics Dataset                                                                                  The Multi-domain Image Characteristic Dataset consists of thousands of images sourced from the internet. Each image falls under one of three domains - animals, birds, or furniture. There are five types under each domain. There are 200 images of each type, summing up the total dataset to 3,000 images. The master file consists of two columns; the image name and the visible characteristics of that image. Every image was manually analyzed and the characteristics for each image were generated, ensuring accuracy. Images falling under the same domain have a similar set of characteristics. For example, pictures under the bird's domain will have a common set of characteristics such as the color of the bird, the presence of a beak, wing, eye, legs, etc. Care has been taken to ensure that each image is as unique as possible by including pictures that have different combinations of visible characteristics present. This includes pictures having variations in the capture angle, etc.
Multidimensional Texture Perception                                                                                  Texture-based studies and designs have been in focus recently. Whisker-based multidimensional surface texture data is missing in the literature. This data is critical for robotics and machine perception algorithms in the classification and regression of textural surfaces. We present a novel sensor design to acquire multidimensional texture information. The surface texture's roughness and hardness were measured experimentally using sweeping and dabbing. The data is made available to the research community for further advancing texture perception studies.
Multispectral Image Database                                                                                  We present a database of multispectral images that were used to emulate the GAP camera. The images are of a wide variety of real-world materials and objects. We are making this database available to the research community. Details of the database can be found in the following publication: 'Generalized Assorted Pixel Camera: Post-Capture Control of Resolution, Dynamic Range and Spectrum,' F. Yasuma, T. Mitsunaga, D. Iso, and S.K. Nayar, Technical Report, Department of Computer Science, Columbia University CUCS-061-08, Nov. 2008.
Munich Motor Imagery dataset                                                                                  BMI/OpenBMI dataset for MI. Dataset from Lee et al 2019 [1]. ### Dataset Description A trial started with the central display of a white fixation cross. After 3 s, a white arrow was superimposed on the fixation cross, either pointing to the left or the right. Subjects were instructed to perform haptic motor imagery of the left or the right hand during display of the arrow, as indicated by the direction of the arrow. After another 7 s, the arrow was removed, indicating the end of the trial and start of the next trial. While subjects were explicitly instructed to perform haptic motor imagery with the specified hand, i.e., to imagine feeling instead of visualizing how their hands moved, the exact choice of which type of imaginary movement, i.e., moving the fingers up and down, gripping an object, etc., was left unspecified. A total of 150 trials per condition were carried out by each subject, with trials presented in pseudorandomized order. Ten healthy subjects (S1--S10) participated in the experimental evaluation. Of these, two were females, eight were right handed, and their average age was 25.6 years with a standard deviation of 2.5 years. Subject S3 had already participated twice in a BCI experiment, while all other subjects were naive to BCIs. EEG was recorded at M=128 electrodes placed according to the extended 10--20 system. Data were recorded at 500 Hz with electrode Cz as reference. Four BrainAmp amplifiers were used for this purpose, using a temporal analog high-pass filter with a time constant of 10 s. The data were re-referenced to common average reference offline. Electrode impedances were below 10 kΩ for all electrodes and subjects at the beginning of each recording session. No trials were rejected and no artifact correction was performed. For each subject, the locations of the 128 electrodes were measured in three dimensions using a Zebris ultrasound tracking system and stored for further offline analysis. ### References [1] Grosse-Wentrup, Moritz, et al. 'Beamforming in noninvasive brain–computer interfaces.' IEEE Transactions on Biomedical Engineering 56.4 (2009): 1209-1219.
N-ImageNet                                                                                  The N-ImageNet dataset is an event-camera counterpart for the ImageNet dataset. The dataset is obtained by moving an event camera around a monitor displaying images from ImageNet. N-ImageNet contains approximately 1,300k training samples and 50k validation samples. In addition, the dataset also contains variants of the validation dataset recorded under a wide range of lighting or camera trajectories. Additional details about the dataset are explained in the paper available through this [link](https://arxiv.org/abs/2112.01041). Please cite this paper if you make use of the dataset.
N-ImageNet (mini)                                                                                  
NAS-Bench-201, ImageNet-16-120                                                                                  
NATS-Bench Size, ImageNet16-120                                                                                  
NATS-Bench Topology, ImageNet16-120                                                                                  
NSText2SQL: An Open Source Text-to-SQL Dataset for Foundation Model Training                                                                                  Numbers Station Text to SQL
NText                                                                                  NText is an eight million words dataset extracted and preprocessed from nuclear research papers and thesis. Source: [NukeBERT: A Pre-trained language model for Low Resource Nuclear Domain](https://arxiv.org/pdf/2003.13821.pdf)
Nagoya University Extremely Low-resolution FIR Image Action Dataset                                                                                  A pedestrian dataset for Person Re-identification. Source: [Nagoya University Extremely Low-resolution FIR Image Action Dataset](https://www.murase.m.is.nagoya-u.ac.jp/~kawanishiy/en/datasets.html)
ODAQ: Open Dataset of Audio Quality                                                                                  A dataset containing the results of a MUSHRA listening test conducted with expert listeners from 2 international laboratories. ODAQ contains 240 audio samples and corresponding quality scores. Each audio sample is rated by 26 listeners. The audio samples are stereo audio signals sampled at 44.1 or 48 kHz and are processed by a total of 6 method classes, each operating at different quality levels. The processing method classes are designed to generate quality degradations possibly encountered during audio coding and source separation, and the quality levels for each method class span the entire quality range. The diversity of the processing methods, the large span of quality levels, the high sampling frequency, and the pool of international listeners make ODAQ particularly suited for further research into subjective and objective audio quality. The dataset is released with permissive licenses, and the software used to conduct the listening test is also made publicly available.
Ontology Enrichment from Texts (OET): A Biomedical Dataset for Concept Discovery and Placement                                                                                  A biomedical dataset supporting ontology enrichment from texts, by concept discovery and placement, adapting the MedMentions dataset (PubMed abstracts) with SNOMED CT of versions in 2014 and 2017 under the Diseases (disorder) sub-category and the broader categories of Clinical finding, Procedure, and Pharmaceutical / biologic (CPP) product. The dataset is documented in the work, Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement, on arXiv: https://arxiv.org/abs/2306.14704 (CIKM 2023). The companion code is available at https://github.com/KRR-Oxford/OET. Out-of-KB mention discovery (including the settings of mention-level data) is further partly documented in the work, Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking, on arXiv: https://arxiv.org/abs/2302.07189 (CIKM 2023). ver4: we made a version of mention-level data for out-of-KB discovery and concept placement separately: the former (for out-of-KB discovery) has out-of-KB mentions in training data, while the latter (for concept placement) has only out-of-KB mentions during the evaluation (validation and test) and not in the training data. Also, we split the original 'test-NIL.jsonl' (now 'test-NIL-all.jsonl') into 'valid-NIL.jsonl' and 'test-NIL.jsonl' for a better evaluation. ver3: we revised and updated mention-level data (syn_full, synonym augmentation setting) and the folder structure, and also updated the edge catalogues with complex edges. ver2: we revised the mention-level data by only keeping out-of-KB mentions (or 'NIL' mentions) associated with one-hop edges (including leaf nodes, as <leaf node, NULL>) and two-hop edges in the ontology (SNOMED CT 20140901). Acknowledgement of data sources and tools below: * SNOMED CT https://www.nlm.nih.gov/healthit/snomedct/archive.html (and use snomed-owl-toolkit to form .owl files) * UMLS https://www.nlm.nih.gov/research/umls/licensedcontent/umlsarchives04.html (and mainly use MRCONSO for mapping UMLS to SNOMED CT) * MedMentions https://github.com/chanzuckerberg/MedMentions (source of entity linking) * Protégé http://protegeproject.github.io/protege/ * snomed-owl-toolkit https://github.com/IHTSDO/snomed-owl-toolkit * DeepOnto https://github.com/KRR-Oxford/DeepOnto (based on OWLAPI https://owlapi.sourceforge.net/) for ontology processing and complex concept verbalisation
Open Images V4                                                                                  Open Images V4 offers large scale across several dimensions: 30.1M image-level labels for 19.8k concepts, 15.4M bounding boxes for 600 object classes, and 375k visual relationship annotations involving 57 classes. For object detection in particular, 15x more bounding boxes than the next largest datasets (15.4M boxes on 1.9M images) are provided. The images often show complex scenes with several objects (8 annotated objects per image on average). Visual relationships between them are annotated, which support visual relationship detection, an emerging task that requires structured reasoning. Source: [The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale](https://arxiv.org/pdf/1811.00982) Image Source: [https://storage.googleapis.com/openimages/web/index.html](https://storage.googleapis.com/openimages/web/index.html)
Open Images V7                                                                                  Open Images is a computer vision dataset covering ~9 million images with labels spanning thousands of object categories. A subset of 1.9M includes diverse annotations types. - 15,851,536 boxes on 600 classes - 2,785,498 instance segmentations on 350 classes - 3,284,280 relationship annotations on 1,466 relationships - 675,155 localized narratives (synchronized voice, mouse trace, and text caption) - 66,391,027 point-level annotations on 5,827 classes - 61,404,966 image-level labels on 20,638 classes Images are under a CC BY 2.0 license, annotations under CC BY 4.0 license.
OpenImage-O                                                                                  It is manually annotated, comes with a naturally diverse distribution, and has a large scale. It is built to overcome several shortcomings of existing OOD benchmarks. OpenImage-O is image-by-image filtered from the test set of OpenImage-V3, which has been collected from Flickr without a predefined list of class names or tags, leading to natural class statistics and avoiding an initial design bias.
OpenImages-v6                                                                                  OpenImages V6 is a large-scale dataset , consists of 9 million training images, 41,620 validation samples, and 125,456 test samples. It is a partially annotated dataset, with 9,600 trainable classes
OpenWebText                                                                                  **OpenWebText** is an open-source recreation of the [WebText](/dataset/webtext) corpus. The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB). Source: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
Oximeter Image Dataset | Medical Device Reading                                                                                  This dataset is an extremely challenging set of over 2000+ original Oximeter images captured and crowdsourced from over 300+ urban and rural areas, where each image is **manually reviewed and verified** by computer vision professionals at Datacluster Labs. ### **Dataset Features** - Dataset size : 2000+ - Captured by : Over 300+ crowdsource contributors - Resolution : 100% images HD and above (1920x1080 and above) - Location : Captured with 50+ cities accross India - Diversity : Various lighting conditions like day, night, varied distances, view points etc. - Device used : Captured using mobile phones in 2020-2021 - Usage : Oximeter Reading detection, Medical Device, Healthcare system detection, etc. ### Available Annotation formats COCO, YOLO, PASCAL-VOC, Tf-Record **To download full datasets or to submit a request for your dataset needs, please ping us at [sales@datacluster.ai](sales@datacluster.ai) Visit [www.datacluster.ai](www.datacluster.ai) to know more.** **Note**: All the images are manually captured and verified by a large contributor base on DataCluster platform
PASCAL Context                                                                                  The **PASCAL Context** dataset is an extension of the PASCAL VOC 2010 detection challenge, and it contains pixel-wise labels for all training images. It contains more than 400 classes (including the original 20 classes plus backgrounds from PASCAL VOC segmentation), divided into three categories (objects, stuff, and hybrids). Many of the object categories of this dataset are too sparse and; therefore, a subset of 59 frequent classes are usually selected for use. Source: [Image Segmentation Using Deep Learning:A Survey](https://arxiv.org/abs/2001.05566) Image Source: [https://cs.stanford.edu/~roozbeh/pascal-context/](https://cs.stanford.edu/~roozbeh/pascal-context/)
PASCAL VOC 2012, 60 proposals per image                                                                                  
PET: A new Dataset for Process Extraction from Natural Language Text                                                                                  The dataset contains 45 documents containing narrative description of business process and their annotations. Annotated with activities, gateways, actors, and flow information. Each document is composed of three files: Doc_name.txt (Process description in CONLL format) Doc_name.process-elements.IOB2.txt (Process elements annotated with IOB2 Schema in CONLL format) Doc_name.relations.tsv (Process relations between process elements. Each line is a triplette (source, relation tag, target). Source and target are in the form: n_sent_x words range.)
PRO-teXt                                                                                  PRO-teXt is an extension of PROXD with the inclusion of text prompts to synthesize objects. There are 180/20 interactions for training/testing in PRO-teXt. Each interaction involves a linguistic command corresponding to an existing room arrangement.
PanoContext                                                                                  The **PanoContext** dataset contains 500 annotated cuboid layouts of indoor environments such as bedrooms and living rooms. Source: [LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image](https://arxiv.org/abs/1803.08999) Image Source: [https://panocontext.cs.princeton.edu/paper.pdf](https://panocontext.cs.princeton.edu/paper.pdf)
Panoramic Image Database                                                                                  The **Panoramic Image Database** is a panoramic image dataset. The databases were collected by Andrew Vardy while visiting with the Computer Engineering group in February and March of 2004. Images were captured by a robot-mounted camera, pointed upwards at a hyperbolic mirror. The camera was an ImagingSource DFK 4303. The robot was an ActivMedia Pioneer 3-DX. The mirror was a large wide-view hyperbolic mirror from Accowle Ltd. The hyperbolic mirror expands the camera's field of view to allow the capture of panoramic images.
Parasitic Egg Detection and Classification in Microscopic Images                                                                                  Parasitic infections have been recognized as one of the most significant causes of illnesses by WHO. Most infected persons shed cysts or eggs in their living environment, and unwittingly cause transmission of parasites to other individuals. Diagnosis of intestinal parasites is usually based on direct examination in the laboratory, of which capacity is obviously limited. Targeting to automate routine fecal examination for parasitic diseases, this challenge aims to gather experts in the field to develop robust automated methods to detect and classify eggs of parasitic worms in a variety of microscopic images. Participants will work with a large-scale dataset, containing 11 types of parasitic eggs from fecal smear samples. They are the main interest because of causing major diseases and illness in developing countries. We open to any techniques used for parasitic egg recognition, ranging from conventional approaches based on statistical models to deep learning techniques. Finally, the organizers expect a new collaboration come out from the challenge.
PartImageNet                                                                                  **PartImageNet** is a large, high-quality dataset with part segmentation annotations. It consists of 158 classes from [ImageNet](/dataset/imagenet) with approximately 24000 images. PartImageNet offers part-level annotations on a general set of classes with non-rigid, articulated objects, while having an order of magnitude larger size compared to existing datasets. It can be utilized in multiple vision tasks including but not limited to: Part Discovery, Semantic Segmentation, Few-shot Learning.
Persian Text Image Segmentation (PTI SEG)                                                                                  Persian Text Image Segmentation (PTI SEG) This dataset is part of a paper titled 'Persis: A Persian Font Recognition Pipeline Using Convolutional Neural Networks'. A dataset in order to solve image segmentation for the Persian texts. For generating the PTI SEG dataset, we use 735 different backgrounds in four types (stock, papers, noisy real world, and texture), and apply four effects (gradient light, folding, subtle noise, and ink bleed) to the image If you use the findings of the paper or this dataset please cite the paper. [GitHub repo](https://github.com/mehrdad-dev/persis)
Phrase-in-Context                                                                                  Phrase in Context is a curated benchmark for phrase understanding and semantic search, consisting of three tasks of increasing difficulty: Phrase Similarity (PS), Phrase Retrieval (PR) and Phrase Sense Disambiguation (PSD). The datasets are annotated by 13 linguistic experts on Upwork and verified by two groups: ~1000 AMT crowdworkers and another set of 5 linguistic experts. PiC benchmark is distributed under CC-BY-NC 4.0.
Physical Audiovisual CommonSense                                                                                  **PACS** (**Physical Audiovisual CommonSense**) is the first audiovisual benchmark annotated for physical commonsense attributes. PACS contains a total of 13,400 question-answer pairs, involving 1,377 unique physical commonsense questions and 1,526 videos. The dataset provides new opportunities to advance the research field of physical reasoning by bringing audio as a core component of this multimodal problem.
Physionet Motor Imagery dataset.                                                                                  ## Physionet MI dataset: https://physionet.org/pn4/eegmmidb/ This data set consists of over 1500 one- and two-minute EEG recordings, obtained from 109 volunteers [2]_. Subjects performed different motor/imagery tasks while 64-channel EEG were recorded using the BCI2000 system (http://www.bci2000.org) [1]_. Each subject performed 14 experimental runs: two one-minute baseline runs (one with eyes open, one with eyes closed), and three two-minute runs of each of the four following tasks: 1. A target appears on either the left or the right side of the screen. The subject opens and closes the corresponding fist until the target disappears. Then the subject relaxes. 2. A target appears on either the left or the right side of the screen. The subject imagines opening and closing the corresponding fist until the target disappears. Then the subject relaxes. 3. A target appears on either the top or the bottom of the screen. The subject opens and closes either both fists (if the target is on top) or both feet (if the target is on the bottom) until the target disappears. Then the subject relaxes. 4. A target appears on either the top or the bottom of the screen. The subject imagines opening and closing either both fists (if the target is on top) or both feet (if the target is on the bottom) until the target disappears. Then the subject relaxes. ## References [1] Schalk, G., McFarland, D.J., Hinterberger, T., Birbaumer, N. and Wolpaw, J.R., 2004. BCI2000: a general-purpose brain-computer interface (BCI) system. IEEE Transactions on biomedical engineering, 51(6), pp.1034-1043. [2] Goldberger, A.L., Amaral, L.A., Glass, L., Hausdorff, J.M., Ivanov, P.C., Mark, R.G., Mietus, J.E., Moody, G.B., Peng, C.K., Stanley, H.E. and PhysioBank, P., PhysioNet: components of a new research resource for complex physiologic signals Circulation 2000 Volume 101 Issue 23 pp. E215–E220.
PhysionetMotorImagery MOABB                                                                                  
Pistachio Image Dataset                                                                                  Citation Request : 1. OZKAN IA., KOKLU M. and SARACOGLU R. (2021). Classification of Pistachio Species Using Improved K-NN Classifier. Progress in Nutrition, Vol. 23, N. 2, pp. DOI:10.23751/pn.v23i2.9686. (Open Access) https://www.mattioli1885journals.com/index.php/progressinnutrition/article/view/9686/9178 ABSTRACT: In order to keep the economic value of pistachio nuts which have an important place in the agricultural economy, the efficiency of post-harvest industrial processes is very important. To provide this efficiency, new methods and technologies are needed for the separation and classification of pistachios. Different pistachio species address different markets, which increases the need for the classification of pistachio species. In this study, it is aimed to develop a classification model different from traditional separation methods, based on image processing and artificial intelligence which are capable to provide the required classification. A computer vision system has been developed to distinguish two different species of pistachios with different characteristics that address different market types. 2148 sample image for these two kinds of pistachios were taken with a high-resolution camera. The image processing techniques, segmentation and feature extraction were applied on the obtained images of the pistachio samples. A pistachio dataset that has sixteen attributes was created. An advanced classifier based on k-NN method, which is a simple and successful classifier, and principal component analysis was designed on the obtained dataset. In this study; a multi-level system including feature extraction, dimension reduction and dimension weighting stages has been proposed. Experimental results showed that the proposed approach achieved a classification success of 94.18%. The presented high-performance classification model provides an important need for the separation of pistachio species and increases the economic value of species. In addition, the developed model is important in terms of its application to similar studies. Keywords: Classification, Image processing, k nearest neighbor classifier, Pistachio species 2. SINGH D, TASPINAR YS, KURSUN R, CINAR I, KOKLU M, OZKAN IA, LEE H-N., (2022). Classification and Analysis of Pistachio Species with Pre-Trained Deep Learning Models, Electronics, 11 (7), 981. https://doi.org/10.3390/electronics11070981. (Open Access) ABSTRACT: Pistachio is a shelled fruit from the anacardiaceae family. The homeland of pistachio is the Middle East. The Kirmizi pistachios and Siirt pistachios are the major types grown and exported in Turkey. Since the prices, tastes, and nutritional values of these types differs, the type of pistachio becomes important when it comes to trade. This study aims to identify these two types of pistachios, which are frequently grown in Turkey, by classifying them via convolutional neural networks. Within the scope of the study, images of Kirmizi and Siirt pistachio types were obtained through the computer vision system. The pre-trained dataset includes a total of 2148 images, 1232 of Kirmizi type and 916 of Siirt type. Three different convolutional neural network models were used to classify these images. Models were trained by using the transfer learning method, with AlexNet and the pre-trained models VGG16 and VGG19. The dataset is divided as 80% training and 20% test. As a result of the performed classifications, the success rates obtained from the AlexNet, VGG16, and VGG19 models are 94.42%, 98.84%, and 98.14%, respectively. Models’ performances were evaluated through sensitivity, specificity, precision, and F-1 score metrics. In addition, ROC curves and AUC values were used in the performance evaluation. The highest classification success was achieved with the VGG16 model. The obtained results reveal that these methods can be used successfully in the determination of pistachio types. Keywords: pistachio; genetic varieties; machine learning; deep learning; food recognition
Plaintext Jokes                                                                                  There are about 208 000 jokes in this database scraped from three sources. Source: [Plaintext Jokes](https://github.com/taivop/joke-dataset)
Publication text: code, data, and new measures                                                                                  Data for novelty and its impact detection in scientific publications from Microsoft Academic Graph (now OpenAlex)
Red MiniImageNet 20% label noise                                                                                  Part of the Controlled Noisy Web Labels Dataset.
Red MiniImageNet 40% label noise                                                                                  Part of the Controlled Noisy Web Labels Dataset.
Red MiniImageNet 80% label noise                                                                                  Part of the Controlled Noisy Web Labels Dataset.
Retinal Fundus MultiDisease Image Dataset (RFMiD)                                                                                  According to the WHO, World report on vision 2019, the number of visually impaired people worldwide is estimated to be 2.2 billion, of whom at least 1 billion have a vision impairment that could have been prevented or is yet to be addressed. The world faces considerable challenges in terms of eye care, including inequalities in the coverage and quality of prevention, treatment, and rehabilitation services. Early detection and diagnosis of ocular pathologies would enable forestall of visual impairment. One challenge that limits the adoption of a computer-aided diagnosis tool by the ophthalmologist is, the sight-threatening rare pathologies such as central retinal artery occlusion or anterior ischemic optic neuropathy and others are usually ignored. In the past two decades, many publicly available datasets of color fundus images have been collected with a primary focus on diabetic retinopathy, glaucoma, and age-related macular degeneration, and few other frequent pathologies. The challenge for which this dataset was introduced aimed to unite the medical image analysis community to develop methods for automatic ocular disease classification of frequent diseases along with the rare pathologies. The Retinal Fundus Multi-disease Image Dataset (RFMiD) consists of a total of 3200 fundus images captured using three different fundus cameras with 46 conditions annotated through adjudicated consensus of two senior retinal experts. To the best of the authors knowledge, the dataset, RFMiD represents the only publicly available dataset that constitutes such a wide variety of diseases that appear in routine clinical settings. This aforementioned challenge promoted the development of generalizable models for screening retina, unlike the previous efforts that focused on the detection of specific diseases.
Rice Image Dataset                                                                                  Citation Request: See the articles for more detailed information on the data. Koklu, M., Cinar, I., & Taspinar, Y. S. (2021). Classification of rice varieties with deep learning methods. Computers and Electronics in Agriculture, 187, 106285. https://doi.org/10.1016/j.compag.2021.106285 Cinar, I., & Koklu, M. (2021). Determination of Effective and Specific Physical Features of Rice Varieties by Computer Vision In Exterior Quality Inspection. Selcuk Journal of Agriculture and Food Sciences, 35(3), 229-243. https://doi.org/10.15316/SJAFS.2021.252 Cinar, I., & Koklu, M. (2022). Identification of Rice Varieties Using Machine Learning Algorithms. Journal of Agricultural Sciences https://doi.org/10.15832/ankutbd.862482 Cinar, I., & Koklu, M. (2019). Classification of Rice Varieties Using Artificial Intelligence Methods. International Journal of Intelligent Systems and Applications in Engineering, 7(3), 188-194. https://doi.org/10.18201/ijisae.2019355381 https://www.kaggle.com/mkoklu42 DATASET: https://www.muratkoklu.com/datasets/
RoadText-1K                                                                                  A dataset for text in driving videos. The dataset is 20 times larger than the existing largest dataset for text in videos. The dataset comprises 1000 video clips of driving without any bias towards text and with annotations for text bounding boxes and transcriptions in every frame. Source: [RoadText-1K: Text Detection & Recognition Dataset for Driving Videos](/paper/roadtext-1k-text-detection-recognition)
Roof-Image Dataset                                                                                  We created a building-image paired dataset that contains more than 3K samples using our roof modeling tools. Image source: [https://github.com/llorz/SGA21_roofOptimization/tree/main/RoofGraphDataset](https://github.com/llorz/SGA21_roofOptimization/tree/main/RoofGraphDataset)
SIDD-Image                                                                                  This is the first image-based network intrusion detection dataset. This large-scale dataset included network traffic protocol communication-based images from 15 different observation locations of different countries in Asia. This dataset is used to identify two different types of anomalies from benign network traffic. Each image with a size of 48 × 48 contains multi-protocol communications within 128 seconds. The SIDD dataset can be to applied to a broad range of tasks such as machine learning-based network intrusion detection, non-iid federated learning, and so forth.
SMC Text Corpus                                                                                  Contents (As on March 4, 2019) -------- The text corpus contains running text from various free licensed sources. - The whole content of Malayalam Wikipedia extracted on January 1, 2019 - News/Article from various sources, source mentioned in respective files: - 251 Mb - 8,60,159 lines - 98,15,533 words - 10,11,11,885 characters The word corpus contains - Classified lexicon prepared for [Malaylam Morphology Analyser project](https://gitlab.com/smc/mlmorph) - Unique words extracted from Malayalam Wikipedia, Wictionary etc. - 14,27,392 words
SMLM CEP152-Complex FITS Images                                                                                  The following files comprise 19 sets of 40,000 images, each set corresponding to a different rendering sigma as described in the paper. * Extracting * To extract the files, execute the following command (under Linux): cat paper_data.tar.gz.* | tar xzvf - * Organisation * The data are grouped into 19 directories, corresponding to the sigma value they were rendered at. These values are 10, 9, 8.1, 7.29, 6.56, 5.9, 5.31, 4.78, 4.3, 3.87, 3.65, 3.28, 2.95, 2.66, 2.39, 2.15, 1.94, 1.743, 1.57, 1.41 Each directory contains 40,000 FITS files - a NASA floating point image standard. The images are single channel and un-normalised. This structure is ready to be used * Recreating the data * If you have the time and compute power, you can regenerate this data set with as many or as few images as you prefer, at any sigma level. The original experimental data is available at <fill in later>. To recreate the data set you need to download the CEPRender program, available on github: https://github.com/OniDaito/CEPrender - details on how to use this program are available with the code.
Satimage                                                                                  The resources for this dataset can be found at https://www.openml.org/d/182 Author: Ashwin Srinivasan, Department of Statistics and Data Modeling, University of Strathclyde Source: UCI - 1993 Please cite: UCI The database consists of the multi-spectral values of pixels in 3x3 neighbourhoods in a satellite image, and the classification associated with the central pixel in each neighbourhood. The aim is to predict this classification, given the multi-spectral values. In the sample database, the class of a pixel is coded as a number. One frame of Landsat MSS imagery consists of four digital images of the same scene in different spectral bands. Two of these are in the visible region (corresponding approximately to green and red regions of the visible spectrum) and two are in the (near) infra-red. Each pixel is a 8-bit binary word, with 0 corresponding to black and 255 to white. The spatial resolution of a pixel is about 80m x 80m. Each image contains 2340 x 3380 such pixels. The database is a (tiny) sub-area of a scene, consisting of 82 x 100 pixels. Each line of data corresponds to a 3x3 square neighbourhood of pixels completely contained within the 82x100 sub-area. Each line contains the pixel values in the four spectral bands (converted to ASCII) of each of the 9 pixels in the 3x3 neighbourhood and a number indicating the classification label of the central pixel. Each pixel is categorized as one of the following classes: 1. red soil 2. cotton crop 3. grey soil 4. damp grey soil 5. soil with vegetation stubble 6. mixture class (all types present) 7. very damp grey soil NB. There are no examples with class 6 in this dataset. The data is given in random order and certain lines of data have been removed so you cannot reconstruct the original image from this dataset.
Selection from FFHQ & StyleGAN2:FFHQ (used in 'Testing Human Ability To Detect Deepfake Images of Human Faces' study)                                                                                  This dataset is the image stimulus pool of 50 deepfake and 50 real images, used for the experiment in the study titled 'Testing Human Ability To Detect Deepfake Images of Human Faces'. Images were obtained through random selection from the Flickr Faces High Quality dataset (https://paperswithcode.com/dataset/ffhq) and likewise from output of the StyleGAN2 algorithm (https://paperswithcode.com/method/stylegan2) as trained on the FFHQ dataset. Also included are the 20 deepfake images, similarly obtained (from StyleGAN2:FFHQ), which were used in the familiarization intervention in the study; and the 20 deepfake images (different images but similarly obtained, albeit with an element of curation to select for images with the specific 'tell-tale signs' / 'visible rendering artefacts') which were used in the advice intervention in the study. The 50 real and 50 deepfake images that were used as test stimuli in the experiment are in /real and /fake respectively; the 20 familiarization images are in /familiarization; and the 20 images used in the advice intervention are in /advice.
Semantic Textual Similarity (2012 - 2016)                                                                                  Semantic Textual Similarity (2012 - 2016) involves a set of semantic textual similarity datasets that were part of previous shared tasks (2012-2016): STS12 - [ Semeval-2012 task 6: A pilot on semantic textual similarity](https://www.aclweb.org/anthology/S12-1051/) STS13 - [SEM 2013 shared task: Semantic Textual Similarity](https://www.aclweb.org/anthology/S13-1004/) STS14 - [SemEval-2014 task 10: Multilingual semantic textual similarity](https://www.aclweb.org/anthology/S14-2010/) STS15 - [SemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability](https://www.aclweb.org/anthology/S15-2045/) STS16 - [SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation](https://www.aclweb.org/anthology/S16-1081/)
SerialTrack Particle Image Dataset                                                                                  This dataset accompanies the linked SerialTrack paper and provides test case data (2D/3D, varying particle density) across a range of synthetic and experimental imaging modalities. Included test cases can be used for further code development, validation of and comparisons for existing particle tracking codes, and/or evaluating and learning to use our SerialTrack code on known data.
Short Text Font Dataset                                                                                  The proposed dataset includes 1,309 short text instances from Adobe Spark. The dataset is a collection of publicly available sample texts created by different designers. It covers a variety of topics found in posters, flyers, motivational quotes and advertisements. Source: [Short Text Font Dataset](https://github.com/RiTUAL-UH/Font-prediction-dataset)
Small ImageNet 150                                                                                  This new dataset represents a subset of the ImageNet1k. It consists of 99000 images and 150 classes. 90000 of them are for training, 600 images for each class. The validation test size is 7500. For testing, we add 1500 images from the ImageNetV2 Top-Images dataset to the validation.
Stairs Image Dataset | Parts of House | Indoor                                                                                  This dataset is an extremely challenging set of over 3000+ originally Stair images captured and crowdsourced from over 500+ urban and rural areas, where each image is **manually reviewed and verified** by computer vision professionals at Datacluster Labs. ### **Dataset Features** - Dataset size : 3000+ - Captured by : Over 500+ crowdsource contributors - Resolution : 100% images HD and above (1920x1080 and above) - Location : Captured with 500+ cities accross India - Diversity : Various lighting conditions like day, night, varied distances, view points etc. - Device used : Captured using mobile phones in 2020-2022 - Usage : Stair detection , Stair Edge detection , Computer Vision , etc. ### Available Annotation formats COCO, YOLO, PASCAL-VOC, Tf-Record **To download full datasets or to submit a request for your dataset needs, please ping us at [sales@datacluster.ai](sales@datacluster.ai) Visit [www.datacluster.ai](www.datacluster.ai) to know more.** **Note**: All the images are manually captured and verified by a large contributor base on DataCluster platform
Statlog image segmentation                                                                                  The instances were drawn randomly from a database of 7 outdoor images. The images were handsegmented to create a classification for every pixel. Each instance is a 3x3 region.
Street View Image, Pose, and 3D Cities Dataset                                                                                  A large-scale dataset composed of object-centric street view scenes along with point correspondences and camera pose information. Source: [Generic 3D Representation via Pose Estimation and Matching](/paper/generic-3d-representation-via-pose-estimation)
Stylized ImageNet                                                                                  The Stylized-ImageNet dataset is created by removing local texture cues in ImageNet while retaining global shape information on natural images via AdaIN style transfer. This nudges CNNs towards learning more about shapes and less about local textures. Source: [Adversarial Examples Improve Image Recognition](https://arxiv.org/abs/1911.09665) Image Source: [https://github.com/rgeirhos/Stylized-ImageNet](https://github.com/rgeirhos/Stylized-ImageNet)
Suitcase/Luggage Dataset Indoor Object Image                                                                                  This dataset is an extremely challenging set of over 7000+ original Suitcase/Luggage images captured and crowdsourced from over 800+ urban and rural areas, where each image is **manually reviewed and verified** by computer vision professionals at ****DC Labs. ### **Dataset Features** - Dataset size : 6000+ - Captured by : Over 1000+ crowdsource contributors - Resolution : 99% images HD and above (1920x1080 and above) - Location : Captured with 800+ cities accross India - Diversity : Various lighting conditions like day, night, varied distances, view points etc. - Device used : Captured using mobile phones in 2021-2022 - Usage : Luggage detection, suitcase detection, etc. ### Available Annotation formats COCO, YOLO, PASCAL-VOC, Tf-Record **To download full datasets or to submit a request for your dataset needs, please ping us at [sales@datacluster.ai](sales@datacluster.ai) Visit [www.datacluster.ai](www.datacluster.ai) to know more.** **Note**: All the images are manually captured and verified by a large contributor base on DataCluster platform
Supplementary software and data: Bayesian multi-exposure image fusion for robust high dynamic range ptychography                                                                                  Accompanying software package and data for the publication titled 'Bayesian multi-exposure image fusion for robust high dynamic range ptychography'.
TAU Audio-Visual Urban Scenes 2021                                                                                  The dataset for this task is TAU Audio-Visual Urban Scenes 2021. The dataset contains synchronized audio and video recordings from 12 European cities in 10 different scenes.
TESTIMAGES                                                                                  A collection of photographic and synthetic images intended for analysis of image processing techniques and quality assessment of displays. Image source: [https://testimages.org/](https://testimages.org/)
Tecnocampus Hand Image Database                                                                                  The acquisition over the VIS and TIR data was performed by a commercial thermal camera Testo 882-3. We have used a second external camera to obtain the NIR data. In this case we have built a NIR camera using a webcam changing the default optical filter for a couple of Kodak filters for IR. We have also used a printed circuit board with 16 infra-red LEDs that provide the infra-red illumination. In order to alleviate the variability on the way the users present their hand we have used a kind of removable mask/template. Users had to put their hand in a neoprene surface with the help of a hand mask. Once the hand is placed the mask was removed and the three images (VIS, NIR, TIR) were shot. The same process was repeated with the palmar hand side, but using the mask in the opposite position (flip up to down). Once the first acquisition was finished (no more than a minute) the user performed some exercise in order to change hand heat conditions. This step was carried out in less than 30 seconds so that when finished, the user proceeded again with the second acquisition. For each session a total of 12 hand images were captured per user.
Tencent ML-Images                                                                                  Tencent ML-Images is a large open-source multi-label image database, including 17,609,752 training and 88,739 validation image URLs, which are annotated with up to 11,166 categories. Source: [Tencent ML-Images](https://github.com/Tencent/tencent-ml-images)
Text-to-3D House Model                                                                                  The dataset contains 2,000 houses, 13,478 rooms and 873 (some rooms have same textures so this number is smaller than the total number of rooms.) texture images with corresponding natural language descriptions. These descriptions are firstly generated from some pre-defined templates and then refined by human workers. The average length of the description is 173.73 and there are 193 unique words. In our experiments, we use 1,600 pairs for training while 400 for testing in the building layout generation. For texture synthesis, we use 503 data for training and 370 data for testing.
Text2KGBench                                                                                  **Text2KGBench** is a benchmark to evaluate the capabilities of language models to generate KGs from natural language text guided by an ontology. Given an input ontology and a set of sentences, the task is to extract facts from the text while complying with the given ontology (concepts, relations, domain/range constraints) and being faithful to the input sentences.
Text2shape                                                                                  A large dataset of natural language descriptions for physical 3D objects in the ShapeNet dataset.
Text8                                                                                  Desc: [About of Text8](http://mattmahoney.net/dc/textdata.html)
TextBox 2.0                                                                                  **TextBox 2.0** is a comprehensive and unified library for text generation, focusing on the use of pre-trained language models (PLMs). The library covers 13 common text generation tasks and their corresponding 83 datasets and further incorporates 45 PLMs covering general, translation, Chinese, dialogue, controllable, distilled, prompting, and lightweight PLMs. Source: [TextBox 2.0: A Text Generation Library with Pre-trained Language Models](https://arxiv.org/pdf/2212.13005v1.pdf) Image Source: [https://arxiv.org/pdf/2212.13005v1.pdf](https://arxiv.org/pdf/2212.13005v1.pdf)
TextCaps                                                                                  Contains 145k captions for 28k images. The dataset challenges a model to recognize text, relate it to its visual context, and decide what part of the text to copy or paraphrase, requiring spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects. Source: [TextCaps: a Dataset for Image Captioning with Reading Comprehension](/paper/textcaps-a-dataset-for-image-captioning-with)
TextCaps 2020                                                                                  
TextComplexityDE                                                                                  TextComplexityDE is a dataset consisting of 1000 sentences in German language taken from 23 Wikipedia articles in 3 different article-genres to be used for developing text-complexity predictor models and automatic text simplification in German language. The dataset includes subjective assessment of different text-complexity aspects provided by German learners in level A and B. In addition, it contains manual simplification of 250 of those sentences provided by native speakers and subjective assessment of the simplified sentences by participants from the target group. The subjective ratings were collected using both laboratory studies and crowdsourcing approach. Source: [Subjective Assessment of Text Complexity: A Dataset for German Language](/paper/subjective-assessment-of-text-complexity-a)
TextOCR                                                                                  **TextOCR** is a dataset to benchmark text recognition on arbitrary shaped scene-text. TextOCR requires models to perform text-recognition on arbitrary shaped scene-text present on natural images. TextOCR provides ~1M high quality word annotations on TextVQA images allowing application of end-to-end reasoning on downstream tasks such as visual question answering or image captioning. Dataset statistics: - 28,134 natural images from TextVQA - 903,069 annotated scene-text words - 32 words per image on average
TextSeg                                                                                  **TextSeg** is a large-scale fine-annotated and multi-purpose text detection and segmentation dataset, collecting scene and design text with six types of annotations: word- and character-wise bounding polygons, masks and transcriptions. Source: [https://github.com/SHI-Labs/Rethinking-Text-Segmentation](https://github.com/SHI-Labs/Rethinking-Text-Segmentation) Image Source: [https://github.com/SHI-Labs/Rethinking-Text-Segmentation](https://github.com/SHI-Labs/Rethinking-Text-Segmentation)
TextVQA                                                                                  TextVQA is a dataset to benchmark visual reasoning based on text in images. TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions. Statistics * 28,408 images from OpenImages * 45,336 questions * 453,360 ground truth answers
TextVQA Test                                                                                  
TextVQA Val                                                                                  
TextVQA test-standard                                                                                  
TextWorld KG                                                                                  **TextWorld KG** is a dynamic Knowledge Graph (KG) extraction dataset. It is based on a set of text-based games generated using. That framework allows to extract the underlying partial KG for every state, i.e., the subgraph that represents the agent’s partial knowledge of the world – what it has observed so far. All games share the same overarching theme: the agent finds itself hungry in a simple modern house with the goal of gathering ingredients and cooking a meal. Source: [https://arxiv.org/abs/1910.09532](https://arxiv.org/abs/1910.09532)
TextZoom                                                                                  **TextZoom** is a super-resolution dataset that consists of paired Low Resolution – High Resolution scene text images. The images are captured by cameras with different focal length in the wild. Source: [https://github.com/JasonBoy1/TextZoom](https://github.com/JasonBoy1/TextZoom) Image Source: [https://github.com/JasonBoy1/TextZoom](https://github.com/JasonBoy1/TextZoom)
Text_VPH                                                                                  Este conjunto de datos consiste en comentarios de publicaciones del MINSA (Perú) en Facebook sobre la vacuna contra el VPH entre los años 2019 y 2020. Se leyó cuidadosamente cada uno de los comentarios, luego se procedió a clasificarlos de manera manual. Para esta clasificación se interpretó los mensajes de las personas, por lo que se analizó los hilos (comentarios y respuestas) por separado y se procedió a etiquetarlos por temas `'Topic'` . Un profesional de salud realizó una segunda clasificación y las discrepancias se resolvieron con un tercer profesional. Luego, se seleccionaron subcategorías que hacían referencia directa a las vacunas contra el VPH. La clasificación se realizó utilizando las siguientes categorías `'topic_c'` : - 0: El comentario tiene una postura contraria a la vacuna contra el VPH (antivacuna) - 1: El comentario tien una postura a favor de la vacuna contra el VPH (provacuna) - 2: El comentario refleja una duda o dudas relacionada con la vacuna contra el VPH - 3: El comentario habla de cualquier otra cosa Citar: - Lewis De La Cruz, Lucy Cordova, &amp; Esperanza Reyes. (2023). <i>Text_VPH</i> [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/6460567
Textual Visual Semantic Dataset                                                                                  Extends the COCO-text [Veit et al. 2016] with information about the scene (such as objects and places appearing in the image) to enable researchers to include semantic relations between texts and scene in their Text Spotting systems, and to offer a common framework for such approaches. Source: [Textual Visual Semantic Dataset for Text Spotting](/paper/textual-visual-semantic-dataset-for-text)
The Contextual TV Dataset                                                                                  Using the Experience-Sampling Method (ESM), participants are asked to report TV consumption multiple times each day for a five week period. Through self-reported data, authors decrease uncertainty of exposure to content, and allow collection of non-trivial information, such as how much attention is paid to the TV. The data is structured to accommodate quantitative analyses, e.g. in the CARS community, and is publicly available under the name **Contextual TV (CTV)** dataset. Source: [Kristoffersen et al.](https://arxiv.org/pdf/1808.00337v2.pdf) Image source: [Kristoffersen et al.](https://arxiv.org/pdf/1808.00337v2.pdf)
Thermal focus image database                                                                                  The database was acquired using a thermographic camera TESTO 880-3. This camera is equipped with an uncooled detector and has a spectral sensitivity range from 8 to 14 μm. It has a removable German optic lens. It provides the following main features: Image resolution: 160 × 120 pixels. Optical field/min. focus distance: 32° × 24°/0.1 m. Thermal sensitivity (NETD) <0.1 °C at 30 °C. Geometric resolution: 3.5 mrad. Detector type: FPA 160 × 120 pixels, temperature-stabilized. The database consists of several image sets. In each set, the camera acquires one image of the scene at each lens position. In our case we have manually moved the lens in 1 mm steps which provides a total of 96 positions. Thus, each set consists of 96 different images of the one scene. For this purpose, we have attached a millimeter tape to the objective, and used a stable tripod in order to acquire the same scene for each scene position. to add a brief description of the dataset (Markdown and LaTeX enabled). Provide: * a high-level explanation of the dataset characteristics * explain motivations and summary of its content * potential use cases of the dataset We acquire different kinds of images according to their information content and depth of focus. It should be easier to focus an image with large amount of detail, because blurring in such an image will generally be more evident. As in visible images, it should be more difficult to completely focus an image with several objects, when each object is located at a different focal distance. We analyzed only static scenes, because of the need for comparability (same position and temperature). We have constructed 10 different databases, as follows. Telematic equipment (TE): this consists of four sets of images of the same scene (an item of telematic equipment). Set one (TE1) is acquired at one meter distance from the scene to the camera, set two (TE2) is acquired at two meters, set three (TE3) at three meters and set four (TE4) at 4 m. Obviously, when moving the camera away from the scene, more objects appear in the image. On the other hand, these four databases contain a scene that can be considered to be contained in a flat plane. Thus, it is acquiring mainly a two dimensional object with a single point of focus. Electronic circuit (EC): this consists of a single set of images of the same scene (an electronic circuit with components at different temperatures and distances from the camera). It is important to emphasize that, in this case, we are acquiring a very near object, in which there is a range of depth. Thus, it is not possible to focus the whole image simultaneously. Laptop transformer (LT): this consists of a single set of images of the same scene (the transformer of a laptop computer). Corridor and fluorescents (CF): this consists of a single set of images of a single scene (a corridor at the university, illuminated by several ceiling fluorescents). Heater (H): this consists of a single set of images of a heater. This scene contains a large amount of detail because the metallic parts are warmer than the spaces between. Face (F): this consists of a single set of images of a human face. This sequence contains a scene that is not fully static because of involuntary physical movement (eyes, breathing, etc.). Hand (Ha): this consists of a single set of images of a hand. The hand rests on a black surface. The database consists of 10 × 96 = 960 images.
Tiered ImageNet 10-way (1-shot)                                                                                  
Tiered ImageNet 10-way (5-shot)                                                                                  
Tiered ImageNet 5-way (1-shot)                                                                                  
Tiered ImageNet 5-way (5-shot)                                                                                  
Time-Lapse Hyperspectral Radiance Images                                                                                  These sequences of hyperspectral radiance images have been taken from scenes undergoing natural illumination changes. In each scene, hyperspectral images were acquired at about 1-hour intervals. Source: [Time-Lapse Hyperspectral Radiance Images of Natural Scenes 2015](https://personalpages.manchester.ac.uk/staff/d.h.foster/Time-Lapse_HSIs/Time-Lapse_HSIs_2015.html)
Tiny ImageNet                                                                                  **Tiny ImageNet** contains 100000 images of 200 classes (500 for each class) downsized to 64×64 colored images. Each class has 500 training images, 50 validation images and 50 test images. Source: [Embedded Encoder-Decoder in Convolutional Networks Towards Explainable AI](https://arxiv.org/abs/2007.06712) Image Source: [https://arxiv.org/pdf/1707.08819.pdf](https://arxiv.org/pdf/1707.08819.pdf)
Tiny ImageNet Classification                                                                                  
Tiny Images                                                                                  The image dataset TinyImages contains 80 million images of size 32×32 collected from the Internet, crawling the words in WordNet. **The authors have decided to withdraw it because it contains offensive content, and have asked the community to stop using it.** Source: [Implementing Randomized Matrix Algorithms in Parallel and Distributed Environments](https://arxiv.org/abs/1502.03032)
Tiny-ImageNet                                                                                  
Tiny-ImageNet-C                                                                                  Tiny-ImageNet-C is an open-source data set comprising algorithmically generated corruptions (blur, noise) applied to the Tiny-ImageNet (ImageNet-200) test-set.
Total-Text                                                                                  **Total-Text** is a text detection dataset that consists of 1,555 images with a variety of text types including horizontal, multi-oriented, and curved text instances. The training split and testing split have 1,255 images and 300 images, respectively. Source: [Convolutional Character Networks](https://arxiv.org/abs/1910.07954) Image Source: [https://github.com/cs-chan/Total-Text-Dataset](https://github.com/cs-chan/Total-Text-Dataset)
Traditional and Context-specific Spam Twitter                                                                                  This data set is being released to support the spam and context-specific spam detection tasks on Twitter data. There are three sets of tweets, parenting-related, #MeToo-related (a social movement focused on tackling issues related to sexual harassment and sexual assault of women), and gun-violence-related tweets. Each set contains 5,000 tweets. These tweets are original tweets in English. There are no retweets, quoted tweets or non-English tweets.
Transparent Object Images | Indoor Object Dataset                                                                                  This dataset is an extremely challenging set of over 3000+ original Transparent object images such as glasses and mirrors are captured and crowdsourced from over 500+ urban and rural areas, where each image is **manually reviewed and verified** by computer vision professionals at Datacluster Labs. ### **Dataset Features** - Dataset size : 3000+ - Captured by : Over 500+ crowdsource contributors - Resolution : 99% images HD and above (1920x1080 and above) - Location : Captured with 600+ cities accross India - Diversity : Diversity in object type, lighting, camera type etc. - Device used : Captured using mobile phones in 2020-2022 - Usage : Glass detection | Mirror detection | Transparent Cups detection | Home automation etc. ### Available Annotation formats COCO, YOLO, PASCAL-VOC, Tf-Record **To download full datasets or to submit a request for your dataset needs, please ping us at [sales@datacluster.ai](sales@datacluster.ai) Visit [www.datacluster.ai](www.datacluster.ai) to know more.** **Note**: All the images are manually captured and verified by a large contributor base on DataCluster platform
Twitter Abusive Context                                                                                  This dataset for abusive content detection in Twitter consists of two sets of annotations for the same set of tweets, one where the human annotators had access to the tweet's content and one where they didn't know the context.
UCLA Protest Image                                                                                  40,764 images (11,659 protest images and hard negatives) with various annotations of visual attributes and sentiments. Source: [Protest Activity Detection and Perceived Violence Estimation from Social Media Images](/paper/protest-activity-detection-and-perceived)
Unpaired haze images                                                                                  Unpaired dataset: The dataset is built by ourselves, and there are all real haze images from websites. 10000 images: Address：Baidu cloud disk Extraction code：zvh6 1000 images: Address：Baidu cloud disk Extraction code:47v9 Paired dataset: The dataset is added haze by ourselves according to the image depth. Address: Baidu cloud disk Extraction code : 63xf
Urban Hyperspectral Image                                                                                  Urban is one of the most widely used hyperspectral data used in the hyperspectral unmixing study. There are 307x307 pixels, each of which corresponds to a 2x2 m2 area. In this image, there are 210 wavelengths ranging from 400 nm to 2500 nm, resulting in a spectral resolution of 10 nm. After the channels 1-4, 76, 87, 101-111, 136-153 and 198-210 are removed (due to dense water vapor and atmospheric effects), we remain 162 channels (this is a common preprocess for hyperspectral unmixing analyses). There are three versions of ground truth, which contain 4, 5 and 6 endmembers respectively, which are introduced in the ground truth. Linda S. Kalman and Edward M. Bassett III 'Classification and material identification in an urban environment using HYDICE hyperspectral data', Proc. SPIE 3118, Imaging Spectrometry III, (31 October 1997); https://doi.org/10.1117/12.283843 Hosted at: - https://rslab.ut.ac.ir/data - http://lesun.weebly.com/hyperspectral-data-set.html - https://erdc-library.erdc.dren.mil/jspui/handle/11681/2925
VG graph-text                                                                                  
ViText2SQL                                                                                  **ViText2SQL** is a dataset for the Vietnamese Text-to-SQL semantic parsing task, consisting of about 10K question and SQL query pairs. Source: [https://github.com/VinAIResearch/ViText2SQL](https://github.com/VinAIResearch/ViText2SQL)
Visiting Card | ID Card Images | Hindi-English                                                                                  This dataset is an extremely challenging set of over 2000+ original Visiting card/ID card images captured and crowdsourced from over 300+ urban and rural areas, where each image is **manually reviewed and verified** by computer vision professionals at Datacluster Labs. ### **Dataset Features** - Dataset size : 2000+ - Captured by : Over 150+ crowdsource contributors - Resolution : 100% images HD and above (1920x1080 and above) - Location : Captured with 300+ cities accross India - Diversity : Covers a wide variety of things such as reflective paper, different fonts, different color and type of cards, etc. - Device used : Captured using mobile phones in 2020-2021 - Usage : Visiting card detection, card edge detection, paper edge detection, ID card OCR, Hindi OCR, etc. ### Available Annotation formats COCO, YOLO, PASCAL-VOC, Tf-Record **To download full datasets or to submit a request for your dataset needs, please ping us at [sales@datacluster.ai](sales@datacluster.ai) Visit [www.datacluster.ai](www.datacluster.ai) to know more.** **Note**: All the images are manually captured and verified by a large contributor base on DataCluster platform
Visual Near-Duplicates Detection in the Context of Social Media                                                                                  The dataset of the paper: ``Dataset and Case Studies for Visual Near-Duplicates Detection in the Context of Social Media'', by Hana Matatov, Mor Naaman, and Ofra Amir. https://figshare.com/articles/dataset/Visual_Near-Duplicates_Detection_in_the_Context_of_Social_Media_Dataset_/19367933 See the Github repository for details: https://github.com/sTechLab/Visual-Near-Duplicates-Detection-in-the-Context-of-Social-Media See the associated paper on arXiv: https://arxiv.org/abs/2203.07167
WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images                                                                                  WHOOPS! Is a dataset and benchmark for visual commonsense. The dataset is comprised of purposefully commonsense-defying images created by designers using publicly-available image generation tools like Midjourney. It contains commonsense-defying image from a wide range of reasons, deviations from expected social norms and everyday knowledge.
Waterloo IVC 3D Image Quality Database                                                                                  Click to add a brief description of the dataset (Markdown and LaTeX enabled). Provide: * a high-level explanation of the dataset characteristics * explain motivations and summary of its content * potential use cases of the dataset
WebText                                                                                  **WebText** is an internal OpenAI corpus created by scraping web pages with emphasis on document quality. The authors scraped all outbound links from Reddit which received at least 3 karma. The authors used the approach as a heuristic indicator for whether other users found the link interesting, educational, or just funny. WebText contains the text subset of these 45 million links. It consists of over 8 million documents for a total of 40 GB of text. All Wikipedia documents were removed from WebText since it is a common data source for other datasets.
WikiText-103                                                                                  The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License. Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies. Source: [The WikiText Long Term Dependency Language Modeling Dataset](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) Image Source: [https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)
WikiText-2                                                                                  The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License. Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies. Source: [The WikiText Long Term Dependency Language Modeling Dataset](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) Image Source: [https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)
WikiText-TL-39                                                                                  WikiText-TL-39 is a benchmark language modeling dataset in Filipino that has 39 million tokens in the training set. Source: [Evaluating Language Model Finetuning Techniques for Low-resource Languages](/paper/evaluating-language-model-finetuning)
Words in Context                                                                                  
X-ray and Visible Spectra Circular Motion Images Dataset                                                                                  Collections of images of the same rotating plastic object made in X-ray and visible spectra. Both parts of the dataset contain 400 images. The images are maid every 0.5 degrees of the object axial rotation. The collection of images is designed for evaluation of the performance of circular motion estimation algorithms as well as for the study of X-ray nature influence on the image analysis algorithms such as keypoints detection and description. Source: [X-ray and Visible Spectra Circular Motion Images Dataset](/paper/x-ray-and-visible-spectra-circular-motion)
XImageNet                                                                                  we introduce XIMAGENET-12, an explainable benchmark dataset with over 200K images and 15,600 manual semantic annotations. Covering 12 categories from ImageNet to represent objects commonly encountered in practical life and simulating six diverse scenarios, including overexposure, blurring, color changing, etc.,
XImageNet-12                                                                                  Enlarge the dataset to understand how image background effect the Computer Vision ML model. With the following topics: Blur Background / Segmented Background / AI generated Background/ Bias of tools during annotation/ Color in Background / Dependent Factor in Background/ LatenSpace Distance of Foreground/ Random Background with Real Environment! We introduce XIMAGENET-12, an explainable benchmark dataset with over 200K images and 15,600 manual semantic annotations. Covering 12 categories from ImageNet to represent objects commonly encountered in practical life and simulating six diverse scenarios, including overexposure, blurring, color changing, etc., Our research builds upon the foundation laid by 'Noise or Signal: The Role of Image Backgrounds in Object Recognition' (Xiao et al., ICLR 2022), 'Explainable AI: Object Recognition With Help From Background' (Qiang et al., ICLR Workshop 2022), reinforced the notion that models trained solely on backgrounds can substantially improve accuracy. One noteworthy discovery highlighted in their studies is that more accurate models tend to rely less on backgrounds.
YouTube8M-MusicTextClips                                                                                  The YouTube8M-MusicTextClips dataset consists of over 4k high-quality human text descriptions of music found in video clips from the YouTube8M dataset. For each selected YouTube music video, we extracted 10 second clips at the middle of the video for annotation. We provided annotators with only the audio corresponding to this clip. Thus, text annotations describe audio alone, not the visual content of the clip. The dataset annotations are divided into train and test split files. As the dataset is meant mainly for evaluation, there are 3169 annotated clips from the test set and only 1000 annotated clips from the train set.
andstor/smart_contracts inflated_plain_text                                                                                  
eTRIMS Image Database                                                                                  The database is comprised of two datasets, the 4-Class eTRIMS Dataset with 4 annotated object classes and the 8-Class eTRIMS Dataset with 8 annotated object classes. Source: [eTRIMS Image Database](http://www.ipb.uni-bonn.de/projects/etrims_db/)
iLur News Texts                                                                                  iLur News Texts is a dataset of over 12000 news articles from iLur.am, categorized into 7 classes: sport, politics, weather, economy, accidents, art, society. The articles are split into train (2242k tokens) and test sets (425k tokens). Source: [iLur News Texts](https://github.com/ispras-texterra/word-embeddings-eval-hy)
image-goal-nav-dataset                                                                                  A dataset for Image-Goal Navigation in Habitat based on Gibson scenes.
imagenet-1k                                                                                  
mini-ImageNet - 100-Way                                                                                  
mini-ImageNet-LT                                                                                  mini-ImageNet was proposed by Matching networks for one-shot learning for few-shot learning evaluation, in an attempt to have a dataset like ImageNet while requiring fewer resources. Similar to the statistics for CIFAR-100-LT with an imbalance factor of 100, we construct a long-tailed variant of mini-ImageNet that features all the 100 classes and an imbalanced training set with $N_1 = 500$ and $N_K = 5$ images. For evaluation, both the validation and test sets are balanced and contain 10K images, 100 samples for each of the 100 categories.
mini-Imagenet                                                                                  mini-Imagenet is proposed by **Matching Networks for One Shot Learning . In NeurIPS, 2016**. This dataset consists of 50000 training images and 10000 testing images, evenly distributed across 100 classes.
miniImagenet                                                                                  
pritamdeka/cord-19-fulltext                                                                                  
standard atomic contexts                                                                                  The dataset contains standard contexts of the lattices of all atomic lattices in the Concept Explorer format.
tieredImageNet                                                                                  The **tieredImageNet** dataset is a larger subset of ILSVRC-12 with 608 classes (779,165 images) grouped into 34 higher-level nodes in the ImageNet human-curated hierarchy. This set of nodes is partitioned into 20, 6, and 8 disjoint sets of training, validation, and testing nodes, and the corresponding classes form the respective meta-sets. As argued in Ren et al. (2018), this split near the root of the ImageNet hierarchy results in a more challenging, yet realistic regime with test classes that are less similar to training classes. Source: [tieredImageNet](https://github.com/yaoyao-liu/tiered-imagenet-tools) Image Source: [https://arxiv.org/pdf/1803.00676.pdf](https://arxiv.org/pdf/1803.00676.pdf)
wikiHow-image                                                                                  The dataset consists of 53,189 wikiHow articles across various categories of everyday tasks, 155,265 methods, and 772,294 steps with corresponding images.
wikitext wikitext-2-raw-v1                                                                                  

375 Rows. -- 286 msec.
